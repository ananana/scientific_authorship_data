<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Diacritics Restoration for Hungarian</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Attila</forename><surname>Novák</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MTA-PPKE Hungarian Language Technology Research Group</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Information Technology and Bionics 50/a Práter street</orgName>
								<orgName type="institution">Pázmány Péter Catholic University</orgName>
								<address>
									<postCode>1083</postCode>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borbála</forename><surname>Siklósi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Information Technology and Bionics 50/a Práter street</orgName>
								<orgName type="institution">Pázmány Péter Catholic University</orgName>
								<address>
									<postCode>1083</postCode>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Diacritics Restoration for Hungarian</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we describe a method based on statistical machine translation (SMT) that is able to restore accents in Hungarian texts with high accuracy. Due to the ag-glutination in Hungarian, there are always plenty of word forms unknown to a system trained on a fixed vocabulary. In order to be able to handle such words, we integrated a morphological analyzer into the system that can suggest accented word candidates for unknown words. We evaluated the system in different setups, achieving an accuracy above 99% at the highest.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Due to clumsy mobile device interfaces and re- luctance of users to spend too much time entering their message, a great amount of text is generated in a format that lacks the diacritic marks normally used in the orthography of the language the text is written in. Whatever the causes for the missing accents are, NLP applications should be able to re- store or generate the accented version of such texts prior to any further syntactic or semantic process- ing to avoid upstream errors.</p><p>In this paper, we aim at solving the problem of restoring accents in Hungarian texts with the com- bined application of a statistical machine transla- tion system and a morphological analyzer. Our method can be applied to any other languages that have an accurate morphological analyzer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>For Hungarian, there have been some attempts at creating accent restoration systems.  and <ref type="bibr" target="#b2">Mihalcea and Nastase (2002)</ref> are examples for ML approaches, where the correct places of diacritics are predicted from the immedi- ate grapheme-level context of the unaccented let- ter with an accuracy of 95%. Thus, unseen words can also be accented, but incorrect forms may also be introduced into the text. Dictionary-based ap- proaches rely on large text corpora and the distri- bution of the different accented forms.  report to have achieved a performance of 98% of accuracy with their dictionary-based method. Nevertheless, their system cannot rec- ognize unseen wordforms quite common in Hun- garian. <ref type="bibr">Németh et al. (2000)</ref> have implemented a complex text processing system for TTS applica- tions, applying morphological and syntactic anal- ysis. The authors report that the performance of accent restoration depends very much on the per- formance of the analyzers (achieving 95% accu- racy at best). Neither the implementations nor the resources used in these systems have been made publicly available.</p><p>A language-independent tool, Charlifter (Scan- nell, 2011), is based on statistical methods relying on a lexicon, a bigram contextual model and char- acter distributions built from a training corpus. Its performance on Hungarian with its pre-built mod- els is compared to our results in Section 5.</p><p>For other languages, similar methods are used. <ref type="bibr" target="#b10">Yarowsky (1994)</ref> presents a comprehensive report on corpus-based techniques used for French and Spanish texts. The role of the context is empha- sized in this report, however, both word form and accent variations are relatively moderate in the investigated languages compared to Hungarian. The study of <ref type="bibr" target="#b12">Zweigenbaum and Grabar (2002)</ref> is also aiming at French, but in the medical domain, which contains a higher ratio of unknown words than general language. In their work, a tagging method is applied in combination with transduc- ers, resulting in a tag sequence corresponding to each letter. The method is successfully (92% pre- cision) applied to single headwords of a medical thesaurus (without exploiting any context). The most similar method to ours is that of <ref type="bibr" target="#b6">Pham et al. (2013)</ref>, who also applied SMT in order to au-tomatically restore accents in Vietnamese texts. In their case, the best results produced an accu- racy of 93%. However, their system is augmented with a dictionary, and the distribution of accents and grammatical behaviour are also quite different from Hungarian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hungarian</head><p>Hungarian is an agglutinating language with an orthography that represents compounds as single word forms. These may result in rather complex word forms and words are often composed of long sequences of morphemes. Thus, agglutination and compounding yield a huge number of different word forms.</p><p>In Hungarian, umlauts and acute accents are used as diacritics for vowels. Acute accents mark long vowels, while umlauts are used to indicate the frontness of rounded vowels o→ö <ref type="bibr">[o→ø]</ref> and u→ü  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>In this research, we considered the problem of ac- cent restoration as a translation task, where the source language is the unaccented version, and the target language is accented Hungarian. Since it is easy to come up with a parallel training corpus for this task, methods of SMT can be applied.</p><p>In our experiments, we used Moses ( <ref type="bibr" target="#b1">Koehn et al., 2007</ref>), a widely used SMT toolkit for build- ing the translation models and performing decod- ing, and SRILM <ref type="bibr" target="#b9">(Stolcke et al., 2011</ref>) to build the necessary language models. Moses was used with its default configuration settings and monotone de- coding (i.e. reordering was not allowed), and with- out the alignment step, which was not needed in our case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The baseline setup</head><p>In the baseline setup, only the translation and lan- guage models built from the training corpus were used. The input for the decoder was Hungar- ian raw texts with all the accents removed. The translation model contained only unigram phrases (larger n-grams were also tried, but did not change the results) and the language model contained phrases up to 5 grams. Thus, the translation model was responsible for predicting the distribution of accented forms and the language model exploited contextual information.</p><p>Another baseline was also created in order to monitor the effect of the SMT system. In this sec- ond baseline, each unaccented word form was re- placed by its most frequent accented form in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Incorporating a morphological analyzer</head><p>In order to be able to restore accents in unseen words as well, a Hungarian morphological an- alyzer <ref type="bibr" target="#b7">(Prószéky and Kis, 1999;</ref><ref type="bibr" target="#b4">Novák, 2003)</ref> was integrated. A special version of the analyzer was created that directly maps unaccented word forms to their possible accented variants while also marking morpheme boundaries and adding morphosyntactic category tags. The segmenta- tion marks (e.g. compound and derivational suf- fix boundaries) and the tags are used when we as- sign a score to the accented candidates. We also reanalyze accented forms to retrieve lemmas not directly returned by the accenting analyzer. In our test set of 1 804 252 tokens, about 1% of the words were not found in the translation model even in the case of the largest, 440 million words, training set. <ref type="table">Table 2</ref> shows the ratio of unknown words (OOV) as a function of the size of the training set used for building the phrase table.</p><p>For these unknown words, all possible correct accented candidates were generated by the mor- phological analyzer. These candidates were then fed to the Moses decoder using its -xml-input train sentences M words OOV in test 100K 100 000 1.738 9.63% 1000K 1 000 000 18.078 3.44% 5000K 5 000 000 89.907 <ref type="table" target="#tab_2">1.23%  10M  10 000 000  180.644  1.68%  ALL  24 048 302</ref> 437.559 0.81% <ref type="table">Table 2</ref>: Ratio of OOV after building a translation model from a training set of a certain size parameter. In order to be able to use this feature of the decoder, a probability for each candidate form had to be estimated. First, we assumed uni- form distribution among the candidates. However, this approach assigned the same probability to the most common and the most nonsensical (although grammatical) candidates as well. Thus, in some cases these forms showed up in the results. In order to avoid the system to make such errors, a more sophisticated distribution was estimated for the candidate set. For this, we applied a linear regression model based on corpus frequency data determined for the lemma and other features of the candidate word (since the actual wordform was not present in the corpus). Thus, for each candi- date, its lemma frequency (LEM ), the number of productively applied compounding (CM P ), the number of productively applied derivational af- fixes (DER), and the frequency of the inflectional suffix sequence returned by the analysis were de- termined. Compounding and derivation were pe- nalized (i.e. they were given a negative sign), be- cause the morphological analyzer could suggest some nonsensical, though grammatical compound or derived forms. Sometimes such forms could be the correct ones, but the more productive com- pounding and derivation there is in a word, the lower score it should get. On the other hand, the frequencies of the lemma and the inflectional pat- tern should increase the score of a candidate, thus these components were given positive weights. Based on these components, a score was assigned to each candidate based on Formula (1).</p><formula xml:id="formula_0">score = −λ c #CM P − λ d #DER + log 10 LEM + λ i log 10 IN F + M S<label>(1)</label></formula><p>, where</p><formula xml:id="formula_1">M S = |minscore| + 1 if minscore ≤ 0 0 otherwise<label>(2)</label></formula><p>The M S component was used to scale up the scores by adding |minscore| + 1, i.e. the low- est score received for any candidate in the actual candidate set in order to evade negative scores. The λ weights were set by the mert tuning of the Moses system. We used a separate develop- ment set for this, on which we observed the distri- bution of compounds, derivational and inflectional suffixes in OOV words analyzed by the morphol- ogy and from which we sampled 1000 words ap- proximating the observed distributions. The target of the optimization in the mert tuning was the ac- curacy of the system on these words, resulting in the optimal values for each λ. Even though, in lin- ear regression, it is standard to use an additional bias weight, we did not find it necessary, because we did not need to bring our estimates in sync with estimates from other sources. And assuming one factor to have a fixed unit value was just an- other simplification that would not affect the over- all ranking, just its scaling. Even though, following an appropriate scaling of the scores, the ranked candidates could be used the same way as the entries in the translation table, the system would never select any accented form other than the most probable one, since the lan- guage model does not include any of these forms. Thus, only the candidate with the highest relative score was made available to the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and results</head><p>In our experiments, the Hungarian webcorpus <ref type="bibr" target="#b0">(Halácsy et al., 2004</ref>) was used for training and testing purposes. A set of 100 000 sentences were separated from the corpus as the test set, and an- other 100 000 sentences were used as a develop- ment set. The rest were used for training in differ- ent settings. The size of each training set is shown in <ref type="table">Table 2</ref>.</p><p>We evaluated the performance on all the 1 804 252 tokens of the test set (56.84% correct without accent) and on a subset of 1 472 200 words that included any vowels (47.09% correct without accent). The experiments were then performed for the baseline system using the most frequent form (BL-FREQ), for the baseline SMT system (BL- SMT) and for the one augmented by the morphol- ogy with the first-ranked candidate (RANK). <ref type="table" target="#tab_2">Ta- ble 3</ref> shows the detailed results for the smallest and largest training sets for all words (ALL) and for words that include vowels (VOWEL). It can be seen that the precision of the system is only   slightly improved when increasing the size of the training corpus, but the values of recall and accu- racy do dramatically improve in the case of the baseline system. However, the integration of the suggestions of the morphology can make up for the lack of information due to the small training set improving recall a great deal while only slightly affecting precision. Even for the biggest 437.6M- word training corpus, incorporating the morpho- logical analyzer with ranking yielded a relative er- ror rate reduction of 39.74%, reducing the word error rate from 1.56% to 0.94%. For the small- est 1.74M-word training corpus tested, the relative error rate reduction was 85.85%. The system in- cluding the morphological analyzer performs bet- ter even with the smallest training corpus in terms of word accuracy than the baseline Moses sys- tem with the biggest corpus. <ref type="figure">Figure 1</ref> shows the learning curves for each system with accuracy as a function of training set size.</p><p>Comparing our results to those we obtained using Charlifter (89.75% with most frequent ac- cented form baseline, 90.00% with the lexicon- lookup+bigram contextual model and 93.31% with lookup+bigram context+character-n-gram- based model), the results reveal that both the con- textual model in the SMT system improves accu- racy better than the bigram context model of Char- lifter, and the performance boost we get by incor- porating morphology vastly exceeds the accuracy improvement yielded by the incorporation of the character-n-gram-based model used in Charlifter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Error analysis</head><p>We performed a detailed error analysis on a 5000- sentence (87786-token) fragment of the test set. The results of the error analysis are presented in <ref type="table">Table 4</ref>. RANK-VOWEL BL-SMT-VOWEL BL-FREQ-VOWEL <ref type="figure">Figure 1</ref>: Accuracy as a function of the size of the training set for each system, measured on words containing vowels.</p><p>The detailed analysis showed that 14.7% of mis- matches between the original and the system out- put is in fact not due to the latter being erroneous. 3.55% are equivalent forms , while the rest is cor- rect in the output and erroneous in the reference, i.e. the system corrected errors in the original.</p><p>Another part of the reference (17.91%) is like- wise erroneous, however, since the error in these cases was not in the accents, the system was not able to correct it. Missing or substituted letters are the most common mistakes (10.81%), and further 6.42% of the errors is due to punctuation errors in the original.</p><p>About 2/3 of the mismatches are real errors. 5.57% of these could be attributed to the stem of the word missing from the database of the morpho- logical analyzer. In 3.55% of the cases, the sys- tem transforms a name to a more frequent word: sometimes to another name, but more often to some common frequent word. A similar case is when some common noun is transformed to a more frequent name (another 1.35%). The num- ber of these errors could be reduced to some ex- tent by making the system rely on case informa- tion (in the case of some proper name-common noun ambiguities), however this could make the system perform worse elsewhere due to increased data sparseness. 2.20% of the errors is due to er- rors in the training corpus. Since rare word forms are quite frequent in Hungarian, the chances are high that a specific form is more often mistyped  <ref type="table">Table 4</ref>: Analysis of mismatches between the system output and the input on a 5000-sentence test sample than not (this is especially true for word forms that occur only once in the training data). 3.72% of the errors in the analyzed test data was due to either transforming arbitrary unaccented letter sequences used as file names in the text being transformed to some meaningful words or to accented words be- ing used in an url in the original text.</p><p>The most common error (51.01% of all mis- matches) is the case where the system is sim- ply unable to correctly disambiguate the word in context, and this is not due to some other error or information loss. Interestingly, more than half (51%) of these errors belong to a sin- gle type where the system is unable to distin- guish a possessive and a non-possessive form of the same nominal lemma: gyereket∼gyerekét 'the child (accusative)' vs.'his/her child (accusative)', gyereken∼gyerekén 'on/about the child' vs. 'on/about his/her child', and gyereke∼gyereké 'his/her child' vs.'(belongs to a) child' (anaphoric possessive).</p><p>Another 26% of the mismatches is due to a sim- ilar problem concerning verbs. In Hungarian, tran- sitive verbs agree with their object in definiteness. Certain past, present and conditional verb forms differing in definiteness are only distinguished by an accent: hajtottak∼hajtották 'they drove' vs. 'they drove it'; hajtanak∼hajtanák 'they drive' vs. 'they would drive it'; hajtana∼hajtaná 'he/she would drive' vs. 'he/she would drive it'.</p><p>A factored model could in theory improve the recognition of these structures. It is questionable however, whether the improvement would justify the costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have described a method to restore accents in Hungarian texts. The baseline method using only a fixed training corpus to build translation and lan- guage models for a statistical machine translation system, which is limited to handling word forms present in the training corpus achieved an accu- racy of 98.44% at best. In order to process un- known words, a morphological analyzer was in- tegrated to produce accented candidates for these unknown words as well, resulting in an improved accuracy of 99.06%. This performance could only be achieved by a system that is able to produce correct word forms and takes context into account. Our method can be applied to any other languages for which a training corpus and a morphological analyzer are available.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>[</head><label></label><figDesc>u→y], like in German. A combination of acutes and umlauts is the double acute diacritic to mark long front rounded vowels ˝ o [ø:] and ˝ u [y:]. Long vowels generally have essentially the same quality as their short counterpart (i-´ ı, ¨ u-˝ u, u-´ u, ¨ o-˝ o, o-´ o). The long pairs of the low vowels a [O] and e [E], on the other hand, also differ in quality: ´ a [a:] andéand´andé [e:].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>100K</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance results for each experimen-
tal settings and training size 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Creating open language resources for hungarian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Péter</forename><surname>Halácsy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">András</forename><surname>Kornai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">László</forename><surname>Németh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC. European Language Resources Association</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>András Rung, István Szakadát, and Viktor Trón</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Moses: Open Source Toolkit for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2007 Demo and Poster Sessions</title>
		<meeting>the ACL 2007 Demo and Poster Sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Letter level learning for language independent diacritics restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivi</forename><surname>Nastase</surname></persName>
		</author>
		<idno>COLING-02</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Conference on Natural Language Learning</title>
		<meeting>the 6th Conference on Natural Language Learning<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The design, implementation, and operation of a Hungarian e-mail reader</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Géza</forename><surname>Németh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Zainkó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">László</forename><surname>Fekete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Olaszy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Endrédi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Péter</forename><surname>Olaszi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Géza</forename><surname>Kiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Péter</forename><surname>Kis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Speech Technology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="217" to="236" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">What is good Humor like?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Attila</forename><surname>Novák</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Milyen a jó Humor?</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">In I. Magyar Számítógépes Nyelvészeti Konferencia</title>
		<imprint>
			<biblScope unit="page" from="138" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vietnamese text accent restoration with statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luan-Nghia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet-Hong</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinh-Van</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Pacific Asia Conference on Language, Information, and Computation (PACLIC 27)</title>
		<meeting>the 27th Pacific Asia Conference on Language, Information, and Computation (PACLIC 27)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="423" to="429" />
		</imprint>
		<respStmt>
			<orgName>Department of English, National Chengchi University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unificationbased approach to morpho-syntactic parsing of agglutinative and other (highly) inflectional languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Prószéky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balázs</forename><surname>Kis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, ACL &apos;99</title>
		<meeting>the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, ACL &apos;99<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="261" to="268" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Statistical unicodification of african languages. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Scannell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="375" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SRILM at sixteen: Update and outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Abrash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Automatic Speech Recognition and Understanding Workshop</title>
		<meeting>IEEE Automatic Speech Recognition and Understanding Workshop<address><addrLine>Waikoloa, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A comparison of corpusbased techniques for restoring accents in spanish and french text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Annual Workshop on Very Large Text Corpora</title>
		<meeting>the 2nd Annual Workshop on Very Large Text Corpora<address><addrLine>Las Cruces</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="19" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zainkó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Németh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Olaszy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordos</surname></persName>
		</author>
		<title level="m">Eljárás adott nyelvenékezetesnyelven´nyelvenékezetes bet˝ uk használata nélkül készített szövegekszövegek´szövegekékezetes bet˝ uinek visszaállítására</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accenting unknown words in a specialized language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Zweigenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Grabar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Workshop on Natural Language Processing in the Biomedical Domain</title>
		<editor>Stephen Johnson</editor>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="21" to="28" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
