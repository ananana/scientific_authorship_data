<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rule Extraction for Tree-to-Tree Transducers by Cost Minimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascual</forename><surname>Martínez-Gómez</surname></persName>
							<email>pascual.mg@aist.go.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Artificial Intelligence Research Center</orgName>
								<orgName type="institution">AIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Artificial Intelligence Research Center</orgName>
								<orgName type="institution">AIST</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">National Institute of Informatics and JST</orgName>
								<address>
									<country>PRESTO</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">The Graduate University for Advanced Studies (SOKENDAI) Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke@nii</forename><forename type="middle">Ac</forename><surname>Jp</surname></persName>
						</author>
						<title level="a" type="main">Rule Extraction for Tree-to-Tree Transducers by Cost Minimization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="12" to="22"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Tree transducers that model expressive linguistic phenomena often require word-alignments and a heuristic rule extractor to induce their grammars. However, when the corpus of tree/string pairs is small compared to the size of the vocabulary or the complexity of the grammar, word-alignments are unreliable. We propose a general rule extraction algorithm that uses cost functions over tree fragments, and formulate the extraction as a cost minimization problem. As a by-product, we are able to introduce back-off states at which some cost functions generate right-hand-sides of previously unseen left-hand-sides, thus creating transducer rules &quot;on-the-fly&quot;. We test the generalization power of our induced tree transducers on a QA task over a large Knowledge Base, obtaining a reasonable syntactic accuracy and effectively overcoming the typical lack of rule coverage.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Tree transducers are general and solid theoreti- cal models that have been applied to a variety of NLP tasks, such as machine translation <ref type="bibr" target="#b12">(Knight and Graehl, 2005</ref>), text summarization <ref type="bibr" target="#b4">(Cohn and Lapata, 2009</ref>), question answering ( <ref type="bibr" target="#b10">Jones et al., 2012</ref>), paraphrasing and textual entailment ( <ref type="bibr" target="#b27">Wu, 2005)</ref>. One strategy to obtain transducer rules is by exhaus- tive enumeration; however, this method is ineffec- tive when there is a high structural language vari- ability and we wish to have an expressive model. Another strategy is to heuristically extract rules from a corpus of tree/string pairs and word-alignments, as GHKM algorithm does ( <ref type="bibr" target="#b6">Galley et al., 2004</ref>); how- ever, word-alignments are difficult to estimate when the corpus is small. This would be the case, for ex- ample, of machine translation for low-resourced lan- guages where there is often small numbers of paral- lel sentences, or in Question Answering (QA) tasks where the number of Knowledge Base (KB) identi- fiers (concepts) is much larger than QA datasets.</p><p>Our main contribution is an algorithm that formu- lates the rule extraction as a cost minimization prob- lem, where the search for the best rules is guided by an ensemble of cost functions over pairs of tree fragments. In GHKM, a tree fragment and a se- quence of words are extracted together if they are minimal and their word alignments do not fall out- side of their respective boundaries. However, given that alignment violations are not allowed, the qual- ity of the extracted rules degrades as the rate of misaligned words increases. In our framework, we can mimic GHKM by assigning an infinite cost to pairs of tree fragments that violate such conditions on word alignments and by adding a cost regular- izer on the size of the tree fragments. Smoother cost functions, however, would permit controlled mis- alignments, contributing to generalization. Given the generality of these cost functions, we believe that the applicability of tree transducers will be extended.</p><p>A by-product of introducing these cost functions is that some of them may act as rule back-offs, where transducer rules are built "on-the-fly" when the transducer is at a predefined back-off state but there is no rule whose left-hand-side (lhs) matches the input subtree. These back-off states can be seen as functions that are capable of generating right-hand-sides (rhs) for unseen input subtrees.</p><p>Our rule extraction algorithm and back-off scheme are general, in the sense that they can be applied to any tree transformation task. However, in this paper, we extrinsically evaluate the quality of the extracted rules in a QA task, where the ob- jective is to transform syntactic trees of questions into constituent trees that represent Sparql queries on Freebase, a large Knowledge Base. Implement- ing all components of a QA system at a sufficient level is out of the scope of this paper; for that reason, in order to evaluate our contribution in isolation, we use the FREE917 corpus released by <ref type="bibr" target="#b3">Cai and Yates (2013)</ref>, for which an entity and predicate lexicon is available <ref type="bibr">1</ref> . We show that a tree-to-tree transducer in- duced using our rule extraction and back-off scheme is accurate and generalizes well, which was not pre- viously achieved with tree transducers in semantic parsing tasks such as QA over large KBs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Tree transducers were first proposed by <ref type="bibr" target="#b20">Rounds (1970)</ref> and <ref type="bibr" target="#b22">Thatcher (1970)</ref>, and have been greatly developed recently <ref type="bibr" target="#b12">(Knight and Graehl, 2005</ref>). <ref type="bibr" target="#b10">Jones et al. (2012)</ref> used tree transducers to seman- tically parse narrow-domain questions into Prolog queries for GeoQuery ( <ref type="bibr" target="#b26">Wong and Mooney, 2006</ref>), a small database of 700 geographical facts. Rules were exhaustively enumerated, which was possible given the small size of the database and low variabil- ity of questions. Another strategy is that of <ref type="bibr" target="#b14">Li et al. (2013)</ref>, where they used a variant of GHKM to in- duce tree transducers that parse into λ-SCFG. Word- to-node alignments could be reliably estimated with the IBM models ( <ref type="bibr" target="#b2">Brown et al., 1993)</ref> given, again, the small vocabulary and database size of GeoQuery. In such small-scale tasks, our rule extraction and back-off scheme offers no obvious advantage. How- ever, when doing QA over larger and more realistic KBs (and other tasks with similar characteristics), exhaustive enumeration of rules or reliable estima- tions of alignments are not possible, which prevents the application of tree transducers. Thus, it is on the latter type of tasks where we focus our contribution.</p><p>A similar problem has been considered in the tree mapping literature in the form of the tree-to-tree edit distance. In that formulation, three edit operations are defined, namely, deleting and inserting single nodes, and replacing the label of a node. These edit operations have a cost associated to them, and the task consists of finding the minimum edit cost and its corresponding edit script 2 that transforms a source into a target tree. The problem was first solved by <ref type="bibr" target="#b21">Tai (1979)</ref>, and later <ref type="bibr" target="#b28">Zhang and Shasha (1989)</ref> pro- posed a simpler and faster dynamic programming al- gorithm that operates in polynomial time, and that has inspired multiple variations <ref type="bibr" target="#b1">(Bille, 2005</ref>). However, we need edit operations that involve tree fragments (e.g., noun phrases or parts of verb phrases), rather than single nodes, when searching for the best mappings. We address this problem by searching for non-isomorphic tree mappings, in line with <ref type="bibr" target="#b5">Eisner (2003)</ref>, except that our rule extraction algorithm is guided by an ensemble of cost functions over pairs of tree fragments. This algorithm is capa- ble of extracting rules more robustly than GHKM by permitting misalignments in a controlled man- ner. Finding a tree mapping solves simultaneously the alignment and the rule extraction problem.</p><p>There is a wide array of tree transducers with dif- ferent expressive capabilities <ref type="bibr" target="#b12">(Knight and Graehl, 2005</ref>). We consider extended 3 root-to-frontier 4 lin- ear 5 transducers ( <ref type="bibr" target="#b18">Maletti et al., 2009)</ref>, possibly with deleting 6 operations. In this paper, we syntactically parse the natural language question and transform it into a meaning representation, similarly to <ref type="bibr" target="#b7">Ge and Mooney (2005)</ref>. But instead of using Prolog formu- lae or λ-SCFG, we use constituent representations of λ−DCS expressions <ref type="bibr" target="#b15">(Liang, 2013)</ref>, which is a for- mal language convenient to represent Sparql queries where variables are eliminated by making existential quantifications implicit (see example in <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>Another challenge is to construct transducers with sufficient rule coverage, which would require bil- lions of lexical rules that map question phrases to database entities or relations. Even if those rules were available, estimating their rule probabilities would be difficult given the small data sets of ques-tions paired with their logical representations. We solve the problem by constructing lexical rules "on- the-fly" at the decoding stage, similarly to the candi- date generation stage of entity linking systems ( <ref type="bibr" target="#b16">Ling et al., 2015)</ref>. Rule weights are also predicted on-the- fly given rule features and model parameters similar to <ref type="bibr" target="#b4">Cohn and Lapata (2009)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>Tree transducers apply to general tree transforma- tion problems, but for illustrative purposes, we use the tree pair s and t in <ref type="figure" target="#fig_0">Figure 1</ref> (from FREE917) as a running example. s is the syntactic constituent tree of the question "how many teams participate in the uefa", whereas t is a constituent tree of an executable meaning representation in the λ−DCS formalism:</p><formula xml:id="formula_0">count(Team.League.Uefa) Its corresponding lambda expression is count(λx.∃a.Team(x, a) ∧ League(a, Uefa))</formula><p>which can be converted into a Sparql KB query:</p><formula xml:id="formula_1">SELECT COUNT(?x) WHERE { ?a Team ?x .</formula><p>?a League Uefa . } Following the terminology of <ref type="bibr" target="#b8">Graehl and Knight (2004)</ref>, we define a tree-to-tree transducer as a 5- tuple (Q, Σ, ∆, q start , R) where Q is the set of states, Σ and ∆ are the sets of symbols of the in- put and output languages, q start is the initial state, and R is the set of rules. For convenience, define T Σ as the set of trees with symbols in Σ, T Σ (A) the set of trees with symbols in Σ ∪ A where symbols in A only appear in the leaves, X as the set of vari- ables {x 1 , . . . , x n }, and A.B for the cross-product of two sets A and B. A rule r ∈ R has the form q.t i s → t o , where q ∈ Q is a state, t i ∈ T Σ (X ) is the left-hand-side (lhs) tree pattern (or elementary tree), t o ∈ T ∆ (Q.X ) the right-hand-side (rhs), and s ∈ R the rule score.</p><p>Tree-to-tree transducers apply a sequence of rules to transform a source s into a target t tree. A root-to- frontier transducer starts at the root of the source tree and searches R for a rule whose i) tree pattern t i on the lhs matches the root of the source tree, and ii) the state q of the rule is the initial state of the transducer. An incipient target tree is created by copying the rhs of the rule. Then, the transducer recursively and in- dependently visits the subtrees of the source tree at the lhs variable positions of the rule from their new states, and copies the results into the same variable on the target tree.</p><p>In <ref type="figure" target="#fig_0">Figure 1</ref>, the sequential application of rules r1 to r5 is a derivation that transforms the question s into the query t. For example, rule r1 consumes a tree fragment of s (e.g. "how", "many", "WRB", etc.) and produces a tree fragment with terminals ("COUNT", x 1 , x 2 ) and non-terminals ("ID") with a specific structure. Rules r2 and r3 only consume but do not produce symbols (other than variables). The rhs of rules are target tree fragments that con- nect to each other at the frontier nodes (those with variables). Rules r4 and r5 are terminal rules, where r4 produces the predicate Team and rule r5 pro- duces the entity Uefa and a disambiguating predi- cate League that has no lexical support on the source side, similarly to the role that bridging predicates play in <ref type="bibr" target="#b0">Berant et al. (2013)</ref>.</p><p>Given a corpus of source and target tree pairs, the learning stage aims to obtain rules such as r1 − r5 in <ref type="figure" target="#fig_0">Figure 1</ref> and their associated probabilities or scores. We discuss our novel approach to rule extraction in Section 5. For the assignment of rule scores, we adopt the latent variable averaged structured percep- tron, a discriminative procedure similar to <ref type="bibr" target="#b23">Tsochantaridis et al. (2005)</ref> and <ref type="bibr" target="#b4">Cohn and Lapata (2009)</ref>. Here, we instantiate feature values f for every rule, and reward the weights w of rules that participate in a derivation (latent variable) that transforms a train- ing source tree into a meaning representation that retrieves the correct answer.</p><p>At decoding stage, rule scores can be predicted as s = w · f . However, we cannot expect to have ex- tracted all necessary rules at the training stage given the small training data and large-scale KB. For that reason, we propose in Section 4 a novel rule back-off scheme to alleviate coverage problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Back-off rules</head><p>As an illustrative example, consider the question "how many teams participate in the nba", and the rules r1 to r5 in <ref type="figure" target="#fig_0">Figure 1</ref>. When the transducer at- tempts to transform the noun phrase (NP (DT the) (NN nba)), no rule's lhs matches it. However, since the transducer is at state bridge (as specified by the rhs of r3), it should be able to produce a list of bridged entities, among which the target subtree (ID League NBA) will be hopefully included. Thus, the following rule should be created for the occasion:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Rule Extraction</head><p>Given a pair of trees, our rule extraction algorithm finds a tree mapping that implicitly describes the rules that transform a source into a target tree. In the search of the best mapping, we need to explore the space of edit operations, which are substitutions of source by target tree fragments. We define cost func- tions for these edit operations, and formulate the tree mapping as a cost minimization problem. Whereas our tree mapping algorithm and back-off scheme are generic and can be used in any tree transformation task, cost functions depend on the application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Cost functions</head><p>In general, cost functions are defined over edit op- erations, which are pairs of source and target tree fragments, cost : T Σ (X ) × T ∆ (Q.X ) → R ≥0 , and they are equivalent to feature functions. Some cost functions are defined over all pairs of tree fragments.</p><p>For this QA application, these are:</p><formula xml:id="formula_2">csize(t 1 , t 2 ) = |nodes(t 1 )| 2 + |nodes(t 2 )| 2</formula><p>which acts as a tree size regularizer, returning a cost quadratic to the size of the tree fragments, thus en- couraging small rules. The cost function ccount as- signs zero cost if (i) "how" and "many" appear in t 1 , and (ii) "COUNT" appears in t 2 . If only either (i) or (ii), the cost is a positive constant. Similarly, other operators (max, min, argmax, etc.) could be recog- nized, but this dataset did not require them.</p><p>Other cost functions only apply to some pairs of tree fragments. These are the back-off functions de- scribed in Section 4, but instead of returning scores for every target tree fragment, they return a cost, e.g. cent : T Σ × T ∆ → R ≥0 . An ensemble will produce up to three different costs for every pair of tree frag- ments, depending on what back-off functions were triggered. In the case of the entity cost function:</p><formula xml:id="formula_3">γ ent (t 1 , t 2 ) = λ 1 · csize(t 1 , t 2 ) + λ 2 · ccount(t 1 , t 2 ) + λ 3 · cent(t 1 , t 2 )<label>(1)</label></formula><p>where λ i ∈ R ≥0 are scaling factors. In the search of the lowest-cost mapping, the labels of the cost functions that are derived from the back-off func- tions (e.g. γ ent , γ pred ) are memorized for the pairs (t 1 , t 2 ) for which they were defined and for which they outputted a cost. These labels are then used as back-off rule state names when constructing rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Tree Mapping: Optimization Problem</head><p>Intuitively, the cost of mapping a source node n s to a target node n t is equal to the cost of transforming a tree fragment T Σ (X ) rooted at node n s into a tree fragment T ∆ (Q.X ) rooted at node n t , plus the sum of costs of mapping the frontier nodes rooted at the variables. In order to formalize our tree mapping, we need a more precise definition of a tree frag- ment where the locations of variables X are spec- ified by paths. The notation to specify subtrees is taken from ( <ref type="bibr" target="#b8">Graehl and Knight, 2004</ref>), and we in- troduce the ⊥ operator for convenience.</p><p>A path p is a tuple, equivalent to a Gorn address, that uniquely identifies the node of a tree by speci- fying the sequence of child indices to the node from the root. In the tree s of <ref type="figure" target="#fig_0">Figure 1</ref>, the path to the VP node is (1, 0), whereas in t, the path to League is <ref type="figure" target="#fig_0">(1, 1, 0)</ref>. The path p = () refers to the root of a tree. We denote by s ↓ p the subtree of tree s that is rooted at path p and that has no variables. In <ref type="figure" target="#fig_0">Figure 1</ref>, the left-hand-side (lhs) of r5 is the sub- tree s ↓ <ref type="figure" target="#fig_0">(1, 0, 1, 1)</ref>. In order to introduce variables, we generalize the notion of subtree into a tree pat- tern s ↓ p ⊥ {p 1 , . . . , p n }, where n variables re- place subtrees s ↓ p i at subpaths p i ∈ {p 1 , . . . , p n }. For example, the lhs of r1 can be represented with the tree pattern s ↓ () ⊥ {(0, 1), (1)}, and r2 with s ↓ (1) ⊥ {(1, 0, 1)}. Note that the order of sub- paths {p 1 , . . . , p n } matters. A tree pattern with no subpaths s ↓ p ⊥ {} is simply a subtree s ↓ p, such as the lhs of rules r4 and r5; a tree pattern with only one subpath equal to its path s ↓ p ⊥ {p} is a single variable, such as the rhs of rules r2 and r3. Note that in s ↓ p ⊥ {p 1 , . . . , p n }, all paths p i to variables are prefixed by p, and that no variables are descendants of any other variable in the same tree pattern. In other words, p = {p 1 , . . . , p n } are disjoint subpaths given p, where p denotes a list of paths.</p><p>We can now formalize the tree mapping algorithm as an optimization problem. Let γ (s ↓ p s ⊥ p, t ↓ p t ⊥ p ) be the cost to transform a source into a target tree pattern, as defined in Equation 1. To transform s ↓ p s into t ↓ p t , we need to find the best combination of source sub- trees rooted at {p 1 , . . . , p n } that can be transformed at minimum cost to the best combination of target subtrees at {p 1 , . . . , p n }. The transformation cost of a certain tree pattern s ↓ p s ⊥ {p 1 , . . . , p n } into t ↓ p t ⊥ {p 1 , . . . , p n } is equal to the cost of trans- forming the source tree pattern into the target tree pattern, plus the minimum cost to transform s ↓ p i into t ↓ p i , for i ≥ 1. That is:</p><formula xml:id="formula_4">C (s ↓ p s , t ↓ p t ) = min p,p {γ s ↓ p s ⊥ p, t ↓ p t ⊥ p + |p| i=1 C s ↓ p i , t ↓ p i }<label>(2)</label></formula><p>subject to |p| = |p |, that is, source and tar- get tree patterns having the same number of vari- ables. Then, the cost of transforming the source into the target tree would be given by the expression C (s ↓ (), t ↓ ()). Since we are only interested in the pairs of source and target tree patterns that lead to the minimum cost, we keep track of subpaths p and p of tree pattern pairs that minimize the cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Overview</head><p>This problem can be solved for small depths of tree patterns and a small number of variables by stor- ing intermediate results in the computation of Eq. 2. However, an exact implementation needs to enumer- ate all pairs of source and target disjoint subpaths (p and p ), which has a computational complexity that grows combinatorially with |p| (variable permuta- tions), and exponentially with the number of descen- dant nodes of p s and p t (powerset of variables).</p><p>Instead, we use a beam search algorithm (see Al- gorithm 1) <ref type="bibr">7</ref> that constructs source and target disjoint paths (p and p ) hierarchically (function GENER- ATEDISJOINT) in a bottom-up order, for any given path pair (p s , p t ). First, n-best solutions (pairs of disjoint paths) are computed for children; then those partial solutions are combined into their parent us- ing the cross-product. Solutions (with their associ- ated cost) for every pair of paths (p s , p t ) are stored in a weighted hypergraph, from which we can extract n-best derivations (sequences of rules). In the pseu- docode, we use a helper function, paths(s ↓ p s ), which denotes the list of subtree paths in bottom-up order: from the leaves up to p s (including the latter).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Detailed Description</head><p>For a certain path pair (p s , p t ), there are three cases. The first case (line 34-35) considers a pair of empty disjoint subpaths (p, p ) = ({}, {}), where the cost c of transforming s ↓ p s ⊥ {} into t ↓ p t ⊥ {} is evaluated and the empty disjoint subpaths are added to the priority queue P , indexed with p s . Such indexing is useful to retrieve the n-best pairs of dis- joint subpaths accumulated at every tree node.</p><p>The second case (line 28 to 31) evaluates the cost of transforming single-variable tree patterns:</p><formula xml:id="formula_5">s ↓ p s ⊥ {p c } into t ↓ p t ⊥ {p c }.</formula><p>In this case, variables substitute entire subtrees rooted at paths p c and p c on the source and target tree pat- terns, respectively. Note that p c ranges over all node Algorithm 1 Extraction of optimal sequence of rules to transform a source s into a target tree t. Input: Trees s and t, and ensemble of cost functions γ. Output: Sequence of optimal rules for s ⇒ * t.</p><p>1: let H = (V, E) be a hypergraph of solutions with V ← {} vertices and E ← {} hyperedges. <ref type="bibr">2:</ref> for (p s , p t ) ∈ paths(s) × paths(t) do 3: add vertex v = (p s , p t ) to V 4: P P ← GENERATEDISJOINT(s ↓ p s , t ↓ p t , γ) 5: for (p, p ) ∈ P P do 6:</p><p>Get cost of tree pattern pair. Costs when variables combined from children.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><formula xml:id="formula_6">c ← γ (s ↓ p s ⊥ p, t ↓ p t ⊥ p ) 8: add edge (p s , p t ) c → (p, p ) to E 9</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16:</head><p>for every p ic immediate child of p c (if any) do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17:</head><p>Retrieve n-best subpaths p and p from p ic .</p><p>18:</p><formula xml:id="formula_7">C ← arg min n (p,p ) {c | (p ic , p, p , c) ∈ P } 19:</formula><p>Combine subpaths with those accumulated <ref type="bibr">20:</ref> from previous siblings and stored at path p c .</p><p>21:</p><formula xml:id="formula_8">A ← arg min n (p,p ) {c | (p c , p, p , c) ∈ P } 22: for (p, p ) ∈ (C ∪ (C.A)) do 23: c ← γ (s ↓ p s ⊥ p, t ↓ p t ⊥ p ) 24:</formula><p>add (p c , p, p , c) to priority queue P</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>25:</head><p>end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>26:</head><p>end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>27:</head><p>Cost of tree patterns with one variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>28:</head><p>for every p c ∈ paths(t ↓ p t ) do 29:</p><formula xml:id="formula_9">c ← γ (s ↓ p s ⊥ {p c }, t ↓ p t ⊥ {p c }) 30:</formula><p>add (p c , {p c }, {p c }, c) to priority queue P</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>31:</head><p>end for 32: end for 33: Cost of tree patterns with no variables.</p><formula xml:id="formula_10">34: c ← γ (s ↓ p s ⊥ {}, t ↓ p t ⊥ {}) 35: add (p s , {}, {}, c) to priority queue P 36: return arg min n (p,p ) {c | (p s , p, p , c) ∈ P } 37: end function</formula><p>addresses that are descendant of p s (including p s ), and similarly for p c . The pairs of disjoint subpaths (p, p ) = ({p c }, {p c }) are added into the priority queue, indexed by p c .</p><p>The third case (line 16 to 26) performs the com- bination of subpaths hierarchically from children to parents, and incrementally across children. For ev- ery path p c ∈ paths(s ↓ p s ), it visits its imme-diate children p ic one by one, and retrieves into C the n-best disjoint subpaths (line 18) that have al- ready been obtained during previous iterations for p ic . Then, we retrieve into A the n-best disjoint sub- paths indexed at p c , which is a list of the best sub- paths that were combined from previous immediate children (the list is empty if this is the first immedi- ate child that we visit). The cross-product of disjoint subpaths in C and A, that is C.A, is then evaluated and the best combinations are stored in the priority queue indexed at p c .</p><p>As an example of a cross-product between two lists C and A of pairs of disjoint paths, let C = {(p 1 , p 1 ), (p 2 , p 2 )} and A = {(p 3 , p 3 )}. Then the cross-product C.A would be:</p><formula xml:id="formula_11">C.A = {(p 1 · p 3 , p 1 · p 3 ), (p 2 · p 3 , p 2 · p 3 )}</formula><p>where p 1 · p 3 = {(0, 1), (0, 2), (0, 3), (0, 4)} if p 1 = {(0, 1), (0, 2)} and p 3 = {(0, 3), (0, 4)}. At this stage, subpaths p i or p i that are not disjoint are discarded, together with disjoint paths that produce tree patterns with depth larger than a certain user- defined threshold, or whose number of subpaths is larger than the number of variables allowed.</p><p>In line 24, the disjoint subpaths of C (in addition to their cross-product C.A) are also evaluated and added to the priority queue indexed by p c , to propa- gate upwards in the hierarchy of solutions the deci- sion of not combining disjoint subpaths.</p><p>Finally, GENERATEDISJOINT returns the n-best pairs of disjoint subpaths of minimum cost (p, p ) that accumulated in the priority queue P for path p s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Other Considerations</head><p>The n-best source and target pairs of disjoint sub- paths are stored at every pair of source and target paths (p s , p t ) (lines 2-10), forming a hypergraph, as in <ref type="figure" target="#fig_1">Figure 2</ref>. Then, with a hypergraph search <ref type="bibr" target="#b9">(Huang and Chiang, 2007)</ref> we can retrieve at least n-best sequences of rules (derivations) that transform the source into the target tree (line 11).</p><p>To maintain diversity of partial disjoint subpaths, we divide P into a matrix of buckets with as many rows and columns as the number of non-variable ter- minals of the source and target tree patterns, trading memory for more effective search <ref type="bibr" target="#b13">(Koehn, 2015)</ref>. This operation is implicit in lines 24, 30 and 35. </p><formula xml:id="formula_12">↓ pt ⊥ {p 1 , p 2 , p 3 }.</formula><p>The one with cost .5, s ↓ ps ⊥ {p1, p3} and t ↓ pt ⊥ {p 2 , p 3 }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experiment Settings</head><p>Data The training data is a corpus of questions an- notated with their logical forms that can be exe- cuted on Freebase to obtain a precise answer. For an unseen set of questions, the task is to obtain au- tomatically their logical forms and retrieve the cor- rect answer. Our objective is to evaluate the gen- eralization capabilities of a transducer induced us- ing our rule extraction on an unseen open-domain test set. We parsed questions from FREE917 into source constituent trees using the Stanford caseless model ( <ref type="bibr" target="#b11">Klein and Manning, 2003</ref>). Target con- stituent (meaning) representations were obtained by a simple heuristic conversion from the λ−DCS ex- pressions released by <ref type="bibr" target="#b0">Berant et al. (2013)</ref>. We evalu- ate on the same training and testing split as in <ref type="bibr" target="#b0">Berant et al. (2013)</ref>. Tree pairs (2.9%) for which the gold executable meaning representation did not retrieve valid results were filtered out.</p><p>Baselines We compared to two baselines. The first one is SEMPRE ( <ref type="bibr" target="#b0">Berant et al., 2013</ref>), a state- of-the-art semantic parser that uses a target language grammar to over-generate trees, and a log-linear model to estimate the parameters that guide the de- coder towards trees that generate correct answers. For FREE917, SEMPRE uses a manually-created entity lexicon released by <ref type="bibr" target="#b3">(Cai and Yates, 2013)</ref>, but an automatically generated predicate lexicon. In-stead, our system and the second baseline use manu- ally created entity and predicate lexicons, where the latter was created by selecting all words from every question that relate to the target predicate. For ex- ample, for the question "what olympics has egypt participated in", we created an entry that maps the discontinuous phrase "olympics participated in" to the predicate OlympicsParticipatedIn.</p><p>The second baseline is a tree-to-tree transducer whose rules are extracted using a straightforward adaptation of the GHKM algorithm ( <ref type="bibr" target="#b6">Galley et al., 2004</ref>) for pairs of trees. Word-to-concept align- ments are extracted using three different strategies: i) ghkm-g uses the IBM models ( <ref type="bibr" target="#b2">Brown et al., 1993)</ref> as implemented in GIZA++ ( <ref type="bibr" target="#b19">Och and Ney, 2003)</ref>, ii) ghkm-m maps KB concepts (target leaves) to as many source words as present in the entity/predicate lexicons, and iii) ghkm-c maps KB concepts as in ii) but only retaining the longest contiguous sequence of source words (or right-most sequence if there is a tie). Bridging predicates are assumed when a KB concept does not align (according to the lexicon) to any source word. Finally, rule state names are set according to the mechanism described in Section 5.</p><p>Our ent, pred and bridge cost/back-off functions assign a low cost (or high score) to source and target tree patterns with no variables whose leaves appear in either the entity or the predicate lexicons. Scal- ing factors λ i (see Eq. 1) were subjectively tuned on 20 training examples. When used as back-off func- tions, they generate as many rhs as entities or pred- icates can be retrieved from the lexicons by at least one of the words in the source tree pattern. Bridging predicates are dispreferred by adding an extra con- stant cost. At back-off, this score function generates a variable predicate, acting as a wildcard in Sparql.</p><p>Our system t2t For the rule extraction, we use a beam size of 10, and we output 100 derivations for every tree pair. We do not impose any limit in the depth of lhs or rhs, or in the number of variables. To increase the coverage of our rules, we produce deleting tree transducers by replacing fully lexical- ized branches that are directly attached to the root of a lhs with a deleting variable.</p><p>For the parameter estimation, we used 3 iterations of the latent variable averaged structured perceptron, where the number of iterations was selected on 20% of held-out training data. To assess the equality be- tween the gold and the decoded tree, we compare their denotations. The features for the discrimina- tive training were the lhs and rhs roots, the number of variables, deleting variables and leaves, the pres- ence of entities or predicates in the rhs, the rule state and children states, and several measures of charac- ter overlaps between the leaves of the source and in- formation associated to leaves in target tree patterns.</p><p>For decoding, we used standard techniques ( <ref type="bibr" target="#b8">Graehl and Knight, 2004</ref>) to constrain and prune weighted regular tree grammars given a tree trans- ducer and a source tree, and used the cube-growing algorithm to generate 10, 000 derivations, converted them to Sparql queries, and retained those that were valid (either syntactically correct or that retrieved any result). We compute the accuracy of the system as the percentage of questions for which the 1-best output tree retrieves the correct answer, and the cov- erage as the percentage for which the correct answer is within the 10, 000 best derivations. The average rule extraction time per tree pair when using beam size 1 was 0.46 seconds (median 0.35, maximum 2.94 seconds). When using beam size 10, the aver- age was 4.7 seconds (median 2.02, maximum 104.4 seconds), which gives us a glimpse of how the beam size influences the computational complexity for the typical tree size of FREE917 questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>Results are in <ref type="table">Table 1</ref>. Note that although we compare our results to those obtained with SEM- PRE ( <ref type="bibr" target="#b0">Berant et al., 2013)</ref>, the systems cannot really be compared since <ref type="bibr" target="#b0">Berant et al. (2013)</ref> did not have access to a manually created lexicon of predicates. When comparing the average number of entity and predicate rules that the back-off functions generate, we see that the number of predicate rules is much larger, implying a higher ambiguity. Despite this, our base system still produces promising results in terms of accuracy and coverage.</p><p>We also carried out several ablation experiments to investigate what are the characteristics of the sys- tem that contribute the most to the accuracy: In no nbest, we only extract one sequence of rules that transform a source into a target tree. In no del, we do not introduce deleting variables. In beam 1, we use beam size 1 in rule extraction. In no size, no tree size regularizer cost function is used. And in</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (s) Constituent tree of a question; (t) executable meaning representation; r1-r5 are typical transducer rules extracted by our algorithm, where q is a generic state, pred and bridge are predicate and bridged entity back-off states.</figDesc><graphic url="image-1.png" coords="3,324.54,57.83,204.11,393.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Hypergraph with 2-best pairs of disjoint subpaths for (ps, pt). Vertices are pairs of source and target paths. Hyperedges are pairs of tree patterns. The hyperedge with cost .3 denotes the pair s ↓ ps ⊥ {p1, p2, p3} and t ↓ pt ⊥ {p 1 , p 2 , p 3 }. The one with cost .5, s ↓ ps ⊥ {p1, p3} and t ↓ pt ⊥ {p 2 , p 3 }.</figDesc><graphic url="image-3.png" coords="7,335.88,57.82,181.45,153.21" type="bitmap" /></figure>

			<note place="foot" n="1"> The entity lexicon was released by the authors of FREE917, and the predicate lexicon is ours.</note>

			<note place="foot" n="2"> Sequence of edit operations. 3 lhs may have depth larger than 1. 4 Top-down transformations. 5 lhs variables appear at most once in the rhs. 6 Some variables on the lhs may not appear in the rhs.</note>

			<note place="foot">This mechanism produces rules &quot;on-the-fly&quot;, allowing us to compensate low rule coverage by consuming and producing tree fragments that were not necessarily observed in the training data. Back-off rules are produced when the transducer is at a back-off state q b ∈ Q b ⊂ Q, similarly as the back-off mechanisms in finite-state language models where we produce estimates (probabilities) of input structures (sequences) under less conditioning. In our scheme, a back-off state (or function) q b produces estimates that are target structures t 2 ∈ T ∆ with score s ∈ R, given some information of the source tree fragment t 1 ∈ T Σ. That is, a function q b : T Σ → {(T ∆ , R),. . .}. In our QA application, we only use the leaves of the input subtree t 1 and use lexicons or entity/predicate linkers to retrieve KB entities, KB relations or a compound of a disambiguating relation and an entity from backoff states ent, pred and bridge, respectively. Other back-off functions would transliterate the leaves of the input tree in machine translation, or produce synonyms/hypernyms in a paraphrasing application. We associate a score s to these newly created rules, which we learn to predict using the discriminative training procedure suggested by Tsochantaridis et al. (2005), as described in Section 3. Back-off rules are then constructed on-demand as q b .t 1 s → t 2 , and the discrete set of rules R is augmented with them. It remains now to recognize those back-off states when inducing tree transducer grammars, which is covered in Section 5.1.</note>

			<note place="foot" n="7"> https://github.com/pasmargo/t2t-qa</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This paper is based on results obtained from a project commissioned by the New Energy and Industrial Technology Development Organization (NEDO), and is also supported by JSPS KAKENHI Grant Number 16K16111. We thank Yoshimasa Tsuruoka, Yo Ehara and the anonymous reviewers for their helpful comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Systems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acc. Cov. # Preds. # Ents. # Rules</head><p>no back, no rule back-offs are used. As we see, removing the back-off rule capabilities is critical in this setting and makes the QA task unfeasible. We also studied the impact of the size of the training data in the generalization of our system, by train- ing the system in {100, 200, . . . , 600} examples. We found that the accuracy saturates at only 400 train- ing instances, which might be advantageous in tasks where training resources are scarce. Finally, in or- der to estimate the upper bound in the coverage and accuracy of our approach on FREE917, we also run our pipeline t2t-e with a refined version of <ref type="bibr" target="#b3">Cai and Yates (2013)</ref>'s entity lexicon, where 65 missing en- tities are added (7.8% of the total). We can observe a significant increase in the accuracy and coverage of the system, suggesting that the bottleneck may lie in the entity/predicate linking procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Future Work</head><p>One step further in the generalization of the rule ex- traction is to remove the necessity of explicitly pro- viding cost functions such as word-to-word hard- alignments or costs between tree fragments. This would allow us to remove the bias introduced by en- gineered cost functions and to obtain rules that are globally optimal. In this setup, the parameters of the cost functions are to be learned with the objective to maximize the likelihood on the training data or a downstream application performance. However, since rules (or tree mappings) would become hid- den variables, this generalized rule extraction may require faster methods to enumerate plausible rules. Another extension would be to make the rule extrac- tion more robust against parsing errors, using pairs of forests instead of pairs of trees, similarly as in <ref type="bibr" target="#b17">Liu et al. (2009)</ref>. Regarding the QA application, there are two nat- ural extensions that we want to address, namely to develop general and automatic entity and predicate linking mechanisms for large knowledge bases, and to test our approach in datasets that require higher levels of compositionality such as the QALD chal- lenges ( <ref type="bibr" target="#b24">Unger et al., 2015)</ref> or those datasets pro- duced by <ref type="bibr" target="#b25">Wang et al. (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We proposed to induce tree to tree transducers us- ing a rule extraction algorithm that uses cost func- tions over pairs of tree fragments (instead of word- alignments), which increases the applicability of these models. Some cost functions may act as rule back-offs, generating new rhs given unseen lhs, thus producing transducer rules "on-the-fly". The scores of these rules are obtained on demand using a discriminative training procedure that estimates weights for rule features. This strategy was useful to compensate the lack of rule coverage when inducing tree transducers from small tree corpora.</p><p>As a proof-of-concept, we tested the tree trans- ducer induced with our algorithm on a QA task over a large KB, a domain in which tree transducers have not been effective before. In this task, lexicon map- pings were naturally introduced as cost functions and rule back-offs, without loss of generality. De- spite using a manually created lexicon of predicates, we showed a high accuracy and coverage of non- final rules, which are promising results.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey on tree edit distance and related problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">337</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="217" to="239" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-scale semantic parsing via schema matching and lexicon extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqing</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="423" to="433" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sentence compression as tree transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="637" to="674" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning non-isomorphic tree mappings for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="205" to="208" />
		</imprint>
	</monogr>
	<note>ACL &apos;03. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">What&apos;s in a translation rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLTNAACL&apos;2004: Main Proceedings</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A statistical semantic parser that integrates syntax and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifang</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Conference on Computational Natural Language Learning, CONLL &apos;05</title>
		<meeting>the Ninth Conference on Computational Natural Language Learning, CONLL &apos;05<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Training tree transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Graehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Main Proceedings</title>
		<editor>Daniel Marcu Susan Dumais and Salim Roukos</editor>
		<meeting><address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004-05-02" />
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Forest rescoring: Faster decoding with integrated language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic parsing with bayesian tree transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keeley</forename><surname>Bevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="488" to="496" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-07" />
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
	<note>Accurate unlexicalized parsing</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An overview of probabilistic tree transducers for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Graehl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing</title>
		<editor>Alexander Gelbukh</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">3406</biblScope>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Moses Manual</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An extended GHKM algorithm for inducing Lambda-SCFG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="605" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Lambda dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno>abs/1309.4408</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Design challenges for entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="315" to="328" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving tree-to-tree translation with packed forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lü</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Suntec, Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009-08" />
			<biblScope unit="page" from="558" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The power of extended topdown tree transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Maletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Graehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="410" to="430" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Mappings and grammars on trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rounds</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="257" to="287" />
		</imprint>
	</monogr>
	<note>Mathematical systems theory</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The tree-to-tree correction problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuo-Chung</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="422" to="433" />
			<date type="published" when="1979-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generalized sequential machine maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">W</forename><surname>Thatcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="339" to="367" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1453" to="1484" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Question Answering over Linked Data (QALD-5)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corina</forename><surname>Forascu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axelcyrille</forename><surname>Ngonga Ngomo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Cimiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of CLEF 2015-Conference and Labs of the Evaluation forum, volume 1391. Working Notes of CLEF 2015-Conference and Labs of the Evaluation forum</title>
		<editor>Linda Cappellato, Nicola Ferro, Gareth Jones, and Eric San Juan</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Building a semantic parser overnight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1332" to="1342" />
		</imprint>
	</monogr>
	<note>Beijing, China, July. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning for semantic parsing with statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL &apos;06</title>
		<meeting>the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL &apos;06<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="439" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recognizing paraphrases and textual entailment using inversion transduction grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, EMSEE &apos;05</title>
		<meeting>the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, EMSEE &apos;05<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Simple fast algorithms for the editing distance between trees and related problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaizhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Shasha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1245" to="1262" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
