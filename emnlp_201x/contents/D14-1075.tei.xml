<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fear the REAPER: A System for Automatic Multi-Document Summarization with Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cody</forename><surname>Rioux</surname></persName>
							<email>cody.rioux@uleth.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Lethbridge Lethbridge</orgName>
								<address>
									<region>AB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadid</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
							<email>sadid.hasan@philips.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Philips Research North America Briarcliff Manor</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yllias</forename><surname>Chali</surname></persName>
							<email>chali@cs.uleth.ca</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Lethbridge Lethbridge</orgName>
								<address>
									<region>AB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fear the REAPER: A System for Automatic Multi-Document Summarization with Reinforcement Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="681" to="690"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper explores alternate algorithms, reward functions and feature sets for performing multi-document summarization using reinforcement learning with a high focus on reproducibility. We show that ROUGE results can be improved using a unigram and bigram similarity metric when training a learner to select sentences for summarization. Learners are trained to summarize document clusters based on various algorithms and reward functions and then evaluated using ROUGE. Our experiments show a statistically significant improvement of 1.33%, 1.58%, and 2.25% for ROUGE-1, ROUGE-2 and ROUGE-L scores, respectively, when compared with the performance of the state of the art in automatic summarization with reinforcement learning on the DUC2004 dataset. Furthermore query focused extensions of our approach show an improvement of 1.37% and 2.31% for ROUGE-2 and ROUGE-SU4 respectively over query focused extensions of the state of the art with reinforcement learning on the DUC2006 dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The multi-document summarization problem has received much attention recently <ref type="bibr" target="#b15">(Lyngbaek, 2013;</ref><ref type="bibr" target="#b23">Sood, 2013;</ref><ref type="bibr" target="#b20">Qian and Liu, 2013</ref>) due to its ability to reduce large quantities of text to a human processable amount as well as its appli- cation in other fields such as question answering ( <ref type="bibr" target="#b14">Liu et al., 2008;</ref><ref type="bibr" target="#b3">Chali et al., 2009a;</ref><ref type="bibr" target="#b4">Chali et al., 2009b;</ref><ref type="bibr" target="#b6">Chali et al., 2011b</ref>). We expect this trend to further increase as the amount of linguistic data on the web from sources such as social media, wikipedia, and online newswire increases. This paper focuses specifically on utilizing reinforce- ment learning <ref type="bibr" target="#b24">(Sutton and Barto, 1998;</ref><ref type="bibr" target="#b25">Szepesv, 2009</ref>) to create a policy for summarizing clusters of multiple documents related to the same topic.</p><p>The task of extractive automated multi- document summarization <ref type="bibr" target="#b17">(Mani, 2001</ref>) is to se- lect a subset of textual units, in this case sentences, from the source document cluster to form a sum- mary of the cluster in question. This extractive approach allows the learner to construct a sum- mary without concern for the linguistic quality of the sentences generated, as the source documents are assumed to be of a certain linguistic quality. This paper aims to expand on the techniques used in <ref type="bibr" target="#b21">Ryang and Abekawa (2012)</ref> which uses a re- inforcement learner, specifically T D(λ), to create summaries of document clusters. We achieve this through introducing a new algorithm, varying the feature space and utilizing alternate reward func- tions.</p><p>The T D(λ) learner used in Ryang and Abekawa (2012) is a very early reinforcement learning implementation. We explore the option of leveraging more recent research in reinforcement learning algorithms to improve results. To this end we explore the use of SARSA which is a deriva- tive of T D(λ) that models the action space in ad- dition to the state space modelled by T D(λ). Fur- thermore we explore the use of an algorithm not based on temporal difference methods, but instead on policy iteration techniques. Approximate Pol- icy Iteration ( <ref type="bibr" target="#b10">Lagoudakis and Parr, 2003)</ref> gener- ates a policy, then evaluates and iterates until con- vergence.</p><p>The reward function in Ryang and Abekawa (2012) is a delayed reward based on tf * idf values. We further explore the reward space by introduc- ing similarity metric calculations used in ROUGE <ref type="bibr" target="#b13">(Lin, 2004</ref>) and base our ideas on <ref type="bibr" target="#b22">Saggion et al. (2010)</ref>. The difference between immediate re- wards and delayed rewards is that the learner re-ceives immediate feedback at every action in the former and feedback only at the end of the episode in the latter. We explore the performance differ- ence of both reward types. Finally we develop query focused extensions to both reward functions and present their results on more recent Document Understanding Conference (DUC) datasets which ran a query focused task.</p><p>We first evaluate our systems using the DUC2004 dataset for comparison with the results in <ref type="bibr" target="#b21">Ryang and Abekawa (2012)</ref>. We then present the results of query focused reward functions against the DUC2006 dataset to provide refer- ence with a more recent dataset and a more recent task, specifically a query-focused summarization task. Evaluations are performed using ROUGE for ROUGE-1, ROUGE-2 and ROUGE-L values for general summarization, while ROUGE-2 and ROUGE-SU4 is used for query-focused summa- rization. Furthermore we selected a small subset of query focused summaries to be subjected to hu- man evaluations and present the results.</p><p>Our implementation is named REAPER (Relatedness-focused Extractive Automatic summary Preparation Exploiting Reinfocement learning) thusly for its ability to harvest a docu- ment cluster for ideal sentences for performing the automatic summarization task. REAPER is not just a reward function and feature set, it is a full framework for implementing summarization tasks using reinforcement learning and is avail- able online for experimentation. <ref type="bibr">1</ref> The primary contributions of our experiments are as follows:</p><p>• Exploration of T D(λ), SARSA and Ap- proximate Policy Iteration.</p><p>• Alternate REAPER reward function.</p><p>• Alternate REAPER feature set.</p><p>• Query focused extensions of automatic sum- marization using reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Work and Motivation</head><p>Previous work using reinforcement learning for natural language processing tasks ( <ref type="bibr" target="#b0">Branavan et al., 2009;</ref><ref type="bibr" target="#b27">Wan, 2007;</ref><ref type="bibr" target="#b21">Ryang and Abekawa, 2012;</ref><ref type="bibr" target="#b5">Chali et al., 2011a;</ref><ref type="bibr" target="#b7">Chali et al., 2012</ref>) inspired us to use a similar approach in our experiments. <ref type="bibr" target="#b21">Ryang and Abekawa (2012)</ref> im- plemented a reinforcement learning approach to 1 https://github.com/codyrioux/REAPER multi-document summarization which they named Automatic Summarization using Reinforcement Learning (ASRL). ASRL uses TD(λ) to learn and then execute a policy for summarizing a cluster of documents. The algorithm performs N summa- rizations from a blank state to termination, updat- ing a set of state-value predictions as it does so. From these N episodes a policy is created using the estimated state-value pairs, this policy greed- ily selects the best action until the summary enters its terminal state. This summary produced is the output of ASRL and is evaluated using ROUGE- 1, ROUGE-2, and ROUGE-L <ref type="bibr" target="#b13">(Lin, 2004</ref>). The results segment of the paper indicates that ASRL outperforms greedy and integer linear program- ming (ILP) techniques for the same task.</p><p>There are two notable details that provide the motivation for our experiments; TD(λ) is rela- tively old as far as reinforcement learning (RL) algorithms are concerned, and the optimal ILP did not outperform ASRL using the same reward func- tion. The intuition gathered from this is that if the optimal ILP algorithm did not outperform the suboptimal ASRL on the ROUGE evaluation, us- ing the same reward function, then there is clearly room for improvement in the reward function's ability to accurately model values in the state space. Furthermore one may expect to achieve a performance boost exploiting more recent re- search by utilizing an algorithm that intends to improve upon the concepts on which T D(λ) is based. These provide the motivation for the re- mainder of the research preformed.</p><p>Query focused multi-document summarization ( <ref type="bibr" target="#b11">Li and Li, 2013;</ref><ref type="bibr" target="#b2">Chali and Hasan, 2012b;</ref><ref type="bibr" target="#b30">Yin et al., 2012;</ref><ref type="bibr" target="#b29">Wang et al., 2013)</ref> has recently gained much attention due to increasing amounts of tex- tual data, as well as increasingly specific user de- mands for extracting information from said data. This is reflected in the query focused tasks run in the Document Understanding Conference (DUC) and Text Analysis Conference (TAC) over the past decade. This has motivated us to design and im- plement query focused extensions to these rein- forcement learning approaches to summarization.</p><p>There has been some research into the effects of sentence compression on the output of automatic summarization systems <ref type="bibr" target="#b1">(Chali and Hasan, 2012a)</ref>, specifically the evaluation results garnered from compressing sentences before evaluation <ref type="bibr" target="#b20">(Qian and Liu, 2013;</ref><ref type="bibr" target="#b12">Lin and Rey, 2003;</ref><ref type="bibr" target="#b21">Ryang and Abekawa, 2012)</ref>. However <ref type="bibr" target="#b21">Ryang and Abekawa (2012)</ref> found this technique to be ineffective in im- proving ROUGE metrics using a similar reinforce- ment learning approach to this paper, as a result we will not perform any further exploration into the effects of sentence compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Definition</head><p>We use an identical problem definition to <ref type="bibr" target="#b21">Ryang and Abekawa (2012)</ref>. Assume the given cluster of documents is represented as a set of textual units D = {x 1 , x 2 , · · · , x n } where |D| = n and x i represents a single textual unit. Textual units for the purposes of this experiment are the indi- vidual sentences in the document cluster, that is</p><formula xml:id="formula_0">D = D 1 ∪ D 2 ∪ · · · ∪ D m</formula><p>where m is the number of documents in the cluster and each D i represents a document.</p><p>The next necessary component is the score function, which is to be used as the reward for the learner. The function score(s) can be applied to any s ⊂ D. s is a summary of the given document or cluster of documents.</p><p>Given these parameters, and a length limitation k we can define an optimal summary s * as:</p><formula xml:id="formula_1">s * = argmax score(s)</formula><p>where s ⊂ D and length(s) ≤ k <ref type="formula">(1)</ref> It is the objective of our learner to create a pol- icy that produces the optimal summary for its pro- vided document cluster D. Henceforth the length limitations used for general summarization will be 665 bytes, and query focused summarization will use 250 words. These limitations on summary length match those set by the Document Under- standing Conferences associated with the dataset utilized in the respective experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Algorithms</head><p>T D(λ) and SARSA (Sutton and Barto, 1998) are temporal difference methods in which the primary difference is that T D(λ) models state value pre- dictions, and SARSA models state-action value predictions. Approximate Policy Iteration (API) follows a different paradigm by iteratively improv- ing a policy for a markov decision process until the policy converges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">TD(λ)</head><p>In the ASRL implementation of T D(λ) the learn- ing rate α k and temperature τ k decay as learning progresses with the following equations with k set to the number of learning episodes that had taken place.</p><formula xml:id="formula_2">α k = 0.001 · 101/(100 + k 1.1 ) (2) τ k = 1.0 * 0.987 k−1<label>(3)</label></formula><p>One can infer from the decreasing values of α k that as the number of elapsed episodes increases the learner adjusts itself at a smaller rate. Simi- larly as the temperature τ k decreases the action se- lection policy becomes greedier and thus performs less exploration, this is evident in (5) below.</p><p>Note that unlike traditional T D(λ) implementa- tions the eligibility trace e resets on every episode. The reasons for this will become evident in the experiments section of the paper in which λ = 1, γ = 1 and thus there is no decay during an episode and complete decay after an episode. The same holds true for SARSA below.</p><p>The action-value estimation Q(s, a) is approxi- mated as:</p><formula xml:id="formula_3">Q(s, a) = r + γV (s )<label>(4)</label></formula><p>The policy is implemented as such:</p><formula xml:id="formula_4">policy(a|s; θ; τ ) = e Q(s,a)/τ a∈A e Q(s,a)/τ<label>(5)</label></formula><p>Actions are selected probabilistically using soft- max selection <ref type="bibr" target="#b24">(Sutton and Barto, 1998</ref>) from a Boltzmann distribution. As the value of τ ap- proaches 0 the distribution becomes greedier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SARSA</head><p>SARSA is implemented in a very similar manner and shares α k , τ k , φ(s), m, and policy(s) with the T D(λ) implementation above. SARSA is also a temporal difference algorithm and thus behaves similarly to T D(λ) with the exception that values are estimated not only on the state s but a state- action pair [s, a].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Approximate Policy Iteration</head><p>The third algorithm in our experiment uses Ap- proximate Policy Iteration ( <ref type="bibr" target="#b10">Lagoudakis and Parr, 2003)</ref> to implement a reinforcement learner. The novelty introduced by <ref type="bibr" target="#b10">(Lagoudakis and Parr, 2003</ref>) is that they eschew standard representations for a policy and instead use a classifier to represent the current policy π. Further details on the algo- rithm can be obtained from <ref type="bibr" target="#b10">Lagoudakis and Parr (2003)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Our state space S is represented simply as a three- tuple [s a f ] in which s is the set of textual units (sentences) that have been added to the summary, a is a sequence of actions that have been per- formed on the summary and f is a boolean with value 0 representing non-terminal states and 1 rep- resenting a summary that has been terminated.</p><p>The individual units in our action space are de- fined as <ref type="bibr">[:insert x i</ref> ] where x i is a textual unit as described earlier, let us define D i as the set [:in- sert x i ] for all x i ∈ D where D is the document set. We also have one additional action [:finish] and thus we can define our action space.</p><formula xml:id="formula_5">A = D i ∪ {[: f inish]}<label>(6)</label></formula><p>The actions eligible to be executed on any given state s is defined by a function actions(A, s):</p><formula xml:id="formula_6">actions(A, s) = [: f inish] if length(s) &gt; k A − a t otherwise (7)</formula><p>The state-action transitions are defined below:</p><formula xml:id="formula_7">[s t , a t , 0] a=insertx i − −−−−−− → [s t ∪ x i , a t ∪ a , 0] (8) [s t , a t , 0] :f inish −−−−→ [s t , a t , 1]<label>(9)</label></formula><formula xml:id="formula_8">[s t , a t+1 , 1] any − − → [s t , a t , 1]<label>(10)</label></formula><p>Insertion adds both the content of the textual unit x i to the set s as well as the action itself to set a. Conversely finishing does not alter s or a but it flips the f bit to on. Notice from (10) that once a state is terminal any further actions have no effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Feature Space</head><p>We present an alternate feature set called REAPER feature set based on the ideas presented in <ref type="bibr" target="#b21">Ryang and Abekawa (2012)</ref>. Our proposed fea- ture set follows a similar format to the previous one but depends on the presence of top bigrams instead of tf * idf words.</p><p>• One bit b ∈ 0, 1 for each of the top n bigrams <ref type="bibr" target="#b18">(Manning and Schütze, 1999</ref>) present in the summary.</p><p>• Coverage ratio calculated as the sum of the bits in the previous feature divided by n.</p><p>• Redundancy Ratio calculated as the number of redundant times a bit in the first feature is flipped on, divided by n.</p><p>• Length Ratio calculated as length(s)/k where k is the length limit.</p><p>• Longest common subsequence length.</p><p>• Length violation bit. Set to 1 if length(s) &gt; k</p><p>Summaries which exceed the length limitation k are subject to the same reduction as the ASRL feature set ( <ref type="bibr" target="#b21">Ryang and Abekawa, 2012)</ref> to an all zero vector with the final bit set to one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Reward Function</head><p>Our reward function (termed as REAPER reward) is based on the n-gram concurrence score metric, and the longest-common-subsequence recall met- ric contained within ROUGE <ref type="bibr" target="#b13">(Lin, 2004</ref>).</p><formula xml:id="formula_9">reward(s) =      −1, if length(s) &gt; k score(s) if s is terminal 0 otherwise (11)</formula><p>Where score is defined identically to ASRL, with the exception that Sim is a new equation based on ROUGE metrics.</p><formula xml:id="formula_10">score(s) = x i ∈S λ s Rel(x i )− x i ,x j ∈S,i&lt;j (1 − λ s )Red(x i , x j ) (12) Rel(x i ) = Sim(x i , D) + P os(x i ) −1<label>(13)</label></formula><p>Red(</p><formula xml:id="formula_11">x i , x j ) = Sim(x i , x j )<label>(14)</label></formula><p>Ryang and Abekawa (2012) experimentally de- termined a value of 0.9 for the λ s parameter. That value is used herein unless otherwise specified. Sim has been redefined as:</p><formula xml:id="formula_12">Sim(s) =1 * ngco(1, D, s)+ 4 * ngco(2, D, s)+ 1 * ngco(3, D, s)+ 1 * ngco(4, D, s)+ 1 * rlcs(D, s)<label>(15)</label></formula><p>and ngco is the ngram co-occurence score met- ric as defined by <ref type="bibr" target="#b13">Lin (2004)</ref>.</p><formula xml:id="formula_13">ngco(n, D, s) = r∈D ngram∈r Count match (ngram) Sr∈D ngram∈r Count(ngram)<label>(16)</label></formula><p>Where n is the n-gram count for example 2 for bigrams, D is the set of documents, and s is the summary in question. Count match is the maxi- mum number of times the ngram occurred in either D or s.</p><p>The rlcs(R, S) is also a recall oriented mea- sure based on longest common subsequence <ref type="bibr" target="#b9">(Hirschberg, 1977)</ref>.</p><p>Recall was selected as DUC2004 tasks favoured a β value for F-Measure </p><p>We are measuring similarity between sentences and our entire reference set, and thusly our D is the set of documents defined in section 3. This is also a delayed reward as the provided reward is zero until the summary is terminal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Query Focused Rewards</head><p>We have proposed an extension to both reward functions to allow for query focused (QF) summa- rization. We define a function score which aims to balance the summarization abilities of the re- ward with a preference for selecting textual units related to the provided query q. Both ASRL and REAPER score functions have been extended in the following manner where Sim is the same sim- ilarity functions used in equation <ref type="formula" target="#formula_2">(13)</ref> and <ref type="bibr">(15)</ref>.</p><formula xml:id="formula_15">score (q, s) = βSim(q, s) + (1 − β)score(s)<label>(18)</label></formula><p>The parameter β is a balancing factor between query similarity and overall summary score in which 0 &lt;= β &lt;= 1, we used an arbitrarily cho- sen value of 0.9 in these experiments. In the case of ASRL the parameter q is the vectorized version of the query function with tf * idf values, and for Sim q is a sequence of tokens which make up the query, stemmed and stop-words removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Immediate Rewards</head><p>Finally we also employ immediate versions of the reward functions which behave similarly to their delayed counterparts with the exception that the score is always provided to the caller regardless of the terminal status of state s.</p><formula xml:id="formula_16">reward(s) = −1, if length(s) &gt; k score(s) otherwise<label>(19)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>We first present results 2 of our experiments, spec- ifying parameters, and withholding discussion un- til the following section. We establish a bench- mark using ASRL and other top-scoring sum- marization systems compared with REAPER us- ing ROUGE. For generic multi-document sum- marization we run experiments on all 50 docu- ment clusters, each containing 10 documents, of DUC2004 task 2 with parameters for REAPER and ASRL fixed at λ = 1, γ = 1, and k = 665. Sentences were stemmed using a Porter Stemmer <ref type="bibr" target="#b19">(Porter, 1980)</ref> and had the ROUGE stop word set removed. All summaries were processed in this manner and then projected back into their original (unstemmed, with stop-words) state and output to disk.  <ref type="table">Table 1</ref>: Experimental results with ROUGE-1, ROUGE-2 and ROUGE-L scores on DUC2004. <ref type="table">Table 1</ref> presents results for REAPER, ASRL <ref type="bibr" target="#b21">(Ryang and Abekawa, 2012)</ref>, MCKP (Takamura and Okumura, 2009), PEER65 ( <ref type="bibr" target="#b8">Conroy et al., 2004</ref>) , and GREEDY ( <ref type="bibr" target="#b21">Ryang and Abekawa, 2012</ref>) algorithms on the same task. This allows us to make a direct comparison with the results of <ref type="bibr" target="#b21">Ryang and Abekawa (2012)</ref>.</p><formula xml:id="formula_17">Config R-1 R-2 R-L REAPER</formula><p>REAPER results are shown using the T D(λ) algorithm, REAPER reward function, and ASRL feature set. This is to establish the validity of the reward function holding all other factors constant. REAPER results for ROUGE-1, ROUGE-2 and ROUGE-L are statistically significant compared to the result set presented in <ref type="table" target="#tab_1">Table 2</ref> of Ryang and Abekawa (2012) using p &lt; 0.01.   <ref type="table">Table 3</ref>: REAPER with delayed and immediate re- wards on DUC2004. <ref type="table">Table 3</ref> shows the performance difference of REAPER when using a delayed and immediate re- ward. The immediate version of REAPER pro- vides feedback on every learning step, unlike the delayed version which only provides score at the end of the episode.  <ref type="table">Table 4</ref>: REAPER with alternate feature spaces on DUC2004.</p><formula xml:id="formula_18">Run R-1 R-2 R-L REAPER</formula><p>We can observe the results of using REAPER with various feature sets in <ref type="table">Table 4</ref>. Experiments were run using REAPER reward, T D(λ), and the specified feature set.  <ref type="table">Table 5</ref>: REAPER with alternate algorithms on DUC2004. <ref type="table">Table 5</ref> displays the performance of REAPER with alternate algorithms. T D(λ) and SARSA are run using the delayed reward feature, while AP I requires an immediate reward and was thus run with the immediate reward.</p><formula xml:id="formula_19">Algorithm R-1 R-2 R-L T D(λ) 0.39773</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>R-2 R-SU4 REAP ER 0.07008 0.11689 ASRL 0.05639 0.09379 S24 0.09505 0.15464 Baseline 0.04947 0.09788 <ref type="table">Table 6</ref>: QF-REAPER on DUC2006.</p><p>For query-focused multi-document summariza- tion we experimented with the DUC2006 system task, which contained 50 document clusters con- sisting of 25 documents each. Parameters were fixed to λ = 1, γ = 1 and k = 250 words. In <ref type="table">Table 6</ref> we can observe the results 3 of our query focused systems against DUC2006's top scorer (S24) for ROUGE-2, and a baseline. The baseline was generated by taking the most recent document in the cluster and outputting the first 250 words.</p><p>Human Evaluations: We had three native English-speaking human annotators evaluate a set of four randomly chosen summaries produced by REAPER on the DUC2004 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1</head><p>A2 A3 AVG Grammaticality 3.00 4.00 4.00 3.67 Redundancy 4.75 4.25 2.75 3.92 Referential Clarity 4.00 4.50 3.50 4.00 Focus 4.50 3.50 2.25 3.42 Structure 3.50 4.00 3.00 3.50 Responsiveness 4.25 3.75 3.00 3.67 <ref type="table">Table 7</ref>: Human evaluation scores on DUC2004. <ref type="table">Table 7</ref> shows the evaluation results accord- ing to the DUC2006 human evaluation guidelines. The first five metrics are related entirely to the lin- guistic quality of the summary in question and the final metric, Responsiveness, rates the summary on its relevance to the source documents. Columns represent the average provided by a given annota- tor over the four summaries, and the AVG column represents the average score for all three annota- tors over all four summaries. Score values are an integer between 1 and 5 inclusive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>First we present a sample of a summary generated from a randomly selected cluster. The following summary was generated from cluster D30017 of the DUC 2004 dataset using REAPER reward with T D(λ) and REAPER feature space.</p><p>A congressman who visited remote parts of North Korea last week said Saturday that the food and health situation there was desperate and de- teriorating, and that millions of North Koreans might have starved to death in the last few years. North Korea is entering its fourth winter of chronic food shortages with its people malnourished and at risk of dying from normally curable illnesses, senior Red Cross officials said Tuesday. More than five years of severe food shortages and a near-total breakdown in the public health system have led to devastating malnutrition in North Korea and prob- ably left an entire generation of children physi- cally and mentally impaired, a new study by in- ternational aid groups has found. Years of food shortages have stunted the growth of millions of North Korean children, with two-thirds of children under age seven suffering malnourishment, U.N. experts said Wednesday. The founder of South Ko- rea's largest conglomerate plans to visit his native North Korea again next week with a gift of 501 cattle, company officials said Thursday. "There is enough rice.</p><p>We can observe that the summary is both syn- tactically sound, and elegantly summarizes the source documents.</p><p>Our baseline results table <ref type="table">(Table 1)</ref> shows REAPER outperforming ASRL in a statistically significant manner on all three ROUGE metrics in question. However we can see from the abso- lute differences in score that very few additional important words were extracted (ROUGE-1) how- ever REAPER showed a significant improvement in the structuring and ordering of those words (ROUGE-2, and ROUGE-L).</p><p>The balancing factors used in the REAPER re- ward function are responsible for the behaviour of the reward function, and thus largely responsible for the behaviour of the reinforcement learner. In equation 15 we can see balance numbers of 1, 4, 1, 1, 1 for 1-grams, 2-grams, 3-grams, 4-grams, and LCS respectively. In adjusting these values a user can express a preference for a single metric or a specific mixture of these metrics. Given that the magnitude of scores for n-grams decrease as n in- creases and given that the magnitude of scores for 1-grams is generally 3 to 4 times larger, in our ex- perience, we can see that this specific reward func- tion favours bigram similarity over unigram simi- larity. These balance values can be adjusted to suit the specific needs of a given situation, however we leave exploration of this concept for future work.</p><p>We can observe in <ref type="figure" target="#fig_4">Figure 1</ref> that ASRL does not converge on a stable value, and dips towards the 300 th episode while in <ref type="figure" target="#fig_5">Figure 2</ref> REAPER does not take nearly such a dramatic dip. These fig- ures display average normalized reward for all 50 document clusters on a single run. Furthermore we can observe that ASRL reaches it's peak re- ward around episode 225 while REAPER does so around episode 175 suggesting that REAPER con- verges faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Delayed vs. Immediate Rewards</head><p>The delayed vs. immediate rewards results in Ta- ble 3 clearly show that delaying the reward pro- vides a significant improvement in globally opti- mizing the summary for ROUGE score. This can be attributed to the λ = 1 and γ = 1 parame- ter values being suboptimal for the immediate re- ward situation. This has the added benefit of be- ing much more performant computationally as far fewer reward calculations need be done. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Feature Space</head><p>The feature space experiments in <ref type="table">Table 4</ref> seem to imply that REAPER performs similarly with both feature sets. We are confident that an improve- ment could be made through further experimenta- tion. Feature engineering, however, is a very broad field and we plan to pursue this topic in depth in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Algorithms</head><p>T D(λ) significantly outperformed both SARSA and AP I in the algorithm comparison. <ref type="bibr" target="#b21">Ryang and Abekawa (2012)</ref> conclude that the feature space is largely responsible for the algorithm performance. This is due to the fact that poor states such as those that are too long, or those that contain few impor- tant words will reduce to the same feature set and receive negative rewards collectively. SARSA loses this benefit as a result of its modelling of state-action pairs. AP I on the other hand may have suffered a per- formance loss due to its requirements of an imme- diate reward, this is because when using a delayed reward if the trajectory of a rollout does not reach a terminal state the algorithm will not be able to make any estimations about the value of the state in question. We propose altering the policy iter- ation algorithm to use a trajectory length of one episode instead of a fixed number of actions in or- der to counter the need for an immediate reward function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Query Focused Rewards</head><p>From the ROUGE results in <ref type="table">Table 6</ref> we can infer that while REAPER outperformed ASRL on the query focused task, however it is notable that both systems under performed when compared to the top system from the DUC2006 conference.</p><p>We can gather from these results that it is not enough to simply naively calculate similarity with the provided query in order to produce a query- focused result. Given that the results produced by the generic summarization task is rather accept- able according to our human evaluations we sug- gest that further research be focused on a proper similarity metric between the query and summary to improve the reward function's overall ability to score summaries in a query-focused setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Future Work</head><p>We have explored alternate reward functions, fea- ture sets, and algorithms for the task of automatic summarization using reinforcement learning. We have shown that REAPER outperforms ASRL on both generic summarization and the query focused tasks. This suggests the effectiveness of our re- ward function and feature space. Our results also confirm that T D(λ) performs best for this task compared to SARSA and AP I.</p><p>Due to the acceptable human evaluation scores on the general summarization task it is clear that the algorithm produces acceptable summaries of newswire data. Given that we have a framework for generating general summaries, and the cur- rent popularity of the query-focused summariza- tion task, we propose that the bulk of future work in this area be focused on the query-focused task specifically in assessing the relevance of a sum- mary to a provided query. Therefore we intend to pursue future research in utilizing word-sense dis- ambiguation and synonyms, as well as other tech- niques for furthering REAPER's query similarity metrics in order to improve its ROUGE and human evaluation scores on query-focused tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>Lin, 2004) high enough that only recall would be considered. lcs is the longest common sub- sequence, and length(D) is the total number of tokens in the reference set D. rlcs(D, s) = lcs(D, s) length(D)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: ASRL normalized reward.</figDesc><graphic url="image-1.png" coords="8,72.00,62.81,200.00,160.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: REAPER normalized reward.</figDesc><graphic url="image-2.png" coords="8,307.28,62.81,200.00,160.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>REAPER run 10 times on the DUC2004. We present the results of 10 runs of REAPER, with REAPER feature set.. As with ASRL, REAPER does not converge on a stable solution which is attributable to the random elements of T D(λ). Results in all three metrics are again statistically significant compared to ASRL results presented in the Ryang and Abekawa (2012) pa- per. All further REAPER experiments use the bi- gram oriented feature space.</figDesc><table>Reward 
R-1 
R-2 
R-L 
Delayed 
0.39773 0.11397 0.36018 
Immediate 0.32981 0.07709 0.30003 

</table></figure>

			<note place="foot" n="2"> ROUGE-1.5.5 run with-m-s-p 0</note>

			<note place="foot" n="3"> ROUGE-1.5.5 run with-n 2-x-m-2 4-u-c 95-r 1000-f A-p 0.5-t 0-l 250</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous review-ers for their useful comments. The research re-ported in this paper was supported by the Nat-ural Sciences and Engineering Research Council (NSERC) of Canada-discovery grant and the Uni-versity of Lethbridge. This work was done when the second author was at the University of Leth-bridge.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reinforcement Learning for Mapping Instructions to Actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP</title>
		<meeting>the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the Effectiveness of Using Sentence Compression Models for QueryFocused Multi-Document Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012)</title>
		<meeting>the 24th International Conference on Computational Linguistics (COLING 2012)<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="457" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Queryfocused Multi-document Summarization: Automatic Data Annotations and Supervised Learning Approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="109" to="145" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Do Automatic Annotation Techniques Have Any Impact on Supervised Complex Question Answering?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Joty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-IJCNLP 2009)</title>
		<meeting>the Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-IJCNLP 2009)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="329" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Complex Question Answering: Unsupervised Learning Approaches and Experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Reinforcement Learning Framework for Answering Complex Questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Imam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Intelligent User Interfaces</title>
		<meeting>the 16th International Conference on Intelligent User Interfaces<address><addrLine>Palo Alto, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="307" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improving Graph-based Random Walks for Complex Question Answering Using Syntactic, Shallow Semantic and Extended String Subsequence Kernels. Information Processing and Management (IPM), Special Issue on Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Joty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="843" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving the Performance of the Reinforcement Learning Model for Answering Complex Questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Imam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM Conference on Information and Knowledge Management (CIKM 2012)</title>
		<meeting>the 21st ACM Conference on Information and Knowledge Management (CIKM 2012)<address><addrLine>Maui, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2499" to="2502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Left-Brain / Right-Brain Multi-Document Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Leary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Document Un-derstanding Conference</title>
		<meeting>the Document Un-derstanding Conference</meeting>
		<imprint>
			<publisher>DUC</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Algorithms for the Longest Common Subsequence Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="664" to="675" />
			<date type="published" when="1977-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reinforcement learning as classification: Leveraging modern classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lagoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on Machine Learning</title>
		<meeting>the Twentieth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">424</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Novel Feature-based Bayesian Model for Query Focused Multi-document Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="89" to="98" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving Summarization Performance by Sentence Compression A Pilot Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Workshop on Information Retrieval with Asian Languages</title>
		<meeting>the Sixth International Workshop on Information Retrieval with Asian Languages</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ROUGE : A Package for Automatic Evaluation of Summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Sciences</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="25" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding and Summarizing Answers in Community-Based Question Answering Services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING &apos;08 Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2008-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">SPORK: A Summarization Pipeline for Online Repositories of Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyngbaek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Sc</forename><surname>Thesis</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>California Polytechnic State University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Automatic Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>John Benjamins Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<title level="m">Foundations of Statistical Natural Language Processing</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">An algorithm for suffix stripping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
			<publisher>Program</publisher>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="130" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast Joint Compression and Summarization via Graph Cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Framework of Automatic Text Summarization Using Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ryang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Abekawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="256" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multilingual Summarization Evaluation without Human Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Saggion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Iria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Juan-Manuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING &apos;10 Proceedings of the 23rd International Conference on Computational Linguistics: Posters</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1059" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Towards Summarization of Written Text Conversations. M.sc. thesis, International Institute of Information Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>Hyderabad, India</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Algorithms for Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Szepesv</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Text Summarization Model based on Maximum Coverage Problem and its Variant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL &apos;09 Proceedings of the 12th Conference of the European Chapter</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009-04" />
			<biblScope unit="page" from="781" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards an Iterative Reinforcement Approach for Simultaneous Document Summarization and Keyword Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th</title>
		<meeting>the 45th</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<title level="m">Annual Meeting of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="552" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Sentence Compression Based Framework to Query-Focused</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Castelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1384" to="1394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Queryfocused multi-document summarization based on query-sensitive feature space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Information and knowledge management-CIKM &apos;12</title>
		<meeting>the 21st ACM international conference on Information and knowledge management-CIKM &apos;12</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">1652</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
