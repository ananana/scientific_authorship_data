<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Related Work Summarization with a Joint Context-driven Attention Mechanism</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongzhen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Maritime Economics and Management</orgName>
								<orgName type="institution">Dalian Maritime University</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhong</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Informatics, Computing and Engineering</orgName>
								<orgName type="institution">Indiana University Bloomington</orgName>
								<address>
									<settlement>Bloomington</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Gao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Informatics, Computing and Engineering</orgName>
								<orgName type="institution">Indiana University Bloomington</orgName>
								<address>
									<settlement>Bloomington</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Related Work Summarization with a Joint Context-driven Attention Mechanism</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1776" to="1786"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1776</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Conventional solutions to automatic related work summarization rely heavily on human-engineered features. In this paper, we develop a neural data-driven summarizer by leverag-ing the seq2seq paradigm, in which a joint context-driven attention mechanism is proposed to measure the contextual relevance within full texts and a heterogeneous bibliography graph simultaneously. Our motivation is to maintain the topic coherency between a related work section and its target document, where both the textual and graphic contexts play a big role in characterizing the relationship among scientific publications accurately. Experimental results on a large dataset show that our approach achieves a considerable improvement over a typical seq2seq summarizer and five classical summarization baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In scientific fields, scholars need to contextualize their contribution to help readers acquire an un- derstanding of their research papers. For this pur- pose, the related work section of an article serves as a pivot to connect prior domain knowledge, in which the innovation and superiority of current work are displayed by a comparison with previ- ous studies. While citation prediction can assist in drafting a reference collection ( <ref type="bibr" target="#b31">Nallapati et al., 2008)</ref>, consuming all these papers is still a labo- rious job, where authors must read every source document carefully and locate the most relevant content cautiously.</p><p>As a solution in saving authors' efforts, auto- matic related work summarization is essentially a topic-biased multi-document problem <ref type="bibr" target="#b12">(Cong and Kan, 2010)</ref>, which relies heavily on human- engineered features to retrieve snippets from the references. Most recently, neural networks enable * Corresponding author a data-driven architecture sequence-to-sequence (seq2seq) for natural language generation <ref type="bibr" target="#b1">(Bahdanau et al., 2014</ref><ref type="bibr" target="#b2">(Bahdanau et al., , 2016</ref>, where an encoder reads a sequence of words/sentences into a context vec- tor, from which a decoder yields a sequence of specific outputs. Nonetheless, compared to sce- narios like machine translation with an end-to-end nature, aligning a related work section to its source documents is far more challenging.</p><p>To address the summarization alignment, for- mer studies try to apply an attention mechanism to measure the saliency/novelty of each candidate word/sentence ( <ref type="bibr" target="#b37">Tan et al., 2017)</ref>, with the aim of locating the most representative content to retain primary coverage. However, toward summarizing a related work section, authors should be more cre- ative when organizing text streams from the refer- ence collection, where the selected content ought to highlight the topic bias of current work, rather than retell each reference in a compressed but bal- anced fashion. This motivates us to introduce the contextual relevance and characterize the relation- ship among scientific publications accurately.</p><p>Generally speaking, for a pair of documents, a larger lexical overlap often implies a higher sim- ilarity in their research backgrounds. Yet such a hypothesis is not always true when sampling con- tent from multiple relevant topics. Take "DSSM" <ref type="bibr">1</ref> as an example, from viewpoint of the abstract sim- ilarity, those references investigating "Information Retrieval", "Latent Semantic Model" or "Click- through Data Mining" could be of more impor- tance in correlation and should be greatly sampled for the related work section. But in reality, this ar- ticle spends a bit larger chunk of texts (about 58%) to elaborate "Deep Learning" during the litera- ture review, which is quite difficult for machines to grasp the contextual relevance therein. In addi-tion, other situations like emerging new concepts also suffer from the terminology variation or para- phrasing in varying degrees.</p><p>In this study, we utilize a heterogeneous bibli- ography graph to embody the relationship within a scalable scholarly database. Over the recent past, there is a surge of interest in exploiting diverse re- lations to analyze bibliometrics, ranging from lit- erature recommendation ( <ref type="bibr" target="#b40">Yu et al., 2015</ref>) to topic evolvement ( <ref type="bibr" target="#b21">Jensen et al., 2016)</ref>. In a graphi- cal sense, interconnected papers transfer the credit among each other directly/indirectly through vari- ous patterns, such as paper citation, author collab- oration, keyword association and releasing on se- ries of venues, which constitutes the graphic con- text for outlining concerned topics. Unfortunately, a variety of edge types may pollute the information inquiry, where a slice of edges are not so important as the others on sampling content. Meanwhile, most existing solutions in mining heterogeneous graphs depend on the human supervision, e.g., hy- peredge ( <ref type="bibr" target="#b3">Bu et al., 2010</ref>) and metapath ( <ref type="bibr" target="#b36">Swami et al., 2017)</ref>. This is usually not easy to access due to the complexity of graph schemas.</p><p>Our contribution is threefold: First, we explore the edge-type usefulness distribution (EUD) on a heterogeneous bibliography graph, which en- ables the relationship discovery (between any pair of papers) for sampling the interested informa- tion. Second, we develop a novel seq2seq summa- rizer for the automatic related work summariza- tion, where a joint context-driven attention mech- anism is proposed to measure the contextual rel- evance within both textual and graphic contexts. Third, we conduct experiments on 8,080 papers with native related work sections, and experimen- tal results show that our approach outperforms a typical seq2seq summarizer and five classical summarization baselines significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>This study touches on several strands of research within automatic related work summarization and seq2seq summarizer as follows.</p><p>The idea of creating a related work section au- tomatically is pioneered by <ref type="bibr" target="#b12">Cong and Kan (2010)</ref> who design two rule-based strategies to extract sentences for general and detailed topics respec- tively. Subsequently, <ref type="bibr" target="#b19">Hu and Wan (2014)</ref> exploit probabilistic latent semantic indexing to split can- didate texts into different topic-biased parts, then</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Authors</head><p>Number of papers <ref type="bibr" target="#b12">Cong and Kan (2010)</ref> 20 <ref type="bibr" target="#b19">Hu and Wan (2014)</ref> 1,050 <ref type="bibr" target="#b39">Widyantoro and Amin (2014)</ref> 50 <ref type="bibr" target="#b6">Chen and Hai (2016)</ref> 3 <ref type="table">Table 1</ref>: Data scales of previous studies on automatic related work summarization.</p><p>apply several regression models to learn the im- portance of each sentence. Similarly, Widyan- toro and Amin (2014) transform the summariza- tion problem into classifying rhetorical categories of sentences, where each sentence is represented as a feature vector containing word frequency, sen- tence length and etc. Most recently, <ref type="bibr" target="#b6">Chen and Hai (2016)</ref> construct a graph of representative key- words, in which a minimum steiner tree is figured out to guide the summarization as finding the least number of sentences to cover the discriminated nodes. In general, compared to traditional sum- maries, the automatic related work summarization receives less concerns over the past. However, these existing solutions cannot work without man- ual intervention, which limits the application scale to an extremely small size (see <ref type="table">Table 1</ref>). The earliest seq2seq summarizer stems from <ref type="bibr" target="#b35">Rush et al. (2015)</ref> which utilizes a feed-forward network for compressing sentences, and later is expanded by <ref type="bibr" target="#b9">Chopra et al. (2016)</ref> with a recur- rent neural network (RNN). On this basis, Nalla- pati et al. (2016a,c) and  both present a set of RNN-based models to address var- ious aspects of abstractive summarization. Typ- ically, <ref type="bibr" target="#b8">Cheng and Lapata (2016)</ref> propose a gen- eral seq2seq summarizer, where an encoder learns the representation of documents while a decoder generates each word/sentence using an attention mechanism. With further research, <ref type="bibr" target="#b29">Nallapati et al. (2016b)</ref> extend the sentence compression by try- ing a hierarchical attention architecture and a lim- ited vocabulary during the decoding phase. Next, <ref type="bibr" target="#b32">Narayan et al. (2017)</ref> leverage the side information as an attention cue to locate focus regions for sum- maries. Recently, inspired by PageRank, <ref type="bibr" target="#b37">Tan et al. (2017)</ref> introduce a graph-based attention mecha- nism to tackle the saliency problem. Nonetheless, these methods all discuss the single-document sce- nario, which is far from the nature of automatic related work summarization.</p><p>In this study, derived from the general seq2seq summarizer of <ref type="bibr" target="#b8">Cheng and Lapata (2016)</ref>, we pro- pose a joint context-driven attention mechanism to measure the contextual relevance within full texts and a heterogeneous bibliography graph simulta- neously. To our best knowledge, we make the first attempt to develop a neural data-driven solution for the automatic related work summarization, and the practice of using the joint context as an atten- tion cue is also less explored to date. Besides, this study is launched on a dataset with up to 8,080 pa- pers, which is much greater than previous studies and makes our results more convincing.</p><p>Since text summarization via word-by-word generation is not mature at present <ref type="bibr" target="#b8">(Cheng and Lapata, 2016;</ref><ref type="bibr" target="#b29">Nallapati et al., 2016b;</ref><ref type="bibr" target="#b37">Tan et al., 2017)</ref>, we adopt the extractive sentential fashion for our summarizer, where a related work section is created by extracting and linking sentences from a reference collection. Meanwhile, this study fol- lows the mode of <ref type="bibr" target="#b12">Cong and Kan (2010)</ref> who as- sume that the collection is given as part of the in- put, and do not consider the citation sentences of each reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>To adapt the seq2seq paradigm, we formulate the automatic related work summarization into a se- quential text generation problem as follows.</p><p>Given an unedited paper t (target document) and its n-size reference collection R t = {r t 1:n }, we draw up a related work section for t by select- ing sentences from R t . To be specific, each refer- ence (source document) will be traversed one time sequentially, and without loss of generality, in the descending order of their significance to t. Con- sequently, all sentences to be selected are concate- nated into an m-length sequence S t = {s t 1:m } to feed the summarizer. For each candidate sentence s t j , once being visited, a label y t j ∈ {0, 1} will be determined synchronously based on whether or not this sentence should be covered into the output. Our objective is to maximize the log- likelihood probability of observed labels Y t = {y t 1:m } under R t , S t and summarizer parameters θ, as shown below. </p><formula xml:id="formula_0">max m j=1 log Pr(y t j | R t ; S t ; θ)<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Random Walk on Heterogeneous Bibliography Graph</head><p>Prior works have illustrated that one of the most promising channels for information recommen- dation is the community network ( <ref type="bibr" target="#b16">Guo and Liu, 2015)</ref>. In this study, we verify this hypothesis to- ward the content sampling of scientific summa- rization, by investigating heterogeneous relations among different kinds of objects such as papers, authors, keywords and venues. For measuring the relationship among scien- tific publications, we introduce a directed graph G = (V, E) to contain various bibliographical con- nections, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, which involves four objects and ten edge types in total. Each edge e j,i ∈ E is assigned a value π(e j,i ) z ∈ [0, 1] to indi- cate the transition probability between two nodes v j , v i ∈ V, where π(e j,i ) ∈ R returns the un- known edge-type usefulness of e j,i , and z ∈ R is a normalizing weight. For most of edge types, we model the weight as one divided by the number of outgoing links of the same kind. But regarding the "contribution" category, the weight modeling is accomplished by PageRank with <ref type="bibr">Priors (White and Smyth, 2003)</ref>. Note that different edge types usually take very uneven importance in one partic- ular task ( <ref type="bibr" target="#b40">Yu et al., 2015)</ref>, and it is quite difficult to enable the classical heterogeneous graph min- ing without expert defined paths for random walk ( <ref type="bibr" target="#b3">Bu et al., 2010;</ref><ref type="bibr" target="#b36">Swami et al., 2017)</ref>.</p><p>In this study, we propose an unsupervised ap- proach to capture the connectivity diversity, by in- troducing an optimal EUD for navigating random walkers on the heterogeneous bibliography graph. Given a target document t, the optimized useful-ness assignment can help those walkers lock a top- n recommendation ¯ R t to best match the reference collection R t , as shown in Eq. 2. On this basis, a well-performing algorithm node2vec ( <ref type="bibr" target="#b15">Grover and Leskovec, 2016</ref>) is adopted to conduct an unsuper- vised random walk to vectorize every node ∀v * ∈ V into a d-dimensional embedding ϕ(v * ) ∈ R d so that any edge ∀e * ∈ E can be calculated there- from. Specifically, we employ evolutionary algo- rithm (EA) to tune the EUD, which enjoys advan- tages over conventional gradient methods in both convergence speed and accuracy.</p><formula xml:id="formula_1">arg max t n j=1 log Pr(r t j ∈ ¯ R t | EUD) (2)</formula><p>EA Setup We use an array of real numbers x 1:10 to code an individual in the population, where x j ∈ [0, 1] denotes the usefulness of j-th edge type. Given an EUD, PageRank (Page, 1998) runs on graph to infer the relative importance of each node for each target document, and a fitness func- tion is applied to judge how well this EUD satis- fies locating the ground truth references as Eq. 3, in which if r t j belongs to ¯ R t , then α(r t j , ¯ R t ) ∈ N returns the ranking of r t j within ¯ R t , and otherwise a big penalty coefficient to prevent irrelevant ref- erences to be recommended. Like most other op- timizations, this procedure starts with a randomly generated population.</p><formula xml:id="formula_2">max 1 t n j=1 j − α(r t j , ¯ R t )<label>(3)</label></formula><p>EA Operator We choose the operator from dif- ferential evolution (Das and Suganthan, 2011) to generate offsprings for each individual. The basic idea is to utilize the difference between different individuals to disturb each trial object. First, three distinct individuals x r 1 1:10 , x r 2 1:10 , x r 3 1:10 are sampled randomly from current population to create a vari- ant x var 1:10 , as shown in Eq. 4, where f ∈ R in- dicates the scaling factor. Next, x var 1:10 is crossed with a trial object x tri 1:10 to build a hybrid one x hyb 1:10</p><p>as Eq. 5, in which c ∈ [0, 1] denotes the crossover factor and u ∈ [0, 1] represents an uniform random number. At last, the fitnesses of x tri 1:10 and x hyb 1:10 are compared, and the better one will be saved as the offspring into a new round of evolution.</p><formula xml:id="formula_3">x var j = x r 1 j + f × (x r 2 j − x r 3 j )<label>(4)</label></formula><formula xml:id="formula_4">x hyb j =    x var j , if u ≤ c x tri j , otherwise<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Neural Extractive Summarization</head><p>As <ref type="figure" target="#fig_1">Figure 2</ref> shows, we model our seq2seq summa- rizer with a hierarchical encoder and an attention- based decoder, as described below. Hierarchical Encoder Our encoder consists of two major layers, namely a convolutional neu- ral network (CNN) and a long-short-term mem- ory (LSTM)-based RNN. Specifically, the CNN deals with word-level texts to derive sentence- level meanings, which are then taken as inputs to the RNN for handling longer-range dependency within lager units like a paragraph and even a whole paper. This conforms to the nature of docu- ment that is composed from words, sentences and higher levels of abstraction ( <ref type="bibr" target="#b32">Narayan et al., 2017)</ref>. Consider a sentence of p words s t j = {w t j,1:p }, where each word w t j,i can be represented by a d- dimensional embedding φ(w t j,i ) ∈ R d . Previ- ous studies have illustrated the strength of CNN in presenting sentences, because of its capability to learn compressed expressions and address sen- tences with variable lengths <ref type="bibr" target="#b22">(Kim, 2014)</ref>. First, a convolution kernel k ∈ R d×q×d is applied to each possible window of q words to construct a list of feature maps as:</p><formula xml:id="formula_5">g t j,i = tanh k × φ(w t j,i:i+q−1 ) + b<label>(6)</label></formula><p>where b ∈ R d denotes the bias term. Next, max- over-time pooling <ref type="bibr" target="#b11">(Collobert et al., 2011</ref>) is per- formed on all generated features to obtain the sen- tence embedding as:</p><formula xml:id="formula_6">φ(s t j ) = max 1≤i≤d g t j,1:p−q+1 [i, :]<label>(7)</label></formula><p>where <ref type="bibr">[i, :]</ref> denotes the i-th row of matrix. Given a sequence of sentences S t = {s t 1:m }, we then take the RNN to yield an equal-length array of hidden states, in which LSTM has proved to al- leviate the vanishing gradient problem when train- ing long sequences <ref type="bibr" target="#b18">(Hochreiter and Schmidhuber, 1997)</ref>. Each hidden state can be viewed as a lo- cal representation with focusing on current and former sentences together, which is updated as:  sentence, and average them to capture the infor- mation inside different n-grams. As <ref type="figure" target="#fig_1">Figure 2</ref> (bot- tom) shows, the sentence s t j involves six words, and two kernels of widths two (orange) and three (green) abstract a set of five and four feature maps respectively. Meanwhile, since rhetorical struc- ture theory <ref type="bibr" target="#b26">(Mann and Thompson, 2009)</ref> points out that association must exist in any two parts of coherent texts, RNN is only applicable to manage the sentence relation within a single document, be- cause we cannot expect the dependency between two sections from different references.</p><formula xml:id="formula_7">h t j = LSTM φ(s t j ), h t j−1 ∈ R d . In</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention-based Decoder</head><p>Attention-based Decoder Our decoder labels each sentence s t j as 0/1 sequentially, according to whether it is salient or novel enough, plus if rele- vant to the target document t or not. As shown in <ref type="figure" target="#fig_1">Figure 2</ref> (top), the binary decision y t j is made by both the hidden state h t j and the context vector ¯ h t j from an attention mechanism (grey background).</p><p>In particular, this attention (red dash line) is acted as an intermediate stage to determine which sen- tences to highlight so as to provide the contextual information for current decision ( <ref type="bibr" target="#b1">Bahdanau et al., 2014)</ref>. Given H t = {h t 1:m }, this decoder returns the probability of y t j = 1 as below:</p><formula xml:id="formula_8">Pr(y t j = 1 | R t ; S t ; θ) = sigmoid δ(h t j , ¯ h t j )<label>(8)</label></formula><formula xml:id="formula_9">¯ h t j = m i=1 a j,i h t i<label>(9)</label></formula><p>where δ(h t j , ¯ h t j ) ∈ R denotes a fully connected layer with as input the concatenation of h t j and ¯ h t j , and a j,i ∈ [0, 1] is the attention weight indicating how much the supporting sentence s t i contributes to extracting the candidate one s t j . Apart from saliency and novelty two traditional attention factors <ref type="bibr" target="#b37">Tan et al., 2017)</ref>, we focus on the contextual relevance within both textual and graphic contexts to distinguish the relationship from near to far, as shown in Eq. 10 and Eq. 11. To be specific: 1) h tT j W s h t i repre- sents the saliency of s t i to s t j ; 2) −d tT j W n h t i indi- cates the novelty of s t i to the dynamic output d t j ; 3) φ(t) T W t h t i denotes the relevance of s t i to t from the textual context; 4) ϕ(t) T W g ϕ(h t i ) refers to the relevance from the graphic context. More concretely, W * ∈ R d characterizes the learnable matrix, φ(t) returns the average of hidden states from t, ϕ(t) and ϕ(h t i ) return the node embed- dings of both t and the source document that h t i belongs to respectively. Note that φ(·) and ϕ(·) represent two distinct embedding spaces, where the former reflects the lexical collocations of cor- pus, and the latter embodies the connectivity pat- terns of associated graph.</p><formula xml:id="formula_10">a j,i = h tT j W s h t i # saliency −d tT j W n h t i # novelty +φ(t) T W t h t i # relevance 1 +ϕ(t) T W g ϕ(h t i ) # relevance 2<label>(10)</label></formula><formula xml:id="formula_11">d t j = j−1 i=1 Pr(y t j = 1 | R t ; S t ; θ) × h t i<label>(11)</label></formula><p>The basic idea behind our attention mechanism is as follows: if a supporting sentence more re- sembles a candidate one, or overlaps less with the dynamic output, or is more relevant to the target document, then it can provide more contextual in- formation to facilitate current decision on being extracted or not, thereby taking a higher weight in the generated context vector. This innovative at- tention will guide our goal related work section to maximize the representativeness of selected sen- tences (saliency &amp; novelty), while minimizing the semantic distance to the target document <ref type="bibr">(relevance)</ref>. This is consistent with the way that schol- ars consume a reference collection, with the min- max objective in their minds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>This section presents the experimental setup for assessing our approach, including 1) dataset used for training and testing, 2) implementation details, 3) contrast methods and evaluation metrics. Dataset We conduct experiments on a dataset 2 created from the ACM digital library, where meta- data and full texts are derived from PDF files. To be detailed, this dataset includes 371,891 papers, <ref type="bibr">2</ref> To help readers reproduce the experiment outcome, we share part of the experiment data while the copyrighted infor- mation is removed. https://github.com/kuadmu/ 2018EMNLP 779,810 authors, 9,204 keywords and 807 venues in total. Note that we ignore the keyword with frequency below a certain threshold, and adopt greedy matching of <ref type="bibr" target="#b17">Guo et al. (2013)</ref> to generate pseudo keywords for papers lacking topic descrip- tions. For each target document, the references are traversed by the descending order of the cited number in related work section (primary) and in full paper (secondary) successively. We first ap- ply a series of pre-processings such as lowercasing and stemming to standardize candidate sentences, then remove those which are too short/long (&lt; 7 or &gt; 80 words). On this basis, a total of 8,080 papers are selected to evaluate our approach, each containing more than 15 references found in the dataset and a related work section of at least 500 words. But as for the heterogeneous bibliography graph, all source data have to be imported to en- sure the structural integrity of communities. Be- sides, this graph should be constructed year-by- year to preclude the effect of later publications on earlier ones.</p><p>Implementation We use Tensorflow for imple- mentation, where both the dimensions of embed- ding and hidden state are equally 128. For the <ref type="bibr">CNN, word2vec (Mikolov et al., 2013</ref>) is utilized to initialize the word embeddings, which can be further tuned during the training phase. Mean- while, we follow the work of <ref type="bibr" target="#b22">Kim (2014)</ref> to ap- ply a list of kernels with widths {3, 4, 5}. As for the RNN, each LSTM module is set to one single layer, and all input documents are padded to the same length, along with a mark to indicate the real number of sentences. Based on these settings, we train our summarizer using Adam with the default in <ref type="bibr" target="#b23">Kingma and Ba (2014)</ref>, and perform mini-batch cross-entropy training with a batch of one target document for 20 epochs.</p><p>To create training data for our summarizer, each reference needs to be annotated with the ground truth in advance, i.e., candidate sentences are tagged with 0/1 for indicating summary-worthy or not. Specifically, we follow a heuristic practice of <ref type="bibr" target="#b4">Cao et al. (2016)</ref> and <ref type="bibr" target="#b29">Nallapati et al. (2016b)</ref> to compute ROUGE-2 score ( <ref type="bibr" target="#b24">Lin and Hovy, 2003)</ref> for each sentence, in terms of the native related work sections (gold standards). Next, those sen- tences with high scores are chosen as the positive samples, and the rest as the negative ones, such that the total score of selected sentences is max- imized with respect to the gold standard. As for testing, we relax the number of sentences to be se- lected, and focus on the classification probability from Eq. 8. In this study, cross validation is ap- plied to split the dataset into ten parts equally at random, in which nine are used for training and the other one for testing. Evaluation We adopt the widely used toolkit ROUGE ( <ref type="bibr" target="#b24">Lin and Hovy, 2003)</ref> to evaluate the summarization performance automatically. In par- ticular, we report ROUGE-1 and ROUGE-2 (uni- gram and bigram overlapping) as a way to assess the informativeness, and ROUGE-L (the longest common subsequence) as a means to assess the fluency, in terms of fixed bytes of gold standards.</p><p>To validate the proposed attention mecha- nism, we compare our approach (denoted as P. S+N+Rteg+EUD ) against six variants, including: 1) P. void : a plain seq2seq summarizer without atten- tions; 2) P. S : use the saliency as an only atten- tion factor; 3) P. S+N : leverage both the saliency and novelty; 4) P. S+N+Rt : incorporate the relevance from the textual context; 5) P. S+N+Rtog : gain the relevance from the graphic context of a homo- geneous citation graph; 6) P. S+N+Rteg : utilize the heterogeneous bibliography graph, but with each edge type the same usefulness.</p><p>In addition, we also select six representative summarization methods as a benchmark group. The first one is the general seq2seq summarizer by <ref type="bibr" target="#b8">Cheng and Lapata (2016)</ref>, denoted as Point- erNet, which employs an attention mechanism to extract sentences directly after reading them. Fol- lowing are five classical generic solutions, includ- ing: 1) Luhn <ref type="bibr" target="#b25">(Luhn, 1958)</ref>: a heuristic summa- rization based on word frequency and distribu- tion; 2) MMR ( <ref type="bibr" target="#b5">Carbonell and Goldstein, 1998</ref>): a diversity-based re-ranking to produce summaries; 3) LexRank ( <ref type="bibr" target="#b14">Erkan et al., 2004</ref>): a graph-based summary technique inspired by PageRank and HITS; 4) SumBasic (Nenkova and <ref type="bibr" target="#b33">Vanderwende, 2005</ref>): a frequency-based summarizer with du- plication removal; 5) NltkSum ( <ref type="bibr" target="#b0">Acanfora et al., 2014</ref>): a natural language tookit (NLTK)-based implementation for summarization.</p><p>For clarity, Luhn, LexRank and SumBasic are analogous to the work of <ref type="bibr" target="#b19">Hu and Wan (2014)</ref> which extracts sentences scoring the highest in significance, and they are also contrasted in the latest studies on neural summarizers <ref type="bibr" target="#b37">Tan et al., 2017)</ref>. Meanwhile, MMR often serves as a part/post-processing of existing tech- niques to avoid the redundancy <ref type="bibr" target="#b10">(Cohan and Goharian, 2017)</ref>, and we introduce NltkSum to inves- tigate the impact of grammatical/semantic analy- sis to the automatic related work summarization. Note that former studies specially for this task re- quire extensive human involvements (see <ref type="table">Table 1</ref>), thus we cannot apply them to such a large dataset of this study. <ref type="table" target="#tab_2">Table 2</ref> reports the evaluation comparison over ROUGE metrics. From the top half, all scores ap- pear a gradual upward trend with incorporation of saliency, novelty, relevance (from both textual and graphic contexts) and EUD into consideration one after another, which demonstrates the validity of our attention mechanism for summarizing related work sections. To be specific, we further reach the following conclusions:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Discussion</head><p>1) P. void vs. P. S vs. P. S+N : Both saliency and novelty are two effective factors to locate the re- quired content for summaries, which is consistent with prior studies.</p><p>2) P. S+N vs. P. S+N+Rt : Contextual relevance does contribute to address the alignment between a re- lated work section and its source documents.</p><p>3) P. S+N+Rt vs. P. S+N+Rtog : Textual context alone cannot provide entire evidence to characterize the relationship among scientific publications exactly. 4) P. S+N+Rtog vs. P. S+N+Rteg : Heterogeneous bib- liography graph involves richer contextual infor- mation than a homogeneous citation graph. 5) P. S+N+Rteg vs. P. S+N+Rteg+EUD : EUD plays an indispensable role in organizing accurate contex- tual relevance on a heterogeneous graph. Continuing the "DSSM", <ref type="figure" target="#fig_2">Figure 3</ref> visualizes the number of extracted words on each reference  cluster 3 under different attention factors. It can be seen that only after adding the relevance es- pecially that from the graphic context into atten- tions, our summarizer can correctly sample the content from "Deep Learning" (yellow line), and eliminate that originated from "Other Sources" by a big margin (green line). As this example falls into the methodology transferring, a host of its in- volved word collocations are not idiomatic com- binations yet, such as "Deep Neural Network" co- occurs with "Clickthrough Data" that is more fre- quently related to "Latent Semantic Analysis" at that time, which results in a somewhat biased tex- tual context. By contrast, the graphic context will suffer less from this bias because it characterizes the connectivity patterns (real-time setup) instead of n-gram statistics, thus offering a more robust measure for the contextual relevance. The bottom half of <ref type="table" target="#tab_2">Table 2</ref> illustrates the superi- ority of our approach over six representative sum- marization methods. Above all, Luhn, LexRank and MMR three summarizers that simply exploit shallow text features (word frequency and asso- ciated sentence similarity) to measure either sig- nificance or redundancy fall far behind the plain variant P. void , which partly reflects the strength of seq2seq paradigm in summarizing a related work section. Second, with combination of sig- nificance and redundancy, SumBasic achieves a drastic increase on ROUGE-1 and a mild raise on <ref type="bibr">3</ref> We pack the references cited in the same subsection of the related work section as one reference cluster. ROUGE-2 respectively, but it still cannot improve ROUGE-L marginally. This is because simple text statistics cannot present deeper levels of natu- ral language understanding to catch larger-grained units of co-occurrence. Third, NltkSum benefits from a NLTK library so as to access grammati- cal/semantic supports, thereby having the best in- formativeness (ROUGE-1 and ROUGE-2) among the five generic baselines, and meanwhile a com- parable fluency (ROUGE-L) with our approach. Finally, as a deep learning solution, although PointerNet takes both hidden states and previously labeled sentences into account, at each decoding step it focuses on only current and just one pre- vious sentences, lacking a comprehensive consid- eration on saliency, novelty and more importantly the contextual relevance (&lt; P. S+N ).</p><formula xml:id="formula_12">Methods ROUGE-1 ROUGE-2 ROUGE-L P</formula><p>To better verify the summarization perfor- mance, we also conduct a human evaluation on 35 papers containing more than 30 references in the dataset. We assign a number of raters to com- pare each generated related work section against the gold standard, and judge by three independent aspects as: 1) How compliant is the related work section to the target document? 2) How intuitive is the related work section for readers to grasp the key content? 3) How useful is the related work section for researchers to prepare their final liter- ature reviews? Note that we do not allow any ties during the comparison, and each property is as- sessed with a 5-point scale of 1 (worst) to 5 (best). <ref type="table" target="#tab_3">Table 3</ref> displays how often raters rank each summarizer as the 1st, 2nd and so on, in terms of <ref type="table" target="#tab_2">Methods  1st  2nd 3rd  4th  5th  6th  7th</ref> Mean Ranking Luhn 0.04 0.07 0.09 0.13 0.17 0.23 0.29 5.26 MMR 0.05 0.07 0.11 0.16 0.19 0.22 0.20 4.82 LexRank 0.06 0.09 0.11 0.14 0.17 0.19 0.27 4.93 SumBasic 0.09 0.13 0.18 0.18 0.18 0.15 0.10 4.10 NltkSum 0.21 0.21 0.20 0.15 0.10 0.07 0.04 3.00 PointerNet 0.14 0.20 0.18 0.15 0.13 0.11 0.08 3.54 P. S+N+Rteg+EUD 0.40 0.22 0.14 0.09 0.06 0.04 0.02 2.34 best-to-worst. Specifically, our approach comes the 1st on 40% of the time, which is followed by NltkSum that is considered the best on 21% of the time (almost half of ours), and Pointer- Net with quite equal proportions on each rank- ing. Furthermore, the other four summarizers ac- count for obviously lower ratings in general. To attain the statistical significance, one-way analy- sis of variance (ANOVA) is performed on the ob- tained ratings, and the results show that our ap- proach is better than all six contrast methods sig- nificantly (p &lt; 0.01), which means that the con- clusion drawn by <ref type="table" target="#tab_2">Table 2</ref> is sustained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we highlight the contextual rele- vance for the automatic related work summariza- tion, and analyze the graphic context to charac- terize the relationship among scientific publica- tions accurately. We develop a neural data-driven summarizer by leveraging the seq2seq paradigm, where a joint context-driven attention mechanism is proposed to measure the contextual relevance within full texts and a heterogeneous bibliogra- phy graph simultaneously. Extensive experiments demonstrate the validity of the proposed attention mechanism, and the superiority of our approach over six representative summarization baselines. In future work, an appealing direction is to or- ganize the selected sentences in a logical fashion, e.g., by leveraging a topic hierarchy tree to deter- mine the arrangement of the related work section <ref type="bibr" target="#b12">(Cong and Kan, 2010)</ref>. We also would like to take the citation sentences of each reference into con- sideration, which is another concise and univer- sal data source for scientific summarization <ref type="bibr" target="#b6">(Chen and Hai, 2016;</ref><ref type="bibr" target="#b10">Cohan and Goharian, 2017)</ref>. At the end of this paper, we believe that extractive meth- ods are by no means the final solutions for litera- ture review generation due to plagiarism concerns, and we are going to put forward a fully abstractive version in further studies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Heterogeneous bibliography graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Framework of our seq2seq summarizer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Number of extracted words on each reference cluster under different attention factors.</figDesc><graphic url="image-1.png" coords="7,307.51,548.50,226.43,143.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>practice, we use multiple kernels with various widths to produce a group of embeddings for each</figDesc><table>hidden state 

( ) 

t 

s j 

¡ 

word embedding 

feature map 

sentence 
embedding 

max-over-time 
pooling 
convolution 

average 

( ) 

t 
,1 

w j 

¡ 

( ) 

t 
,6 

w j 


( ) 

t 
,2 

w j 


... 

Hierarchical Encoder h 

t 

h i 

context 
vector 
binary decision 

( ) 

t 

£ 

( ) 

t 
1 

r 

¤ 

( ) 

t 
2 

r 

¤ 

( ) 

t 

r n 

¥ 

node 
embedding 

t 
1 

r 

t 
2 

r 

t 

r n 

t 

... 

... 

... 

... 
... 

... 

... 

... 

... 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Rouge evaluation (%) on 8,080 papers from ACM digital library. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Human evaluation (proportion) on 35 papers with more than 30 references in the dataset. 

</table></figure>

			<note place="foot" n="1"> Learning deep structured semantic models for web search using clickthrough data (Huang et al., 2013)</note>

			<note place="foot">t 1 y j¢ t y j t y m t 1 y i+ t 2 y i+ t 1 y t 2 y t y i t h j t 1 h j¢ t h m t 1 h i+ t 2 h i+ t 1 h t 2</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank the anonymous reviewers for their valuable comments. This work is partially supported by the National Science Foundation of China under grant No. 71271034.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Natural language processing: generating a summary of flood disasters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Acanfora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Evangelista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Keimig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myron</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="383" to="94" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Endto-end attention-based large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st IEEE ICASSP International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the 41st IEEE ICASSP International Conference on Acoustics, Speech and Signal Processing<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4945" to="4949" />
		</imprint>
	</monogr>
	<note>Philemon Brakel, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Music recommendation by unified hypergraph:combining social media information and music content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shulong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMM International Conference on Multimedia</title>
		<meeting>the ACM SIGMM International Conference on Multimedia<address><addrLine>Amsterdam, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="391" to="400" />
		</imprint>
	</monogr>
	<note>Lijun Zhang, and Xiaofei He</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00125</idno>
		<title level="m">Attsum: Joint learning of focusing and summarization with neural attention</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The use of mmr, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 21st International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Summarization of related work through citations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuge</forename><surname>Hai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th IEEE SKG International Conference on Semantics, Knowledge and Grids</title>
		<meeting>the 12th IEEE SKG International Conference on Semantics, Knowledge and Grids<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="54" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distraction-based neural networks for modeling documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM IJCAI International Joint Conference on Artificial Intelligence</title>
		<meeting>the ACM IJCAI International Joint Conference on Artificial Intelligence<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2754" to="2760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural summarization by extracting sentences and words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th ACL Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th ACL Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the NAACL Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Scientific article summarization using citation-context and article&apos;s discourse structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06619</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="390" to="400" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards automated related work summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoang</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><forename type="middle">Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM COLING International Conference on Computational Linguistics</title>
		<meeting>the 23rd ACM COLING International Conference on Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="427" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Differential evolution: A survey of the state-of-the-art</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<editor>Swagatam Das and Ponnuthurai Nagaratnam Suganthan</editor>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="31" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lexrank: graphbased lexical centrality as salience in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radev</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dragomir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Qiqihar Junior Teachers College</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Francisco, Usa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic feature generation on heterogeneous graph for music recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="807" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Scientific metadata quality enhancement for scholarly publications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhong</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Ischools</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic generation of related work sections in scientific papers: an optimization approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL EMNLP Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the ACL EMNLP Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1624" to="1633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM CIKM international Conference on Information &amp; Knowledge Management</title>
		<meeting>the 22nd ACM CIKM international Conference on Information &amp; Knowledge Management<address><addrLine>San Francisco, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generation of topic evolution trees from heterogeneous bibliographic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stasa</forename><surname>Milojevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Informetrics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="606" to="621" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Eprint Arxiv</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic evaluation of summaries using n-gram cooccurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Chin Yew Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL The Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the NAACL The Annual Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>Stroudsburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The automatic creation of literature abstracts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Luhn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1958" />
			<publisher>IBM Corp</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rhetorical structure theory: Toward a functional theory of text organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text &amp; Talk</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="281" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence rnns for text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations, Workshop track</title>
		<meeting>the International Conference on Learning Representations, Workshop track<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Summarunner: A recurrent neural network based sequence model for extractive summarization of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04230v1</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Abstractive text summarization using sequenceto-sequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06023v5</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Cicero Nogueira Dos Santos, Caglar Gulcehre, and Bing Xiang</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint latent topic models for text and citations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><forename type="middle">M</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Las Vegas, Usa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="542" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Papasarantopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04530</idno>
		<title level="m">Neural extractive summarization with side information</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The impact of frequency on summarization. Microsoft Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking : Bringing order to the web, online manuscript</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
<note type="report_type">Stanford Digital Libraries Working Paper</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL EMNLP Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the ACL EMNLP Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Halifax, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Abstractive document summarization with a graph-based attentional neural model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th ACL Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th ACL Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1171" to="1181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Algorithms for estimating relative importance in networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 9th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="266" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Citation sentence identification and classification for related work summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dwi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imaduddin</forename><surname>Widyantoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICACSIS International Conference on Advanced Computer Science and Information Systems</title>
		<meeting>the ICACSIS International Conference on Advanced Computer Science and Information Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="291" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Random walk and feedback on scholarly network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoren</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM GSB@SIGIR International Workshop on Graph Search and Beyond</title>
		<meeting>the 1st ACM GSB@SIGIR International Workshop on Graph Search and Beyond<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="33" to="37" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
