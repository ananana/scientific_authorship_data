<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:13+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Juman++: A Morphological Analysis Toolkit for Scriptio Continua</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arseny</forename><surname>Tolmachev</surname></persName>
							<email>arseny@kotonoha.ws</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Graduate School of Informatics</orgName>
								<orgName type="department" key="dep2">Graduate School of Informatics</orgName>
								<orgName type="institution">Kyoto University Yoshida-honmachi</orgName>
								<address>
									<addrLine>Sakyo-ku Kyoto</addrLine>
									<postCode>606-8501</postCode>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Kyoto University</orgName>
								<address>
									<addrLine>Yoshida-honmachi, Sakyo-ku Kyoto</addrLine>
									<postCode>606-8501</postCode>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Kyoto University</orgName>
								<address>
									<addrLine>Yoshida-honmachi, Sakyo-ku Kyoto</addrLine>
									<postCode>606-8501</postCode>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Juman++: A Morphological Analysis Toolkit for Scriptio Continua</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations)</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations) <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="54" to="59"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>54</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a three-part toolkit for developing morphological analyzers for languages without natural word boundaries. The first part is a lattice-based morphological analysis library that uses a combination of linear and recurrent neural net language models for analysis. The other parts are a tool for exposing problems in the trained model and a partial annotation tool. Our morphological analyzer for Japanese achieves new SOTA on Jumandic-based corpora while being 250 times faster than the previous one. We also perform a small experiment and quantitive analysis of using our toolkit.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Processing scriptio continua natural languages, or languages without natural word boundaries, like space in English, frequently requires performing tokenization into morphemes in the natural lan- guage processing pipeline. Usually, tokenization is done together with part of speech (POS) tag- ging, pronunciation estimation or other subtasks. This process is usually referred to as morphologi- cal analysis.</p><p>A morphological analyzer is useful from a prac- tical point of view only if it is fast as well as highly accurate in its analysis. Because morphological analysis is very important to languages without word boundaries (like Japanese), there already ex- ist many approaches and tools for performing it. Morphological analyzers usually use two kinds of resources: a dictionary which defines possible morphemes and an annotated corpus which is used to train an analysis model for connecting mor- phemes together.</p><p>Modern morphological analyzers achieve high accuracy (a segmentation F1 score of &gt; .99 for Japanese) on established domains like newspaper. However, when using them on out-of-domain or open domain data (like web texts) the accuracy de- creases, and it is difficult to improve that accuracy without creating costly annotations by trained ex- perts.</p><p>The Juman++ Japanese morphological analyzer ( <ref type="bibr" target="#b1">Morita et al. 2015</ref>, referred to also as V1), which uses a combination of a linear model and a neu- ral network-based language model (RNNLM) to compute a semantic plausibility of a segmentation. Juman++ has achieved state-of-the-art analysis ac- curacy on Jumandic (the JUMAN dictionary and segmentation standard <ref type="bibr" target="#b0">(Kurohashi and Kawahara, 2012)</ref>) based corpora, and drastically reduced the number of intolerable analysis errors. Unfortu- nately, its execution speed was extremely slow and this has limited the practical usage of Juman++.</p><p>We have developed a morphological analysis toolkit consisting of three components: a morpho- logical analyzer and two support tools which help with the development of analysis models. The an- alyzer is a complete rewrite of core ideas of Ju- man++, released as Juman++ V2 1 <ref type="bibr" target="#b6">(Tolmachev and Kurohashi, 2018)</ref>. Our reimplementation is more than 250 times faster than V1, reaching the speed of traditional analyzers, at the same time achieving better accuracy than V1.</p><p>We have also developed a tool which uses raw, unannotated data and gives an insight into prob- lematic dictionary and grammar points, together with finding sentences which contain situations not present in training data by exploiting differ- ences in the analysis that arise from using different beam search configurations. It is bundled with V2. Also, we have developed a partial annotation tool which makes it easy to review problematic sen- tences and create partial annotation data for im- proving the analysis 2 . To the best of our knowl-edge, a similar set of tools has never been devel- oped before. Juman++ V2 and the development tools are language and segmentation standard in- dependent and are released under the permissive Apache 2 open source license.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>KyTea ( <ref type="bibr" target="#b2">Neubig et al., 2011</ref>) is a similar tool that can perform morphological analysis for languages with the continuous script. It can also be trained using partial annotation data and output point-wise confidence scores for the analysis result which were used for creating partially annotated data in an active learning scenario. Still, by using a point- wise approach and estimating auxiliary tags (like POS) after computing segmentation, KyTea trades off accuracy for simplicity. Juman++ is faster, has better accuracy, does tag estimation jointly with segmentation, uses an online learning approach and can use longer contexts in forms of RNNLM and trigram features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Juman++ Morphological Analyzer V2</head><p>Juman++ V2 is implemented with modern C++, with the intention to be used not only as a pro- gram, but also as an embeddable library, usable in a multi-threaded environment. Additionally, V2 is not hardwired to a particular dictionary and can use partially annotated data for training.</p><p>Juman++ is a lattice-based analyzer. For an in- put sentence, it looks up all possible morphemes from a dictionary, makes a lattice from them, and assigns a score s to each path going through the lattice. The path with the highest score is consid- ered to be the analysis result. The score s consists of two components: a feature-based linear model score s l and an RNNLM score s RNN , which are combined as</p><formula xml:id="formula_0">s = s l + α(s RNN + β),</formula><p>where α and β are scale and bias hyperparam- eters respectively. The RNNLM score is a log- probability score from the language model.</p><p>The linear score is defined as</p><formula xml:id="formula_1">s l = t∈p f (t, p)w,</formula><p>where f (t, p) is an indicator vector which contains features, extracted for a node t in a path p, and w is a model weight vector. Features use dictionary information of the node t and at most two previous nodes and their surface surroundings. Weights are learned using the Soft Confidence Weighted algo- rithm ( <ref type="bibr" target="#b7">Wang et al., 2016)</ref>. In practice, it is intractable to compute scores for all paths through the lattice. Instead, we use beam search. Additionally, because the RNNLM is computationally-heavy, we compute the RNN score only for the paths which remain in the beam of the special end-of-string token. The overall de- sign is to cut off improbable analysis results with a simple model and then re-rank by RNNLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Search Space Trimming</head><p>One of important optimizations of V2 is the re- duction of search space by changing the beam search operation mode. Because the Juman++ lin- ear model uses up to 3-gram features, the beam searching procedure has to deal with combinato- rial explosion caused by higher order n-gram fea- tures. V1 uses local beams of width j, meaning that each lattice node keeps j incoming paths with top scores. The rest of the paths are discarded. The problem is that paths are discarded after evaluating their scores and the number of evaluations is still large. Most of the sentences have several bound- aries which have 20-30 both left and right nodes. Because almost all these paths are useless, we do not want to consider them in the analysis at all.</p><p>The first improvement is to use only paths end- ing on left nodes with top k scores instead of using all left paths. Connections for the remaining paths are not considered. We refer to this process as left global beam. The application of the left global beam is shown in <ref type="figure" target="#fig_2">Figure 1a</ref>.</p><p>The second improvement is the right global beam, displayed in <ref type="figure" target="#fig_2">Figure 1b</ref>. As the first sub- step, we use top l(≤ k) paths ending at left nodes to compute scores of right nodes. After that, we evaluate the connections from the remaining k − l left paths to the top-scored m right nodes. The rest of connections are not considered.</p><p>We call search using local beams as full beam. In comparison to that, a combination of left and right global beams is referred to as trimmed beam because it uses significantly smaller search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Partially Annotated Data</head><p>Juman++ V2 supports both training with partially annotated data and soft-constrainted analysis us- ing partially annotated data as constraints.   There exist three types of annotations: bound- ary (break here), non-boundary (no break here) and word. A word annotation means that a se- quence of characters must not contain boundaries inside it; additionally, the word can have tags at- tached to it, meaning that the analysis result also must have these tags.</p><p>For training, we need to provide sets of correct and incorrect features for the training algorithm. A feature computed from the correct path would be correct, and a feature containing any lattice node that is not contained in the correct path would be incorrect.</p><p>When using the fully annotated data, there ex- ist only a single correct path. However, for partial annotated data, there could exist multiple allowed paths through the lattice. Because of this, when training using the partially annotated data, we use several highly-scored candidates which do not vi- olate annotation requirements instead of the only correct path as in the case of fully annotated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Performance Comparison</head><p>We compare both speed and accuracy of five different Japanese morphological analyzers: JU- MAN 3 , MeCab 4 , KyTea, Juman++ (V1) and Ju- man++ (V2). For both versions of Juman++ we report results both using and not using RNNLM (noRNN). ing the same Juman++ dictionary, the Kyoto Uni- versity 5 (KU) and KWDLC 6 corpora for all mor- phological analyzers except JUMAN, which is not trainable. For KyTea we also report the throughput using the Unidic-based models, which are avail- able for download from the KyTea website. A Jumandic-based model for KyTea was learned us- ing default parameters. V1 uses the full beam of width j = 5. V2 uses trimmed beam with param- eters j = 5, k = 6, l = 1, m = 5. All analyzers were using only a single thread. <ref type="table">Table 1</ref> shows the analysis speed of the consid- ered morphological analyzers and speed ratio as compared to JUMAN. The speed was measured by analyzing 50k sentences from a web corpus. Each analyzer was launched five times and the median time was used for computing the analysis speed. V2 noRNN is only 20% slower than JUMAN while having a considerably complex model. V2  RNN has 1.8 times the execution speed of JU- MAN and is more than 250 times faster than V1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis speed</head><p>Accuracy <ref type="table" target="#tab_2">Table 2</ref> shows F1 scores for both the KU and KWDLC corpora. A concatenation of training sections of both corpora was used to train a combined model; the reported scores are for the test sections. MeCab and V2 have hyperpa- rameters optimized using Spearmint ( <ref type="bibr" target="#b5">Snoek et al., 2012</ref>). V2 RNN achieves a higher F1 score than the previous SOTA of V1 RNN. Even the scores of V2 noRNN are higher in some cases than those of V1 RNN. Note that the scores of V1 noRNN are one of the lowest, and thus we hypothesize that the number of training iterations of the V1 linear model was not sufficient. However, it was difficult to increase it because of very slow analysis speed.</p><p>With V2, we could find an optimal number of it- erations for learning the linear model with the best accuracy. The other reason for the improved ac- curacy for V2 is that it uses surface character and character type features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Beam Search Diffs as Active Learning</head><p>We compared the accuracy of the trimmed beam search to the full configuration in the in-domain setting. For this experiment, we trained the model using 1 pass of full beam search followed by 4 passes of trimmed beam search for the optimal number of iterations for different trimmed beam parameters. <ref type="figure" target="#fig_3">Figure 2</ref> shows the F1 score average on 10-fold cross-validation over the KU corpus. It can be seen that the accuracy of the models does not fall even if using very small global beam sizes. Nevertheless, we noticed that there exist sentences  when the full and trimmed search configurations do not agree on the top-scoring path when analyz- ing random web sentences and produce diffs.</p><p>To get a better picture, we analyzed a large number of sentences from a raw Japanese cor- pus, crawled from the web.</p><p>On average, 0.38% (1969/510595) of sentences were diffs, a relatively small number. In contrary to our expec- tations, the full beam analysis was correct only in around 50% of the cases. The rest of sentences had both of analysis variants incorrect either be- cause the dictionary did not support the language phenomena (20%) or lack of coverage by the train- ing data (10%); the trimmed beam analysis was the correct one (10%); and other situations that were difficult to decide or impossible to analyze correctly like typos.</p><p>Based on these insights, we believe that the diffs can be treated as a result of selection by the Query- by-Committee active learning algorithm with two committee members <ref type="bibr" target="#b3">(Settles and Craven, 2008;</ref><ref type="bibr" target="#b4">Seung et al., 1992</ref>). We think that diffs actually reveal problems not only with a training corpus (the goal of active learning) but also with an anal- ysis dictionary as well. Note that we are using exactly the same model in both beam modes and the only difference is beam configuration. So, a diff can form only if trimmed beam ranking was not learned correctly either because the model ca- pacity was not enough, features were not strong enough, or the situation was not present in the training data.</p><p>When the beam size at training is very small, the model does not learn to rank for the trimmed case well enough, lowering the accuracy on larger beam sizes. Increasing the beam size above three makes the trimmed beam score indistinguishable with the full beam. On the other hand, the model loses accuracy on smaller beam sizes at test time, because the trimmed ranking fails in those situ- ations. Thus we believe that the model capacity and the feature set should be enough to capture the ranking and the diffs are caused mostly by the lack of training data. The fact that the diffs in- clude situations when it is impossible to produce the correct analysis at all, namely lack of dictio- nary words and typos, confirms our belief.</p><p>On the other hand, we also believe that the cor- rections of places pointed by diffs are not going to significantly improve benchmark scores exactly because of the same reason. The benchmark cor- pus will usually be relatively in-domain and not contain dictionary or grammar problems, because they would be fixed when creating the corpus. So we hope that this method would be useful to im- prove real world morphological analysis accuracy and for domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Partial Annotation Tool</head><p>Because the diffs must be reviewed by human an- notators to be useful as training data, we have de- veloped a partial annotation tool based on a simple idea: to allow annotators to select a correct anal- ysis from two candidates. The output of the tool can be used as partially annotated training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Tool Description</head><p>The tool is a web application, implemented in Scala. As an input, it uses sentences with some parts being diffs, which are produced by a tool bundled with the Juman++ V2 distribution. For each sentence, annotators are asked to select a cor- rect variant, correct an analysis if both are not cor- rect, or report if it is impossible to select a correct analysis or there is a problem with the sentence itself.</p><p>A sentence diff view is shown in <ref type="figure" target="#fig_4">Figure 3</ref>. The annotation targets are diffs: we want annotators to choose a correct analysis from the possible vari- ants, which are displayed side-by-side. A sen- tence can contain more than one annotation target in general and each variant can contain more than one morpheme. Non-diff parts form contexts and are displayed in grey. The tool shows diffs in an unspecified order. In the shown example, the left variant is selected as the correct one. The area in the middle of the target section contains a button which activates an interactive analysis mode and buttons for assigning special tags in the case when the mistake in the target part is due to a typo or a phenomenon that we do not want to support in the automatic analysis, such as netspeak.</p><p>In the case when both analyses are incorrect, an annotator can use the interactive analysis mode, shown in <ref type="figure" target="#fig_5">Figure 4</ref>, which performs constrained morphological analysis. Constraint nodes are shown in blue. A full beam analysis becomes ini- tial constraints.</p><p>The interactive analysis mode uses lattice infor- mation from the Juman++ to provide morpheme candidates for constraints. It is possible to show either all morphemes containing a focused charac- ter or morphemes spanning exactly selected char- acters. The corrected analysis replaces the closest diff variant which was not selected by any annota- tor yet, or creates a new variant if replacing is not possible. Finally, if there are no variants, annota- tors can select a character span and report that it is impossible to choose a correct morpheme in this sentence for that span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Annotation Experiment</head><p>We performed a small scale annotation experiment using the developed annotation tool. For the ex- periment, we trained a model on the concatenation of the training and test data from both benchmark corpora and the copy of data augmented by re- moving "ga", "ha" and "wo" case markers, which are often omitted in the spoken language so that the model would be more robust to case marker omission. Using this model we have collected sen- tences which contained diffs, as described in Sec- tion 4. We asked two annotators to work for three hours each.</p><p>During the allocated time, the two annotators could review 111 and 112 sentences each. Both the annotators started annotating rather slowly, while gradually increasing their annotation speed. An inter-annotator agreement was 0.819. The an- notators have selected at least one of analysis can- didates in most (82%) cases; the rest were special tags.</p><p>We also checked sentences where the annotators did not agree on the correct analysis, but both the annotations were analysis results (9 in total). Each of them was difficult to decide even for us. We be- lieve that the relatively large number of sentences which caused annotators to spend a long time on annotation is caused by the fact that the diff ex- tractor selects difficult cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>BOS (a) Left global beam. k = 2. Top left paths are displayed in orange. Remaining left paths are displayed in solid gray. Non-considered paths are displayed as dashed blue arrows. Considered paths are solid blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>BOS</head><label></label><figDesc>(b) Right global beam. l = m = 1. A top left path and a top right node are displayed in green. Orange paths via the boundary were used for scoring right nodes. Solid blue path via the boundary connects the remaining k − l top left paths the top right nodes. Dashed blue paths are not considered.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Trimming the search space using the global beams</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Cross-validation test F1 score average on the KU corpus for POS tags when using different global beam parameters k = m = ρ, l = 1. Red dotted line at the top is F1 score of a model without global beams.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Annotation tool: diff view. UI explanations are in blue. Variant 1 is selected.</figDesc><graphic url="image-1.png" coords="5,310.12,62.81,218.70,186.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Annotation tool: interactive analysis. Constraint nodes background is blue. Other analysis variants for the last constraint node are displayed after the gray indent. The "CL" button removes a constraint.</figDesc><graphic url="image-2.png" coords="6,77.46,62.81,207.35,250.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>F1 scores of morphological analyzers on 
Jumandic-based corpora. Seg is segmentation; 
+Pos is correctly guessing the POS-tags after seg-
mentation. 

</table></figure>

			<note place="foot" n="1"> https://github.com/ku-nlp/jumanpp 2 https://github.com/eiennohito/ nlp-tools-demo</note>

			<note place="foot" n="5"> http://nlp.ist.i.kyoto-u.ac.jp/EN/ index.php?Kyoto University Text Corpus 6 http://nlp.ist.i.kyoto-u.ac.jp/EN/ index.php?KWDLC</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We present Juman++ V2: a morphological anal-ysis and tokenization toolkit for languages with the continuous script. Our current implementa-tion focuses on Japanese and the Jumandic-based segmentation standard, but the core library is lan-guage independent. Juman++ V2 achieves a new state-of-the-art accuracy for both the Kyoto Uni-versity and KWDLC corpora while drastically re-ducing the analysis time compared to Juman++ V1. Juman++ V2 can be used as a library and can use both fully and partially annotated data for training.</p><p>We also release a morphological analysis devel-oper tool which reveals problematic places of a dictionary and segmentation standard using only unannotated data by comparing the analysis re-sults in the two different beam search configura-tions. Also, we release the partial annotation tool for easily viewing and fixing these problems.</p><p>We plan to create a Unidic version of Ju-man++ V2 and use it to annotate readings of the Jumandic-based corpora, which are currently rather arbitrary, enabling the Jumandic-based models to estimate correct readings as well. We also plan to continue collecting partially annotated data and release a partially annotated web corpus.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Japanese morphological analysis system juman 7.0 users manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Kyoto University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Morphological analysis for unsegmented languages using recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Morita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2292" to="2297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pointwise prediction for robust, adaptable japanese morphological analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosuke</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An analysis of active learning strategies for sequence labeling tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1070" to="1079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Query by committee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sompolinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Annual Workshop on Computational Learning Theory, COLT &apos;92</title>
		<meeting>the Fifth Annual Workshop on Computational Learning Theory, COLT &apos;92<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="287" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2951" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Juman++ v2: A practical and modern morphological analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arseny</forename><surname>Tolmachev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ANLP</title>
		<meeting><address><addrLine>Okayama, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="917" to="920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Soft confidence-weighted learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACM TIST</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
