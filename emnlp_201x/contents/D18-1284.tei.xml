<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Personas from Dialogue with Attentive Memory Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 2638</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chu</surname></persName>
							<email>echu@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">MIT Media Lab</orgName>
								<orgName type="department" key="dep2">MIT Media Lab</orgName>
								<orgName type="department" key="dep3">MIT Media Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashanth</forename><surname>Vijayaraghavan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">MIT Media Lab</orgName>
								<orgName type="department" key="dep2">MIT Media Lab</orgName>
								<orgName type="department" key="dep3">MIT Media Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deb</forename><surname>Roy</surname></persName>
							<email>dkroy@media.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">MIT Media Lab</orgName>
								<orgName type="department" key="dep2">MIT Media Lab</orgName>
								<orgName type="department" key="dep3">MIT Media Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Personas from Dialogue with Attentive Memory Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2638" to="2646"/>
							<date type="published">October 31-November 4, 2018. 2018. 2638</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The ability to infer persona from dialogue can have applications in areas ranging from computational narrative analysis to personal-ized dialogue generation. We introduce neu-ral models to learn persona embeddings in a supervised character trope classification task. The models encode dialogue snippets from IMDB into representations that can capture the various categories of film characters. The best-performing models use a multi-level attention mechanism over a set of utterances. We also utilize prior knowledge in the form of tex-tual descriptions of the different tropes. We apply the learned embeddings to find similar characters across different movies, and cluster movies according to the distribution of the embeddings. The use of short conversational text as input, and the ability to learn from prior knowledge using memory, suggests these methods could be applied to other domains.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Individual personality plays a deep and pervasive role in shaping social life. Research indicates that it can relate to the professional and personal rela- tionships we develop ( <ref type="bibr" target="#b2">Barrick and Mount, 1993)</ref>, <ref type="bibr" target="#b19">(Shaver and Brennan, 1992)</ref>, the technological in- terfaces we prefer <ref type="bibr" target="#b16">(Nass and Lee, 2000</ref>), the be- havior we exhibit on social media networks <ref type="bibr" target="#b17">(Selfhout et al., 2010)</ref>, and the political stances we take <ref type="bibr" target="#b9">(Jost et al., 2009)</ref>.</p><p>With increasing advances in human-machine di- alogue systems, and widespread use of social me- dia in which people express themselves via short text messages, there is growing interest in systems that have an ability to understand different person- ality types. Automated personality analysis based on short text analysis could open up a range of po- tential applications, such as dialogue agents that * The first two authors contributed equally to this <ref type="bibr">work.</ref> sense personality in order to generate more inter- esting and varied conversations.</p><p>We define persona as a person's social role, which can be categorized according to their con- versations, beliefs, and actions. To learn personas, we start with the character tropes data provided in the CMU Movie Summary Corpus by <ref type="bibr" target="#b1">(Bamman et al., 2014)</ref>. It consists of 72 manually identified commonly occurring character archetypes and ex- amples of each. In the character trope classifica- tion task, we predict the character trope based on a batch of dialogue snippets.</p><p>In their original work, the authors use Wikipedia plot summaries to learn latent variable models that provide a clustering from words to topics and topics to personas -their persona clus- terings were then evaluated by measuring similar- ity to the ground-truth character trope clusters. We asked the question -could personas also be in- ferred through dialogue? Because we use quotes as a primary input and not plot summaries, we be- lieve our model is extensible to areas such as dia- logue generation and conversational analysis.</p><p>Our contributions are:</p><p>1. Data collection of IMDB quotes and charac- ter trope descriptions for characters from the CMU Movie Summary Corpus. 2. Models that greatly outperform the baseline model in the character trope classification task. Our experiments show the importance of multi-level attention over words in dia- logue, and over a set of dialogue snippets. 3. We also examine how prior knowledge in the form of textual descriptions of the per- sona categories may be used. We find that a 'Knowledge-Store' memory initialized with descriptions of the tropes is particularly use- ful. This ability may allow these models to be used more flexibly in new domains and with</p><p>Character Trope Character <ref type="table">Movie  Corrupt corporate executive Les Grossman  Tropic Thunder  Retired outlaw  Butch Cassidy Butch Cassidy and the Sundance Kid  Lovable rogue  Wolverine  X-Men   Table 1: Example tropes and characters</ref> different persona categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Prior to data-driven approaches, personalities were largely measured by asking people questions and assigning traits according to some fixed set of di- mensions, such as the Big Five traits of openness, conscientiousness, extraversion, agreeability, and neuroticism <ref type="bibr" target="#b22">(Tupes and Christal, 1992)</ref>. Compu- tational approaches have since advanced to infer these personalities based on observable behaviors such as the actions people take and the language they use <ref type="bibr" target="#b5">(Golbeck et al., 2011</ref>). Our work builds on recent advances in neural networks that have been used for natural language processing tasks such as reading comprehension <ref type="bibr" target="#b21">(Sukhbaatar et al., 2015)</ref> and dialogue modeling and generation <ref type="bibr" target="#b24">(Vinyals and Le, 2015;</ref><ref type="bibr" target="#b13">Li et al., 2016;</ref><ref type="bibr" target="#b18">Shang et al., 2015)</ref>. This includes the grow- ing literature in attention mechanisms and mem- ory networks ( <ref type="bibr" target="#b0">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b21">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b12">Kumar et al., 2016)</ref>.</p><p>The ability to infer and model personality has applications in storytelling agents, dialogue sys- tems, and psychometric analysis. In particular, personality-infused agents can help "chit-chat" bots avoid repetitive and uninteresting utterances ( <ref type="bibr" target="#b25">Walker et al., 1997;</ref><ref type="bibr" target="#b14">Mairesse and Walker, 2007;</ref><ref type="bibr" target="#b13">Li et al., 2016;</ref><ref type="bibr" target="#b26">Zhang et al., 2018)</ref>. The more recent neural models do so by conditioning on a 'persona' embedding -our model could help pro- duce those embeddings.</p><p>Finally, in the field of literary analysis, graphi- cal models have been proposed for learning char- acter personas in novels ( <ref type="bibr" target="#b4">Flekova and Gurevych, 2015;</ref><ref type="bibr" target="#b20">Srivastava et al., 2016)</ref>, folktales (Valls- <ref type="bibr" target="#b23">Vargas et al., 2014</ref>), and movies ( <ref type="bibr" target="#b1">Bamman et al., 2014</ref>). However, these models often use more structured inputs than dialogue to learn personas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets</head><p>Characters in movies can often be categorized into archetypal roles and personalities. To understand the relationship between dialogue and personas, we utilized three different datasets for our mod- els: (a) the Movie Character Trope dataset, (b) the IMDB Dialogue Dataset, and (c) the Charac- ter Trope Description Dataset. We collected the IMDB Dialogue and Trope Description datasets, and these datasets are made publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Character Tropes Dataset</head><p>The CMU Movie Summary dataset provides tropes commonly occurring in stories and media <ref type="bibr" target="#b1">(Bamman et al., 2014</ref>). There are a total of 72 tropes, which span 433 characters and 384 movies. Each trope contains between 1 and 25 characters, with a median of 6 characters per trope. Tropes and canonical examples are shown in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">IMDB Dialogue Snippet Dataset</head><p>To obtain the utterances spoken by the charac- ters, we crawled the IMDB Quotes page for each movie. Though not every single utterance spoken by the character may be available, as the quotes are submitted by IMDB users, many quotes from most of the characters are typically found, espe- cially for the famous characters found in the Char- acter Tropes dataset. The distribution of quotes per trope is displayed in <ref type="figure" target="#fig_0">Figure 1</ref>. Our models were trained on 13,874 quotes and validated and tested on a set of 1,734 quotes each.</p><p>We refer to each IMDB quote as a (contextu- alized) dialogue snippet, as each quote can con- tain several lines between multiple characters, as well as italicized text giving context to what might be happening when the quote took place. <ref type="figure">Fig- ure 2</ref> show a typical dialogue snippet. 70.3% of the quotes are multi-turn exchanges, with a mean of 3.34 turns per multi-turn exchange. While the character's own lines alone can be highly indica- tive of the trope, our models show that account- ing for context and the other characters' lines and context improves performance. The context, for instance, can give clues to typical scenes and ac- tions that are associated with certain tropes, while the other characters' lines give further detail into  the relationship between the character and his or her environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Character Trope Description Dataset</head><p>We also incorporate descriptions of each of the character tropes by using the corresponding de- scriptions scraped from TVTropes 2 . Each descrip- tion contains several paragraphs describing typical characteristics, actions, personalities, etc. about the trope. As we demonstrate in our experiments, the use of these descriptions improves classifica- tion performance. This could allow our model to be applied more flexibly beyond the movie char- acter tropes -as one example, we could store de- scriptions of personalities based on the Big Five traits in our Knowledge-Store memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Problem Formulation</head><p>Our goal is to train a model that can take a batch of dialogue snippets from the IMDB dataset and predict the character trope. Formally, let N P be the total number of char- acter tropes in the character tropes dataset. Each character C is associated with a correspond- ing ground-truth trope category P . Let S = (D, E, O) be a dialog snippet associated with a character C, where</p><formula xml:id="formula_0">D = [w D 1 , w D 2 ..., w D T ] refers to the character's own lines, E = 2 http://tvtropes.org [w E 1 , w E 2 ..., w E T ] is the contextual information and O = [w O 1 , w O 2 ..., w O T ]</formula><p>denotes the other characters' lines. We define all three components of S to have fixed sequence length T and pad when necessary. Let N S be the total number of dialogue snippets for a trope. We sample a set of N diag (where N diag N S ) snippets from N S snippets related to the trope as inputs to our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Attentive Memory Network</head><p>The Attentive Memory Network consists of two major components: (a) Attentive Encoders, and (b) a Knowledge-Store Memory Module. <ref type="figure" target="#fig_1">Figure 3</ref> outlines the overall model. We describe the com- ponents in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Attentive Encoders</head><p>Not every piece of dialogue may be reflective of a latent persona. In order to learn to ignore words and dialogue snippets that are not informa- tive about the trope we use a multi-level attentive encoder that operates at (a) the individual snippet level, and (b) across multiple snippets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attentive Snippet Encoder</head><p>The snippet encoder extracts features from a sin- gle dialogue snippet S, with attention over the words in the snippet. A snippet S = (D, E, O) is fed to the encoder to extract features from each of these textual inputs and encode them into an embedding space. We use a recurrent neural net- work as our encoder, explained in detail in Sec- tion 5.1.1. In order to capture the trope-reflective words from the input text, we augment our model with a self-attention layer which scores each word in the given text for its relevance. Section 5.1.2 explains how the attention weights are computed. The output of this encoder is an encoded snippet embedding</p><formula xml:id="formula_1">S e = (D e , E e , O e ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attentive Inter-Snippet Encoder</head><p>As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, the N diag snippet em- beddings S e from the snippet encoder are fed to our inter-snippet encoder. This encoder captures inter-snippet relationship using recurrence over the N diag snippet embeddings for a given trope and determines their importance. Some of the di- alogue snippets may not be informative about the trope, and the model learns to assign low attention scores to such snippets. The resulting attended <ref type="figure" target="#fig_1">Figure 3</ref>: Illustration of the Attentive Memory Network. The network takes dialogue snippets as input and predicts its asso- ciated character trope. In this example, dialogue snippets associated with the character trope "Bruiser with a Soft Corner" is given as input to the model. summary vector from this phase is the persona rep- resentation z, defined as:</p><formula xml:id="formula_2">z = γ s D D s + γ s E E s + γ s O O s γ s D + γ s E + γ s O = 1<label>(1)</label></formula><p>where γ s D , γ s E , γ s O are learnable weight parame- ters. D s , E s , O s refers to summary vectors of the N diag character's lines, contextual informa- tion, and other characters' lines, respectively. In Section 7, we experiment with models that have γ s E and γ s O set to 0 to understand how the contex- tual information and other characters' lines con- tribute to the overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Encoder</head><p>Given an input sequence (x 1 , x 2 , ..., x T ), we use a recurrent neural network to encode the sequence into hidden states (h 1 , h 2 , ..., h T ). In our exper- iments, we use a gated recurrent network (GRU) ( <ref type="bibr" target="#b3">Chung et al., 2014</ref>) over LSTMs <ref type="bibr" target="#b7">(Hochreiter and Schmidhuber, 1997</ref>) because the latter is more computationally expensive. We use bidirectional GRUs and concatenate our forward and backwards hidden states to get ← → h t for t = 1, ..., T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Attention</head><p>We define an attention mechanism Attn that com- putes s from the resultant hidden states ← → h t of a GRU by learning to generate weights α t . This can be interpreted as the relative importance given to a hidden state h t to form an overall summary vector for the sequence. Formally, we define it as:</p><formula xml:id="formula_3">at = fattn(ht) (2) αt = sof tmax(at)<label>(3)</label></formula><formula xml:id="formula_4">s = T t=1 αtht (4)</formula><p>where f attn is a two layer fully connected network in which the first layer projects h t ∈ IR d h to an attention hidden space g t ∈ IR da , and the second layer produces a relevance score for every hidden state at timestep t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Memory Modules</head><p>Our model consists of a read-only 'Knowledge- Store' memory, and we also test a recent read- write memory. External memories have been shown to help on natural language processing tasks ( <ref type="bibr" target="#b21">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b12">Kumar et al., 2016;</ref><ref type="bibr" target="#b10">Kaiser and Nachum, 2017)</ref>, and we find similar improvements in learning capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Knowledge-Store Memory</head><p>The main motivation behind the Knowledge-Store memory module is to incorporate prior domain knowledge. In our work, this knowledge refers to the trope descriptions described in Section 3.3.</p><p>Related works have initialized their memory networks with positional encoding using word em- beddings ( <ref type="bibr" target="#b21">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b12">Kumar et al., 2016;</ref><ref type="bibr">Miller et al., 2016)</ref>. To incorporate the de- scriptions, we represent them with skip thought vectors ( <ref type="bibr" target="#b11">Kiros et al., 2015)</ref> and use them to ini- tialize the memory keys K M ∈ IR N P ×d K , where N P is the number of tropes, and d K is set to the size of embedded trope description R D , i.e.</p><formula xml:id="formula_5">d K = ||R D ||.</formula><p>The values in the memory represent learnable embeddings of corresponding trope categories V M ∈ IR N P ×d V , where d V is the size of the trope category embeddings. The network learns to use the persona representation z from the encoder phase to find relevant matches in the memory. This corresponds to calculating similarities between z and the keys K M . Formally, this is calculated as:</p><formula xml:id="formula_6">zM = fz(z)<label>(5)</label></formula><formula xml:id="formula_7">p (i) M = sof tmax(zM · KM [i]) (6) ∀i ∈ {1, .., NP } where f z : IR d h → IR d K is a</formula><note type="other">fully-connected layer that projects the persona representation in the space of memory keys K M . Based on the match probabilities p (i) M , the values V M are weighted and cumulatively added to the original persona repre- sentation as:</note><formula xml:id="formula_8">rout = N P i=1 p (i) M · VM [i]<label>(7)</label></formula><p>We iteratively combine our mapped persona representation z M with information from the memory r out . The above process is repeated n hop times. The memory mapped persona representa- tion z M is updated as follows:</p><formula xml:id="formula_9">z hop M = fr(z hop−1 M ) + rout<label>(8)</label></formula><p>where z 0 M = z M , and f r : IR d V → IR d K is a fully- connected layer. Finally, we transform the re- sulting z n hop M using another fully-connected layer,</p><formula xml:id="formula_10">f out ∈ IR d K → IR d h , via: ˆ zM = fout(z n hop M )<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Read-Write Memory</head><p>We also tested a Read-Write Memory following Kaiser et. al <ref type="bibr" target="#b10">(Kaiser and Nachum, 2017)</ref>, which was originally designed to remember rare events. In our case, these 'rare' events might be key di- alogue snippets that are particularly indicative of latent persona. It consists of keys, which are ac- tivations of a specific layer of model, i.e. the persona representation z, and values, which are the ground-truth labels, i.e. the trope categories. Over time, it is able to facilitate predictions based on past data with similar activations stored in the memory. For every new example, the network writes to memory for future look up. A memory with memory size N M is defined as:</p><formula xml:id="formula_11">M = (K N M ×d H , VN M , AN M )<label>(10)</label></formula><p>Memory Read We use the persona embedding z as a query to the memory. We calculate the co- sine similarities between z and the keys in M , take the softmax on the top-k neighbors, and compute a weighted embeddingˆzembeddingˆ embeddingˆz M using those scores.</p><p>Memory Write We update the memory in a similar fashion to the original work by <ref type="bibr" target="#b10">(Kaiser and Nachum, 2017)</ref>, which takes into account the max- imum age of items as stored in A N M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Objective Losses</head><p>To train our model, we utilize the different objec- tive losses described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Classification Loss</head><p>We calculate the probability of a character belong- ing to a particular trope category P through Equa- tion 11, where f P : IR d h → IR N P is a fully- connected layer, and z is the persona representa- tion produced by the multi-level attentive encoders described in Equation 1. We then optimize the cat- egorical cross-entropy loss between the predicted and true tropes as in Equation 12, where N P is the total number of tropes, q j is the predicted distribu- tion that the input character fulls under trope j, and p j ∈ {0, 1} denotes the ground-truth of whether the input snippets come from characters from the j th trope.</p><formula xml:id="formula_12">q = sof tmax(fP (z))<label>(11)</label></formula><formula xml:id="formula_13">JCE = N P j=1 −pj log(qj)<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Trope Description Triplet Loss</head><p>In addition to using trope descriptions to initialize the Knowledge-Store Memory, we also test learn- ing from the trope descriptions through a triplet loss <ref type="bibr" target="#b8">(Hoffer and Ailon, 2015)</ref>. We again use the skip thought vectors to represent the descriptions. Specifically, we want to maximize the similarity of representations obtained from dialogue snippets with their corresponding description, and mini- mize their similarity with negative examples. We implement this as:</p><formula xml:id="formula_14">R P = fD(z)<label>(13)</label></formula><formula xml:id="formula_15">JT = max(0, s(R P , R D n ) − s(R P , R D p ) + αT )<label>(14)</label></formula><p>where </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trope Description Triplet Loss with Memory Module</head><p>If a memory module is used, we compute a new triplet loss in place of the one described in Equa- tion 14. Models that use a memory module should learn a representationˆzrepresentationˆ representationˆz M , based on ei- ther the prior knowledge stored in the memory (as in Knowledge-Store memory) or the top-k key matches (as in Read-Write memory), that is simi- lar to the representation of the trope descriptions. This is achieved by replacing the persona em- bedding z in Equation 13 with the memory out- putˆzputˆ putˆz M as shown in Equation 15, where f D M : IR d h → IR ||R D || is a fully-connected layer. To compute the new loss, we combine the representa- tions obtained from Equations 13 and 15 through a learnable parameter γ that determines the impor- tance of each representation. Finally, we utilize this combined representationˆRrepresentationˆ representationˆR P to calculate the loss as shown in Equation 17.</p><formula xml:id="formula_16">R P M = fD M (ˆ zM ) (15) ˆ R P = γR P + (1 − γ)R P M (16) JMT = max(0, s( ˆ R P , R D n ) − s( ˆ R P , R D p ) + αMT ) (17)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Read-Write Memory Losses</head><p>When the Read-Write memory is used, we use two extra loss functions. The first is a Memory Rank- ing Loss J M R as done in <ref type="bibr" target="#b10">(Kaiser and Nachum, 2017)</ref>, which learns based on whether a query with the persona embedding z returns nearest neighbors with the correct trope. The second is a Memory Classification Loss J M CE that uses the values re- turned by the memory to predict the trope. The full details for both are found in Supplementary Section A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Overall Loss</head><p>We combine the above losses through:</p><formula xml:id="formula_17">J = βCE · JCE + βT · ˆ JT + βMR · JMR + βMCE · JMCEˆJT JMCEˆ JMCEˆJT = JMT if memory module is used JT otherwise.<label>(18)</label></formula><p>where β = [β CE , β M CE , , β T , β M R ] are learn- able weights such that i β i = 1. Depending on which variant of the model is being used, the list β is modified to contain only relevant losses. For example, when the Knowledge-Store memory is used, we set β M R = β M CE = 0 and β is modified to β = [β CE , β T ]. We discuss different variants of our model in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>We experimented with combinations of our var- ious modules and losses. The experimental re- sults and ablation studies are described in the fol- lowing sections, and the experimental details are described in Supplementary Section B. The dif- ferent model permutation names in <ref type="table" target="#tab_3">Table 2</ref>, e.g. "attn 3 tropetrip ks-mem ndialog16", are defined as follows:</p><p>• baseline vs attn: The 'baseline' model uses only one dialogue snippet S to predict the trope, i.e. N diag = 1. Hence, the inter- snippet encoder is not used. The 'attn' model operates on N diag dialogue snippets using the inter-snippet encoder to assign an attention score for each snippet S i . cates that the triplet loss on the trope de- scriptions was used. If '-500' is appended to 'tropetrip', then the 4800-dimensional skip embeddings representing the descriptions in Equations 15 and 17 are projected to 500 di- mensions using a fully connected layer.</p><p>• ks-mem vs. rw-mem: 'ks-mem' refers to the Knowledge-Store memory, and 'rw-mem' refers to the Read-Write memory.</p><p>• ndialog: The number of dialogue snippets N diag used as input for the attention mod- els. Any attention model without the explicit N diag listed uses N diag = 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Ablation Results</head><p>Baseline vs. Attention Model. The attention model shows a large improvement over the base- line models. This matches our intuition that not every quote is strongly indicative of character trope. Some may be largely expository or 'chit- chat' pieces of dialogue. Example attention scores are shown in Section 7.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.615 attn 3 ks-mem</head><p>0.678 0.648 0.676 0.657 attn 3 rw-mem 0.635 0.598 0.635 0.611 attn 3 tropetrip ks-mem 0.663 0.628 0.662 0.639 attn 3 tropetrip-500 ks-mem 0.654 0.618 0.652 0.629 attn 3 tropetrip rw-mem 0.649 0.602 0.648 0.617 attn 3 tropetrip-500 rw-mem 0.644 0.608 0.644 0.619 attn 3 tropetrip-500 ks-mem ndialog16 0.740 0.707 0.741 0.718 attn 3 tropetrip-500 ks-mem ndialog32 0.750 0.750 0.750 0.750 attn 3 tropetrip ks-mem ndialog16 0.740 0.722 0.741 0.728 attn 3 tropetrip ks-mem ndialog32 0.731 0.712 0.731 0.718   Though our experiments showed marginal im- provement between using the 'char' data and the '3' data, we found that using all 3 inputs had greater performance for models with the triplet loss and read-only memory. This is likely because the others' lines and context capture more of the social dynamics and situations that are described in the trope descriptions. Subsequent results are shown only for the 'attn 3' models.</p><p>Trope Description Triplet Loss. Adding the trope description loss alone provided relatively small gains in performance, though we see greater gains when combined with memory. While both use the descriptions, perhaps the Knowledge Store memory matches an embedding against all the tropes, whereas the trope triplet loss is only pro- vided information from one positive and one neg- ative example.</p><p>Memory Modules.</p><p>The Knowledge-Store memory in particular was helpful. Initialized with the trope descriptions, this memory can 'sharpen' queries toward one of the tropes. The Read-Write memory had smaller gains in performance. It may be that more data is required to take advantage of the write capabilities.</p><p>Combined Trope Description Triplet Loss and Memory Modules. Using the triplet loss with memory modules led to greater performance when compared to the attn 3 model, but the per- formance sits around the use of either triplet only or memory only. However, when we increase the N diag to 16 or 32, we find a jump in performance. This is likely the case because the model has both increased learning capacity and a larger sample of data at every batch, which means at least some of the N diag quotes should be informative about the trope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Attention Scores</head><p>Because the inter-snippet encoder provides such a large gain in performance compared to the base- line model, we provide an example illustrating the weights placed on a batch of N diag snippets. <ref type="figure" target="#fig_3">Fig- ure 4</ref> shows the attention scores for the charac- ter's lines in the "byronic hero" trope. Matching what we might expect for an antihero personality, we find the top weighted line to be full of confi- dence and heroic bluster, while the middle lines hint at the characters' personal turmoil. We also find the lowly weighted sixth and seventh lines to be largely uninformative (e.g. "I heard things."), and the last line to be perhaps too pessimistic and negative for a hero, even a byronic one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Purity scores of character clusters</head><p>Finally, we measure our ability to recover the trope 'clusters' (with one trope being a cluster of its characters) with our embeddings through the pu- rity score used in ( <ref type="bibr" target="#b1">Bamman et al., 2014</ref>). Equa- tion 19 measures the amount of overlap between two clusterings, where N is the total number of characters, g i is the i-ith ground truth cluster, and c j is the j-th predicted cluster.</p><formula xml:id="formula_18">Purity = 1 N i maxj|gi ∩ cj|<label>(19)</label></formula><p>We use a simple agglomerative clustering method on our embeddings with a parameter k for the number of clusters. The methods in ( <ref type="bibr" target="#b1">Bamman et al., 2014</ref>) contain a similar hyper-parameter for the number of persona clusters. We note that the metrics are not completely comparable because not every character in the original dataset was found on IMDB. The results are shown in <ref type="table" target="#tab_5">Table  3</ref>. It might be expected that our model perform better because we use the character tropes them- selves as training data. However, dialogue may be noisier than the movie summary data; their better performing Persona Regression (PR) model also uses useful metadata features such as the movie genre and character gender. We simply note that our scores are comparable or higher.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Application: Narrative Analysis</head><p>We collected IMDB quotes for the top 250 movies on IMDB. For every character, we calculated a character embedding by taking the average em- bedding produced by passing all the dialogues through our model. We then calculated movie embeddings by taking the weighted sum of all the character embeddings in the movie, with the weight as the percentage of quotes they had in the movie. By computing distances between pairs of character or movie embeddings, we could poten- tially unearth notable similarities. We note some of the interesting clusters below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Clustering Characters</head><p>• </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We used the character trope classification task as a test bed for learning personas from dialogue. Our experiments demonstrate that the use of a multi- level attention mechanism greatly outperforms a baseline GRU model. We were also able to lever- age prior knowledge in the form of textual descrip- tions of the trope. In particular, using these de- scriptions to initialize our Knowledge-Store mem- ory helped improved performance. Because we use short text and can leverage domain knowl- edge, we believe future work could use our models for applications such as personalized dialogue sys- tems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Number of IMDB dialogue snippets per trope</figDesc><graphic url="image-1.png" coords="3,72.00,258.90,218.27,51.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>• char vs. 3 :</head><label>3</label><figDesc>To measure the importance of context and other characters' lines, we have two variants -'char' uses only the char- acter's lines, while '3' uses the character's lines, other character's lines, and all context lines. Formally, in 'char' mode, we set γ s E and γ s O to 0 in Equation 1. In 'attn' mode, (γ s E , γ s O , γ s D ) are learned by the model. • tropetrip: The presence of 'tropetrip' indi-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Attention scores for a batch of dialogues for the "byronic hero" trope 3 .</figDesc><graphic url="image-5.png" coords="7,93.46,361.86,121.69,121.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Grumpy old men: Carl Fredricksen (Up); Walk Kowalski (Gran Torino) • Shady tricksters, crooks, well versed in de- ceit: Ugarte (Casablanca); Eames (Inception) • Intrepid heroes, adventurers: Indiana Jones (Indiana Jones and the Last Crusade); Nemo (Finding Nemo); Murph (Interstellar) 8.2 Clustering Movies • Epics, historical tales: Amadeus, Ben-Hur • Tortured individuals, dark, violent: Donnie Darko, Taxi Driver, Inception, The Prestige • Gangsters, excess: Scarface, Goodfellas, Reservoir Dogs, The Departed, Wolf of Wall Street</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Experimental results. Details and analysis are given in Section 7.1. The best performing results in each block are 

bolded. The first block examines the baseline model vs. the attention model, as well as use of different inputs. The second block 
uses the triplet loss, and the third block uses our memory modules. The fourth block combines the triplet loss and memory 
module, which the fifth block extends to larger N diag . 

Rumors of my death have been greatly exaggerated. 

What does it matter? Nothing's real down there. Our life is here. 

I don't give a shit who he's connected to. … I want you vacate this 
guy off the premises, and I want you to exit him off his feet and use 
his head to open the fucking door. 

I had to see you. Stacy, I swear... I-I don't blame you for hating 
me. Or for wanting to break up. I just-Let me explain. About my 
family. I ju-I didn't want you to know. See, my dad's a drunk. 
Alright? … Cause when you love somebody, you never give up 
on them. 

Not so bad... Oh! 'Ello, beastie. 

I heard things. 

Fuck! Fuck! That's it, I'm screwed. It's over. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Cluster purity scores. k is the number of clusters, 

PR and DP are the Persona Regression and Dirichlet Persona 
models from (Bamman et al., 2014), and AMN is our atten-
tion memory network. 

</table></figure>

			<note place="foot" n="1"> https://pralav.github.io/emnlp_ personas/</note>

			<note place="foot" n="3"> TVtropes.org defines a byronic hero as &quot;Sometimes an Anti-Hero, others an Anti-Villain, or even Just a Villain, Byronic heroes are charismatic characters with strong passions and ideals, but who are nonetheless deeply flawed individuals....&quot;</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning latent personas of film characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">352</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Autonomy as a moderator of the relationships between the big five personality dimensions and job performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael K</forename><surname>Barrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mount</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of applied Psychology</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">111</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Personality profiling of fictional characters using sense-level links between lexical resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucie</forename><surname>Flekova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1805" to="1816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predicting personality from twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Golbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Robles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michon</forename><surname>Edmondson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Privacy, Security, Risk and Trust (PASSAT) and 2011 IEEE Third Inernational Conference on Social Computing (SocialCom)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<title level="m">IEEE Third International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="149" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Similarity-Based Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="84" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Personality and ideology as determinants of candidate preferences and obama conversion in the 2008 us presidential election</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tessa</forename><forename type="middle">V</forename><surname>John T Jost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gosling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Du Bois Review: Social Science Research on Race</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="124" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<title level="m">Aurko roy, and samy bengio. Learning to remember rare events</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Georgios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dolan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06155</idno>
		<title level="m">A persona-based neural conversation model</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Personage: Personality generation for dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Mairesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirhossein</forename><surname>Karimi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03126</idno>
		<title level="m">Antoine Bordes, and Jason Weston. 2016. Key-value memory networks for directly reading documents</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Does computer-generated speech manifest personality? an experimental test of similarity-attraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clifford</forename><surname>Nass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwan Min Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="329" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Emerging late adolescent friendship networks and big five personality traits: A social network approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Selfhout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Burk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Branje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaap</forename><surname>Denissen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Van Aken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wim</forename><surname>Meeus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="509" to="538" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02364</idno>
		<title level="m">Neural responding machine for short-text conversation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attachment styles and the&quot; big five&quot; personality traits: Their connections with each other and with romantic relationship outcomes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Phillip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kelly A Brennan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Social Psychology Bulletin</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="536" to="545" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inferring interpersonal relations in narrative summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2807" to="2813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Recurrent personality factors based on trait ratings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ernest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Tupes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="225" to="251" />
		</imprint>
	</monogr>
	<note>Journal of personality</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Toward automatic role identification in unannotated folk tales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josep</forename><surname>Valls-Vargas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jichen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanón</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth Artificial Intelligence and Interactive Digital Entertainment Conference</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05869</idno>
		<title level="m">A neural conversational model</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improvising linguistic style: Social and affective bases for agent personality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marilyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><forename type="middle">E</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen J</forename><surname>Cahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whittaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first international conference on Autonomous agents</title>
		<meeting>the first international conference on Autonomous agents</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="96" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Personalizing dialogue agents: I have a dog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07243</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>do you have pets too? arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
