<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Long-Short Range Context Neural Networks for Language Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Oualil</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Spoken Language Systems (LSV)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Collaborative Research Center on Information Density and Linguistic Encoding</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mittul</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Spoken Language Systems (LSV)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Graduate School of Computer Science</orgName>
								<orgName type="institution">Saarland University</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Greenberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Spoken Language Systems (LSV)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Collaborative Research Center on Information Density and Linguistic Encoding</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Graduate School of Computer Science</orgName>
								<orgName type="institution">Saarland University</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Spoken Language Systems (LSV)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Collaborative Research Center on Information Density and Linguistic Encoding</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Graduate School of Computer Science</orgName>
								<orgName type="institution">Saarland University</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Long-Short Range Context Neural Networks for Language Modeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1473" to="1481"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The goal of language modeling techniques is to capture the statistical and structural properties of natural languages from training corpora. This task typically involves the learning of short range dependencies, which generally model the syntactic properties of a language and/or long range dependencies, which are semantic in nature. We propose in this paper a new multi-span architecture, which separately models the short and long context information while it dynamically merges them to perform the language modeling task. This is done through a novel recurrent Long-Short Range Context (LSRC) network, which explicitly models the local (short) and global (long) context using two separate hidden states that evolve in time. This new architecture is an adaptation of the Long-Short Term Memory network (LSTM) to take into account the linguistic properties. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art language modeling techniques.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A high quality Language Model (LM) is considered to be an integral component of many systems for speech and language technology applications, such as machine translation <ref type="bibr" target="#b2">(Brown et al., 1990</ref>), speech recognition <ref type="bibr" target="#b5">(Katz, 1987)</ref>, etc. The goal of an LM is to identify and predict probable sequences of pre- defined linguistic units, which are typically words.</p><p>These predictions are typically guided by the seman- tic and syntactic properties encoded by the LM.</p><p>In order to capture these properties, classical LMs were typically developed as fixed (short) context techniques such as, the word count-based meth- ods <ref type="bibr" target="#b8">(Rosenfeld, 2000;</ref><ref type="bibr" target="#b5">Kneser and Ney, 1995)</ref>, com- monly known as N -gram language models, as well as the Feedforward Neural Networks (FFNN) <ref type="bibr" target="#b2">(Bengio et al., 2003)</ref>, which were introduced as an al- ternative to overcome the exponential growth of pa- rameters required for larger context sizes in N -gram models.</p><p>In order to overcome the short context constraint and capture long range dependencies known to be present in language, <ref type="bibr" target="#b1">Bellegarda (1998a)</ref> proposed to use Latent Semantic Analysis (LSA) to capture the global context, and then combine it with the standard N -gram models, which capture the local context. In a similar but more recent approach, <ref type="bibr">Mikolov and Zweig (2012)</ref> showed that Recurrent Neural Net- work (RNN)-based LM performance can be signif- icantly improved using an additional global topic information obtained using Latent Dirichlet Allo- cation (LDA). In fact, although recurrent architec- tures theoretically allow the context to indefinitely cycle in the network, Hai <ref type="bibr">Son et al. (2012)</ref> have shown that, in practice, this information changes quickly in the classical RNN ( <ref type="bibr" target="#b7">Mikolov et al., 2010)</ref> structure, and that it is experimentally equivalent to an 8-gram FFNN. Another alternative to model linguistic dependencies, Long-Short Term Memory (LSTM) <ref type="bibr" target="#b9">(Sundermeyer et al., 2012)</ref>, addresses some learning issues from the original RNN by control- ling the longevity of context information in the net-work. This architecture, however, does not particu- larly model long/short context but rather uses a sin- gle state to model the global linguistic context.</p><p>Motivated by the works in <ref type="bibr" target="#b1">(Bellegarda, 1998a;</ref><ref type="bibr">Mikolov and Zweig, 2012)</ref>, this paper proposes a novel neural architecture which explicitly models 1) the local (short) context information, generally syn- tactic, as well as 2) the global (long) context, which is semantic in nature, using two separate recurrent hidden states. These states evolve in parallel within a long-short range context network. In doing so, the proposed architecture is particularly adapted to model natural languages that manifest local-global context information in their linguistic properties.</p><p>We proceed as follows. Section 2 presents a brief overview of short vs long range context lan- guage modeling techniques. Section 3 introduces the novel architecture, Long-Short Range Context (LSRC), which explicitly models these two depen- dencies. Then, Section 4 evaluates the proposed net- work in comparison to different state-of-the-art lan- guage models on the PTB and the LTCB corpus. Fi- nally, we conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Short vs Long Context Language Models</head><p>The goal of a language model is to estimate the probability distribution p(w T 1 ) of word sequences w T 1 = w 1 , · · · , w T . Using the chain rule, this dis- tribution can be expressed as</p><formula xml:id="formula_0">p(w T 1 ) = T t=1 p(w t |w t−1 1 )<label>(1)</label></formula><p>This probability is generally approximated under different simplifying assumptions, which are typi- cally derived based on different linguistic observa- tions. All these assumptions, however, aim at mod- eling the optimal context information, be it syntac- tic and/or semantic, to perform the word prediction.</p><p>The resulting models can be broadly classified into two main categories: long and short range context models. The rest of this section presents a brief overview of these categories with a particular focus on Neural Network (NN)-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Short Range Context</head><p>This category includes models that approximate (1) based on the Markov dependence assumption of or- der N −1. That is, the prediction of the current word depends only on the last N − 1 words in the history. In this case, (1) becomes</p><formula xml:id="formula_1">p(w T 1 ) ≈ T t=1 p(w t |w t−1 t−N +1 )<label>(2)</label></formula><p>The most popular methods that subscribe in this category are the N -gram models <ref type="bibr" target="#b8">(Rosenfeld, 2000;</ref><ref type="bibr" target="#b5">Kneser and Ney, 1995)</ref> as well as the FFNN model ( <ref type="bibr" target="#b2">Bengio et al., 2003)</ref>, which estimates each of the terms involved in this product, i.e, p(w t |w t−1 t−N +1 ) in a single bottom-up evaluation of the network.</p><p>Although these methods perform well and are easy to learn, the natural languages that they try to encode, however, are not generated under a Markov model due to their dynamic nature and the long range dependencies they manifest. Alleviating this assumption led to an extensive research to develop more suitable modeling techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Long Range Context</head><p>Conventionally, N-gram related LMs have not been built to capture long linguistic dependencies, al- though significant word triggering information is still available for large contexts. To illustrate such triggering correlations spread over a large context, we use correlation defined over a distance d, given by c d (w 1 , w 2 ) = P d (w 1 ,w 2 ) P (w 1 )P (w 2 ) . A value greater than 1 shows that it is more likely that the word w 1 fol- lows w 2 at a distance d than expected without the occurrence of w 2 . In <ref type="figure" target="#fig_0">Figure 1</ref>, we show the variation of this correlation for pronouns with the distance d. It can be observed that seeing another "he" about twenty words after having seen a first "he" is much more likely. A similar observation can be made for the word "she". It is, however, surprising that seeing "he" after "he" is three times more likely than see- ing "she" after "she", so "he" is much more predic- tive. In the cases of cross-word triggering of "he" → "she" and "she" → "he", we find that the correlation is suppressed in comparison to the same word trig- gering for distances larger than three. In summary, <ref type="figure" target="#fig_0">Figure 1</ref> demonstrates that word triggering informa- tion exists at large distances, even up to one thou- sand words. These conclusions were confirmed by similar correlation experiments that we conducted 1474 for different types of words and triggering relations. In order to model this long-term correlation and overcome the restrictive Markov assumption, recur- rent language models have been proposed to approx- imate (1) according to</p><formula xml:id="formula_2">p(w T 1 ) ≈ T t=1 p(w t |w t−1 , h t−1 ) = T t=1 p(w t |h t ) (3)</formula><p>In NN-based recurrent models, h t is a context vector which represents the complete history, and modeled as a hidden state that evolves within the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Elman-Type RNN-based LM</head><p>The classical RNN ( <ref type="bibr" target="#b7">Mikolov et al., 2010</ref>) esti- mates each of the product terms in (3) according to</p><formula xml:id="formula_3">H t = f (X t−1 + V · H t−1 )<label>(4)</label></formula><formula xml:id="formula_4">P t = g (W · H t )<label>(5)</label></formula><p>where X t−1 is a continuous representation (i.e, embedding) of the word w t−1 , V encodes the re- current connection weights and W is the hidden-to- output connection weights. These parameters define the network and are learned during training. More- over, f (·) is an activation function, whereas g(·) is the softmax function. <ref type="figure">Figure (</ref> Theoretically, the recurrent connections of an RNN allow the context to indefinitely cycle in the  <ref type="formula" target="#formula_0">(2012)</ref> have shown that this information changes quickly over time, and that it is experimentally equivalent to an 8-gram FFNN. This observation was confirmed by the experiments that we report in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Long-Short Term Memory Network</head><p>In order to alleviate the rapidly changing context issue in standard RNNs and control the longevity of the dependencies modeling in the network, the LSTM architecture ( <ref type="bibr" target="#b9">Sundermeyer et al., 2012</ref>) in- troduces an internal memory state C t , which explic- itly controls the amount of information, to forget or to add to the network, before estimating the current hidden state. Formally, this is done according to</p><formula xml:id="formula_5">{i, f, o} t = σ U i,f,o · X t−1 + V i,f,o · H t−1 (6) ˜ C t = f (U c · X t−1 + V c · H t−1 )<label>(7)</label></formula><formula xml:id="formula_6">C t = f t C t−1 + i t ˜ C t (8) H t = o t f (C t )<label>(9)</label></formula><formula xml:id="formula_7">P t = g (W · H t ) (10)</formula><p>where is the element-wise multiplication opera- tor, ˜ C t is the memory candidate, whereas i t , f t and o t are the input, forget and output gates of the net- work, respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multi-Span Language Models</head><p>The attempts to learn and combine short and long range dependencies in language modeling led to what is known as multi-span LMs <ref type="bibr" target="#b1">(Bellegarda, 1998a)</ref>. The goal of these models is to learn the various constraints, both local and global, that are present in a language. This is typically done using two different models, which separately learn the lo- cal and global context, and then combine their re- sulting linguistic information to perform the word prediction. For instance, Bellegarda (1998b) pro- posed to use Latent Semantics Analysis (LSA) to capture the global context, and then combine it with the standard N -gram models, which capture the lo- cal context, whereas <ref type="bibr">Mikolov and Zweig (2012)</ref> proposed to model the global topic information us- ing Latent Dirichlet Allocation (LDA), which is then combined with an RNN-based LM. This idea is not particular to language modeling but has been also used in other Natural Language Processing (NLP) tasks, e.g., <ref type="bibr" target="#b0">Anastasakos et al. (2014)</ref> proposed to use a local/global model to perform a spoken language understanding task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Long-Short Range Context Network</head><p>Following the line of thoughts in <ref type="bibr" target="#b2">(Bellegarda, 1998b;</ref><ref type="bibr">Mikolov and Zweig, 2012)</ref>, we propose a new multi-span model, which takes advantage of the LSTM ability to model long range context while, simultaneously, learning and integrating the short context through an additional recurrent, local state. In doing so, the resulting Long-Short Range Con- text (LSRC) network is able to separately model the short/long context while it dynamically combines them to perform the next word prediction task. For- mally, this new model is defined as</p><formula xml:id="formula_8">H l t = f X t−1 + U c l · H l t−1 (11) {i, f, o} t = σ V i,f,o l · H l t + V i,f,o g · H g t−1<label>(12)</label></formula><formula xml:id="formula_9">˜ C t = f V c l · H l t + V c g · H g t−1<label>(13)</label></formula><formula xml:id="formula_10">C t = f t C t−1 + i t ˜ C t<label>(14)</label></formula><formula xml:id="formula_11">H g t = o t f (C t )<label>(15)</label></formula><formula xml:id="formula_12">P t = g (W · H g t )<label>(16)</label></formula><p>Learning of an LSRC model requires the training of the local parameters V i,f,o,c l and U c l , the global parameters V i,f,o,c g and the hidden-to-output connec- tion weights W . This can be done using the standard Back-Propagation Through Time (BPTT) algorithm, which is typically used to train recurrent networks.</p><p>The proposed approach uses two hidden states, namely, H l t and H g t to model short and long range context, respectively. More particularly, the local state H l t evolves according to (11) which is noth- ing but a simple recurrent model as it is defined in (4). In doing so, H l t is expected to have a similar be- havior to RNN, which has been shown to capture local/short context (up to 10 words), whereas the global state H g t follows the LSTM model, which is known to capture longer dependencies (see example in <ref type="figure" target="#fig_6">Figure 5</ref>). The main difference here, however, is the dependence of the network modules (gates and memory candidate) on the previous local state H l t instead of the last seen word X t−1 . This model is based on the assumption that the local context car- ries more linguistic information, and is therefore, more suitable to combine with the global context and update LSTM, compared to the last seen word. <ref type="figure" target="#fig_5">Fig- ure 4</ref> illustrates the recurrent module of an LSRC network. It is worth mentioning that this model was not particularly developed to separately learn syn- tactic and semantic information. This may come, however, as a result of the inherent local and global nature of these two types of linguistic properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Context Range Estimation</head><p>For many NLP applications, capturing the global context information can be a crucial component to develop successful systems. This is mainly due to the inherent nature of languages, where a single idea or topic can span over few sentences, paragraphs or a complete document. LSA-like approaches take ad- vantage of this property, and aim at extracting some hidden "concepts" that best explain the data in a low- dimension "semantic space". To some extent, the hidden layer of LSRC/LSTM can be seen as a vec- tor in a similar space. The information stored in this vector, however, changes continuously based on the processed words. Moreover, interpreting its content is generally difficult. As an alternative, measuring the temporal correlation of this hidden vector can be used as an indicator of the ability of the network to model short and long context dependencies. For- mally, the temporal correlation of a hidden state H over a distance d is given by</p><formula xml:id="formula_13">c d = 1 D t=D t=1 SM (H t , H t+d )<label>(17)</label></formula><p>where D is the test data size in words and SM is a similarity measure such as the cosine similarity. This measure allows us to evaluate how fast does the information stored in the hidden state change over time.</p><p>In <ref type="figure" target="#fig_6">Figure 5</ref>, we show the variation of this tempo- ral correlation for the local and global states of the proposed LSRC network in comparison to RNN and LSTM for various values of the distance d (up to 3000). This figure was obtained on the test set of the Penn Treebank (PTB) corpus, described in Sec- tion (4). The main conclusion we can draw from this figure is the ability of the LSRC local and global states (trained jointly) to behave in a similar fash- ion to RNN and LSTM states (trained separately), respectively. We can also conclude that the LSRC global state and LSTM are able to capture long range correlations, whereas the context changes rapidly over time in RNN and LSRC local state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>We evaluated the proposed architecture on two dif- ferent benchmark tasks. The first set of experi- ments was conducted on the commonly used Penn Treebank (PTB) corpus using the same experimental setup adopted in (Mikolov et al., 2011) and ( <ref type="bibr" target="#b10">Zhang et al., 2015)</ref>. Namely, sections 0-20 are used for training while sections 21-22 and 23-24 are used for validation an testing, respectively. The vocabulary was limited to the most 10k frequent words while the remaining words were mapped to the token &lt;unk&gt;.</p><p>In order to evaluate how the proposed approach performs on large corpora in comparison to other methods, we run a second set of experiments on the Large Text Compression Benchmark (LTCB) (Ma- honey, 2011). This corpus is based on the enwik9 dataset which contains the first 10 9 bytes of enwiki- 20060303-pages-articles.xml. We adopted the same training-test-validation data split as well as the the same data processing 1 which were used in ( <ref type="bibr" target="#b10">Zhang et al., 2015</ref>). The vocabulary is limited to the most 80k <ref type="bibr">1</ref> All the data processing steps described here for PTB and LTCB were performed using the FOFE toolkit in ( <ref type="bibr" target="#b10">Zhang et al., 2015)</ref>, which is available at https://wiki.eecs.yorku.ca/lab/MLL/_media/ projects:fofe:fofe-code.zip frequent words with all remaining words replaced by &lt;unk&gt;. Details about the sizes of these two corpora can be found in <ref type="table">Table 1</ref> Similarly to the RNN LM toolkit 2 (Mikolov et al., 2011), we have used a single end sentence tag be- tween each two consecutive sentences, whereas the begin sentence tag was not included 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Models</head><p>The proposed LSRC architecture is compared to different LM approaches that model short or long range context. These include the commonly used N -gram Kneser-Ney (KN) <ref type="bibr" target="#b5">(Kneser and Ney, 1995)</ref> model with and without cache ( <ref type="bibr">Kuhn and De Mori, 1990)</ref>, as well as different feedforward and recurrent neural architectures. For short (fixed) size context models, we compare our method to 1) the FFNN- based LM ( <ref type="bibr" target="#b2">Bengio et al., 2003)</ref>, as well as 2) the Fixed-size Ordinally Forgetting Encoding (FOFE) approach, which is implemented in ( <ref type="bibr" target="#b10">Zhang et al., 2015</ref>) as a sentence-based model. For these short size context models, we report the results of dif- ferent history window sizes (1, 2 and 4). The 1 st , 2 nd and 4 th -order FOFE results were either reported in ( <ref type="bibr" target="#b10">Zhang et al., 2015)</ref> or obtained using the freely available FOFE toolkit <ref type="bibr">1</ref> .</p><p>For recurrent models that were designed to cap- ture long term context, we compared the pro- posed approach to 3) the full RNN (without classes) <ref type="bibr" target="#b7">(Mikolov et al., 2011</ref>), 4) to a deep RNN (D-RNN) <ref type="bibr">4</ref> ( <ref type="bibr">Pascanu et al., 2013)</ref>, which investigates different approaches to construct mutli-layer RNNs, and finally 5) to the LSTM model <ref type="bibr" target="#b9">(Sundermeyer et al., 2012)</ref>, which explicitly regulates the amount of information that propagates in the network. The recurrent models results are reported for different numbers of hidden layers (1 or 2). In order to inves- tigate the impact of deep models on the LSRC ar- chitecture, we added a single hidden, non-recurrent layer (of size 400 for PTB and 600 for the LTCB ex- periments) to the LSRC model <ref type="bibr">(D-LSRC)</ref>. This was sufficient to improve the performance with a negli- gible increase in the number of model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">PTB Experiments</head><p>For the PTB experiments, the FFNN and FOFE models use a word embedding size of 200, whereas the hidden layer(s) size is fixed at 400, with all hid- den units using the Rectified Linear Unit (ReLu) i.e., f (x) = max(0, x) as activation function. We also use the same learning setup adopted in ( <ref type="bibr" target="#b10">Zhang et al., 2015</ref>). Namely, we use the stochastic gra- dient descent algorithm with a mini-batch size of 200, the learning rate is initialized to 0.4, the mo- mentum is set to 0.9, the weight decay is fixed at 4×10 −5 , whereas the training is done in epochs. The weights initialization follows the normalized initial- ization proposed in <ref type="bibr">(Glorot and Bengio, 2010)</ref>. Sim- ilarly to <ref type="bibr" target="#b7">(Mikolov et al., 2010)</ref>, the learning rate is halved when no significant improvement of the val- idation data log-likelihood is observed. Then, we continue with seven more epochs while halving the learning rate after each epoch.</p><p>Regarding the recurrent models, we use f = tanh(·) as activation function for all recurrent lay- ers, whereas "f = sigmoid(·)" is used for the input, forget and output gates of LSTM and LSRC. The additional non-recurrent layer in D-LSRC, however, uses the ReLu activation function. The word em- bedding size was set to 200 for LSTM and LSRC whereas it is the same as the hidden layer size for RNN (result of the RNN equation 4). In order to illustrate the effectiveness of the LSRC model, we also report the results when the embedding size is fixed at 100, LSRC(100). The training uses the BPTT algorithm for 5 time steps. Similarly to short context models, the mini-batch was set to 200. The learning rate, however, was set to 1.0 and the weight decay to 5 × 10 −5 . The use of momentum did not lead to any additional improvement. Moreover, the data is processed sequentially without any sentence independence assumption. Thus, the recurrent mod-els will be able to capture long range dependencies that exist beyond the sentence boundary.</p><p>In order to compare the model sizes, we also re- port the Number of Parameters (NoP) to train for each of the models above.   <ref type="table" target="#tab_2">Table 2</ref> shows the perplexity evaluation on the PTB test set. As a first observation, we can clearly see that the proposed approach outperforms all other models for all configurations, in particular, RNN and LSTM. This observation includes other models that were reported in the literature, such as random for- est LM ( <ref type="bibr">Xu and Jelinek, 2007)</ref>, structured LM <ref type="bibr">(Filimonov and Harper, 2009</ref>) and syntactic neural net- work LM ( <ref type="bibr">Emami and Jelinek, 2004</ref>). More partic- ularly, we can conclude that LSRC, with an embed- ding size of 100, achieves a better performance than all other models while reducing the number of pa- rameters by ≈ 29% and ≈ 17% compared to RNN and LSTM, respectively. Increasing the embedding size to 200, which is used by the other models, im- proves significantly the performance with a resulting NoP comparable to LSTM. The significance of the improvements obtained here over LSTM were con- firmed through a statistical significance t-test, which led to p-values ≤ 10 −10 for a significance level of 5% and 0.01%, respectively.</p><p>The results of the deep models in <ref type="table" target="#tab_2">Table 2 also</ref> show that adding a single non-recurrent hidden layer to LSRC can significantly improve the performance. In fact, the additional layer bridges the gap between the LSRC models with an embedding size of 100 and 200, respectively. The resulting architectures outperform the other deep recurrent models with a significant reduction of the number of parameters (for the embedding size 100), and without usage of dropout regularization, L p and maxout units or gradient control techniques compared to the deep RNN 4 (D-RNN).</p><p>We can conclude from these experiments that the explicit modeling of short and long range dependen- cies using two separate hidden states improves the performance while significantly reducing the num- ber of parameters. In order to show the consistency of the LSRC im- provement over the other recurrent models, we re- port the variation of the models performance with respect to the hidden layer size in <ref type="figure" target="#fig_7">Figure 6</ref>. This fig- ure shows that increasing the LSTM or RNN hidden layer size could not achieve a similar performance to the one obtained using LSRC with a small layer size (e.g., 300). It is also worth mentioning that this ob- servation holds when comparing a 2-recurrent lay- ers LSTM to LSRC with an additional non-recurrent layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">LTCB Experiments</head><p>The LTCB experiments use the same PTB setup with minor modifications. The results shown in <ref type="table" target="#tab_4">Ta- ble 3</ref> follow the same experimental setup proposed in ( <ref type="bibr" target="#b10">Zhang et al., 2015)</ref>. More precisely, these results were obtained without use of momentum or weight decay (due to the long training time required for this corpus), the mini-batch size was set to 400, the learning rate was set to 0.4 and the BPTT step was fixed at 5. The FFNN and FOFE architectures use 2 hidden layers of size 600, whereas RNN, LSTM and LSRC have a single hidden layer of size 600. More- over, the word embedding size was set to 200 for all models except RNN, which was set to 600. We also report results for an LSTM with 2 recurrent layers as well as for LSRC with an additional non-recurrent layer. The recurrent layers are marked with an "R" in <ref type="table" target="#tab_4">Table 3</ref>.  The results shown in <ref type="table" target="#tab_4">Table 3</ref> generally confirm the conclusions we drew from the PTB experiments above. In particular, we can see that the proposed LSRC model largely outperforms all other models. In particular, LSRC clearly outperforms LSTM with a negligible increase in the number of parameters (resulting from the additional 200 × 200 = 0.04M local connection weights U c l ) for the single layer results. We can also see that this improvement is maintained for deep models (2 hidden layers), where the LSRC model achieves a slightly better perfor- mance while reducing the number of parameters by ≈ 2.5M and speeding up the training time by ≈ 20% compared to deep LSTM.</p><p>The PTB and LTCB results clearly highlight the importance of recurrent models to capture long range dependencies for LM tasks. The training of these models, however, requires large amounts of data to significantly outperform short context mod- els. This can be seen in the performance of RNN and LSTM in the PTB and LTCB tables above. We can also conclude from these results that the explicit modeling of long and short context in a multi-span model can lead to a significant improvement over state-of-the are models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>We investigated in this paper the importance, fol- lowed by the ability, of standard neural networks to encode long and short range dependencies for lan- guage modeling tasks. We also showed that these models were not particularly designed to, explicitly and separately, capture these two linguistic informa- tion. As an alternative solution, we proposed a novel long-short range context network, which takes ad- vantage of the LSTM ability to capture long range dependencies, and combines it with a classical RNN network, which typically encodes a much shorter range of context. In doing so, this network is able to encode the short and long range linguistic de- pendencies using two separate network states that evolve in time. Experiments conducted on the PTB and the large LTCB corpus have shown that the pro- posed approach significantly outperforms different state-of-the are neural network architectures, includ- ing LSTM and RNN, even when smaller architec- tures are used. This work, however, did not investi- gate the nature of the long and short context encoded by this network or its possible applications for other NLP tasks. This is part of our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Variation of word triggering correlations for pronouns over large distances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>2) shows an example of the standard RNN architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Elman RNN architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 illustrates</head><label>3</label><figDesc>the recurrent module of an LSTM network. Learning of an LSTM model requires the training of the network parame- ters U i,f,o,c , V i,f,o,c and W . Although LSTM models have been shown to out- perform classical RNN in modeling long range de- pendencies, they do not explicitly model long/short context but rather use a single state to encode the global linguistic context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Block diagram of the recurrent module of an LSTM network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Block diagram of the recurrent module of an LSRC network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Temporal correlation of the proposed network in comparison to LSTM and RNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Perplexity of the different NN-based LMs with different hidden layer sizes on the PTB test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>.</head><label></label><figDesc></figDesc><table>Corpus Train 
Dev 
Test 
PTB 
930K 74K 
82K 
LTCB 
133M 7.8M 7.9M 

Table 1: Corpus size in number of words. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : LMs performance on the PTB test set.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : LMs performance on the LTCB test set.</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> The RNN LM toolkit is available at http://www. rnnlm.org/ 3 This explains the difference in the corpus size compared to the one reported in (Zhang et al., 2015). 4 The deep RNN results were obtained using Lp and maxout units, dropout regularization and gradient control techniques, which are known to significantly improve the performance. None of these techniques, however, were used in our experiments.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was in part supported by the Cluster of Excellence for Multimodal Computing and Interac-tion, the German Research Foundation (DFG) as part of SFB 1102, the EU FP7 Metalogue project (grant agreement number: 611073) and the EU Mal-orca project (grant agreement number: 698824).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Task specific continuous word representations for mono and multi-lingual spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anastasakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05-04" />
			<biblScope unit="page" from="3246" to="3250" />
		</imprint>
	</monogr>
	<note>Tasos Anastasakos, Young-Bum Kim, and Anoop Deoras</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A multispan language modeling framework for large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Bellegarda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="456" to="467" />
			<date type="published" when="1998-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exact training of a neural syntactic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><forename type="middle">R</forename><surname>Bellegarda ; Yoshua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jauvin ; Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A Della</forename><surname>Cocke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrick</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">S</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roossin</surname></persName>
		</author>
		<idno>Brown et al.1990</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP &apos;98</title>
		<editor>Emami and Jelinek2004] Ahmad Emami and Frederick Jelinek</editor>
		<meeting>the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP &apos;98<address><addrLine>Seattle, Washington, USA; Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-06" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="245" to="248" />
		</imprint>
	</monogr>
	<note>IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). Filimonov and Harper2009] Denis Filimonov</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">P</forename><surname>Harper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS)<address><addrLine>Singapore; Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-08" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
		<respStmt>
			<orgName>Chia Laguna Resort</orgName>
		</respStmt>
	</monogr>
	<note>Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Measuring the influence of long range dependencies with neural network language models</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the Ngram Model? On the Future of Language Modeling for HLT</title>
		<editor>Hai Son et al.2012] Le Hai Son, Alexandre Allauzen, and François Yvon</editor>
		<meeting>the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the Ngram Model? On the Future of Language Modeling for HLT<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Estimation of probabilities from sparse data for the language model component of a speech recognizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech, and Signal Processing</title>
		<editor>Kuhn and De Mori1990] Roland Kuhn and Renato De Mori</editor>
		<meeting><address><addrLine>Detroit, Michigan, USA, May</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987-03" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="570" to="583" />
		</imprint>
	</monogr>
	<note>IEEE Trans. Pattern Anal. Mach. Intell.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Context dependent recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mikolov and Zweig2012] Tomas Mikolov and Geoffrey Zweig</title>
		<meeting><address><addrLine>Miami, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="234" to="239" />
		</imprint>
	</monogr>
	<note>2012 IEEE Spoken Language Technology Workshop (SLT)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Extensions of recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th Annual Conference of the International Speech Communication Association (INTERSPEECH)</title>
		<imprint>
			<publisher>ICASSP</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
	<note>2011 IEEE International Conference on. Pascanu et al.2013] Razvan Pascanu, C ¸ aglar Gülçehre. How to construct deep recurrent neural networks. CoRR, abs/1312.6026</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Two decades of statistical language modeling: Where do we go from here?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="1270" to="1278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Random forests and the data sparseness problem in language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<editor>Xu and Jelinek2007] Peng Xu and Frederick Jelinek</editor>
		<meeting><address><addrLine>Portland, OR, USA, sep</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="105" to="152" />
		</imprint>
	</monogr>
	<note>LSTM neural networks for language modeling</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The fixedsize ordinally-forgetting encoding method for neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd</title>
		<meeting>the 53rd</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing ACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="495" to="500" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
