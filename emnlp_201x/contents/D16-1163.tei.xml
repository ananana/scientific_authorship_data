<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:54+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transfer Learning for Low-Resource Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
							<email>barretzoph@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Information Sciences Institute Computer Engineering Information Sciences Institute University of Southern California Koç University</orgName>
								<orgName type="institution" key="instit2">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
							<email>dyuret@ku.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Information Sciences Institute Computer Engineering Information Sciences Institute University of Southern California Koç University</orgName>
								<orgName type="institution" key="instit2">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Information Sciences Institute Computer Engineering Information Sciences Institute University of Southern California Koç University</orgName>
								<orgName type="institution" key="instit2">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Information Sciences Institute Computer Engineering Information Sciences Institute University of Southern California Koç University</orgName>
								<orgName type="institution" key="instit2">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transfer Learning for Low-Resource Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1568" to="1575"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages. We present a transfer learning method that significantly improves BLEU scores across a range of low-resource languages. Our key idea is to first train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an average of 5.6 BLEU on four low-resource language pairs. Ensembling and unknown word replacement add another 2 BLEU which brings the NMT performance on low-resource machine translation close to a strong syntax based machine translation (SBMT) system, exceeding its performance on one language pair. Additionally , using the transfer learning model for re-scoring, we can improve the SBMT system by an average of 1.3 BLEU, improving the state-of-the-art on low-resource machine translation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation (NMT) ( ) is a promising paradigm for extracting trans- lation knowledge from parallel text. NMT sys- tems have achieved competitive accuracy rates un- der large-data training conditions for language pairs <ref type="bibr">This</ref>   such as English-French. However, neural methods are data-hungry and learn poorly from low-count events. This behavior makes vanilla NMT a poor choice for low-resource languages, where parallel data is scarce. <ref type="table" target="#tab_1">Table 1</ref> shows that for 4 low-resource languages, a standard string-to-tree statistical MT system (SBMT) ( <ref type="bibr" target="#b9">Galley et al., 2004;</ref><ref type="bibr" target="#b10">Galley et al., 2006</ref>) strongly outperforms NMT, even when NMT uses the state-of-the-art local attention plus feed- input techniques from <ref type="bibr" target="#b15">Luong et al. (2015a)</ref>. In this paper, we describe a method for substan- tially improving NMT results on these languages.</p><p>Our key idea is to first train a high-resource lan- guage pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model). We find that we can optimize our results by fixing certain parameters of the parent model and letting the rest be fine-tuned by the child model. We re- port NMT improvements from transfer learning of 5.6 BLEU on average, and we provide an analysis of why the method works. The final NMT system approaches strong SBMT baselines in all four lan- guage pairs, and exceeds SBMT performance in one of them. Furthermore, we show that NMT is an ex- ceptional re-scorer of 'traditional' MT output; even NMT that on its own is worse than SBMT is con- sistently able to improve upon SBMT system output when incorporated as a re-scoring model.</p><p>We provide a brief description of our NMT model in Section 2. Section 3 gives some background on transfer learning and explains how we use it to im- prove machine translation performance. Our main experiments translating Hausa, Turkish, Uzbek, and Urdu into English with the help of a French-English parent model are presented in Section 4. Section 5 explores alternatives to our model to enhance under- standing. We find that the choice of parent language pair affects performance, and provide an empirical upper bound on transfer performance using an arti- ficial language. We experiment with English-only language models, copy models, and word-sorting models to show that what we transfer goes beyond monolingual information and that using a transla- tion model trained on bilingual corpora as a parent is essential. We show the effects of freezing, fine- tuning, and smarter initialization of different com- ponents of the attention-based NMT system during transfer. We compare the learning curves of transfer and no-transfer models, showing that transfer solves an overfitting problem, not a search problem. We summarize our contributions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">NMT Background</head><p>In the neural encoder-decoder framework for MT <ref type="bibr" target="#b18">(Neco and Forcada, 1997;</ref><ref type="bibr" target="#b4">Castaño and Casacuberta, 1997;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b15">Luong et al., 2015a</ref>), we use a recurrent neural network (encoder) to convert a source sen- tence into a dense, fixed-length vector. We then use another recurrent network (decoder) to convert that vector to a target sentence. In this paper, we use a two-layer encoder-decoder system <ref type="figure" target="#fig_0">(Figure 1</ref>) with long short-term memory (LSTM) units <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber, 1997</ref>). The models were trained to optimize maximum likelihood (via a softmax layer) with back-propagation through time <ref type="bibr" target="#b24">(Werbos, 1990)</ref>. Additionally, we use an attention mecha- nism that allows the target decoder to look back at the source encoder, specifically the local attention model from <ref type="bibr" target="#b15">Luong et al. (2015a)</ref>. In our model we also use the feed-input input connection from <ref type="bibr" target="#b15">Luong et al. (2015a)</ref> where at each timestep on the decoder we feed in the top layer's hidden state into the lowest layer of the next timestep.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Transfer Learning</head><p>Transfer learning uses knowledge from a learned task to improve the performance on a related task, typically reducing the amount of required training data ( <ref type="bibr" target="#b21">Torrey and Shavlik, 2009;</ref><ref type="bibr" target="#b19">Pan and Yang, 2010)</ref>. In natural language processing, transfer learning methods have been successfully applied to speech recognition, document classification and sen- timent analysis ( <ref type="bibr" target="#b23">Wang and Zheng, 2015)</ref>. Deep learning models discover multiple levels of repre- sentation, some of which may be useful across tasks, which makes them particularly suited to transfer learning <ref type="bibr" target="#b2">(Bengio, 2012)</ref>. For example, Cires¸anCires¸an et al. (2012) use a convolutional neural network to rec- ognize handwritten characters and show positive ef- fects of transfer between models for Latin and Chi- nese characters. Ours is the first study to apply trans- fer learning to neural machine translation.</p><p>There has also been work on using data from multiple language pairs in NMT to improve perfor- mance. Recently, <ref type="bibr" target="#b7">Dong et al. (2015)</ref> showed that sharing a source encoder for one language helps performance when using different target decoders  for different languages. In that paper the authors showed that using this framework improves perfor- mance for low-resource languages by incorporating a mix of low-resource and high-resource languages. <ref type="bibr" target="#b8">Firat et al. (2016)</ref> used a similar approach, employ- ing a separate encoder for each source language, a separate decoder for each target language, and a shared attention mechanism across all languages. They then trained these components jointly across multiple different language pairs to show improve- ments in a lower-resource setting.</p><p>There are a few key differences between our work and theirs. One is that we are working with truly small amounts of training data. <ref type="bibr" target="#b7">Dong et al. (2015)</ref> used a training corpus of about 8m English words for the low-resource experiments, and <ref type="bibr" target="#b8">Firat et al. (2016)</ref> used from 2m to 4m words, while we have at most 1.8m words, and as few as 0.2m. Additionally, the aforementioned previous work used the same do- main for both low-resource and high-resource lan- guages, while in our case the datasets come from vastly different domains, which makes the task much harder and more realistic. Our approach only requires using one additional high-resource language, while the other papers used many. Our approach also allows for easy training of new low- resource languages, while <ref type="bibr" target="#b7">Dong et al. (2015)</ref>   <ref type="table">Table 3</ref>: Our transfer method applied to re-scoring output n- best lists from the SBMT system. The first row shows the SBMT performance with no re-scoring and the other 3 rows</p><p>show the performance after re-scoring with the selected model.</p><p>Note: the 'LM' row shows the results when an RNN LM trained on the large English corpus was used to re-score.</p><p>large corpus of parallel data (e.g., French-English).</p><p>We call this the parent model. Next, we initialize an NMT model with the already-trained parent model. This new model is then trained on a very small par- allel corpus (e.g., Uzbek-English). We call this the child model. Rather than starting from a random po- sition, the child model is initialized with the weights from the parent model. A justification for this approach is that in scenar- ios where we have limited training data, we need a strong prior distribution over models. The parent model trained on a large amount of bilingual data can be considered an anchor point, the peak of our prior distribution in model space. When we train the child model initialized with the parent model, we fix parameters likely to be useful across tasks so that they will not be changed during child model train- ing. In the French-English to Uzbek-English ex- ample, as a result of the initialization, the English word embeddings from the parent model are copied, but the Uzbek words are initially mapped to random French embeddings. The parameters of the English embeddings are then frozen, while the Uzbek em- beddings' parameters are allowed to be modified, i.e. fine-tuned, during training of the child model. Freezing certain transferred parameters and fine tun- ing others can be considered a hard approximation to a tight prior or strong regularization applied to some of the parameter space. We also experiment with ordinary L2 regularization, but find it does not sig- nificantly improve over the parameter freezing de- scribed above.</p><p>Our method results in large BLEU increases for a variety of low resource languages. In one of the Language Pair</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Role Train Dev Test</head><p>Size Size Size Spanish-English child 2.5m 58k 59k French-English parent 53m 58k 59k German-English parent 53m 58k 59k <ref type="table">Table 4</ref>: Data used for a low-resource Spanish-English task.</p><p>Sizes are English-side token counts.</p><p>four language pairs our NMT system using trans- fer beats a strong SBMT baseline. Not only do these transfer models do well on their own, they also give large gains when used for re-scoring n-best lists (n = 1000) from the SBMT system. Section 4 de- tails these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To evaluate how well our transfer method works we apply it to a variety of low-resource languages, both stand-alone and for re-scoring a strong SBMT base- line. We report large BLEU increases across the board with our transfer method.</p><p>For all of our experiments with low-resource lan- guages we use French as the parent source language and for child source languages we use Hausa, Turk- ish, Uzbek, and Urdu. The target language is al- ways English. <ref type="table" target="#tab_1">Table 1</ref> shows parallel training data set sizes for the child languages, where the language with the most data has only 1.8m English tokens. For comparison, our parent French-English model uses a training set with 300 million English tokens and achieves 26 BLEU on the development set. Ta- ble 1 also shows the SBMT system scores along with the NMT baselines that do not use transfer. There is a large gap between the SBMT and NMT systems when our transfer method is not used.</p><p>The SBMT system used in this paper is a string- to-tree statistical machine translation system ( <ref type="bibr" target="#b10">Galley et al., 2006;</ref><ref type="bibr" target="#b9">Galley et al., 2004</ref>). In this system there are two count-based 5-gram language mod- els. One is trained on the English side of the WMT 2015 English-French dataset and the other is trained on the English side of the low-resource bi- text. Additionally, the SBMT models use thousands of sparsely-occurring, lexicalized syntactic features ( <ref type="bibr" target="#b5">Chiang et al., 2009</ref>  ing rate, parameter initialization range, dropout rate, and hidden state size for all the experiments. For training we use a minibatch size of 128, hidden state size of 1000, a target vocabulary size of 15K, and a source vocabulary size of 30K. The child models are trained with a dropout probability of 0.5, as in <ref type="bibr" target="#b26">Zaremba et al. (2014)</ref>. The common parent model is trained with a dropout probability of 0.2. The learning rate used for both child and parent mod- els is 0.5 with a decay rate of 0.9 when the de- velopment perplexity does not improve. The child models are all trained for 100 epochs. We re-scale the gradient when the gradient norm of all param- eters is greater than 5. The initial parameter range is [-0.08, +0.08]. We also initialize our forget-gate biases to 1 as specified by <ref type="bibr">Józefowicz et al. (2015)</ref> and <ref type="bibr" target="#b11">Gers et al. (2000)</ref>. For decoding we use a beam search of width 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Transfer Results</head><p>The results for our transfer learning method applied to the four languages above are in <ref type="table" target="#tab_3">Table 2</ref>  During transfer learning, we expect the source-language related blocks to change more than the target-language related blocks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Pair</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Re-scoring Results</head><p>We also use the NMT model with transfer learn- ing as a feature when re-scoring output n-best lists (n = 1000) from the SBMT system. <ref type="table">Table 3</ref> shows the results of re-scoring. We compare re-scoring with transfer NMT to re-scoring with baseline (i.e. non-transfer) NMT and to re-scoring with a neural language model. The neural language model is an LSTM RNN with 2 layers and 1000 hidden states. It has a target vocabulary of 100K and is trained using noise-contrastive estimation <ref type="bibr" target="#b17">(Mnih and Teh, 2012;</ref><ref type="bibr" target="#b22">Vaswani et al., 2013;</ref><ref type="bibr" target="#b1">Baltescu and Blunsom, 2015;</ref><ref type="bibr" target="#b25">Williams et al., 2015)</ref>. Additionally, it is trained us- ing dropout with a dropout probability of 0.2 as sug- gested by <ref type="bibr" target="#b26">Zaremba et al. (2014)</ref>. Re-scoring with the transfer NMT model yields an improvement of 1.1- 1.6 BLEU points above the strong SBMT system; we find that transfer NMT is a better re-scoring feature than baseline NMT or neural language models.</p><p>In the next section, we describe a number of ad- ditional experiments designed to help us understand the contribution of the various components of our transfer model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>We analyze the effects of using different parent mod- els, regularizing different parts of the child model, and trying different regularization techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Different Parent Languages</head><p>In the above experiments we use French-English as the parent language pair. Here, we experiment with different parent languages. In this set of experiments we use Spanish-English as the child language pair. A description of the data used in this section is pre- sented in <ref type="table">Table 4</ref>.</p><p>Our experimental results are shown in <ref type="table" target="#tab_6">Table 5</ref>, where we use French and German as parent lan- guages. If we just train a model with no transfer on a small Spanish-English training set we get a BLEU score of 16.4. When using our transfer method we get Spanish-English BLEU scores of 31.0 and 29.8 via French and German parent languages, respec- tively. As expected, French is a better parent than German for Spanish, which could be the result of the parent language being more similar to the child language. We suspect using closely-related parent language pairs would improve overall quality. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effects of having Similar Parent Language</head><p>Next, we look at a best-case scenario in which the parent language is as similar as possible to the child language.</p><p>Here we devise a synthetic child language (called French ) which is exactly like French, except its vo- cabulary is shuffled randomly. (e.g., "internationale" is now "pomme," etc). This language, which looks unintelligible to human eyes, nevertheless has the same distributional and relational properties as ac- tual French, i.e. the word that, prior to vocabu- lary reassignment, was 'roi' (king) is likely to share distributional characteristics, and hence embedding similarity, to the word that, prior to reassignment, was 'reine' (queen). French should be the ideal par- ent model for French .</p><p>The results of this experiment are shown in Ta- ble 6. We get a 4.3 BLEU improvement with an unrelated parent (i.e. French-parent and Uzbek- child), but we get a 6.7 BLEU improvement with a 'closely related' parent (i.e. French-parent and French -child). We conclude that the choice of par- ent model can have a strong impact on transfer mod- els, and choosing better parents for our low-resource languages (if data for such parents can be obtained) could improve the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Analysis</head><p>In all the above experiments, only the target input and output embeddings are fixed during training. In this section we analyze what happens when different parts of the model are fixed, in order to determine the scenario that yields optimal performance. <ref type="figure" target="#fig_2">Figure 2</ref> shows a diagram of the components of a sequence- to-sequence model. <ref type="table" target="#tab_10">Table 7</ref> shows the effects of al- lowing various components of the child NMT model to be trained. We find that the optimal setting for transferring from French-English to Uzbek-English in terms of BLEU performance is to allow all of the components of the child model to be trained except for the input and output target embeddings.</p><p>Even though we use this setting for our main experiments, the optimum setting is likely to be language-and corpus-dependent. For Turkish, ex- periments show that freezing attention parameters as well gives slightly better results. For parent-child models with closely related languages we expect freezing, or strongly regularizing, more components of the model to give better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Learning Curve</head><p>In <ref type="figure" target="#fig_3">Figure 3</ref> we plot learning curves for both a trans- fer and a non-transfer model on training and devel- opment sets. We see that the final training set per- plexities for both the transfer and non-transfer model are very similar, but the development set perplexity for the transfer model is much better.</p><p>The fact that the two models start from and con- verge to very different points, yet have similar train- ing set performances, indicates that our architecture 13.8 13.7</p><p>14.4 and training algorithm are able to reach a good min- imum of the training objective regardless of the ini- tialization. However, the training objective seems to have a large basin of models with similar perfor- mance and not all of them generalize well to the de- velopment set. The transfer model, starting with and staying close to a point known to perform well on a related task, is guided to a final point in the weight space that generalizes to the development set much better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Dictionary Initialization</head><p>Using the transfer method, we always initialize input language embeddings for the child model with randomly-assigned embeddings from the par- ent (which has a different input language). A smarter method might be to initialize child embeddings with similar parent embeddings, where similarity is mea- sured by word-to-word t-table probabilities. To get these probabilities we compose Uzbek-English and English-French t-tables obtained from the Berke- ley Aligner ( <ref type="bibr" target="#b14">Liang et al., 2006</ref>). We see from Fig- ure 4 that this dictionary-based assignment results in faster improvement in the early part of the train- ing. However the final performance is similar to our standard model, indicating that the training is able to untangle the dictionary permutation introduced by randomly-assigned embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Different Parent Models</head><p>In the above experiments, we use a parent model trained on a large French-English corpus. One might hypothesize that our gains come from exploit-  ing the English half of the corpus as an additional language model resource. Therefore, we explore transfer learning for the child model with parent models that only use the English side of the French- English corpus. We consider the following parent models in our ablative transfer learning scenarios:</p><p>• The results, in <ref type="table" target="#tab_12">Table 8</ref>, show that transfer learning does not simply import an English language model, but makes use of translation parameters learned from the parent's large bilingual text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Overall, our transfer method improves NMT scores on low-resource languages by a large margin and al- lows our transfer NMT system to come close to the performance of a very strong SBMT system, even exceeding its performance on Hausa-English. In addition, we consistently and significantly improve state-of-the-art SBMT systems on low-resource lan- guages when the transfer NMT system is used for re- scoring. Our experiments suggest that there is still room for improvement in selecting parent languages that are more similar to child languages, provided data for such parents can be found.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The encoder-decoder framework for neural machine translation (NMT) (Sutskever et al., 2014). Here, a source sentence C B A (presented in reverse order as A B C) is translated into a target sentence W X Y Z. At each step, an evolving real-valued vector summarizes the state of the encoder (blue, checkerboard) and decoder (red, lattice). Not shown here are the attention connections present in our model used by the decoder to access encoder states.</figDesc><graphic url="image-1.png" coords="2,313.22,57.82,226.78,99.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>.</head><label></label><figDesc>The par- ent models were trained on the WMT 2015 (Bojar et al., 2015) French-English corpus for 5 epochs. Our baseline NMT systems ('NMT' row) all receive a large BLEU improvement when using the transfer method (the 'Xfer' row) with an average BLEU im- provement of 5.6. Additionally, when we use un- known word replacement from Luong et al. (2015b) and ensemble together 8 models (the 'Final' row) we further improve upon our BLEU scores, bringing the average BLEU improvement to 7.5. Overall our method allows the NMT system to reach competi- tive scores and outperform the SBMT system in one of the four language pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our NMT model architecture, showing six blocks of parameters, in addition to source/target words and predictions.</figDesc><graphic url="image-2.png" coords="5,110.14,57.83,391.72,144.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Uzbek-English learning curves for the NMT attention model with and without transfer learning. The training perplexity converges to a similar value in both cases. However, the development perplexity for the transfer model is significantly better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Uzbek-English learning curves for the transfer model with and without dictionary-based assignment of Uzbek word types to French word embeddings (from the parent model). Dictionary-based assignment enables faster improvement in early epochs. The model variants converge, showing that the unaided model is able to untangle the initial random Uzbek/French word-type mapping without help.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>A true translation model (French-English Par- ent) • A word-for-word English copying model (English-English Parent) • A model that unpermutes scrambled English (EngPerm-English Parent) • (The parameters of) an RNN language model (LM Xfer)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>NMT models with attention are outperformed by stan-

dard string-to-tree statistical MT (SBMT) when translating low-

resource languages into English. Train/test bitext corpus sizes 

are English token counts. Single-reference, case-insensitive 

BLEU scores are given for held-out test corpora. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Our method significantly improves NMT results for 

the translation of low-resource languages into English. Results 

show test-set BLEU scores. The 'NMT' row shows results with-

out transfer, and the 'Xfer' row shows results with transfer. The 

'Final' row shows BLEU after we ensemble 8 models and use 

unknown word replacement. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 : For a low-resource Spanish-English task, we exper- iment with several choices of parent model: none, French-</head><label>5</label><figDesc></figDesc><table>English, and German-English. We hypothesize that French-

English is best because French and Spanish are similar. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 : A better match between parent and child languages should improve transfer results. We devised a child language called</head><label>6</label><figDesc></figDesc><table>French , identical to French except for word spellings. We observe that French transfer learning helps French (13.3→20.0) more 

than it helps Uzbek (10.7→15.0). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Starting with the parent French-English model (BLEU =24.4, PPL=6.2), we randomly assign Uzbek word types to French 

word embeddings, freeze various parameters of the neural network model (), and allow Uzbek-English (child model) training 

to modify other parts (). The table shows how Uzbek-English BLEU and perplexity vary as we allow more parameters to be 

re-trained. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Transfer for Uzbek-English NMT with parent models 

trained only on English data. The English-English parent learns 

to copy English sentences, and the EngPerm-English learns to 

un-permute scrambled English sentences. The LM is a 2-layer 

LSTM RNN language model. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by ARL/ARO (W911NF-10-1-0533), DARPA (HR0011-15-C-0115), and the Scientific and Technological Research Council of Turkey (T ¨ UB ˙ ITAK) (grants 114E628 and 215E201).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pragmatic neural language modelling in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baltescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HLTNAACL</title>
		<meeting>HLTNAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning of representations for unsupervised and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hokamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turchi</surname></persName>
		</author>
		<title level="m">Findings of the 2015 workshop on statistical machine translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Proc. WMT</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A connectionist approach to machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Castaño</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Casacuberta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurospeech</title>
		<meeting>Eurospeech</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">11,001 new features for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transfer learning for Latin and Chinese characters with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Cires¸ancires¸an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-task learning for multiple language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-way, multilingual neural machine translation with a shared attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What&apos;s in a translation rule?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scalable inference and training of context-rich syntactic translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Graehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deneefe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Thayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL-COLING</title>
		<meeting>ACL-COLING</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to forget: Continual prediction with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Alignment by agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Asynchronous translations with recurrent neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Neco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Forcada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Neural Networks</title>
		<meeting>International Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shavlik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">E</forename><surname>Soria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Magdalena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Serrano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Research on Machine Learning Applications and Trends: Algorithms, Methods, and Techniques</title>
		<imprint>
			<publisher>IGI Global</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Decoding with large-scale neural language models improves translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Transfer learning for speech and language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Fang</forename><surname>Zheng</surname></persName>
		</author>
		<idno>abs/1511.06066</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">78</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scaling recurrent neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mrva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Recurrent neural network regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1409.2329</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
