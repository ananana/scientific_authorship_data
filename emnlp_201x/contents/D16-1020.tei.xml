<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Connective-based Word Representations for Implicit Discourse Relation Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Braud</surname></persName>
							<email>braud@di.ku.dk</email>
							<affiliation key="aff0">
								<orgName type="department">Dep of Computer Science</orgName>
								<orgName type="institution">University of Copenhagen University</orgName>
								<address>
									<addrLine>Park 5, 59650 Villeneuve dAscq</addrLine>
									<postCode>2100</postCode>
									<settlement>Copenhagen</settlement>
									<country>Denmark, France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
							<email>pascal.denis@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Dep of Computer Science</orgName>
								<orgName type="institution">University of Copenhagen University</orgName>
								<address>
									<addrLine>Park 5, 59650 Villeneuve dAscq</addrLine>
									<postCode>2100</postCode>
									<settlement>Copenhagen</settlement>
									<country>Denmark, France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Connective-based Word Representations for Implicit Discourse Relation Identification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="203" to="213"/>
							<date type="published">November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce a simple semi-supervised approach to improve implicit discourse relation identification. This approach harnesses large amounts of automatically extracted discourse connectives along with their arguments to construct new distributional word representations. Specifically, we represent words in the space of discourse connectives as a way to directly encode their rhetorical function. Experiments on the Penn Discourse Treebank demonstrate the effectiveness of these task-tailored representations in predicting implicit discourse relations. Our results indeed show that, despite their simplicity, these connective-based representations outperform various off-the-shelf word embeddings, and achieve state-of-the-art performance on this problem.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A natural distinction is often made between ex- plicit and implicit discourse relations depending on whether they are lexicalized by a connective or not, respectively. To illustrate, the Contrast relation in example (1a) is triggered by the connective but, while it is not overtly marked in example (1b). <ref type="bibr">1</ref> Given the lack of strong explicit cues, the identi- fication of implicit relations is a much more chal- lenging and still open problem. The typically low performance scores for this task also hinder the de- velopment of text-level discourse parsers ( <ref type="bibr" target="#b17">Lin et al., 2010;</ref>: implicit discourse relations The difficulty of this task lies in its dependence on a wide variety of linguistic factors, ranging from syntax, lexical semantics and also world knowl- edge <ref type="bibr" target="#b0">(Asher and Lascarides, 2003)</ref>. In order to deal with this issue, a common approach is to exploit hand-crafted resources to design features captur- ing lexical, temporal, modal, or syntactic informa- tion ( <ref type="bibr" target="#b23">Park and Cardie, 2012)</ref>. By contrast, more recent work show that using simple low-dimensional word-based representations, either cluster-based or distributed (aka word embeddings), yield comparable or better performance <ref type="bibr" target="#b27">(Rutherford and Xue, 2014;</ref><ref type="bibr" target="#b3">Braud and Denis, 2015)</ref>, while dis- pensing with feature engineering.</p><p>While standard low-dimensional word represen- tations appear to encode relevant linguistic infor- mation, they have not been built with the specific rhetorical task in mind. A natural question is there- fore whether one could improve implicit discourse relation identification by using word representations that are more directly related to the task. The problem of learning good representation for dis- course has been recently tackled by <ref type="bibr" target="#b9">Ji and Eisenstein (2014)</ref> on the problem of text-level discourse parsing. Their approach uses two recursive neural networks to jointly learn the task and a transforma- tion of the discourse segments to be attached. While this type of joint learning yields encouraging results, it is also computationally intensive, requiring long training times, and could be limited by the relatively small amount of manually annotated data available.</p><p>In this paper, we explore the possibility of learn- ing a distributional word representation adapted to the task by selecting relevant rhetorical contexts, in this case discourse connectives, extracted from large amounts of automatically detected connectives along with their arguments. Informally, the as- sumption is that the estimated word-connective co- occurrence statistics will in effect give us an im- portant insight to the rhetorical function of different words. The learning phase in this case is extremely simple, as it amounts to merely estimating co- occurrence frequencies, potentially combined with a reweighting scheme, between each word appearing in a discourse segment and its co-occurring connec- tive. To assess the usefulness of these connective- based representations, 2 we compare them with pre- trained word representations, like Brown clusters and other word embeddings, on the task of implicit discourse relation identification. Our experiments on the Penn Discourse Treebank (PDTB) <ref type="bibr" target="#b26">(Prasad et al., 2008)</ref> show that these new representations de- liver improvements over systems using these generic representations and yield state-of-the-art results, and this without the use of other hand-crafted features, thus also alleviating the need for external linguis- tic resources (like lexical databases). Thus, our ap- proach could be easily extended to resource-poor languages as long as connectives can be reliably identified on raw texts. Section 2 summarizes related work. In Section 3, we detail our connective-based distributional word representation approach. Section 4 presents the au- tomatic annotation of the explicit examples used to build the word representation. In Section 5, we de- scribe our comparative experiments on the PDTB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Implicit discourse relation identification has at- tracted growing attention since the release of the PDTB, the first discourse corpus to make the distinc- tion between explicit and implicit examples. Within the large body of research on this problem, we iden- tify two main strands directly relevant to our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Finding the Right Input Representation</head><p>The first work on this task ( <ref type="bibr" target="#b20">Marcu and Echihabi, 2002</ref>), which pre-dates the release of the PDTB, pro- posed a simple word-based representation: they use the Cartesian product of words appearing in the two segments. Given the knowledge-rich nature of the task, following studies attempted to exploit various hand-crafted resources and pre-processing systems to enrich their model with information on modality, polarity, tense, lexical semantics, and syntax, possi- bly combined with feature selection methods ( <ref type="bibr" target="#b16">Lin et al., 2009;</ref><ref type="bibr" target="#b23">Park and Cardie, 2012;</ref><ref type="bibr" target="#b1">Biran and McKeown, 2013;</ref><ref type="bibr" target="#b15">Li and Nenkova, 2014</ref>). Interestingly, Park and Cardie (2012) con- cluded on the worthlessness of word-based features, as long as hand-crafted linguistic features were used. More recent studies however reversed this conclu- sion ( <ref type="bibr" target="#b27">Rutherford and Xue, 2014;</ref><ref type="bibr" target="#b3">Braud and Denis, 2015)</ref>, demonstrating that word-based features can be effective provided they were not encoded using the sparse one-hot representation, but instead with a denser one (cluster based or distributed). This paper takes one step further by testing whether learning a simple task-specific, distributional word representa- tion could lead to further improvements.</p><p>As noted, some previous work have also at- tempted to learn discourse-specific representation for the related problem of discourse parsing. Thus, <ref type="bibr" target="#b9">Ji and Eisenstein (2014)</ref> reports improvements on the RST Discourse Treebank ( <ref type="bibr" target="#b5">Carlson et al., 2001</ref>), by jointly learning a combination of the discourse units, represented by bag-of-words in a one-hot en- coding, along with the sequence of actions of their shift-reduce parser. Our approach is attractively sim- pler, since training reduces to collecting frequency counts, and it can easily generate representations for unseen words without having to retrain the whole system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Leveraging Explicit Discourse Data</head><p>Another line of work, also initiated in ( <ref type="bibr" target="#b20">Marcu and Echihabi, 2002</ref>), propose to deal with the sparseness of the word pair representation by using additional data automatically annotated using discourse con- nectives. An appeal of this strategy is that one can easily identify explicit relations in raw data, as per- formance are high on this task (  and it is even possible to rely on simple heuris- tics ( <ref type="bibr" target="#b20">Marcu and Echihabi, 2002;</ref><ref type="bibr" target="#b30">Sporleder and Lascarides, 2005;</ref><ref type="bibr" target="#b13">Lan et al., 2013)</ref>. It has been shown, however, that using explicit examples as additional data for training an implicit relation classifier de- grades performance, due to important distribution differences <ref type="bibr" target="#b31">(Sporleder and Lascarides, 2008)</ref>.</p><p>Recent attempts to overcome this issue involve domain adaptation strategies <ref type="bibr" target="#b2">(Braud and Denis, 2014;</ref><ref type="bibr" target="#b10">Ji et al., 2015)</ref>, sample selection ( <ref type="bibr" target="#b38">Wang et al., 2012</ref>), or multi-task al- gorithms <ref type="bibr" target="#b13">(Lan et al., 2013</ref>). However, it generally involves longer training time since models are built on a massive amount of data, the strategy requir- ing a large corpus of explicit examples to overcome the noise induced by the automatic annotation strat- egy. In this paper, we circumvent this problem by using explicit data only for learning our word repre- sentations and not for estimating the parameters of our implicit classification model. Some aspects of the present work are similar to <ref type="bibr" target="#b1">Biran and McKeown (2013)</ref> in that they also exploit explicit data to com- pute co-occurrence statistics between word pairs and connectives. But the perspective is reversed, as they represent connectives in the contexts of co-occurring word pairs, with the aim of deriving similarity fea- tures between each implicit example and each con- nective. Furthermore, their approach did not outper- form state-of-the-art systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Connective Vector Space Model</head><p>Our discourse-based word representation model is a simple variant of the standard vector space model <ref type="bibr" target="#b36">(Turney and Pantel, 2010)</ref>: that is, it represents in- dividual words in specific co-occurring contexts (in this case, discourse connectives) that define the di- mensions of the underlying vector space. Our spe- cific choice of contexts was guided by two main con- siderations. On the one hand, we aim at learning word representations that live in a relatively low- dimensional space, so as to make learning a classifi- cation function over that space feasible. The number of parameters of that function grows proportionally with that of the input size. Although there is often a lack of consensus among linguists as to the exact definition of discourse connectives, they neverthe- less form a closed class. For English, the PDTB rec- ognizes 100 distinct connectives. On the other hand, we want to learn a vectorial representation that cap- tures relevant aspects of the problem, in this case the rhetorical contribution of words. Adapting Har- ris (1954)'s famous quote, we make the assumption that words occurring in similar rhetorical contexts tend to have similar rhetorical meanings. Discourse connectives are by definition strong rhetorical cues. As an illustration,  found that con- nectives alone unambiguously predict a single rela- tion in 94% of the PDTB level 1 data. By using con- nectives as contexts, we are thus linking each word to a relation (or a small set of relations), namely those that can be triggered by this connective. Note that for level 2 relations in the PDTB, the connec- tives are much more ambiguous (86.77% reported in ( <ref type="bibr" target="#b17">Lin et al., 2010)</ref>), and it could be also the case if we expand the list of forms considered as connec- tives for English, or if we try to deal with other lan- guages and domains. We however believe that the set of relations that can be triggered by a connective is limited (not all relations can be expressed by the same connective), and that one attractive feature of our strategy is precisely to keep this ambiguity.</p><p>Before turning to the details of how we construct our distributional connective-based model, note that we decided to learn a unique representation for any individual word, irrespective of its position (with)in a particular segment. That is, we represent both ar- guments of a connective as a single bag of words. Other designs are of course possible: we could di- rectly learn distinct word representation for left and right segment words, or even the pair of words <ref type="bibr" target="#b7">(Conrath et al., 2014</ref>), to take into account the fact that some relations are oriented (e.g. Reason contains the cause in the first argument and Result in the second one). An obvious drawback of these more expres- sive representations is that they would need much more data to compute a robust estimate of the fre- quency counts. but while before</p><p>Word Freq. TF-IDF PPMI-IDF Freq. TF-IDF PPMI-IDF Freq. TF-IDF PPMI-IDF <ref type="table">Table 1</ref>: Illustrative example of association measures between connectives and words.</p><formula xml:id="formula_0">reality 12 0.0 0.0 13 0.0 0.0 10 0.0 0.0 not 142 0.37 0.36 201 0.18 0.06 0 0.0 0.0 week 0 0.0 0.0 110 0.10 0.04 90 0.12 0.12</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Building the Distributional Representation</head><p>Our discourse-based representations of words are obtained by computing a matrix of co-occurrence between the words and the chosen contexts. The frequency counts are then weighted in order to high- light relevant associations. More formally, we note V the set of the n words appearing in the arguments, and C the set of the m connective contexts. We build the matrix F, of size n × m, by computing the fre- quency of each element of V with each element of C. We note f i,j the frequency of the word w i ∈ V appearing in one argument of the connective c j ∈ C. We use two standard weighting functions on these raw frequencies: the normalized Term Frequency (TF), eq. <ref type="formula" target="#formula_1">(1)</ref>, and the Positive Pointwise Mutual In- formation (PPMI), eq. <ref type="formula" target="#formula_2">(2)</ref>, which is a version of the PMI where negative values are ignored (with p i,j the joint probability that the word w i appears with connective c j , and p i, * and p * ,j , relative frequency of resp. w i and c j ). These two measures are then normalized by multiplying the value by the Inverse Document Frequency (IDF) for a word w i , eq. <ref type="formula" target="#formula_3">(3)</ref>, as in ( <ref type="bibr" target="#b1">Biran and McKeown, 2013</ref>). In the final ma- trices, the i th row corresponds to the m-dimensional vector for the i th word of V. The j th column is a vector corresponding to the j th connective. <ref type="table">Table 1</ref> illustrates the weighting of the words using the TF and the PPMI normalized with IDF. For in- stance, the presence of the negation "not" is pos- itively linked to Contrast through but and while whereas it receives a null or a very small weight with the temporal connective before. The final vec- tor for this word, &lt; 0.37, 0.18, 0.0 &gt; with TF-IDF or &lt; 0.36, 0.06, 0.0 &gt; with PPMI-IDF, is intended to guide the implicit model toward a contrastive re- lation, thus potentially helping in identifying the re- lation in example (1b). In contrast, the word "week" is more likely to be found in the arguments of tem- poral relations that can be triggered by before but also while, an ambiguity kept in our representation whereas approaches based on using explicit exam- ples as new training data generally choose to anno- tate them using the most frequent sense associated with the connective, often limiting themselves to the less ambiguous ones ( <ref type="bibr" target="#b20">Marcu and Echihabi, 2002;</ref><ref type="bibr" target="#b31">Sporleder and Lascarides, 2008;</ref><ref type="bibr" target="#b13">Lan et al., 2013;</ref><ref type="bibr" target="#b2">Braud and Denis, 2014;</ref>. Finally, a word occuring with all connectives, not discriminant, such as "reality" is associated with a null weight for all dimensions: it thus has no impact on the model. Since we have 100 connectives for the PDTB, the representation is already of quite low dimensional- ity. However, it has been shown <ref type="bibr" target="#b36">(Turney and Pantel, 2010</ref>) that using a dimensionality reduction al- gorithm could help capturing the latent dimensions between the words and their contexts and reducing the noise. We thus also test versions with a reduction Components Analysis (PCA) (Jolliffe, 2002).</p><formula xml:id="formula_1">TF i,j = f i,j n k=1 f k,j<label>(1)</label></formula><formula xml:id="formula_2">PPMI i,j = max(0, log p i,j p i, * p * ,j )<label>(2)</label></formula><formula xml:id="formula_3">IDF i = log m m k=1 f i,k<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Using the Word-based Representation</head><p>So far, our distributional framework associates a word with a d-dimensional vector (where d ≤ m). We now need to represent a pair of arguments (i.e., the spans of text linked by a relation), mod- eled here as a pair of bags of words. Following <ref type="bibr" target="#b3">(Braud and Denis, 2015)</ref>, we first sum all word vec- tors contained in each segment, thus obtaining a d- dimensional vector for each segment. We then com- bine the two segment vectors to build a compos- ite vector representing the pair of arguments, by ei-ther concatenating the two segment vectors (lead- ing to a 2d-dimensional vector) or by computing the Kronecker product between them (leading to a d 2 -dimensional vector). Finally, these segment- pair representations will be normalized using the L 2 norm to avoid segment size effects. These will then be used as the input of a classification model, as described in Section 5. Given these combination schemes, it should be clear that despite the fact that each individual word receives a unique vectorial rep- resentation irrespective of its position, the param- eters of the classification model associated with a given word are likely to be different depending of whether it appears in the left or right segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Automatic Annotation of Explicit Examples</head><p>In order to collect reliable word-connective co- occurrence frequencies, we need a large corpus where the connectives and their arguments have been identified. We therefore rely on automatic annotation of raw data, instead of using the rela- tively small amount of explicit examples manually annotated in the PDTB (roughly 18, 000 examples). Specifically, we used the Bllip corpus 3 composed of news articles from the LA Times, the Washington Post, the New York Times and Reuters and containing 310 millions of words automatically POS-tagged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identifying the Connectives and their Arguments</head><p>We have two tasks to perform: identifying the con- nectives and extracting their arguments. <ref type="bibr">4</ref> Rather than relying on manually defined patterns to anno- tate explicit examples ( <ref type="bibr" target="#b20">Marcu and Echihabi, 2002;</ref><ref type="bibr" target="#b31">Sporleder and Lascarides, 2008;</ref>, we use two binary classification models inspired by previous works on the PDTB ( <ref type="bibr" target="#b17">Lin et al., 2010)</ref>: the first one iden- tifies the connectives and the second one localizes the arguments between inter-and intra-sentential, an heuristic being then used to decide on the exact boundaries of the arguments. Discourse connectives are words (e.g., but, since)</p><p>3 https://catalog.ldc.upenn.edu/ LDC2008T13 4 Note that contrary to studies using automatically annotated explicit examples as new training data, we do not need to anno- tate the relation triggered by the connective. or grammaticalized multi-word expressions (e.g., as soon as, on the other hand) that may trigger a dis- course relation. However, these forms can also ap- pear without any discourse reading, such as because in: He can't sleep because of the deadline. We thus need to disambiguate these forms between discourse and non discourse readings, a task that has proven to be quite easy on the PDTB ( ). This is the task performed by our first binary classifier: a pattern-matching is used to identify all potential connectives, and the model predicts if they have discourse reading in context.</p><p>We then need to extract the arguments of the iden- tified connectives, that is the two spans of text linked by the connective. This latter task has proven to be extremely hard on the PDTB ( <ref type="bibr" target="#b17">Lin et al., 2010;</ref>) because of some annotation principles that make the possible types of argument very di- verse. As first proposed in ( <ref type="bibr" target="#b17">Lin et al., 2010)</ref>, we thus split this task into two subtasks: identifying the rel- ative positions of the arguments and delimiting their exact boundaries.</p><p>For an explicit example in the PDTB, one argu- ment, called Arg2, is linked to the connective, and thus considered as easy to extract ( <ref type="bibr" target="#b17">Lin et al., 2010)</ref>. The other argument, called Arg1, may be located at different places relative to <ref type="bibr">Arg2 (Prasad et al., 2008)</ref>: we call intra-sentential the examples where Arg1 is a clause within the same sentence as Arg2 (60.9% of the explicit examples in the PDTB), and inter- sentential the other examples, that is Arg1 is found in the previous sentence, in a non-adjacent previ- ous sentence (9%) or in a following sentence (less than 0.1%). In this work, we build a localization model by only considering these two coarse cases - the example is either intra-or inter-sentential. Note that this distinction is similar to what has been done in ( <ref type="bibr" target="#b17">Lin et al., 2010)</ref>: more precisely, these authors distinguish between "same-sentence" and "previous sentence" and ignore the cases where the Arg1 is in a following sentence. We rather choose to include them as being also inter-sentential. When the posi- tion of Arg1 has been predicted, an heuristic is in charge of finding the exact boundaries of the argu- ments.</p><p>Here, the problem is that in addition to the vari- ety of locations, the annotators were almost free to choose any boundary for an argument in the PDTB: an argument can cover only a part of a sentence, an entire sentence or several sentences. Statistical ap- proaches intended to solve this task lead for now to low performance even when complex sequential models are used, and they often rely on the syntactic configurations ( <ref type="bibr" target="#b17">Lin et al., 2010;</ref>). We thus decided to define an heuristic to perform this task, following the simplifying assumptions also used in previous work since ( <ref type="bibr" target="#b20">Marcu and Echihabi, 2002</ref>). We assume that: (1) Arg1 is either in the same sentence as Arg2 or in the previous one, (2) an argument covers at most one sentence and <ref type="formula" target="#formula_3">(3)</ref> a sentence contains at most two arguments. As it can be deduced from (1), our final model ignores the finer distinctions one can make for the position of inter-sentential examples (i.e. we never extract Arg1 from a non-adjacent previous sentence or a follow- ing one).</p><p>According to these assumptions, once a connec- tive is identified, knowing its localization is almost enough to identify the boundaries of its arguments. More precisely, if a connective is predicted as inter- sentential, then our heuristic picks the entire pre- ceding sentence as Arg1, Arg2 being the sentence containing the connective, according to assumptions (1) and (2). If a connective is predicted as intra- sentential, then the sentence containing the connec- tive is split into two segments -according to (3) -, more precisely, the sentence is split around the con- nective using the punctuation and making it neces- sary to have a verb in each argument.</p><p>Settings We thus built two models using the PDTB: one to identify the discourse markers (con- nective vs not connective), and one to identify the position of the arguments with respect to the con- nective (inter-vs intra-sentential). The PDTB con- tains 18, 459 explicit examples for 100 connectives. For both models, we use the same split of the data as in ( <ref type="bibr" target="#b19">Lin et al., 2014</ref>). The test set contains 923 positive instances of connectives and 2, 075 nega- tive instances, and 546 inter-sentential and 377 intra- sentential examples. Both models are built using a logistic regression model optimized on the develop- ment set (see Section 5), and the same simple feature set ( <ref type="bibr" target="#b19">Lin et al., 2014;</ref><ref type="bibr" target="#b11">Johannsen and Sgaard, 2013)</ref> without syntactic information. With C the connec- tive, F the following word and P the previous one, our features are: C, P+C, C+F, C-POS 5 , P-POS, F- POS, P-POS+C-POS and C-POS+F-POS.</p><p>Results Our model identifies discourse connective with a micro-accuracy of 92.9% (macro-F 1 91.5%). These scores are slightly lower than the state-of-the- art in micro-accuracy, but high enough to rely on this annotation. When applying our model to the Bllip data, we found 4 connectives that correspond to no examples. We thus have examples for only 96 connectives. For distinguishing between inter-and intra-sentential examples, we get a micro-accuracy of 96.1% (macro-F 1 96.0), with an F 1 of 96.7 for the intra-and 95.3 for the inter-sentential class, again close enough to the state-of-the-art ( <ref type="bibr" target="#b19">Lin et al., 2014</ref>).</p><p>Coverage Using these models on Bllip, we are able to extract around 3 million connectives, along with their arguments. Our word representation has a large vocabulary (see <ref type="table" target="#tab_1">Table 2</ref>) compared to exist- ing off-the-shelf word vectors, with only 2, 902 out of vocabulary (OOV) tokens in set of implicit rela- tions. <ref type="bibr">6</ref>    <ref type="bibr" target="#b22">and Hinton, 2007</ref>) using the implementation in ( <ref type="bibr" target="#b35">Turian et al., 2010</ref>), Hellinger PCA (H-PCA) <ref type="bibr" target="#b14">(Lebret and Collobert, 2014)</ref> and our connective-based representation (Bllip).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># words # OOV</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Our experiments investigate the relevance of our connective-based representations for implicit dis- course relation identification, recast here as multi- class classification problem. That is, we aim at eval- uating the usefulness of having a word representa- tion linked to the task, compared to using generic  word representations (either one-hot, cluster-based or distributed), and whether they encode all the in- formation relevant to the task, thus comparing sys- tems with or without additional hand-crafted fea- tures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data</head><p>The PDTB ( <ref type="bibr" target="#b26">Prasad et al., 2008</ref>) is the largest corpus annotated for discourse relations, formed by news- paper articles from the Wall Street Journal. It con- tains 16, 053 pairs of spans of text annotated with one or more implicit relations. The relation set is organized in a three-level hierarchy. We focus on the level 1 coarse-grained relations and keep only the first relation annotated. We use the most spread split of the data, used in ( <ref type="bibr" target="#b27">Rutherford and Xue, 2014;</ref><ref type="bibr" target="#b3">Braud and Denis, 2015)</ref> among others, that is sections 2-20 for training and 21-22 for testing. The other sections are used for de- velopment. The number of examples per relation is reported in <ref type="table" target="#tab_3">Table 3</ref>. It can be seen that the dataset is highly imbalanced, with the relation Expansion ac- counting for more than 50% of the examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Settings</head><p>Feature Set Our main features are based on the words occurring in the arguments. We test simple baselines using raw tokens. The first one uses the Cartesian product of the tokens, a feature template, generally called "Word pairs", used in most of the previous study for this task as in ( <ref type="bibr" target="#b20">Marcu and Echihabi, 2002;</ref><ref type="bibr" target="#b18">Lin et al., 2011;</ref><ref type="bibr" target="#b3">Braud and Denis, 2015;</ref><ref type="bibr" target="#b10">Ji et al., 2015)</ref>. It is the sparsest representation one can build from words, and it cor- responds to using the combination scheme based on the Kronecker product to combine the one-hot vec- tors representing each word. We also report results with a less sparse version where the vectors are com- bined using concatenation. We also compare our systems to previous ap- proaches that make use of word based representa- tions but not linked to the task. We implement the systems proposed in <ref type="bibr" target="#b3">(Braud and Denis, 2015</ref>) in multiclass, that is using the Brown clusters ( <ref type="bibr" target="#b4">Brown et al., 1992)</ref>, the Collobert and Weston <ref type="bibr" target="#b6">(Collobert and Weston, 2008)</ref> and the hierarchical log-bilinear embeddings <ref type="bibr" target="#b22">(Mnih and Hinton, 2007</ref>) using the implementation in <ref type="figure" target="#fig_0">(Turian et al., 2010)</ref>  <ref type="bibr">7</ref> , and the HPCA ( <ref type="bibr" target="#b14">Lebret and Collobert, 2014)</ref>  <ref type="bibr">8</ref> . We use the combination schemes described in Section 3 to build vector representations for pairs of segments. For these systems and ours, using the connective- based representations, the dimensionality of the final model depends on the number of dimensions d of the representation used and on the combination scheme -the concatenation leading to 2d dimensions and the Kronecker product to d 2 .</p><p>All the word representations used -the off-the- shelf representations as well as our connective-based representation (see Section 4) -are solely or mainly trained on newswire data, thus on the same domain as our evaluation data. The CnW embeddings we use in this paper, with the implementation in ( <ref type="bibr" target="#b35">Turian et al., 2010)</ref>, as well as the HLBL embeddings have been obtained using the RCV1 corpus, that is one year of Reuters English newswire. The H-PCA have been built on the Wikipedia, the Reuters corpus and the Wall street Journal. We thus do not expect any out-of-domain issue when using these representa- tions.</p><p>Finally, we experiment with additional features proposed in previous studies and well described in <ref type="bibr" target="#b23">Park and Cardie, 2012)</ref>: pro- duction rules 9 , information on verbs (average verb phrases length and Levin classes), polarity <ref type="bibr" target="#b39">(Wilson et al., 2005</ref>), General Inquirer tags <ref type="bibr" target="#b32">(Stone and Kirsh, 1966)</ref>, information about the presence of numbers and modals, and first, last and first three words. We concatenate these features to the ones built using word representations.</p><p>Model We train a multinomial multiclass logistic regression model. <ref type="bibr">10</ref> In order to deal with the class imbalance issue, we use a sample weighting scheme where each instance has a weight inversely propor- tional to the frequency of the class it belongs to.</p><p>Parameters We optimize the hyper-parameters of the algorithm, that is the regularization norm (L1 or L2), and the strength of the regularization C ∈ {0.001, 0.005, 0.01, 0.1, 0.5, 1, 5, 10, 100}. When using additional features or one-hot sparse encod- ings over the pairs of raw tokens, we also optimize a filter on the features by defining a frequency cut- off t ∈ {1, 2, 5, 10, 15, 20}. We evaluate the un- supervised representations with different number of dimensions. We test versions of the Brown clus- ters with 100, 320, 1, 000 and 3, 200 clusters, of the Collobert and Weston embeddings with 25, 50, 100 and 200 dimensions, of the hierarchical log- bilinear embeddings with 50 and 100 dimensions, and of the Hellinger PCA with 50, 100 and 200 di- mensions. Finally, the distributional representations of words based on the connective are built using ei- ther no PCA -thus corresponding to 96 dimensions- , or a PCA 11 keeping the first k dimensions with k ∈ {2, 5, 10, 50}. 12 We optimize both the hyper- parameters of the algorithm and the number of di- mensions of the unsupervised representation on the development set based on the macro-F 1 score, the most relevant measure to track when dealing with imbalanced data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>Our results are summarized in <ref type="table" target="#tab_5">Table 4</ref>. Using our connective-based word representation allows im- provements of above 2% in macro-F 1 over the base- line systems based on raw tokens (One-hot), the competitive systems using pre-trained representa- tions (Brown and Embed.) and the state-of-the-art results in terms of macro-F 1 (R&amp;X 15). These im- provements demonstrate the efficiency of the repre- sentation for this task.</p><p>We found that using an unsupervised word repre- sentation generally leads to improvements over the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representation</head><p>Macro-F 1 Acc.  use of raw tokens (One-hot), a conclusion in line with the results reported in <ref type="bibr" target="#b3">(Braud and Denis, 2015)</ref> for binary systems. However, contrary to their find- ings, in multiclass, the best results are not obtained using the Brown clusters, but rather the dense, real valued representations (Embed. and Bllip). Further- more, concerning the combination schemes, the con- catenation (⊕) generally outperforms the Kronecker product (⊗), in effect favoring lower dimensional models. More importantly, the distributional representa- tions based on connectives (Bllip) allows perfor- mance at least similar or even better than those ob- tained with the other dense representations uncon- nected to the task (Embed.). While simply based on weighted co-occurrence counts, thus really easy and fast to build, these representations generally outper- form the ones learned using neural networks (see CnW and HLBL in <ref type="figure" target="#fig_0">Figure 1)</ref>. Besides, our sec- ond best representation is also distributional, namely HPCA (see <ref type="figure" target="#fig_0">Figure 1</ref>). These result are thus in line with the conclusions in ( <ref type="bibr" target="#b14">Lebret and Collobert, 2014</ref>) for other NLP tasks: distributional representations, while simpler to obtain, may allow similar results than distributed ones. Our best results with Bllip are obtained without the use of a dimensionality reduction method, thus keeping the 96 dimensions corresponding to the con- nectives identified in the raw data. Our new word representation like the other low-dimensional ones yield higher scores as one increases the number of dimensions (see <ref type="figure" target="#fig_0">Figure 1</ref>). This could be a limita- tion of our strategy, since the number of connectives in the PDTB is fixed. However, one could easily expand our model to include additional lexical ele- ments that might have a rhetorical function such as modals or specific expressions such as one reason is.</p><p>We also tested the addition of hand-crafted fea- tures traditionally used for the task. We found that, either using a pre-trained word representation or our representation based on connectives, adding these features leads to small or even no improvements and suggest that these representations already encode the information provided by these features. This con- clusion has however to be nuanced: when looking at the scores per relation reported in <ref type="table" target="#tab_7">Table 5</ref>, the use of the connective based word representation alone allows the best performance for Temporal and Con- tingency, but the addition of new features dramat- ically increase the scores for Comparison showing that some information are missing for this relation. Moreover, this relation is the one taking the most advantage of the addition of explicit data in , demonstrating that these data could probably provide even more information than the ones we leverage through our representations. Finally, our results are similar or even better than those reported in ) in terms of macro-F 1 . Our systems correspond how- ever to a lower micro-accuracy. Looking at the scores per relation in <ref type="table" target="#tab_7">Table 5</ref>, we found that we ob- tain better results for all the relations except Expan- sion, the most represented, which could explain the loss in accuracy. It is noteworthy that we generally obtain better results even without the additional fea- tures used in this work. Moreover, our systems re- quires lower training time (since we only train on implicit examples) and alleviate the need for the sample selection strategy used to deal with the dis- tribution differences between the two types of data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented a new approach to leverage infor- mation from explicit examples for implicit relation identification. We showed that building distribu- tional representations linked to the task through con- nectives allows state-of-the-art performance and al- leviates the need for additional features. Future work includes extending the representations to new contexts -such as the Alternative Lexicalization an- notated in the PDTB, the modals or some adverbs -using more sophisticated weighting schemes <ref type="bibr" target="#b14">(Lebret and Collobert, 2014)</ref> and testing this strategy for other languages and domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: F1 scores on dev against the number of dimensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Lexicon coverage for Brown clusters (Brown et al., 

1992), Collobert and Weston (CnW ) (Collobert and Weston, 

2008) and hierarchical log-bilinear embeddings (HLBL) (Mnih 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Number of examples in train, dev, test. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results for multiclass experiments. R&amp;X 15 are the 

scores reported in (Rutherford and Xue, 2015) ; One-hot: one-

hot encoding of raw tokens ; Brown and Embed.: pre-trained 

representations ; Bllip: connective based representation.  *  p ≤ 

0.1 compared to One-hot ⊗ with t-test and Wilcoxon. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Scores per relation for multiclass experiments, "R&amp;X 

15" are the scores reported in (Rutherford and Xue, 2015). 

</table></figure>

			<note place="foot" n="1"> These examples are taken from documents wsj 0008 and wsj 0037, respectively, of the PDTB.</note>

			<note place="foot" n="2"> Available at https://bitbucket.org/chloebt/ discourse-data.</note>

			<note place="foot" n="5"> The connective POS is either the node covering the connective, or the POS of its first word if no such node exists. 6 Training and development sets, only.</note>

			<note place="foot" n="7"> http://metaoptimize.com/projects/ wordreprs/ 8 http://lebret.ch/words/ 9 We use the gold standard parses provided in the Penn Treebank (Marcus et al., 1993).</note>

			<note place="foot" n="10"> http://scikit-learn.org/dev/index.html. 11 Implemented in scikit-learn, applied with default settings. 12 Keeping resp. 11.3%, 36.6%, 56.2% or 95.3% of the variance of the data.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the three anonymous reviewers for their comments. Chloé Braud was funded by the ERC Starting Grant LOWLANDS No. 313695. Pas-cal Denis was supported by ERC Grant STAC No. 269427, and by a grant from CPER Nord-Pas de Calais/FEDER DATA Advanced data science and technologies <ref type="bibr">2015-2020.</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Logics of Conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Asher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lascarides</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Aggregated word pair features for implicit discourse relation disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Biran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining natural and artificial examples to improve implicit discourse relation identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Comparing word representations for implicit discourse relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Classbased n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer</forename><forename type="middle">C</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Building a discourse-tagged corpus in the framework of rhetorical structure theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ellen</forename><surname>Okurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second SIGdial Workshop on Discourse and Dialogue</title>
		<meeting>the Second SIGdial Workshop on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised extraction of semantic relations using discourse cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliette</forename><surname>Conrath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stergos</forename><surname>Afantenos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Asher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Coling</title>
		<meeting>Coling</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Representation learning for text-level discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Closing the gap: Domain adaptation from explicit to implicit discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Disambiguating explicit discourse connectives without oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Johannsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Sgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP</title>
		<meeting>IJCNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Leveraging synthetic discourse data via multi-task learning for implicit discourse relation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyu</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Word emdeddings through Hellinger PCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reducing sparsity improves the recognition of implicit discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessy</forename><surname>Junyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGDIAL</title>
		<meeting>SIGDIAL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recognizing implicit discourse relations in the Penn Discourse Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A PDTB-styled end-to-end discourse parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>National University of Singapore</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatically evaluating text coherence using discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-HLT</title>
		<meeting>ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A PDTB-styled end-to-end discourse parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="151" to="184" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An unsupervised approach to recognizing discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdessamad</forename><surname>Echihabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving implicit discourse relation recognition through feature set optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonsuk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGDIAL Conference</title>
		<meeting>SIGDIAL Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using syntax to disambiguate explicit discourse connectives in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-IJCNLP</title>
		<meeting>the ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic sense prediction for implicit discourse relations in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Penn Discourse Treebank 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discovering implicit discourse relations through Brown cluster pair representation and coreference patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Attapol</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving the inference of implicit discourse relations via classifying explicit discourse connectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Attapol</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lexical marking of discourse relations-some experimental findings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Soria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giacomo</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Discourse Relations and Discourse Markers</title>
		<meeting>the ACL Workshop on Discourse Relations and Discourse Markers</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploiting linguistic cues to classify rhetorical relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Sporleder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lascarides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RANLP-05</title>
		<meeting>RANLP-05</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Using automatically labelled examples to classify rhetorical relations: An assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Sporleder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lascarides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="369" to="416" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The General Inquirer: A Computer Approach to Content Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kirsh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An effective discourse parser that uses rich linguistic information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Subba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><forename type="middle">Di</forename><surname>Eugenio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-HLT</title>
		<meeting>ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discourse markers as signals (or not) of rhetorical relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maite</forename><surname>Taboada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Pragmatics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="567" to="592" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">From frequency to meaning : Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Linguistic tests for discourse relations in the TüBa-D/Z corpus of written German</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Versley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Gastel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue &amp; Discourse</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="142" to="173" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Implicit discourse relation recognition by selecting typical training examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012: Technical Papers</title>
		<meeting>COLING 2012: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recognizing contextual polarity in phrase-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-EMNLP</title>
		<meeting>HLT-EMNLP</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The CoNLL-2015 shared task on shallow discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Attapol</forename><surname>Rutherford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
