<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Do we need bigram alignment models? On the effect of alignment quality on transduction accuracy in G2P</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Eger</surname></persName>
							<email>steeger@em.uni-frankfurt.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Text Technology Lab Goethe University Frankfurt am Main</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Do we need bigram alignment models? On the effect of alignment quality on transduction accuracy in G2P</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We investigate the need for bigram alignment models and the benefit of supervised alignment techniques in grapheme-to-phoneme (G2P) conversion. Moreover, we quantitatively estimate the relationship between alignment quality and overall G2P system performance. We find that, in English, bigram alignment models do perform better than unigram alignment models on the G2P task. Moreover, we find that supervised alignment techniques may perform considerably better than their unsupervised brethren and that few manually aligned training pairs suffice for them to do so. Finally, we estimate a highly significant impact of alignment quality on overall G2P transcription performance and that this relationship is linear in nature.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Grapheme-to-phoneme (G2P) conversion is the problem of converting a string of letters into a string of phonetic symbols. Closely related to G2P are other string transduction problems in nat- ural language processing (NLP) such as transliter- ation ), lemmatization <ref type="bibr" target="#b5">(Dreyer et al., 2008)</ref>, and spelling error correc- tion <ref type="bibr" target="#b2">(Brill and Moore, 2000</ref>). The classical learn- ing paradigm in each of these settings is to train a model on pairs of strings {(x, y)} and then to evaluate model performance on test data. While there are exceptions (e.g., <ref type="bibr" target="#b26">(Rao et al., 2015)</ref>), most state-of-the-art modelings (e.g., <ref type="bibr" target="#b13">(Jiampojamarn et al., 2007;</ref><ref type="bibr" target="#b1">Bisani and Ney, 2008;</ref><ref type="bibr" target="#b15">Jiampojamarn et al., 2008;</ref><ref type="bibr" target="#b24">Novak et al., 2012)</ref>) view string transduction as a two-stage process in which string pairs (x, y) in the train- ing data are first aligned, and then a subsequent (e.g., sequence labeling) module is learned on the aligned data. ph oe n i x f i n I ks <ref type="table">Table 1</ref>: Sample monotone many-to-many align- ment between x = phoenix and y = finIks.</p><p>State-of-the-art alignments in G2P are charac- terized by the following properties:</p><p>(i) Alignments are monotone in that the ordering of characters in input and output sequences is preserved by the alignments. Furthermore, they are many-to-many in the sense that sev- eral x sequence characters may be matched up with several y sequence characters as il- lustrated in <ref type="table">Table 1</ref>.</p><p>(ii) The alignment is a latent variable and learnt in an unsupervised manner from pairs of strings in the training data.</p><p>(iii) The unsupervised alignment models are un- igram alignment models insofar as the over- all score that the alignment model assigns an alignment is the same for all orderings of the matched-up subsequences (context indepen- dence).</p><p>To illustrate point (iii), consider, in the field of lemmatization, the case of aligning an inflected word form with the extended infinitive in German, such as absagt ('rejects') with abzusagen ('to re- ject'). Critically, the insertion -zu-appears in in- fixal position and a plausible alignment might be as in <ref type="table">Table 2</ref>. Then, correctly aligning certain a b s a g t a b zu s a g en <ref type="table">Table 2</ref>: Alignment between absagen and abzusagen. Empty string denoted by .</p><p>analogous forms such as zusagt ('accepts') with their corresponding extended infinitive zuzusagen ('to accept') is beyond the scope of a unigram alignment model since this cannot distinguish the linguistically correct alignment from the following linguistically incorrect alignment z u s a g t zu z u s a g en precisely because it has no notion of context.</p><p>In this work, we firstly address bigram align- ment models in G2P. We investigate whether there are phenomena in G2P that require bigram align- ment models and, more generally, whether bigram alignment models produce better alignments - with respect to a human gold standard -than un- igram alignment models within the G2P setting. We do so, secondly, in a supervised setting where the model learns from gold-standard alignments. While this may seem an odd scenario at first sight, modern alignment toolkits in the related field of machine translation typically include the possibil- ity to learn both in a supervised and unsupervised manner ( <ref type="bibr" target="#b22">Liu et al., 2010;</ref><ref type="bibr" target="#b21">Liu and Sun, 2015)</ref>. The rationale behind supervised learning models may be that they perform better than unsupervised models, and if alignment quality has a large impact upon subsequent string translation performance, then a supervised model may be a suitable alterna- tive. Thirdly, we investigate how alignment qual- ity affects overall G2P performance. This allows us to address whether it is worthwhile to work on better alignment models, which bigram and supervised alignment models promise to be. To our knowledge, all three outlined aspects of align- ments -bigram models, supervised learning, and systematically estimating the relationship between alignment quality and overall string transduction performance -are novel in the G2P setting and its related fields as outlined; however, see also the related work section.</p><p>This work is structured as follows. Section 2 presents definitions and algorithms for uni-and bi- gram alignment models. Section 3 surveys related work. Section 4 presents our data and Section 5 our experiments. We conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Uni-and bigram alignment models</head><p>We first formally define the problem of aligning two strings x and y over arbitrary alphabets in a monotone and many-to-many manner. Let x = |x| and y = |y| denote the lengths of x and y, respectively. Let N = {0, 1, 2, . . .}, and let S ⊆ N 2 \{(0, 0)} be a set defining the valid match-up operations between x characters and y characters. In other words, when (s, t) ∈ S, then this means we allow matches of subsequences of x of length s and subsequences of y of length t. <ref type="bibr">1</ref> It is convenient to define a monotone many-to- many alignment of x and y as a 2×k (for k ≥ 1 ar- bitrary) nonnegative integer matrix A x,y ∈ N 2×k</p><p>satisfying A x,y 1 k = x y , i.e., the two rows of A x,y sum up to the lengths of the respective strings, 2 and where each column of A x,y lies in S. For any such alignment, we let (x 1 , . . . , x k ) be the corresponding induced segmentation of x and (y 1 , . . . , y k ) be the corresponding induced seg- mentation of y.</p><p>Example. For any S ⊇ {(1, 1), (1, 2), (2, 1)}, the alignment of x = phoenix and y = finIks shown in <ref type="table">Table 1</ref> may be represented by the ma- trix A x,y = 2 2 1 1 1 1 1 1 1 2 . The correspond- ing induced segmentations are (ph,oe,n,i,x) and (f,i,n,I,ks).</p><p>Let A S (x, y) denote the class of all alignments of x and y. We call a function f : A S (x, y) → R an alignment model. We call an alignment model f a unigram alignment model if f takes the form, for any A x,y ∈ A S (x, y),</p><formula xml:id="formula_0">f (A x,y ) = k i=1 sim 1 (x i , y i )<label>(1)</label></formula><p>where sim 1 is an arbitrary (real-valued) similar- ity function measuring similarity of two subse- quences. We call an alignment model f a bigram alignment model if f takes the form</p><formula xml:id="formula_1">f (A x,y ) = k i=1 sim 2 (x i , y i ), (x i−1 , y i−1 )<label>(2)</label></formula><p>where sim 2 is an arbitrary (real-valued) similarity function measuring similarity of successive pairs of subsequences.</p><p>Example. Let sim 1 (u, v) be equal to |u| · |v| and let f uni (A x,y ) be as in Eq. (1). Then, f uni is a unigram alignment model that assigns the score 1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment given in <ref type="table">Table 2</ref>.</p><p>Example. Let sim 2 (u, v), (u , v ) = (|u| · |v|) |v | if |u| = |u | − 1 or u = v and −2 oth- erwise. Let f bi (A x,y ) be as in Eq. (2). Then, f bi is a bigram alignment model assigning the score <ref type="table">Table 2</ref>.</p><formula xml:id="formula_2">(1 · 1) 0 + (1 · 1) 1 + (0 · 2) 1 + (1 · 1) 2 + (1 · 1) 1 + (1 · 1) 1 − 2 = 3 to the alignment in</formula><p>In statistical alignment modeling, the task is to find an optimal alignment (i.e., one with maxi- mal score) given strings x and y and given the alignment model f . When f is a unigram model, this can be solved efficiently via dynamic pro- gramming (DP). When f is a bigram alignment model, then finding the optimal alignment can still be solved via DP, by introducing a variable M ijqw denoting the score of the best alignment of x(1 : i) and y(1 : j) that ends in the match- up of x(q : i) with y(w : j). <ref type="bibr">3</ref> The variable M ijqw satisfies a recurrence leading to a DP al- gorithm, shown in Algorithm 1. The actual align- ment can be found by storing pointers to the maxi- mizing steps taken. Running time of the algorithm is O( 2 x 2 y |S|). Note also that the sketched algo- rithm is supervised insofar as it assumes that the similarity values sim 2 (·, ·) are known. Typically, such alignment algorithms can be converted into unsupervised algorithms in which similarity mea- sures sim are learnt iteratively, e.g., in an EM-like fashion (cf., e.g., Eger (2012), Eger (2013)); how- ever, in this paper, we only investigate the super- vised base version as indicated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related work</head><p>Monotone alignments have a long tradition in NLP. The classical Needleman-Wunsch algo- rithm <ref type="bibr" target="#b23">(Needleman and Wunsch, 1970)</ref> computes the optimal alignment between two sequences when only single character matches, mismatches, and skips are allowed. It is a special case of the unigram model (1) for which S = {(1, 0), (0, 1), (1, 1)} and sim 1 takes on values from {0, −1}, depending on whether compared subsequences match or not. As is well-known, this alignment specification is equivalent to the edit distance problem <ref type="bibr" target="#b20">(Levenshtein, 1966</ref>) in which the minimal number of insertions, deletions and substitutions is sought that transforms one string into another. Substring-to-substring edit oper- ations -or equivalently, (monotone) many-to- many alignments -have appeared in the NLP context, e.g., in <ref type="bibr" target="#b4">Deligne et al. (1995)</ref>, <ref type="bibr" target="#b2">Brill and Moore (2000)</ref>, <ref type="bibr" target="#b13">Jiampojamarn et al. (2007)</ref>, <ref type="bibr" target="#b1">Bisani and Ney (2008)</ref>, , or, significantly earlier, in <ref type="bibr" target="#b30">Ukkonen (1985)</ref>, <ref type="bibr">Véronis (1988)</ref>. Learning edit distance/monotone align- ments in an unsupervised manner has been the topic of, e.g., <ref type="bibr" target="#b28">Ristad and</ref><ref type="bibr">Yianilos (1998), Cotterell et al. (2014)</ref>, besides the works already men- tioned. All of these approaches are special cases of our unigram model -i.e., they consider particular S (most prominently, S = {(1, 0), (0, 1), (1, 1)}) and sim 1 . 4 Eger (2015b), <ref type="bibr" target="#b32">Yao and Kondrak (2015)</ref>, and Eger (2015a) generalize to alignments of multiple strings, but likewise only consider un- igram alignment models in their experiments.</p><p>Probably the most closely related work to ours is . There, older and specialized alignment techniques such as ALINE <ref type="bibr" target="#b17">(Kondrak, 2000</ref>) (as well as partly heuristic/semi-automatic alignment methods) are compared with variants of the M2M alignment algorithm, which we also survey. This work does not consider supervised alignments or bigram alignments, as we do. Moreover,  also evaluate the impact of alignment quality on overall G2P system accuracy by running a few experiments, finding that better alignment quality does not always translate into better G2P accuracy, but that there is a "strong correlation" between the two. We more thorougly investigate this question, using, arguably, more heterogeneous aligners, and many more experi- ments. We also quantitatively estimate how align- ment quality influences G2P system accuracy on two different languages via linear regression. <ref type="bibr" target="#b11">Goldwater et al. (2006</ref>) study the effect of context in (unsupervised) word/sequence seg- mentation, which may be considered the one- dimensional specialization of sequence alignment, using a Bayesian method. They find that bigram models greatly outperform unigram models for their task.</p><p>Of course, our study is also related to the field of machine translation and its studies on the rela-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1</head><p>1: procedure BIGRAM-ALIGN(x = x 1 . . . x n , y = y 1 . . . y m ; S, sim 2 ) 2:</p><formula xml:id="formula_3">M ijqw ← −∞ for all (i, j, q, w) ∈ Z 4</formula><p>3:</p><formula xml:id="formula_4">M 0000 ← 0 4:</formula><p>for i = 0 . . . n do tionship between alignment quality and translation performance ( <ref type="bibr" target="#b10">Ganchev et al., 2008)</ref>. In machine translation, the monotonicity assumption of string transduction does typically not hold, however, ren- dering alignment and translation techniques differ- ent and more heuristic in nature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data and systems 4.1 Data</head><p>For English, we conduct experiments on the Gen- eral American (GA) variant of the Combilex data set ( <ref type="bibr" target="#b27">Richmond et al., 2009)</ref>. This contains about 128 000 grapheme-phoneme pairs as exemplified in <ref type="table" target="#tab_0">Table 3</ref>. Importantly, Combilex provides gold- standard alignments, which we will make use of for the supervised alignment models as well as for measuring alignment quality. For German, we ran- domly extract 3 000 G2P string pairs from CELEX ( <ref type="bibr" target="#b0">Baayen et al., 1995)</ref>. We had a native speaker manually align them so that gold standard align- ments are available here, too. Both data sets con- tain quite complex match-ups of character subse- quences such as (2,3) as in English s-oi-r-ee-s/s- wOA-r-P-z or (4,1) as in w-eigh-t/w-P-t but the majority of match-ups are of type (1,1), (2,1), and, to a lesser degree, (1,2) and (3,1).</p><formula xml:id="formula_5">Grapheme string Phoneme string g-e-n-e-r-a-l dZ-E-n-@-r-@-l p-r-o-b-a-t-ion-a-r-y p-r-@U-b-eI-S-n=-E-r-i w-oo-d-e-d w-U-d-@-d M-u-r-m-a-n-s-k m-U@-r-m-A-n-s-k</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Alignment toolkits/models</head><p>The M2M aligner <ref type="bibr" target="#b13">(Jiampojamarn et al., 2007)</ref>, which is based on EM maximum likelihood es- timation of alignment parameters, is the classi- cal unsupervised unigram many-to-many aligner in G2P. As has been pointed out ( <ref type="bibr" target="#b18">Kubo et al., 2011</ref>), M2M greatly overfits the data. <ref type="bibr">5</ref> This means that when the M2M aligner is given the freedom to align two sequences without restric- tions, it matches them up as a whole. The rea- son is that a (probabilistic) unigram alignment model adds log-probabilities of matched-up sub- sequences, which, if not appropriately corrected for, makes alignments with few match-ups a pri- ori more likely than alignments with many match- ups, when probabilities of individual match-ups are uniformly or randomly initialized (as is typi- cally the case for EM maximum likelihood esti- mation in unsupervised models). To address this, M2M must artifically restrain, in our language, the set S to be {(1, 1), (1, 2), (2, 1)}. In contrast, the Mpaligner (Kubo et al., 2011) introduces a prior (or penalty) in the alignment model which favors 'short' matches (s, t) over 'long' ones. Finally, the Phonetisaurus aligner (Novak et al., 2012) mod- ifies the M2M aligner by adding additional soft constraints. Our own alignment model is, as indicated, su- pervised. We implement a unigram alignment model where we specify sim 1 (u, v) as</p><formula xml:id="formula_6">α · logp((u, v)) + β · logp((|u|, |v|)) +γ · logp(u) + δ · logp(v).</formula><p>Here, logp(z) denotes the log-probability -esti- mated from the training data -of observing the object z, and α, β, γ and δ are parameters. This specification says that the subsequences u and v are similar insofar as (i) u and v have been paired frequently in the training data, (ii) the length of u and the length of v have been paired frequently, (iii)/(iv) u/v by itself is likely. We refer to this unigram alignment model as uni α,β,γ,δ . We also implement a bigram alignment model where we specify sim 2 (u, v), (u , v ) as</p><formula xml:id="formula_7">α · logp (u, v) | (u , v ) +β · logp (|u|, |v|) | (|u |, |v |) +γ · logp u|u + δ · logp v|v .</formula><p>Here, logp(z | z ) denotes the logarithm of the con- ditional probability of observing the object z fol- lowing the object z . We refer to this bigram align- ment model as bi α,β,γ,δ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Transduction systems</head><p>We use two string transduction systems for our ex- periments. The first one is DirecTL+ (Jiampo- jamarn et al., 2010), a discriminative string-to- string translation system incorporating joint n- gram features. DirecTL+ is an extension of the model presented in <ref type="bibr" target="#b15">Jiampojamarn et al. (2008)</ref> which treats string transduction as a source se- quence segmentation and subsequent sequence la- beling task. In addition, we use Phonetisaurus (Novak et al., 2012), a weighted finite state-based joint n-gram model employing recurrent neural network language model N -best rescoring in de- coding. Both systems take aligned pairs of strings as input and from this construct a monotone trans- lation model. <ref type="bibr">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Measuring alignment quality</head><p>We employ two measures of alignment quality. First, we use word accuracy, defined as the frac- tion of correctly aligned sequence pairs in a test sample. This is a very strict measure that penalizes even tiny deviations from the gold standard. Addi- tionally, we measure the edit distance between the true alignment A x,y and the predicted alignmentˆA alignmentˆ alignmentˆA x,y . To implement this, we view the two induced segmentations that constitute an alignment -e.g., (ph,oe,n,i,x) and (f,i,n,I,ks) -as strings includ- ing splitting signs. Thus, we can compute the edit distance between the gold-standard segmented x string and the predicted segmentation, and analo- gously for the y sequence. Then, we define the edit distance between A x,y andˆAandˆ andˆA x,y as the sum of these two string edit distances. For a test sam- ple, we indicate so-defined average edit distance, averaged over all pairs in the sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Alignment quality</head><p>To measure alignment quality for the different sys- tems, for English, we run experiments on sets of size x+5 000, where x = 1 000, 2 000, 5 000, 10 000, and 20 000. For the supervised models, we consider x as the training data and the 5 000 additional string pairs as test data. <ref type="bibr">7</ref> To quantify effects when training data is very little, we let x also range over 100 and 500 string pairs for the supervised models. For the unsupervised models, we simply take all x+5 000 string pairs as data to learn from (but evaluate performance only on the 5 000 string pairs, for comparability). Results are shown in <ref type="table" target="#tab_2">Tables 4, 5</ref>, and 6. We first note <ref type="bibr">(Table 4</ref>) that the unsupervised mod- els perform decently, obtaining accuracy rates of 80% and beyond under appropriate parametriza- tions. We also observe the M2M aligner's de- terioration in performance as we increase its de- grees of freedom (allowing it to match subse- quences of larger length), confirming our previous remarks. The Mpaligner does not suffer from this problem as it penalizes large matches. Phoneti- saurus suffers from the same problems as M2M, but to a lesser degree. Overall, we find that, under optimal parametrizations, Phonetisaurus produces best alignments, followed by Mpalign and M2M. However, peak performances of all three unsu- pervised aligners are close. Unsurprisingly, the supervised alignment models perform better than the unsupervised ones <ref type="table" target="#tab_2">(Tables 5 and 6</ref>). Surpris- ingly, however, they do so with very little train- ing data; fewer than 100 aligned string pairs suf- fice to outperform the unsupervised models under good calibrations. When there is sufficient train- ing data, the supervised models perform splen- didly, with a peak accuracy of 99.43% for the bi- gram alignment model that includes appropriate features (scoring lengths of aligned subsequences, etc.). We also note that the bigram alignment model is almost consistently better than the uni- gram alignment model, with a surplus of about 1% point, depending on specific parametrizations.</p><p>We performed an analogous analysis for the German data. Results are quite similar except that unigram and bigram alignment model have indis- tinguishable performance on the German data, in- dicating (the known fact) that G2P is a more com- plex task in English, apparently not requiring bi- gram alignment models.    While all match-ups in both alignments are plau- sible, the bigram model assigns here higher proba- bility to the correct ed/d match-up in terminal po- sition (consistently favored in the data set), which has a particular meaning there, namely, that of a suffix marker for past tense. <ref type="bibr">8,</ref><ref type="bibr">9</ref> In the German data, there is a single instance where the unigram and bigram alignment model disagree, namely, in the alignment of s-t-o-ff-f-l-a-sch-e/S-t-O-f-f-l-&amp;- S-@, which the unigram model falsely aligns as s-t-o-f-ff-l-a-sch-e/S-t-O-f-f-l-&amp;-S-@; note that in the correct alignment f must follow ff, not vice versa, which depends on context information, e.g., that o/O signifies a short vowel which is followed by a double consonant, not a single consonant.</p><formula xml:id="formula_8">x uni 0,0,1,1 uni 1,0,0,0 uni 1,1,1</formula><p>All remaining errors that the bigram align- ment models commits are, for the best considered parametrization and training set size, typically due to match-up types not seen in the training data, and thus mostly concern foreign names or writings (e.g., Bh-u-tt-o/b-u-t-F, falsely aligned as B-hu-tt- o/b-u-t-F). A few other errors might be corrected when the feature coefficients α, β, γ, δ were opti- mized on a development set rather than set manu- ally. We find no indication that our G2P data, ei- ther for English or German, would further benefit from n-gram alignment models of order n &gt; 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Alignment quality vs. overall G2P performance</head><p>Next, we estimate the relationship between align- ment quality and overall G2P performance (tran- scription accuracy). To this end, for the English data, we use the 5 000 aligned string pairs from the previous experiment on alignment quality and feed them in -as training data -to either Di- recTL+ or Phonetisaurus as outlined in Section 4. We then evaluate G2P performance -in terms of word accuracy (fraction of correctly transcribed strings) -on a distinct test set of size 10 000. <ref type="figure">Figure 1</ref> shows a plot of overall G2P accuracy vs. training set size for the aligner (ranging over the x values in the last section); and a second plot that sketches G2P accuracy as a function of corre- sponding alignment accuracy. We first note that, as the supervised aligner receives more training   data from which to align the 5 000 string pairs, the overall G2P accuracy of both DirecTL+ and Phonetisaurus increase substantially (and as a con- vex function of training set size). Apparently, the better alignments produced by more training data for the particular supervised aligner considered di- rectly translate into better overall G2P accuracy. The other plot in the figure shows that, indeed, there seems to be a linear trend coupling align- ment quality with overall G2P performance. <ref type="table">Table  7</ref> pairs G2P accuracy with alignment accuracy of selected systems, all run in the x = 20 000 set- ting. While, in the table, better alignments do not necessarily imply better overall G2P performance, the two best alignments also lead to the two best overall G2P performances (although, in this case, the second best alignment is paired with the best overall G2P performance); conversely, the worst alignment quality is coupled with the worst over- all G2P performance.</p><note type="other">x Mpalign M2M 2,2 M2M 3,3 M2M 6,6 Phon 2,2 Phon 3,3 Phon 6,6 1000 76.</note><p>Overall, we ran 249 experiments (including the German data) in which we trained DirecTL+ or Phonetisaurus with alignments of specific quali-  <ref type="table">Table 7</ref>: Systems, alignment accuracies of corre- sponding produced alignments and transcription accuracy of Phonetisaurus and DirecTL+ when trained with the respective alignments.</p><p>ties obtained from particularly parametrized align- ers. In each of these cases, we obtained an align- ment quality score and a subsequent overall G2P system performance. The English part of this data is sketched in <ref type="figure">Figure 2</ref>. This figure seems to corroborate the linear relationship (apparently present in <ref type="figure">Figure 1</ref>) between alignment quality and overall G2P system accuracy, particularly, when alignment quality is measured in the more fine- grained metric of edit distance. To formally test </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">2 3 4 5 6 7 8 9 Transduction Accuracy</head><p>Alignment quality (edit distance)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phonetisaurus DirecTL</head><p>Figure 2: Overall G2P accuracy vs. alignment quality. Left: Alignment quality measured in accuracy.</p><p>Right: Alignment quality measured in edit distance. English data only.</p><p>this, we regress overall G2P system performance (measured in word accuracy) on edit distance and other variables. 10 This yielded the coefficients as given in <ref type="table" target="#tab_9">Table 8</ref>; in each case, the goodness-of- fit of the linear model was quite large, with R 2 values above 90% for the English data and about 84% for the German data. Also, the coefficients on alignment quality were highly significantly dif- ferent from zero. The table shows that the co- efficients are on the order of about −3.80% to −4.70%, meaning that, all else being equal, in- creasing alignment quality by 1 edit distance to the gold-standard alignment increases overall G2P by about 3.80 to 4.70%.  So far, we have estimated the effects of align- ment quality on overall G2P system performance for a fixed size of training data, namely, 5 000 aligned string pairs. To see whether this relation- ship changes when we vary the amount of train- ing data, we run several more experiments. In these, we align training sets of sizes 100, 500, 1 000, 2 000, 10 000, 20 000, 40 000 and 60 000 via our several alignment systems. Then we feed the aligned data to the Phonetisaurus system (we omit DirecTL+ here because of its long run times) and compute overall G2P accuracy on a disjoint test set of size 28 000 approximately. This time, we only use the unsupervised aligners and the gold-standard alignments directly, omitting results for our various supervised aligners. Note, how- ever, that these aligners could, in principle, imi- tate the gold-standard alignments with a very high degree of precision, as previously seen.  shows that training G2P systems from the human gold standard alignments in each case yields bet- ter overall G2P transcriptions than training them from either of the three unsupervised alignments considered here. However, we note that the sur- plus over the unsupervised alignments decreases as training set size increases. This may be due to the fact that the unsupervised aligners them- selves create better alignments once they are boot-strapped from larger data sets (cf. <ref type="table">Table 4</ref>). Ad- ditionally, the effect of alignment quality on over- all G2P system performance may simply vanish as training set sizes become large enough because the translation modules can better accomodate 'noisy' data as long as its size is sufficiently large. <ref type="figure">Figure   1</ref>.00</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DirecTL+ Phonetisaurus</head><p>1.02</p><p>1.04</p><p>1.06 Figure 3: Ratio of transcription accuracy when us- ing gold standard alignments (GOLD) and when using alignments generated by T = M2M 3,3 , Mpalign, and Phon 3,3 , respectively, as a function of size of aligned training set.</p><p>3 sketches the decreasing influence of alignment system on overall G2P system performance as size of the aligned data increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have investigated the need for bigram align- ment models and the benefit of supervised align- ment techniques in G2P. We have also quantita- tively estimated the relationship between align- ment quality and overall G2P system performance. We have found that, in English, bigram alignment models do perform better than unigram alignment models on the G2P task (we find almost no dif- ferences between unigram and bigram models for the German sample of G2P data we considered). Moreover, we have found that supervised align- ment techniques may perform considerably better than their unsupervised brethren and that few man- ually aligned training pairs suffice for them to do so. Finally, we have estimated a highly significant impact of alignment quality on overall G2P tran- scription performance and that this relationship is linear in nature. At a particular training size, a linear regression model has estimated that improv- ing alignment quality by 1 edit distance toward the gold standard alignments leads to an 3.80-4.70% increase in G2P transcription accuracy. However, we have also found that the importance of good alignments on G2P accuracy appears to dimish as data set size increases, possibly because the trans- lation modules can accomodate more 'noisy' data in this scenario. As a 'policy' implication, we recommend the use of supervised alignment techniques particu- larly when the size of the G2P corpus is small or when high quality alignments, as an end in them- selves, are required. In this case, constructing a few dozen or few hundred alignments in an unsu- pervised manner and correcting them by hand (to serve as an input for a supervised technique) may be highly beneficial.</p><p>In future work, it may be worthwhile to study the impact of alignment techniques on overall sys- tem performance in other string transduction prob- lems such as transliteration, lemmatization, and spelling error correction.</p><p>Our supervised uni-and bigram aligners are available via https://github.com/ SteffenEger/.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Error analysis Concerning errors that the uni- gram model commits and the bigram model does not, the majority of errors (roughly 80%) involve match-ups of ed/d and d. For example, the uni- gram model aligns as in t w i n k le d t w I N k @l d while the gold-standard alignment is t w i n k l ed t w I N k @l d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4 :Figure 1 :</head><label>41</label><figDesc>Figure 1: Left: Overall G2P accuracy as a function of training set size of supervised aligner uni 1,0,0,0. Right: G2P accuracy as a function of alignment quality (measured in accuracy).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Sample grapheme-phoneme string pairs 
in Combilex, using Combilex notation for the 
phoneme strings. Gold-standard alignments indi-
cated in an intuitive manner. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Unigram model and its alignment accura-
cies in % for various training sizes. 

x bi 0,0,1,1 bi 1,0,0,0 bi 1,1,1,1 
100 
73.96 
58.02 
87.28 
500 
87.62 
85.31 
95.26 
1000 
91.87 
90.73 
97.32 
2000 
93.29 
94.11 
97.96 
5000 
95.58 
97.01 
99.03 
10000 
96.07 
98.12 
99.17 
20000 
97.21 
98.73 
99.43 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Bigram model and its alignment accura-
cies in % for various training sizes. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Coefficients on edit distance in the regres-
sion of G2P accuracy on edit distance and further 
variables. For German, DirecTL+ is omitted due 
to its long run times. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="true"><head>Table 9</head><label>9</label><figDesc></figDesc><table>M2M 3,3 Mpalign Phon 3,3 Gold 
100 
5.38 
6.43 
0.19 
9.60 
500 
16.80 
22.43 
5.08 
23.93 
1K 
25.79 
31.46 
18.70 
33.37 
2K 
35.31 
42.01 
37.74 
43.64 
10K 
58.44 
64.05 
63.06 
64.60 
20K 
67.70 
71.70 
71.51 
72.21 
40K 
74.69 
78.45 
78.13 
78.65 
60K 
78.00 
81.07 
80.92 
81.17 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table>Overall G2P accuracy in % as a function 
training size of aligned data and alignment system. 

</table></figure>

			<note place="foot" n="1"> This is sometimes denoted in the manner M-N (e.g., 32, 1-0), indicating that M characters of one string may be matched up with N characters of the other string. Analogously, we could write here s-t rather than (s, t). 2 Here, 1 k denotes the unit vector of dimension k.</note>

			<note place="foot" n="3"> We denote by x(a : b) the substring xaxa+1 · · · x b of the string x1x2 · · · xt.</note>

			<note place="foot" n="4"> In Cotterell et al. (2014), context influences alignments, so that the approach goes beyond the unigram model sketched in (1) (but does not allow for many-to-many match-ups). The contextual dependencies in this model are set up differently from the bigram dependencies in our paper.</note>

			<note place="foot" n="5"> See also the discussion in (Goldwater et al., 2006) for the related word segmentation problem.</note>

			<note place="foot" n="6"> We run both systems with parameters determined by some manual tuning, without trying to systematically optimize their individual performances, however.</note>

			<note place="foot" n="7"> For all our below experiments involving the supervised aligners, we set S to a (&apos;pessimistically&apos; large) value of {(a, b) | 1 ≤ a ≤ 6, 1 ≤ b ≤ 6}. Also, for the bigram models, we add special sequence boundary markers.</note>

			<note place="foot" n="8"> Similar cases are, e.g., alignments of the type f-ee-d-ba-ck/f-i-d-b-a-k, which the unigram model falsely aligns as f-e-ed-b-a-ck/f-i-d-b-a-k. Here, too, the unigram is unable to account for the almost exclusive terminal position of the ed/d match-up in the data. 9 Other errors involve &apos;unusual/foreign&apos; spelling/pronunciation pairs such as Ph-oe-n-i-c-ia/f-@n-i-S-@ (wrongly aligned as Ph-o-en-i-c-ia/f-@-n-i-S-@ by the unigram model) or m-a-d-e-m-oi-s-e-ll-e-&apos;s/m-a-d-@-mw@-z-E-l-0-z (m-a-d-e-m-o-i-s-e-ll-e-&apos;s/m-a-d-@-m-w-@z-E-l-0-z), where the bigram alignment model has apparently gathered the more appropriate statistics.</note>

			<note place="foot" n="10"> These include binary dummy variables for the specific systems as well as alignment consistency and its squaremeasured in conditional entropy H(Y |X) (Pervouchine et al., 2009)-in the regression.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>I thank three anonymous reviewers and Tim vor der Brück for valuable suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The CELEX2 lexical database. ldc96l14</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Piepenbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gulikers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Jointsequence models for grapheme-to-phoneme conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Bisani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="434" to="451" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An improved error model for noisy channel spelling correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL &apos;00</title>
		<meeting>the 38th Annual Meeting on Association for Computational Linguistics, ACL &apos;00<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="286" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stochastic contextual edit distance and probabilistic FSTs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Baltimore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="625" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Variable-length sequence matching for phonetic transcription using joint multigrams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Deligne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franois</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bimbot</surname></persName>
		</author>
		<editor>EUROSPEECH. ISCA</editor>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent-variable modeling of string transductions with finite-state methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1080" to="1089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">S-restricted monotone alignments: Algorithm, search space, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Eger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING&apos;12</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="781" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sequence alignment with arbitrary steps and further generalizations, with applications to alignments in linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Eger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="page" from="287" to="304" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving g2p from wiktionary and other (web) resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Eger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>accepted</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiple many-to-many sequence alignment for combining string-valued variables: A G2P experiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Eger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="909" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Better alignments = better translations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo</forename><surname>Graa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computational Linguistics</title>
		<editor>Kathleen McKeown, Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui</editor>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="986" to="993" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Contextual dependencies in unsupervised word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="673" to="680" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Letter-phoneme alignment: An exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sittichai</forename><surname>Jiampojamarn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Kondrak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computational Linguistics</title>
		<meeting><address><addrLine>Sandra Carberry, and Stephen Clark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="780" to="788" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Applying many-to-many alignments and hidden markov models to letter-to-phoneme conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Sittichai Jiampojamarn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarek</forename><surname>Kondrak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sherif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<title level="m">Proceedings of the Main Conference</title>
		<meeting>the Main Conference<address><addrLine>Rochester, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="372" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint processing and discriminative training for letter-to-phoneme conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Sittichai Jiampojamarn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kondrak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT, pages 905-913, Columbus</title>
		<meeting>ACL-08: HLT, pages 905-913, Columbus<address><addrLine>Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Integrating joint n-gram features into a discriminative training framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Sittichai Jiampojamarn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kondrak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="697" to="700" />
		</imprint>
	</monogr>
	<note>HLTNAACL</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A new algorithm for the alignment of phonetic sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Kondrak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference, NAACL 2000</title>
		<meeting>the 1st North American Chapter of the Association for Computational Linguistics Conference, NAACL 2000<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="288" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unconstrained many-to-many alignment for automatic pronunciation annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keigo</forename><surname>Kubo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiromichi</forename><surname>Kawanami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Saruwatari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyohiro</forename><surname>Shikano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Asia-Pacific</surname></persName>
		</author>
		<title level="m">Signal and Information Processing Association Annual Summit and Conference 2011 (APSIPA2011)</title>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Binary Codes Capable of Correcting Deletions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vi Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Insertions and Reversals. Soviet Physics Doklady</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">707</biblScope>
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Contrastive unsupervised word alignment with non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI 2015</title>
		<meeting>AAAI 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discriminative word alignment by linear modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="303" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A general method applicable to the search for similarities in the amino acid sequence of two proteins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><forename type="middle">D</forename><surname>Needleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wunsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="443" to="453" />
			<date type="published" when="1970-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">WFST-based grapheme-to-phoneme conversion: Open source tools for alignment, model-building and decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><forename type="middle">R</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobuaki</forename><surname>Minematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keikichi</forename><surname>Hirose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing</title>
		<meeting>the 10th International Workshop on Finite State Methods and Natural Language Processing<address><addrLine>Donostia-San Sebastin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="45" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Transliteration alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Pervouchine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
	<note>ACL &apos;09. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Grapheme-to-phoneme conversion using long short-term memory recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Peng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Hasim Sak, and Franoise Beaufays</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust LTS rules with the Combilex speech technology lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Korin</forename><surname>Richmond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1295" to="1298" />
		</imprint>
	</monogr>
	<note>ISCA</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning string-edit distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sven Ristad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yianilos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="522" to="532" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Substringbased transliteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarek</forename><surname>Sherif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Kondrak</surname></persName>
		</author>
		<editor>John A. Carroll, Antal van den Bosch, and Annie Zaenen</editor>
		<imprint>
			<date type="published" when="2007" />
			<publisher>The Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Algorithms for approximate string matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Esko Ukkonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="100" to="118" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Computerized correction of phonographic errors. Computers and the Humanities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Véronis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="43" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joint generation of transliterations from multiple representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Kondrak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="943" to="952" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
