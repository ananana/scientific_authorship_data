<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Reinforcement Learning for Mention-Ranking Coreference Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
							<email>kevclark@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<email>manning@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Reinforcement Learning for Mention-Ranking Coreference Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2256" to="2262"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Coreference resolution systems are typically trained with heuristic loss functions that require careful tuning. In this paper we instead apply reinforcement learning to directly optimize a neural mention-ranking model for coreference evaluation metrics. We experiment with two approaches: the REINFORCE policy gradient algorithm and a reward-rescaled max-margin objective. We find the latter to be more effective, resulting in a significant improvement over the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Coreference resolution systems typically operate by making sequences of local decisions (e.g., adding a coreference link between two mentions). How- ever, most measures of coreference resolution per- formance do not decompose over local decisions, which means the utility of a particular decision is not known until all other decisions have been made.</p><p>Due to this difficulty, coreference systems are usually trained with loss functions that heuristically define the goodness of a particular coreference deci- sion. These losses contain hyperparameters that are carefully selected to ensure the model performs well according to coreference evaluation metrics. This complicates training, especially across different lan- guages and datasets where systems may work best with different settings of the hyperparameters.</p><p>To address this, we explore using two variants of reinforcement learning to directly optimize a coref- erence system for coreference evaluation metrics. In particular, we modify the max-margin coreference objective proposed by <ref type="bibr" target="#b23">Wiseman et al. (2015)</ref> by in- corporating the reward associated with each coref- erence decision into the loss's slack rescaling. We also test the REINFORCE policy gradient algorithm <ref type="bibr" target="#b22">(Williams, 1992)</ref>.</p><p>Our model is a neural mention-ranking model. Mention-ranking models score pairs of mentions for their likelihood of coreference rather than compar- ing partial coreference clusters. Hence they operate in a simple setting where coreference decisions are made independently. Although they are less expres- sive than entity-centric approaches to coreference (e.g., <ref type="bibr" target="#b10">Haghighi and Klein, 2010)</ref>, mention-ranking models are fast, scalable, and simple to train, caus- ing them to be the dominant approach to coreference in recent years ( <ref type="bibr" target="#b23">Wiseman et al., 2015)</ref>. Having independent actions is partic- ularly useful when applying reinforcement learning because it means a particular action's effect on the final reward can be computed efficiently.</p><p>We evaluate the models on the English and Chi- nese portions of the CoNLL 2012 Shared Task. The REINFORCE algorithm is competitive with a heuris- tic loss function while the reward-rescaled objective significantly outperforms both <ref type="bibr">1</ref> . We attribute this to reward rescaling being well suited for a ranking task due to its max-margin loss as well as benefiting from directly optimizing for coreference metrics. Error analysis shows that using the reward-rescaling loss results in a similar number of mistakes as the heuris- tic loss, but the mistakes tend to be less severe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Mention-Ranking Model</head><p>We use the neural mention-ranking model described in <ref type="bibr" target="#b3">Clark and Manning (2016)</ref>, which we briefly go over in this section. Given a mention m and candidate antecedent c, the mention-ranking model produces a score for the pair s(c, m) indicating their compatibility for coreference with a feedforward neural network. The candidate antecedent may be any mention that occurs before m in the document or NA, indicating that m has no antecedent.</p><p>Input Layer. For each mention, the model extracts various words (e.g., the mention's head word) and groups of words (e.g., all words in the mention's sentence) that are fed into the neural network. Each word is represented by a vector w i ∈ R dw . Each group of words is represented by the average of the vectors of each word in the group. In addition to the embeddings, a small number of additional fea- tures are used, including distance, string matching, and speaker identification features. See <ref type="bibr" target="#b3">Clark and Manning (2016)</ref> for the full set of features and an ablation study.</p><p>These features are concatenated to produce an I- dimensional vector h 0 , the input to the neural net- work. If c = NA, features defined over pairs of mentions are not included. For this case, we train a separate network with an identical architecture to the pair network except for the input layer to pro- duce anaphoricity scores.</p><p>Hidden Layers. The input gets passed through three hidden layers of rectified linear (ReLU) units <ref type="bibr" target="#b16">(Nair and Hinton, 2010)</ref>. Each unit in a hidden layer is fully connected to the previous layer:</p><formula xml:id="formula_0">h i (c, m) = max(0, W i h i−1 (c, m) + b i ) where W 1 is a M 1 × I weight matrix, W 2 is a M 2 × M 1 matrix, and W 3 is a M 3 × M 2 matrix.</formula><p>Scoring Layer. The final layer is a fully connected layer of size 1:</p><formula xml:id="formula_1">s(c, m) = W 4 h 3 (c, m) + b 4</formula><p>where W 4 is a 1 × M 3 weight matrix. At test time, the mention-ranking model links each mention with its highest scoring candidate antecedent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Algorithms</head><p>Mention-ranking models are typically trained with heuristic loss functions that are tuned via hyperpa- rameters. These hyperparameters are usually given as costs for different error types, which are used to bias the coreference system towards making more or fewer coreference links. In this section we first describe a heuristic loss function incorporating this idea from <ref type="bibr" target="#b23">Wiseman et al. (2015)</ref>. We then propose new training procedures based on reinforcement learning that instead directly optimize for corefer- ence evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Heuristic Max-Margin Objective</head><p>The heuristic loss from Wiseman et al. is governed by the following error types, which were first pro- posed by .</p><p>Suppose the training set consists of N mentions m 1 , m 2 , ..., m N . Let C(m i ) denote the set of can- didate antecedents of a mention m i (i.e., mentions preceding m i and NA) and T (m i ) denote the set of true antecedents of m i (i.e., mentions preceding m i that are coreferent with it or {NA} if m i has no an- tecedent). Then we define the following costs for linking m i to a candidate antecedent c ∈ C(m i ):</p><formula xml:id="formula_2">∆ h (c, m i ) =            α FN if c = NA ∧ T (m i ) = {NA} α FA if c = NA ∧ T (m i ) = {NA} α WL if c = NA ∧ a / ∈ T (m i ) 0 if a ∈ T (m i )</formula><p>for "false new," "false anaphor," "wrong link", and correct coreference decisions. The heuristic loss is a slack-rescaled max-margin objective parameterized by these error costs. LetˆtLetˆ Letˆt i be the highest scoring true antecedent of m i :</p><formula xml:id="formula_3">ˆ t i = argmax c∈C(m i )∧∆ h (c,m i )=0 s(c, m i )</formula><p>Then the heuristic loss is given as</p><formula xml:id="formula_4">L(θ) = N i=1 max c∈C(m i ) ∆ h (c, m i )(1 + s(c, m i ) − s( ˆ t i , m i ))</formula><p>Finding Effective Error Penalties.</p><p>We fix α WL = 1.0 and search for α FA and α FN out of {0.1, 0.2, ..., 1.5} with a variant of grid search. Each new trial uses the unexplored set of hyperparame-ters that has the closest Manhattan distance to the best setting found so far on the dev set. The search is halted when all immediate neighbors (within 0.1 distance) of the best setting have been explored. We found (α FN , α FA , α WL ) = (0.8, 0.4, 1.0) to be best for English and (α FN , α FA , α WL ) = (0.8, 0.5, 1.0) to be best for Chinese on the CoNLL 2012 data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Reinforcement Learning</head><p>Finding the best hyperparameter settings for the heuristic loss requires training many variants of the model, and at best results in an objective that is correlated with coreference evaluation metrics. To address this, we pose mention ranking in the rein- forcement learning framework <ref type="bibr" target="#b20">(Sutton and Barto, 1998</ref>) and propose methods that directly optimize the model for coreference metrics.</p><p>We can view the mention-ranking model as an agent taking a series of actions a 1:T = a 1 , a 2 , ..., a T , where T is the number of mentions in the current document. Each action a i links the ith mention in the document m i to a candidate an- tecedent. Formally, we denote the set of actions available for the ith mention as A i = {(c, m i ) : c ∈ C(m i )}, where an action (c, m) adds a corefer- ence link between mentions m and c. The mention- ranking model assigns each action the score s(c, m) and takes the highest-scoring action at each step.</p><p>Once the agent has executed a sequence of ac- tions, it observes a reward R(a 1:T ), which can be any function. We use the B 3 coreference metric for this reward ( <ref type="bibr" target="#b0">Bagga and Baldwin, 1998</ref>). Although our system evaluation also includes the MUC <ref type="bibr" target="#b21">(Vilain et al., 1995)</ref> and CEAF φ 4 ( <ref type="bibr" target="#b13">Luo, 2005</ref>) metrics, we do not incorporate them into the loss because MUC has the flaw of treating all errors equally and CEAF φ 4 is slow to compute.</p><p>Reward Rescaling. Crucially, the actions taken by a mention-ranking model are independent. This means it is possible to change any action a i to a dif- ferent one a i ∈ A i and see what reward the model would have gotten by taking that action instead: R(a 1 , ..., a i−1 , a i , a i+1 , ..., a T ). We use this idea to improve the slack-rescaling parameter ∆ in the max- margin loss L(θ). Instead of setting its value based on the error type, we compute exactly how much each action hurts the final reward:</p><formula xml:id="formula_5">∆ r (c, m i ) = −R(a 1 , ..., (c, m i ), ..., a T ) + max a i ∈A i R(a 1 , ..., a i , ..., a T )</formula><p>where a 1:T is the highest scoring sequence of actions according to the model's current parameters. Other- wise the model is trained in the same way as with the heuristic loss.</p><p>The REINFORCE Algorithm. We also explore using the REINFORCE policy gradient algorithm <ref type="bibr" target="#b22">(Williams, 1992)</ref>. We can define a probability dis- tribution over actions using the mention-ranking model's scoring function as follows:</p><formula xml:id="formula_6">p θ (a) ∝ e s(c,m)</formula><p>for any action a = (c, m). The REINFORCE algo- rithm seeks to maximize the expected reward</p><formula xml:id="formula_7">J(θ) = E [a 1:T ∼p θ ] R(a 1:T )</formula><p>It does this through gradient ascent. Computing the full gradient is prohibitive because of the expecta- tion over all possible action sequences, which is ex- ponential in the length of the sequence. Instead, it gets an unbiased estimate of the gradient by sam- pling a sequence of actions a 1:T according to p θ and computing the gradient only over the sample. We take advantage of the independence of actions by using the following gradient estimate, which has lower variance than the standard REINFORCE gradi- ent estimate.</p><formula xml:id="formula_8">θ J(θ) ≈ T i=1 a i ∈A i [ θ p θ (a i )](R(a 1 , ..., a i , ..., a T ) − b i )</formula><p>where b i is a baseline used to reduce the variance, which we set to E a i ∈A i ∼p θ R(a 1 , ..., a i , ..., a T ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>We ( <ref type="bibr" target="#b11">Hinton and Tieleman, 2012</ref>), dropout (Hinton et al., 2012) with a rate of 0.5, and pretraining with the all pairs classification and top pairs classification tasks. However, we improve on the previous system by us- ing using better mention detection, more effective hyperparameters, and more epochs of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>We compare the heuristic loss, REINFORCE, and re- ward rescaling approaches on both datasets. We find that REINFORCE does slightly better than the heuris- tic loss, but reward rescaling performs significantly better than both across both languages. We attribute the modest improvement of REIN- FORCE to it being poorly suited for a ranking task. During training it optimizes the model's per- formance in expectation, but at test-time it takes the most probable sequence of actions. This mismatch occurs even at the level of an individual decision: the model only links the current mention to a single antecedent, but is trained to assign high probability to all correct antecedents. We believe the benefit of REINFORCE being guided by coreference evalu- ation metrics is offset by this disadvantage, which does not occur in the max-margin approaches. The reward-rescaled max-margin loss combines the best of both worlds, resulting in superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Benefits of Reinforcement Learning</head><p>In this section we examine the reward-based cost function ∆ r and perform error analysis to determine CosW . how reward rescaling improves the mention-ranking model's accuracy.</p><p>Comparison with Heuristic Costs. We compare the reward-based cost function ∆ r with the error types used in the heuristic loss. For English, the average value of ∆ r is 0.79 for FN errors and 0.38 for FA errors when the costs are scaled so the aver- age value of a WL error is 1.0. These are very close to the hyperparameter values (α FN , α FA , α WL ) = (0.8, 0.4, 1.0) found by grid search. However, there is a high variance in costs for each error type, sug- gesting that using a fixed penalty for each type as in the heuristic loss is insufficient (see <ref type="figure" target="#fig_1">Figure 1)</ref>.</p><p>Avoiding Costly Mistakes. Embedding the costs of actions into the loss function causes the reward- rescaling model to prioritize getting the more im- portant coreference decisions (i.e., the ones with the biggest impact on the final score) correct. As a   result, it makes fewer costly mistakes at test time. Costly mistakes often involve large clusters of men- tions: incorrectly combining two coreference clus- ters of size ten is much worse than incorrectly com- bining two clusters of size one. However, the cost of an action also depends on other factors like the number of errors already present in the clusters and the utilities of the other available actions. <ref type="table" target="#tab_3">Table 2</ref> shows the breakdown of errors made by the heuristic and reward-rescaling models on the test set. The reward-rescaling model makes slightly more errors, meaning its improvement in perfor- mance must come from its errors being less severe.</p><p>Example Improvements. <ref type="table" target="#tab_2">Table 3</ref> shows two classes of mentions where the reward-rescaling loss particularly improves over the heuristic loss.</p><p>Proper nouns have a higher average cost for "false new" errors (0.90) than other mentions types (0.77). This is perhaps because proper nouns are important for connecting clusters of mentions far apart in a document, so incorrectly linking a proper noun to NA could result in a large decrease in recall. Be- cause it more heavily weights these high-cost errors during training, the reward-rescaling model makes fewer "false new" errors for proper nouns than the heuristic loss. Although there is an increase in other kinds of errors as a result, most of these are low-cost "false anaphoric" errors.</p><p>The pronouns in the "telephone conversation" genre often group into extremely large coreference clusters, which means a "wrong link" error can have a very large negative effect on the score. This is re- flected in its high average cost of 1.21. After prior- itizing these examples during training, the reward- rescaling model creates significantly fewer wrong links than the heuristic loss, which is trained using a fixed cost of 1.0 for all wrong links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Mention-ranking models have been widely used for coreference resolution <ref type="bibr" target="#b6">(Denis and Baldridge, 2007;</ref><ref type="bibr" target="#b19">Rahman and Ng, 2009;</ref>). These models are typically trained with heuristic loss functions that assign costs to different error types, as in the heuristic loss we describe in Sec- tion 3.1 <ref type="bibr" target="#b9">(Fernandes et al., 2012;</ref><ref type="bibr" target="#b1">Björkelund and Kuhn, 2014;</ref><ref type="bibr" target="#b23">Wiseman et al., 2015;</ref><ref type="bibr" target="#b15">Martschat and Strube, 2015;</ref><ref type="bibr" target="#b24">Wiseman et al., 2016)</ref>.</p><p>To the best of our knowledge reinforcement learn- ing has not been applied to coreference resolution before. However, imitation learning algorithms such as SEARN <ref type="bibr" target="#b4">(Daumé III et al., 2009)</ref> have been used to train coreference resolvers <ref type="bibr" target="#b5">(Daumé III, 2006;</ref><ref type="bibr" target="#b14">Ma et al., 2014;</ref><ref type="bibr" target="#b2">Clark and Manning, 2015)</ref>. These algo- rithms also directly optimize for coreference eval- uation metrics, but they require an expert policy to learn from instead of relying on rewards alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose using reinforcement learning to directly optimize mention-ranking models for coreference evaluation metrics, obviating the need for hyperpa- rameters that must be carefully selected for each particular language, dataset, and evaluation metric. Our reward-rescaling approach also increases the model's accuracy, resulting in significant gains over the current state-of-the-art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Density plot of the costs ∆r associated with different error types on the English CoNLL 2012 test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : Examples of classes of mention on which the reward-rescaling loss significantly improves upon the heuristic loss due to its reward-based cost function. Reported numbers are from the English CoNLL 2012 test set.</head><label>3</label><figDesc></figDesc><table>Model 

FN 
FA 
WL 

Heuristic Loss 
1719 
1956 
1258 
Reward Rescaling 
1725 
1994 
1247 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Number of "false new," "false anaphoric," and 
"wrong link" errors produced by the models on the English 
CoNLL 2012 test set. 

</table></figure>

			<note place="foot" n="1"> Code and trained models are available at https://github.com/clarkkev/deep-coref.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Kelvin Guu, William Hamilton, Will Mon-roe, and the anonymous reviewers for their thought-ful comments and suggestions. This work was sup-ported by NSF Award IIS-1514268.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Algorithms for scoring coreference chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Breck</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="563" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning structured perceptrons for coreference resolution with latent antecedents and non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association of Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Entitycentric coreference resolution with model stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving coreference resolution with entity-level distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Search-based structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="325" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Practical structured learning techniques for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>Los Angeles, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Southern California</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A ranking approach to pronoun resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1588" to="1593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Easy victories and uphill battles in coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1971" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Decentralized entity-level modeling for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David Leo Wright</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="114" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Latent structure perceptron with feature induction for unrestricted coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero Nogueira Dos</forename><surname>Eraldo Rezende Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruy Luiz</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Milidiú</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning-Shared Task</title>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning-Shared Task</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Coreference resolution in a modular, entity-centered model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology and North American Association for Computational Linguistics (HLT-NAACL)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="385" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lecture 6.5-RmsProp: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing coadaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On coreference resolution performance metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Prune-and-score: Learning for greedy coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janardhan</forename><surname>Rao Doppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">, J Walker</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashanth</forename><surname>Mannem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Latent structures for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Martschat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="405" to="418" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Conll-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning-Shared Task</title>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning-Shared Task</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A multipass sieve for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoung</forename><surname>Karthik Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Sudarshan Rangarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="492" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Supervised models for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Altaf</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="968" to="977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A modeltheoretic coreference scoring scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Vilain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Aberdeen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th conference on Message understanding</title>
		<meeting>the 6th conference on Message understanding</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
	<note>Dennis Connolly, and Lynette Hirschman</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning anaphoricity and antecedent ranking features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association of Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning global features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology and North American Association for Computational Linguistics (HLT-NAACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
