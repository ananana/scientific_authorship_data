<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Arabic Diacritization with Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Massachusetts Institute of Technology Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Massachusetts Institute of Technology Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Arabic Diacritization with Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Arabic, Hebrew, and similar languages are typically written without diacritics, leading to ambiguity and posing a major challenge for core language processing tasks like speech recognition. Previous approaches to automatic diacritization employed a variety of machine learning techniques. However, they typically rely on existing tools like morphological analyzers and therefore cannot be easily extended to new genres and languages. We develop a recurrent neural network with long short-term memory layers for predicting diacritics in Arabic text. Our language-independent approach is trained solely from diacritized text without relying on external tools. We show experimentally that our model can rival state-of-the-art methods that have access to additional resources.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hebrew, Arabic, and other languages based on the Arabic script usually represent only consonants in writing and do not mark vowels. In such writ- ing systems, diacritics are used for marking short vowels, gemination, and other phonetic units. In practice, diacritics are usually restricted to specific settings such as language teaching or to religious texts. Faced with a non-diacritized word, readers infer missing diacritics based on their prior knowl- edge and the context of the word in order to re- solve ambiguities. For example, <ref type="bibr">Maamouri</ref>  level. In practice, a morphological analyzer like MADA ( <ref type="bibr" target="#b5">Habash et al., 2009)</ref> produces at least 13 different diacritized forms for this word, a subset of which is shown in <ref type="table">Table 1</ref>. <ref type="bibr">1</ref> The ambiguity in Arabic orthography presents a problem for many language processing tasks, in- cluding acoustic modeling for speech recognition, language modeling, text-to-speech, and morpho- logical analysis. Automatic methods for diacriti- zation aim to restore diacritics in a non-diacritized text. While earlier work used rule-based meth- ods, more recent studies attempted to learn a di- acritization model from diacritized text. A vari- ety of methods have been used, including hidden Markov models, finite-state transducers, and max- imum entropy -see the review in <ref type="bibr" target="#b12">(Zitouni and Sarikaya, 2009</ref>) -and more recently, deep neu- ral networks <ref type="bibr" target="#b1">(Al Sallab et al., 2014</ref>). In addi- tion to learning from diacritized text, these meth- ods typically rely on external resources such as part-of-speech taggers and morphological analyz- ers like the MADA tool <ref type="bibr" target="#b3">(Habash and Rambow, 2007)</ref>. However, building such resources is a labor-intensive task and cannot be easily extended to new languages, dialects, and domains. In this work, we propose a diacritization method based solely on diacritized text. We treat the prob- lem as a sequence classification task, where each character has a corresponding diacritic label. The sequence is modeled with a recurrent neural net- work whose input is a sequence of characters and whose output is a probability distribution over the diacritics. Any RNN architecture can be used in this framework; here we focus on long short-term memory (LSTM) networks, which have shown re- cent success in a number of NLP tasks. We exper- iment with several architectures and show that we can achieve state-of-the-art results, without rely- ing on external resources. Error analysis demon- strates the benefit of using LSTM over simpler neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diacritic Transliteration Transcription</head><formula xml:id="formula_0">a /a/ u /u/ i /i/ F /an/ N /un K /in/ ~ Gemination o No vowel</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Linguistic Background</head><p>Languages based on the Arabic script typically employ an abjad writing system, where each sym- bol represents a consonant while vowels and other phonetic units, commonly known as diacritics, are usually omitted in writing. In modern standard and classical Arabic, these include the short vow- els a, u, and i, the case endings F, N, and K, the gemination marker ~, and the silence marker o. 2  <ref type="figure">Figure 1</ref>: An illustration of our network topology.</p><p>Let w = (w 1 , ..., w T ) denote a sequence of char- acters, where each character w t is associated with a label l t . A label may represent 0, 1, or more di- acritics, depending on the language. Assume fur- ther that each character w in the alphabet is rep- resented as a real-valued vector x w . This charac- ter embedding may be learned during training or fixed.</p><p>Our neural network has the following structure, illustrated in Figure 1:</p><p>• Input layer: mapping the letter sequence w to a vector sequence x.</p><p>• Hidden layer(s): mapping the vector se- quence x to a hidden sequence h.</p><p>• Output layer: mapping each hidden vector h t to a probability distribution over labels l.</p><p>During training, each sequence is fed into this network to create a prediction for each character. As errors are back-propagated down the network, the weights at each layer are updated. During test- ing, the learned weights are used in a forward step to compute a prediction over the labels. We always take the best predicted label for evaluation.</p><p>Hidden layer Our main system relies on long short-term memory networks (LSTM) (Hochre- iter and Schmidhuber, 1997; <ref type="bibr" target="#b2">Graves et al., 2013</ref>). Here we describe a single LSTM layer and refer to <ref type="bibr" target="#b2">Graves et al. (2013)</ref> for the extension to bidi- rectional LSTM (B-LSTM) and to multiple layers. The LSTM computes the hidden representation for Train Dev Test Words 470K 81K 80K Letters 2.6M 438K 434K <ref type="table">Table 3</ref>: Arabic diacritization corpus statistics. input x t with the following iterative process:</p><formula xml:id="formula_1">i t = σ(W xi x t + W hi h t−1 + W ci c t−1 + b i ) f t = σ(W xf x t + W hf h t−1 + W cf c t−1 + b f ) c t = f t c t−1 + i t tanh(W xc x t + W hc h t−1 + b c ) o t = σ(W xo x t + W ho h t−1 + W co c t + b o ) h t = o t tanh(c t )</formula><p>where σ is the sigmoid function, is element- wise multiplication, and i, f , o, and c are input, forget, output, and memory cell activation vectors. The crucial element is the memory cell c that is able to store and reuse long term dependencies over the sequence. The W matrices and b bias vec- tors are learned during training.</p><p>Implementation details The input layer maps the character sequence to a sequence of letter vec- tors, initialized randomly. We also tried initializ- ing with letter vectors trained from raw text with word2vec ( <ref type="bibr" target="#b8">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b9">Mikolov et al., 2013b</ref>), but did not notice any improvement, prob- ably due to the small letter vocabulary size. The input layer also stacks previous and future letter vectors, enabling the model to learn contextual in- formation. We use a letter embedding size of 10 and a window size of 5 characters, so the input size is 110.</p><p>We experiment with several types of hidden lay- ers, ranging from one feed-forward layer to mul- tiple B-LSTM layers. We also add a linear pro- jection after the input layer. This has the effect of learning a new representation for the letter embed- dings. The output layer is a Softmax over labels:   </p><formula xml:id="formula_2">P (l|w t ) = exp(y t [l]) l exp(y t [l ])</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Data We extract diacritizied and non-diacritized texts from the Arabic treebank, following the Train/Dev/Test split in ( <ref type="bibr" target="#b12">Zitouni and Sarikaya, 2009)</ref>. <ref type="table">Table 3</ref> provides statistics for the corpus. Every character in our corpus has a label cor- responding to 0, 1, or 2 diacritics, in the case of the gemination marker combining with another di- acritic. Thus the label set almost doubles. We opted for this formulation due to its simplicity and generalizability to other languages, even though previous work reported improved results by first predicting gemination and then all other diacrit- ics ( <ref type="bibr" target="#b12">Zitouni and Sarikaya, 2009)</ref>.</p><p>Results <ref type="table" target="#tab_4">Table 4</ref> shows the results of our models on the Dev set in terms of the diacritic error rate (DER). Clearly, LSTM models perform much bet- ter than simple feed-forward networks. To make the comparison fair, we increased the number of parameters in the feed-forward model to match that of the LSTM. In this setting, the LSTM is still much better, indicating that it is far more success- ful at exploiting the larger parameter set. Interest- ingly, the bidirectional LSTM works better than a unidirectional one, despite having less parameters. Finally, deeper models achieve the best results.</p><p>On the Test set <ref type="table" target="#tab_5">(Table 5)</ref>, our 3-layer B-LSTM model beats the lexical variant of Zitouni and Sarikaya (2009) by 3.25% DER, a 40% error reduction. Moreover, we outperform their best model, which also used a segmenter and part-of- speech tagger. This shows that our model can ef- fectively learn to diacritize without relying on any resources other than diacritized text.</p><p>Finally, some studies report work on a Train/Test data split, without a dedicated Dev set ( <ref type="bibr" target="#b13">Zitouni et al., 2006;</ref><ref type="bibr" target="#b3">Habash and Rambow, 2007;</ref><ref type="bibr" target="#b10">Rashwan et al., 2011;</ref><ref type="bibr" target="#b1">Al Sallab et al., 2014</ref>). We were reluctant to follow this setting so we per- formed all development on the Dev set of <ref type="bibr" target="#b12">(Zitouni and Sarikaya, 2009</ref>). Still, we ran our best model on the Train/Test split and achieved a DER of 5.39% on all diacritics and 8.74% on case end- ings. The first result is behind the state-of-the- art <ref type="bibr" target="#b1">(Al Sallab et al., 2014</ref>) by 2% but the second one is better by 3%. Given that we did not tune the system for this data set, this result is encouraging.</p><p>Error Analysis A quantitative analysis of the er- rors produced by one of our models on the Dev set is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. The heat map denotes the number of errors produced. The major source of errors comes from confusing the short vowels a, i, and u, among themselves and with no diacritic. This is expected due to the high rate of short vow- els in Arabic compared to other diacritics. It also explains why methods that take the confusion ma- trix into account in their classification algorithm do quite well <ref type="bibr" target="#b1">(Al Sallab et al., 2014</ref>).</p><p>We also analyzed some errors qualitatively. <ref type="figure" target="#fig_1">Fig- ure 3</ref> shows the errors produced by several of our diacritization models on a sample sentence. In-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Diacritization Gold</p><p>AiEotabara Almudiyru AlEAm~u l " Aln~ahAri " juborAn tuwayoniy~ Aan Alt~a$okiylAti AlqaDA}iy~apa jA'at lita- moyiyEi milaf~i maHaT~api Al " Aim . tiy . fiy . " Feed- forward AiEotabara Almudiyru AlEAm~u l " Aln~ahAr " jaborAn tuwayoniy Aan Alt~a$okiylAti AlqaDA}iy~api jA'at litamayiyEi malaf~i maHaT~api Al " A m . tiy . fiy . " LSTM AiEotabara Almudiyru AlEAm~u l " Aln~ahAri " juborAn t w yoniy Ain Alt~a$okiylAti AlqaDA}iy~apa jA'at litamoyiyEa milaf~i maHaT~api Al " Aim . tiy . fiy . " B-LSTM AiEotabara Almudiyru AlEAm~u l " Aln~ahAri " juborAn t wayoniy Aan Alt~a$okiylAti AlqaDA}iy~apa jA'at lita- moyiyEi milaf~i maHaT~api Al " Aim . tiy . fiy . " terestingly, the simple feed-forward model fails to predict the correct case ending on the word AlqaDA}iy~ap ("judicial"), while both LSTM models succeed. This may indicate that LSTM in- deed captures the kind of long-distance dependen- cies that are responsible for case marking. Other errors are more difficult to explain, but note that all models struggle with the proper name tuwayoniy~ ("Tueini"), which is difficult to solve without ex- ternal resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we develop a recurrent neural net- work that predicts diacritics in non-diacritized texts. Our model is language agnostic: it is trained solely from diacritized text without relying on additional resources. Using LSTM units, we demonstrate that our model can effectively learn to diacritize Arabic texts and rivals state-of-the-art methods that rely on language-specific tools.</p><p>In future work, we intend to incorporate our di- acritization system in a speech recognizer. Recent work has shown improvements in Arabic speech recognition by diacritizing with MADA <ref type="bibr" target="#b0">(Al Hanai and Glass, 2014</ref>). Since creating such tools is a labor-intensive task, we expect our diacritization approach to promote the development of speech recognizers for other languages and dialects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A confusion matrix of errors made by our system. "#" marks word boundary. Best viewed in color.</figDesc><graphic url="image-5.png" coords="4,72.00,62.81,229.39,212.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Sample errors by selected diacritization models. Wrong predicted diacritics are underlined and in red; missing diacritics are noted by underscore. Translation: "The editor of An Nahar, Gebran Tueni, thought that the judicial formations came to dilute the issue of MTV station".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Arabic diacritics.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 ,</head><label>2</label><figDesc></figDesc><table>modified from (Habash et al., 2007), lists 
the diacritics. Importantly, the gemination marker 
~ can combine with short vowels and case endings 
(e.g. Table 1, row 3). </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>where y t = W hy h t + b y and y t [l] is the l th element of y t . Training is done with stochastic gradient de- scent with momentum, optimizing the cross- entropy objective function. Layer sizes and other hyper-parameters are tuned on the Dev set. Our implementation is based on Currennt (Weninger et al., 2015).</figDesc><table>DER 
Model 
All 
End # params 
Feed-forward 
11.76 22.90 63K 
Feed-forward (large) 11.55 23.40 908K 
LSTM 
6.98 
10.36 838K 
B-LSTM 
6.16 
9.85 
518K 
2-layer B-LSTM 
5.77 
9.18 
916K 
3-layer B-LSTM 
5.08 
8.14 
1,498K 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Diacrtic error rates (DERs) on the Dev 
set, over all diacritics and only at word ending. 

MaxEnt (only lexical) 8.1 
MaxEnt (full) 
5.1 
3-layer B-LSTM 
4.85 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Results (DER) on the Test set. MaxEnt 
results from (Zitouni and Sarikaya, 2009) 

</table></figure>

			<note place="foot" n="1"> Arabic transliteration follows the Buckwalter scheme: http://www.qamus.org/transliteration.htm.</note>

			<note place="foot" n="3"> Approach We define the following sequence classification task, similarly to (Zitouni and Sarikaya, 2009). 2 We also include the low-frequency superscript Alif &apos; that is usually ignored due to its limitation to fixed lexical items.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by the Qatar Comput-ing Research Institute (QCRI).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lexical Modeling for Arabic ASR: A Systematic Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><surname>Tuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hanai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic Arabic diacritics restoration based on deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><forename type="middle">Al</forename><surname>Sallab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Rashwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hazem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Raafat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rafea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP 2014 Workshop on Arabic Natural Language Processing</title>
		<meeting>the EMNLP 2014 Workshop on Arabic Natural Language Processing</meeting>
		<imprint>
			<publisher>ANLP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Arabic Diacritization through Full Morphological Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On Arabic Transliteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelhadi</forename><surname>Soudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Buckwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Arabic Computational Morphology: Knowledge-based and Empirical Methods</title>
		<editor>Abdelhadi Soudi, Günter Neumann, and Antal Van den Bosch</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="15" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MADA+TOKAN: A Toolkit for Arabic Tokenization, Diacritization, Morphological Disambiguation, POS Tagging, Stemming and Lemmatization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Arabic Language Resources and Tools</title>
		<meeting>the Second International Conference on Arabic Language Resources and Tools</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Diacritization: A challenge to Arabic treebank annotation and parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Maamouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Bies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Kulick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Computer Society Arabic NLP/MT Conference</title>
		<meeting>the British Computer Society Arabic NLP/MT Conference</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop at ICLR</title>
		<meeting>Workshop at ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Stochastic Arabic Diacritizer Based on a Hybrid of Factorized and Unfactorized Textual Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A A</forename><surname>Rashwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A S A A</forename><surname>Al-Badrashiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Attia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Abdou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rafea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="166" to="175" />
			<date type="published" when="2011-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
		<title level="m">Introducing CURRENNT: The Munich Open-Source CUDA RecurREnt Neural Network Toolkit. JMLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="547" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Arabic Diacritic Restoration Approach Based on Maximum Entropy Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imed</forename><surname>Zitouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Speech Lang</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Maximum Entropy Based Restoration of Arabic Diacritics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imed</forename><surname>Zitouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">S</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING and ACL</title>
		<meeting>COLING and ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
