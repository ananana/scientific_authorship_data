<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:13+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Noisy Or-based model for Relation Extraction using Distant Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Nagesh</surname></persName>
							<email>ajaynagesh@cse.iitb.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">IITB-Monash Research Academy</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Faculty of IT</orgName>
								<orgName type="institution">Monash University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Dept. of CSE, IIT Bombay</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
							<email>gholamreza.haffari@monash.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Ramakrishnan</surname></persName>
							<email>ganesh@cse.iitb.ac.in</email>
						</author>
						<title level="a" type="main">Noisy Or-based model for Relation Extraction using Distant Supervision</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1937" to="1941"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Distant supervision, a paradigm of relation extraction where training data is created by aligning facts in a database with a large unannotated corpus, is an attractive approach for training relation extractors. Various models are proposed in recent literature to align the facts in the database to their mentions in the corpus. In this paper, we discuss and critically analyse a popular alignment strategy called the &quot;at least one&quot; heuristic. We provide a simple , yet effective relaxation to this strategy. We formulate the inference procedures in training as integer linear programming (ILP) problems and implement the relaxation to the &quot;at least one &quot; heuris-tic via a soft constraint in this formulation. Empirically, we demonstrate that this simple strategy leads to a better performance under certain settings over the existing approaches .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Although supervised approaches to relation ex- traction ( <ref type="bibr" target="#b1">GuoDong et al., 2005;</ref><ref type="bibr" target="#b7">Surdeanu and Ciaramita, 2007</ref>) achieve very high accuracies, they do not scale as they are data intensive and the cost of creating annotated data is quite high. To alle- viate this problem, <ref type="bibr" target="#b3">Mintz et al. (2009)</ref> proposed relation extraction in the paradigm of distant su- pervision. In this approach, given a database of facts (e.g. Freebase <ref type="bibr">1</ref> ) and an unannotated docu- ment collection, the goal is to heuristically align the facts in the database to the sentences in the corpus which contain the entities mentioned in the fact. This is done to create weakly labeled train- ing data to train a classifier for relation extraction. The underlying assumption is that all mentions of 1 www.freebase.com an entity pair 2 (i.e. sentences containing the en- tity pair) in the corpus express the same relation as stated in the database.</p><p>The above assumption is a weak one and is often violated in natural language text. For in- stance, the entity pair, <ref type="bibr">(Barack Obama, United States)</ref> participate in more than one relation: citizenOf, presidentOf, bornIn and every men- tion expresses either one of these fixed set of rela- tions or none of them.</p><p>Consequently, a number of models have been proposed in literature to provide better heuristics for the mapping between the entity pair in the database and its mentions in the sentences of the corpus. <ref type="bibr" target="#b4">Riedel et al. (2010)</ref> tightens the assump- tion of distant supervision in the following man- ner: "Given a pair of entities and their mentions in sentences from a corpus, at least one of the men- tions express the relation given in the database". In other words, it models the problem as that of multi-instance (mentions) single-label (relation) learning. Following this, <ref type="bibr" target="#b2">Hoffmann et al. (2011)</ref> and <ref type="bibr" target="#b9">Surdeanu et al. (2012)</ref> propose models that consider the mapping as that of multi-instance multi-label learning. The instances are the men- tions of the entity pair in the sentences of the cor- pus and the entity pair can participate in more than one relation.</p><p>Although, these models work very well in prac- tice, they have a number of shortcomings. One of them is the possibility that during the align- ment, a fact in the database might not have an in- stantiation in the corpus. For instance, if our cor- pus only contains documents from the years 2000 to 2005, the fact presidentOf(Barack Obama, United States) will not be present in the corpus. In such cases, the distant supervision assumption fails to provide a mapping for the fact in the cor- pus.</p><p>In this paper, we address this situation with a noisy-or model <ref type="bibr" target="#b6">(Srinivas, 2013</ref>) in training the re- lation extractor by relaxing the "at least one" as- sumption discussed above. Our contributions in this paper are as follows: (i) We formulate the in- ference procedures in the training algorithm as in- teger linear programming (ILP) problems, (ii) We introduce a soft-constraint in the ILP objective to model noisy-or in training, and (iii) Empirically, our algorithm performs better than <ref type="bibr" target="#b2">Hoffmann et al. (2011)</ref> procedure under certain settings on two benchmark datasets. Our paper is organized as follows. In Section 2, we discuss our methodology. We review the ap- proach of <ref type="bibr" target="#b2">Hoffmann et al. (2011)</ref> and explain our modifications to it. In Section 3, we discuss re- lated work. In Section 4, we discuss the experi- mental setup and our preliminary results. We con- clude in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Our work extends the work of <ref type="bibr" target="#b2">Hoffmann et al. (2011)</ref>. So, we recapitulate Hoffmann's model in the following subsection. Following which our ad- ditions to this model is explained in detail. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hoffmann's model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Objective function</head><p>Consider an entity pair (e 1 , e 2 ) denoted by the in- dex i. The set of sentences containing the entity pair is denoted x i and the set of relation labels for the entity pair from the database is denoted by y i . The mention-level labels are denoted by the latent variable z (there is one variable z j for each sen- tence j).</p><p>To learn the parameters θ, the training objective to maximize is the likelihood of the facts observed in the database conditioned on the sentences in the text corpus.</p><formula xml:id="formula_0">θ * = arg max θ i P r(y i |x i ; θ) = arg max θ i z P r(y i , z|x i ; θ)</formula><p>The expression P r(y i , z|x i ) for a given entity pair is defined by two types of factors in the factor graph. They are extract factors for each mention and mention factors between a relation label and all the mentions.</p><p>The extract factors capture the local signal for each mention and consists of a bunch of lexical and syntactic features like POS tags, dependency path between the entities and so on ( <ref type="bibr" target="#b3">Mintz et al., 2009)</ref>.</p><p>The mention factors capture the dependency be- tween relation label and its mentions. Here, the at least one assumption that was discussed in Section 1 is modeled. It is implemented as a simple deter- ministic OR operator as given below:</p><formula xml:id="formula_1">f mention (y r , z) = 1</formula><p>if yr is true ∧∃i : zi = r 0 otherwise</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training algorithm</head><p>The learning algorithm is a perceptron-style parameter update scheme with 2 modifications: i) online learning ii) Viterbi approximation. The inference is shown to reduce to the well-known weighted edge-cover problem which can be solved exactly, although <ref type="bibr" target="#b2">Hoffmann et al. (2011)</ref> provide an approximate solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Hoffmann et al. (2011) : Train- ing</head><p>Input : i) Σ: set of sentences, ii) E: set of entities mentioned in the sentences, iii) R: set of relation labels, iv) ∆: database of facts Output: Extraction model :</p><formula xml:id="formula_2">Θ begin for t ← 1 to T ; / * training iterations * / do for i ← 1 to N ; / * No. of entity pairs * / do y, z, = arg max y,z P r y, z x i ; Θ if y! = y i then z * = arg max z P r z y i , x i ; Θ Θ new = Θ old +Φ(x i , z * )−Φ(x i , z)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>end</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our additions to Hoffmann's model</head><p>In the training algorithm described above, there are two MAP inference procedures. Our con- tributions in this space is two-fold. Firstly, we have formulated these as ILP problems. As a re- sult of this, the approximate inference therein is replaced by an exact inference procedure. Sec- ondly, we replace the deterministic-or by a noisy- or which provides a soft-constraint instead of the hard-constraint of Hoffmann. ("at least one" as- sumption)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ILP formulations</head><p>Some notations:</p><formula xml:id="formula_3">z ji :</formula><p>The mention variable z j (or jth sen- tence) taking the relation value i s ji : Score for z j taking the value of i. Scores are computed from the extract factors y i : relation label being i m : number of mentions (sentences) for the given entity pair R: total number of relation labels (excluding the nil label)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deterministic OR</head><p>The following is the ILP formulation for the exact inference arg max P r(y, z|x i ) in the model based on the deterministic-or: The first constraint restricts a mention to have only one label. The second and third constraints impose the at least one assumption. This is the same formulation as Hoffmann but expressed as an ILP problem. However, posing the inference as an ILP allows us to easily add more constraints to it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noisy OR</head><p>As a case-study, we add the noisy-or soft- constraint in the above objective function. The idea is to model the situation where a fact is present in the database but it is not instantiated in the text. This is a common scenario, as the facts populated in the database and the text of the corpus can come from different domains and there might not be a very good match. zji + ǫi ∀i where zji ∈ {0, 1}, yi ∈ {0, 1}, ǫi ∈ {0, 1}</p><p>In the above formulation, the objective function is augmented with a soft penalty. Also the third constraint is modified with this penalty term. We call this new term ǫ i and it is a binary variable to model noise. Through this term we encourage at least one type of configuration but will not disal- low a configuration that does not conform to this. Essentially, the consequence of this is to allow the case where a fact is present in the database but is not instantiated in the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Relation Extraction in the paradigm of distant su- pervision was introduced by <ref type="bibr" target="#b0">Craven and Kumlien (1999)</ref>. They used a biological database as the source of distant supervision to discover rela- tions between biological entities. The progression of models for information extraction using distant supervision was presented in Section 1. <ref type="bibr" target="#b9">Surdeanu et al. (2012)</ref> discuss a noisy-or method for combining the scores of various sen- tence level models to rank a relation during evalu- ation. In our approach, we introduce the noisy-or mechanism in the training phase of the algorithm.</p><p>Our work is inspired from previous works like Roth and tau <ref type="bibr" target="#b5">Yih (2004)</ref>. The use of ILP for this problem facilitates easy incorporation of different constraints and to the best of our knowl- edge, has not been investigated by the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The experimental runs were carried out using the publicly available Stanford's distantly supervised slot-filling system 3 (Surdeanu et al., 2011) and <ref type="bibr" target="#b2">Hoffmann et al. (2011)</ref> code-base 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and Evaluation</head><p>We report results on two standard datasets used as benchmarks by the community namely KBP and Riedel datasets. A complete description of these datasets is provided in <ref type="bibr" target="#b9">Surdeanu et al. (2012)</ref>.</p><p>The evaluation setup and module is the same as that described in <ref type="bibr" target="#b9">Surdeanu et al. (2012)</ref>. We also use the same set of features used by the var- ious systems in the package to ensure that the ap- proaches are comparable. As in previous work, we report precision/recall (P/R) graphs to evaluate the various techniques.</p><p>We used the publicly available lp solve pack- age 5 to solve our inference problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance of ILP</head><p>Use of ILP raises concerns about performance as it is NP-hard. In our problem we solve a separate ILP for every entity pair. The number of variables is limited by the number of mentions for the given entity pair. Empirically, on the KBP dataset (larger of the two datasets), Hoffmann takes around 1hr to run. Our ILP formulation takes around 8.5hrs. However, MIMLRE algorithm (EM-based) takes around 23hrs to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We would primarily like to highlight two settings on which we report the P/R curves and contrast it with <ref type="bibr" target="#b2">Hoffmann et al. (2011)</ref>. Firstly, we re- place the approximate inference in that work with our ILP-based exact inference; we call this set- ting the hoffmann-ilp. Secondly, we replace the deterministic-or in the model with a noisy-or, and call this setting the noisy-or. We further compare our approach with <ref type="bibr" target="#b9">Surdeanu et al. (2012)</ref>. The P/R curves for the various techniques on the two datasets are shown in <ref type="figure" target="#fig_3">Figures 1 and 2</ref>.</p><p>We further report the highest F1 point in the P/R curve for both the datasets in <ref type="table" target="#tab_0">Tables 1 and 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>We would like to discuss the results in the above two scenarios. On the KBP dataset, we observe that hoffmann-ilp has higher precision in the range of 0.05 to 0.1 at lower recall (0 to 0.04). In other parts of the curve it is very close to the baseline (although hoffmann's algorithm is slightly better). In <ref type="table" target="#tab_0">Table 1</ref>, we notice that recall of hoffmann-ilp is lower in comparison with hoffmann's algorithm.</p><p>On the Riedel dataset, we observe that hoffmann-ilp has better precision (0.15 to 0.2) than MIMLRE within recall of 0.1. At recall &gt; 0.1, precision drops drastically. This is because, hoffmann-ilp predicts signif- icantly more nil labels. However, nil labels are not part of the label-set in the P/R curves reported in the community. In <ref type="table" target="#tab_1">Table 2</ref>, we see that hoffmann-ilp has higher precision (0.04) compared to Hoffmann's algorithm. In <ref type="figure">Figure 1</ref> we see that there is a big jump in precision (around 0.4) of noisy-or com- pared to Hoffmann's model in most parts of the curve on the KBP dataset. However, in <ref type="figure" target="#fig_3">Figure 2</ref> (Riedel dataset), we do not see such a trend. Although, we do perform better than MIMLRE ( <ref type="bibr" target="#b9">Surdeanu et al., 2012</ref>) (precision &gt; 0.15 for recall &lt; 0.15).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Performance of noisy-or</head><p>On both datasets, noisy-or has higher preci- sion than MIMLRE, as seen from <ref type="table" target="#tab_0">Tables 1  and 2</ref>. However, the recall reduces. More in- vestigation in this direction is part of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we described an important addition to Hoffmann's model by the use of the noisy-or soft constraint to further relax the at least one assump- tion. Since we posed the inference procedures in Hoffmann using ILP, we could easily add this con- straint during the training and inference. Empirically, we showed that the resulting P/R curves have a significant performance boost over Hoffmann's algorithm as a result of this newly added constraint. Although our system has a lower recall when compared to MIMLRE ( <ref type="bibr" target="#b9">Surdeanu et al., 2012)</ref>, it performs competitively w.r.t the pre- cision at low recall.</p><p>As part of immediate future work, we would like to improve the system recall. Our ILP for- mulation provides a good framework to add new type of constraints to the problem. In the future, we would like to experiment with other constraints like modeling the selectional preferences of entity types.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Hoffmann et al. (2011) present a multi-instance multi-label model for relation extraction through distant supervision. In this model, a pair of enti- ties have multiple mentions (sentence containing the entity pair) in the corpus. An entity pair can have one or more relation labels (obtained from the database).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>zji ≤ yi ∀j, ∀i 3. yi ≤ m j=1 zji ∀i where zji ∈ {0, 1}, yi ∈ {0, 1}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 1: Results : KBP dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Highest F1 point in P/R curve : KBP Dataset 
Precision 
Recall 
F1 
Hoffmann 
0.306451619 0.197916672 0.2405063349 
MIMLRE 
0.28061223 0.286458343 0.2835051518 
Noisy-OR 
0.297002733 0.189236104 0.2311770916 
Hoffmann-ilp 0.293010741 0.189236104 0.2299577976 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Highest F1 point in P/R curve : Riedel Dataset 
Precision 
Recall 
F1 
Hoffmann 
0.32054795 0.24049332 0.27480916 
MIMLRE 
0.28061223 0.28645834 0.28350515 
Noisy-OR 
0.317 0.18139774 0.23075178 
Hoffmann-ilp 
0.36701337 0.12692702 0.18862161 

</table></figure>

			<note place="foot" n="2"> In this paper we restrict ourselves to binary relations</note>

			<note place="foot" n="3"> http://nlp.stanford.edu/software/ mimlre.shtml 4 http://www.cs.washington.edu/ai/ raphaelh/mr/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Constructing biological knowledge bases by extracting information from text sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Kumlien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology</title>
		<meeting>the Seventh International Conference on Intelligent Systems for Molecular Biology</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Guodong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Min</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Knowledge-based weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
	<note>ACL &apos;09. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 European conference on Machine learning and knowledge discovery in databases: Part III, ECML PKDD&apos;10</title>
		<meeting>the 2010 European conference on Machine learning and knowledge discovery in databases: Part III, ECML PKDD&apos;10<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A linear programming formulation for global inference in natural language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2004</title>
		<meeting>CoNLL-2004</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A generalization of the noisyor model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampath</forename><surname>Srinivas</surname></persName>
		</author>
		<idno>abs/1303.1479</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust information extraction with perceptrons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIST 2007 Automatic Content Extraction Workshop (ACE07)</title>
		<meeting>the NIST 2007 Automatic Content Extraction Workshop (ACE07)</meeting>
		<imprint>
			<date type="published" when="2007-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stanford&apos;s distantly-supervised slot-filling system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><forename type="middle">I</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Text Analysis Conference</title>
		<meeting>the Fourth Text Analysis Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
