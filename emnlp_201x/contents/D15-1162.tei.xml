<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Improved Non-monotonic Transition System for Dependency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Macquarie University Sydney</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country>Germany, Australia</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
							<email>mark.johnson@mq.edu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Macquarie University Sydney</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country>Germany, Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Improved Non-monotonic Transition System for Dependency Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Transition-based dependency parsers usually use transition systems that monotoni-cally extend partial parse states until they identify a complete parse tree. Honni-bal et al. (2013) showed that greedy one-best parsing accuracy can be improved by adding additional non-monotonic transitions that permit the parser to &quot;repair&quot; earlier parsing mistakes by &quot;over-writing&quot; earlier parsing decisions. This increases the size of the set of complete parse trees that each partial parse state can derive, enabling such a parser to escape the &quot;gar-den paths&quot; that can trap monotonic greedy transition-based dependency parsers. We describe a new set of non-monotonic transitions that permits a partial parse state to derive a larger set of completed parse trees than previous work, which allows our parser to escape from a larger set of garden paths. A parser with our new non-monotonic transition system has 91.85% directed attachment accuracy, an improvement of 0.6% over a comparable parser using the standard monotonic arc-eager transitions .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent work from <ref type="bibr" target="#b2">Dyer et al. (2015)</ref> and <ref type="bibr" target="#b11">Weiss et al. (2015)</ref> show that neural network models can improve greedy transition-based parsers dramat- ically, even beyond the 20% error reduction re- ported by <ref type="bibr" target="#b1">Chen and Manning (2014)</ref>. Improve- ments on beam-search parsing are much more lim- ited, due to the difficulty of applying neural net- works to structured prediction.</p><p>We suggest that the lack of a ready search so- lution may present the next barrier to further im- provements in accuracy. Some degree of search flexibility seems inherently necessary, no mat- ter how powerful the local model becomes, as even the human sentence processor can be 'garden pathed' by local structural ambiguities.</p><p>We take inspiration from <ref type="bibr" target="#b3">Frazier and Rayner (1982)</ref> and other psycholinguists and propose re- pair actions as a light-weight alternative to beam- search. In a transition-based dependency parser, transitions map parse states to parse states, ulti- mately producing completed parse trees. This pro- cess is non-deterministic, since usually more than one transition can apply to a parse state. This means that each partial parse state can be associ- ated with a set of complete parse trees (i.e., the complete parses that can be produced by applying sequences of transitions to the partial parse state). In general adding additional transitions (mono- tonic or non-monotonic) increases the number of complete parse trees that any given partial parse state can derive.</p><p>We explore adding non-monotonic parsing tran- sitions to a greedy arc-eager dependency parser in this paper, in order to permit the parser to recover from attachment errors made early in the parsing process. These additional non-monotonic transi- tions permit the parser to modify what would have been irrevocable parsing decisions in the mono- tonic arc-eager system when later information jus- tifies this action. Thus one effect of adding the non-monotonic parsing transitions is to effectively delay the location in the input where the parser must ultimately commit to a particular attachment.</p><p>Our transition-system builds on the work of <ref type="bibr" target="#b6">Honnibal et al. (2013)</ref> and <ref type="bibr">Nivre and FernandezGonzalez (2014)</ref>, who each present modifications to the arc-eager transition system that introduce some non-monotonic behaviour, resulting in small improvements in accuracy. However, these sys- tems only apply non-monotonic transitions to a relatively small number of configurations, so they can only have a small impact on parse accuracy.</p><p>We introduce a non-monotonic transition sys- tem that combines ideas from these two ap- proaches, and allows substantially more repair ca- pability (and hence search flexibility). We ob- serve a 0.6% improvement in accuracy on the OntoNotes corpus, which is an error reduction of 6.25% over a competitive baseline. A parser using our transition system is guaranteed to run in linear time, and the modifications to the algorithm have no negative impact on run-time in our implemen- tation.</p><p>Very recently there has been considerable suc- cess in applying neural network models to pre- dict which transition to apply in greedy one-best transition-based parsing. In their preprints, both <ref type="bibr" target="#b2">Dyer et al. (2015)</ref> and <ref type="bibr" target="#b11">Weiss et al. (2015)</ref> re- port error reductions of around 20-30% for greedy one-best parsing, and much more modest im- provements for transition-based parsers with beam search. Because the neural network approaches improve the local model that predicts which tran- sition to apply next, while this paper suggests changes to the transition system itself, it is rea- sonable to expect that the improvements reported here are largely orthogonal to those obtained us- ing the neural network techniques. In future work we would like to explore integrating such neural network models of transition prediction with the extended transition system proposed here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Improved non-monotonic transition system</head><p>Our transition-system is based on the tree- constrained arc-eager system of Nivre and <ref type="bibr" target="#b9">Fernandez-Gonzalez (2014)</ref>, which extends the classic arc-eager system (Nivre, 2003) with a new non-monotonic operation that they call "Unshift". They introduce the Unshift action to repair con- figurations where the buffer is exhausted and the stack contains multiple words that are without in- coming arcs (i.e. without governors). The origi- nal arc-eager configuration outputs partial parses in this situation. Nivre and Fernandez-Gonzalez restrict their Unshift action, such that it can only be applied when the buffer is exhausted and the word on top of the stack has no incoming arc. In this config- uration, the Unshift action is the only action that can be applied. The use of the new action is there- fore entirely deterministic, and they do not need to produce example configurations for the Unshift action during training. They train their model with what Goldberg and Nivre (2012) term a 'static or- acle', which can only label configurations that are consistent with the gold-standard parse.</p><p>We take the Nivre and Fernandez-Gonzalez (2014) Unshift operation, and import it into the non-monotonic parsing model of <ref type="bibr" target="#b6">Honnibal et al. (2013)</ref>, which uses a dynamic oracle to determine the gold-standard actions for configurations pro- duced by the parser. This training strategy is crit- ical to the success of a non-monotonic transition system. The model cannot learn to recover from previous errors if the training data cannot contain configurations that result from incorrect actions. <ref type="bibr" target="#b6">Honnibal et al. (2013)</ref> allow the parser to cor- rect prior misclassifications between the Shift and Right-Arc actions. Both of these actions push the first word of the buffer onto the stack, but the Right-Arc action also adds an arc. After the Right- Arc is applied, the top two words of the stack are connected.</p><p>In the original arc-eager system, the presence or absence of this arc determines which of the two pop moves, Reduce or Left-Arc, is valid. If the arc is present, then Left-Arc is excluded; if it is absent, the Reduce action is excluded. <ref type="bibr" target="#b6">Honnibal et al. (2013)</ref> argue that these deterministic constraints are unmotivated when the parser is trained using a dynamic, instead of static, oracle. Instead of a con- straint, they suggest that consistency be achieved by refining the logic of the actions, so that they have a broader applicability. Instead of preventing the Left-Arc from applying when the word on top of the stack has an incoming arc, they update the definition of the Left-Arc so that it first deletes the existing arc if necessary. A corresponding change is made to the Reduce action: if the model predicts Reduce when the word on top of the stack has no incoming arc, the 'missing' arc is inserted. The arc is labelled by noting the best-scoring Right- Arc label on each Shift action, so that the label can be assigned during non-monotonic Reduce.</p><p>We show that the Nivre and Fernandez- Gonzalez Unshift operation serves as a far supe- rior non-monotonic Reduce action than the one Notation (σ, β, A, S) is a configuration, where σ|s is a stack of word indices with topmost element s b|β is a buffer of word indices with first element b A is a vector of head indices A(i) = j denotes an arc w j → w i S is a bit-vector used to prevent Shift/Unshift cycles <ref type="table">Table 1</ref>: Our non-monotonic transition system, which integrates the Unshift action of Nivre and Fernandez- <ref type="bibr" target="#b9">Gonzalez (2014)</ref> into the model of <ref type="bibr" target="#b6">Honnibal et al. (2013)</ref>.</p><formula xml:id="formula_0">Initial ([ ], [1...n], A(1) = 1) Terminal ([i], [ ], A) Shift (σ, b|β, A, S(b) = 0) ⇒ (σ|b, β, A, S(b) = 1) Right-Arc (σ|s, b|β, A, S) ⇒ (σ|s|b, β, A(b) = s, S) Reduce (σ|s, β, A(s) = 0, S) ⇒ (σ, β, A, S) Unshift (σ|s, β, A(s) = 0, S) ⇒ (σ, s|β, A, S) Left-Arc (σ|s, b|β, A, S) ⇒ (σ, s|β, A(s) = b, S</formula><p>Honnibal et al. use in their system, and that the re- sulting transition system improves parse accuracy by considerably more than either the Honnibal et al or Nivre et al systems do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Definition of Transition System</head><p>The hybrid transition system is defined in <ref type="table">Table  1</ref>. Arcs are stored in a vector, A, where the entry A(i) = j denotes an arc w j → w i . Words are pushed from the buffer β onto the stack σ, using either the Shift or the Right-Arc actions. If a word was pushed with the Shift action, it will not have an incoming arc. The new Unshift action will then be valid, at any point at which the word is on top of the stack -even after many ac- tions have been performed.</p><p>The Unshift action pops the top word of the stack, s, and places it at the start of the buffer. Parsing then proceeds as normal. To prevent cy- cles, the Shift action checks and sets a bit in the new boolean vector S. The Shift action is invalid if S(b) = 1, for a word b at the front of the buffer. This bit will be set if the word was previously Shifted, and then Unshifted.</p><p>At worst, each word can be pushed and popped from the stack twice, so parsing is guaranteed to terminate after a maximum of 4n transitions for a sentence of length n.</p><p>The terminal condition is reached when the buffer is exhausted and exactly one word remains on the stack. This word will be deemed the root of the sentence. No 'dummy' root token is neces- sary, removing the need to choose whether the to- ken is placed at the beginning or end of the buffer ( <ref type="bibr" target="#b0">Ballesteros and Nivre, 2013)</ref>.</p><p>Note that if the two words each seem like the governor of the sentence, such that the parser deems all incoming arcs to these words unlikely, the transition system is guaranteed to arrive at a configuration where these two words are adjacent to each other. The model can then predict an arc between them, initiated by either word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dynamic Training Oracle</head><p>Goldberg and Nivre (2013) describe three ques- tions that need to be answered in order to imple- ment their training algorithm. Exploration Policy: When do we follow an incor- rect transition, and which one do we follow? We always follow the predicted transition, i.e. their two hyper-parameters are set k = 1 and p = 1.0. Optimality: What constitutes an optimal transi- tion in configurations from which the gold tree is not reachable? We follow <ref type="bibr" target="#b6">Honnibal et al. (2013)</ref> in defining a transition as optimal if it:</p><p>1. Renders no additional arcs unreachable using the monotonic arc-eager transitions; and 2. Renders no additional arcs unreachable using the non-monotonic transitions.</p><p>Said another way, we mark a transition as opti- mal if it leads to an analysis with as few errors as possible, and in cases of ties, uses as few non- monotonic transitions as possible.</p><p>For example, given the input string I saw Jack, consider a configuration where saw is on the stack, Jack is at the front of the buffer, and I is attached to saw. The gold arcs are saw → I and saw → Jack. In the monotonic system, the Shift action would make the gold arc saw → Jack newly unreachable. In our system, this arc is still reachable after Shift, via the Unshift action, but we consider the Shift move non-optimal, so that the non-monotonic ac- tions are reserved as "repair" operations. Oracle: Given a definition of optimality, how do we calculate the set of optimal transitions in a given configuration? <ref type="bibr" target="#b5">Goldberg and Nivre (2013)</ref> show that with the monotonic arc-eager actions, the following arcs are reachable from an arbitrary configuration:</p><p>1. Arcs {w i → w j : i ∈ σ, j ∈ β} -i.e. all arcs from stack words to buffer words;</p><p>2. Arcs {w i → w j : i ∈ β, j ∈ σ, A(j) = 0} -i.e. all arcs from buffer words to headless stack words;</p><p>3. Arcs {w i → w j : i ∈ β, j ∈ β} -i.e. all arcs between words in buffer.</p><p>Our non-monotonic actions additionally allow the following arcs to be reached:</p><p>4. Arcs {w i → w j : i ∈ β, j ∈ σ, A(j) = 0} (LeftArc can now "clobber" existing heads)</p><p>5. Arcs {w i → w j or w j → w i : i, j ∈ σ, i &lt; j, A(j) = 0} -i.e. if a word i is on the stack, it can reach an arc to or from a word j ahead of it on the stack if that word does not have a head set.</p><p>In practice, we therefore only need to add two rules to determine the set of optimal transitions:</p><p>1. If σ 0 has a head, and its true head is in the buffer, the Reduce action is now non-optimal.</p><p>2. If σ 0 does not have a head, and its true head is in the stack, the LeftArc action is now non- optimal.</p><p>The oracle calculation is simple because the sys- tem preserves the arc decomposition property that Goldberg and Nivre (2013) prove for the arc eager system: if two arcs of a projective tree are individ- ually reachable from a configuration, a projective tree that includes both arcs is also reachable. To see that this property is preserved in our system, consider that an arc h → d between two stack words is only unreachable if h &lt; d and A(d) = 0. But a projective tree with arc h → d cannot also have an arc x → y such that h &lt; x &lt; d &lt; y. So there can be no other arc part of the same pro- jective tree as h → d that would require d to be assigned to some other head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training Procedure</head><p>We follow <ref type="bibr" target="#b6">Honnibal et al. (2013)</ref> in using the dynamic oracle-based search-and-learn training strategy introduced by <ref type="bibr" target="#b4">Goldberg and Nivre (2012)</ref>. A dynamic oracle is a function that labels config- urations with gold-standard actions. Importantly, a dynamic oracle can label arbitrary configura- tions, while a so-called 'static' oracle can only as- sign labels to configurations that are part of gold- standard derivations.</p><p>We employ the dynamic oracle in an on- line learning strategy, similar to imitation-based learning, where the examples are configurations produced by following the current model's pre- dictions. The configurations are labelled by the dynamic oracle, which determines which of the available actions excludes the fewest gold- standard arcs.</p><p>Often, multiple actions will be labelled as gold- standard for a given configuration. This implies ei- ther spurious ambiguity (the same analysis reach- able via different derivations) or previous errors, such that the best parse reachable by different ac- tions are equally bad. When this occurs, we base the perceptron update on the highest-scoring gold- standard label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Single class for Unshift/Reduce</head><p>The Unshift and Reduce actions are applicable to a disjoint set of configurations. If the word on top of the stack already has an incoming arc, the Re- duce move is valid; otherwise, the Unshift move is valid. For the purpose of training and predic- tion, we therefore model these actions as a single class, which we interpret based on the configura- tion. This allows us to learn the Unshift action more effectively, as it is allowed to share a repre- sentation with the Reduce move. In preliminary development, we found that assigning a distinct class to the Unshift action was not effective. We plan to evaluate this option more rigorously in fu- ture work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We implemented a greedy transition-based parser, and used rich contextual features following <ref type="bibr" target="#b12">Zhang and Nivre (2011)</ref>. We extended the feature set to include Brown cluster features, using the cluster prefix trick described by <ref type="bibr" target="#b7">Koo and Collins (2010)</ref>. Brown clusters are a standard way to improve the cross-domain performance of supervised lin- ear models. The use of Brown cluster features ac- counts for the 0.7% improvement in accuracy ob- served between our baseline parser and the Gold- berg and Nivre (2012) result shown in <ref type="table">Table 2</ref>. The two models are otherwise the same.</p><p>Part-of-speech tags were predicted using a greedy averaged perceptron model that achieved 97.2% accuracy on the evaluation data. Most pre- vious work uses a n-way jack-knifing to train the stacked tagger/parser model. For convenience, we instead train the tagger at the same time as the parser, as both allow online learning. We find this makes no difference to accuracy.</p><p>Our parsers are trained and evaluated on the same data used by <ref type="bibr" target="#b10">Tetreault et al. (2015)</ref> in their recent 'bake-off' of leading dependency pars- ing models. Specifically, we use the OntoNotes corpus converted into dependencies using the ClearNLP 3.1 converter, with the train / dev / test split of the CoNLL 2012 shared task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We implemented three previous versions of the arc-eager transition system, in order to evaluate the effect of our proposed transition-system on parser accuracy. The four systems differ only in their transition system -they are otherwise iden- tical. All use identical features, and all are trained with the dynamic oracle.</p><p>Orig. Arc Eager (Nivre, 2003): the origi- nal arc-eager system, which constrains the Re- duce and Left-Arc actions to ensure monotonic- ity; Prev. Non-Monotic ( <ref type="bibr" target="#b6">Honnibal et al., 2013)</ref>: relaxes the monotonicity constraints, allowing Left-Arc to "clobber" existing arcs, and insert- ing missing arcs on Reduce with a simple heuris- tic; Tree Constrained <ref type="bibr">(Nivre and FernandezGonzalez, 2014)</ref>: adds an Unshift action to the arc-eager system, that is only employed when the buffer is exhausted; This work: merges the Un- shift action into our previous non-monotonic tran- sition system.  <ref type="table">Table 2</ref>: Our non-monotonic transition system improves accuracy by 0.6% unlabelled attachment score, for a final score of 91.85 on the OntoNotes corpus. <ref type="table">Table 2</ref> shows the unlabelled and labelled at- tachment scores of the parsers on the evaluation data. The two previous non-monotonic systems, Prev. Non-monotonic and Tree Constrained, were slightly more accurate than the Orig. Arc Eager system. Our new transition-system had a much bigger impact, improving UAS by 0.6% and LAS by 0.51%. To put the scores in context, we have also included figures reported in a recent sur- vey of the current state-of-the-art ( <ref type="bibr" target="#b10">Tetreault et al., 2015)</ref>. Our parser out-performs existing greedy parsers, and is much more efficient than non- greedy parsers.</p></div>		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>This paper integrates innovations from <ref type="bibr" target="#b6">Honnibal et al. (2013)</ref> and <ref type="bibr" target="#b9">Nivre and Fernandez-Gonzalez (2014)</ref> to produce a novel non-monotonic set of transitions for transition-based dependency pars-ing. Doing this required us to use the dynamic oracle of <ref type="bibr" target="#b4">Goldberg and Nivre (2012)</ref> during train-ing in order to produce configurations that exercise the non-monotonic transitions. We show that this combination of innovations results in a parser with 91.85% directed accuracy, which is an improve-ment of 0.6% directed accuracy over an equivalent arc-standard parser. Interestingly, the Honnibal et al and Nivre et al innovations applied on their own only produce improvements of 0.11% and 0.15% respectively, so it seems that these improvements taken together do interact synergistically.</p><p>Because our innovation largely affects the search space of a greedy one-best parser, it is likely to be independent of the recent improve-ments in parsing accuracy that come from using neural networks to predict the best next parsing transition. In future work we plan to combine such neural network models with a version of our parser that incorporates a much larger set of non-monotonic parsing transitions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Going to the roots of dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with stack long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyn</forename><surname>Frazier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Rayner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="178" to="210" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A dynamic oracle for arc-eager dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Computational Linguistics</title>
		<meeting>the 24th International Conference on Computational Linguistics<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Training deterministic parsers with non-deterministic oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="403" to="414" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A non-monotonic arc-eager transition system for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="163" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient third-order dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An efficient algorithm for projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Parsing Technologies (IWPT)</title>
		<meeting>the 8th International Workshop on Parsing Technologies (IWPT)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="149" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Arc-eager parsing with the tree constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fernandez-Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="259" to="267" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">It depends: Dependency parser comparison using a web-based evaluation tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Ho Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structured training for neural network transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with rich non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="188" to="193" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
