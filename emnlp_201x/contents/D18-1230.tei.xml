<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Named Entity Tagger using Domain-Specific Dictionary</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 2054</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
							<email>xiangren@usc.edu ‡ teng.ren@cootek.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Ren</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">CooTek Inc</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country>China †</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Named Entity Tagger using Domain-Specific Dictionary</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2054" to="2064"/>
							<date type="published">October 31-November 4, 2018. 2018. 2054</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recent advances in deep neural models allow us to build reliable named entity recognition (NER) systems without handcrafting features. However, such methods require large amounts of manually-labeled training data. There have been efforts on replacing human annotations with distant supervision (in conjunction with external dictionaries), but the generated noisy labels pose significant challenges on learning effective neural models. Here we propose two neural models to suit noisy distant supervision from the dictionary. First, under the traditional sequence labeling framework, we propose a revised fuzzy CRF layer to handle tokens with multiple possible labels. After identifying the nature of noisy labels in distant supervision, we go beyond the traditional framework and propose a novel, more effective neural model AutoNER with a new Tie or Break scheme. In addition, we discuss how to refine distant supervision for better NER performance. Extensive experiments on three benchmark datasets demonstrate that AutoNER achieves the best performance when only using dictionaries with no additional human effort, and delivers competitive results with state-of-the-art supervised benchmarks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, extensive efforts have been made on building reliable named entity recognition (NER) models without handcrafting features ( <ref type="bibr" target="#b14">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b10">Lample et al., 2016)</ref>. However, most existing methods require large amounts of manually annotated sentences for training supervised models (e.g., neural se- quence models) ( <ref type="bibr" target="#b14">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b10">Lample et al., 2016;</ref><ref type="bibr" target="#b3">Finkel et al., 2005</ref>). This is particularly challenging in specific do- * Equal contribution.</p><p>mains, where domain-expert annotation is expen- sive and/or slow to obtain.</p><p>To alleviate human effort, distant supervision has been applied to automatically generate labeled data, and has gained successes in various natural language processing tasks, including phrase min- ing ( , entity recognition ( <ref type="bibr" target="#b19">Ren et al., 2015;</ref><ref type="bibr" target="#b4">Fries et al., 2017;</ref><ref type="bibr" target="#b7">He, 2017)</ref>, aspect term extraction ( <ref type="bibr" target="#b5">Giannakopoulos et al., 2017)</ref>, and relation extraction ( <ref type="bibr" target="#b15">Mintz et al., 2009)</ref>. Mean- while, open knowledge bases (or dictionaries) are becoming increasingly popular, such as WikiData and YAGO in the general domain, as well as MeSH and CTD in the biomedical domain. The existence of such dictionaries makes it possible to generate training data for NER at a large scale without additional human effort.</p><p>Existing distantly supervised NER models usu- ally tackle the entity span detection problem by heuristic matching rules, such as POS tag-based regular expressions <ref type="bibr" target="#b19">(Ren et al., 2015;</ref><ref type="bibr" target="#b4">Fries et al., 2017)</ref> and exact string matching ( <ref type="bibr" target="#b5">Giannakopoulos et al., 2017;</ref><ref type="bibr" target="#b7">He, 2017)</ref>. In these models, every unmatched token will be tagged as non- entity. However, as most existing dictionaries have limited coverage on entities, simply ignoring un- matched tokens may introduce false-negative la- bels (e.g., "prostaglandin synthesis" in <ref type="figure">Fig. 1</ref>). Therefore, we propose to extract high-quality out- of-dictionary phrases from the corpus, and mark them as potential entities with a special "un- known" type. Moreover, every entity span in a sentence can be tagged with multiple types, since two entities of different types may share the same surface name in the dictionary. To address these challenges, we propose and compare two neural architectures with customized tagging schemes.</p><p>We start with adjusting models under the tra- ditional sequence labeling framework. Typically, NER models are built upon conditional random fields (CRF) with the IOB or IOBES tagging scheme ( <ref type="bibr" target="#b14">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b10">Lample et al., 2016;</ref><ref type="bibr" target="#b18">Ratinov and Roth, 2009;</ref><ref type="bibr" target="#b3">Finkel et al., 2005</ref>). However, such design can- not deal with multi-label tokens. Therefore, we customize the conventional CRF layer in LSTM- CRF ( <ref type="bibr" target="#b10">Lample et al., 2016</ref>) into a Fuzzy CRF layer, which allows each token to have multiple labels without sacrificing computing efficiency.</p><p>To adapt to imperfect labels generated by dis- tant supervision, we go beyond the traditional se- quence labeling framework and propose a new prediction model. Specifically, instead of predict- ing the label of each single token, we propose to predict whether two adjacent tokens are tied in the same entity mention or not (i.e., broken). The key motivation is that, even the boundaries of an en- tity mention are mismatched by distant supervi- sion, most of its inner ties are not affected, and thus more robust to noise. Therefore, we design a new Tie or Break tagging scheme to bet- ter exploit the noisy distant supervision. Accord- ingly, we design a novel neural architecture that first forms all possible entity spans by detecting such ties, then identifies the entity type for each span. The new scheme and neural architecture form our new model, AutoNER, which proves to work better than the Fuzzy CRF model in our ex- periments.</p><p>We summarize our major contributions as</p><p>• We propose AutoNER, a novel neural model with the new Tie or Break scheme for the distantly supervised NER task.</p><p>• We revise the traditional NER model to the Fuzzy-LSTM-CRF model, which serves as a strong distantly supervised baseline.</p><p>• We explore to refine distant supervision for bet- ter NER performance, such as incorporating high-quality phrases to reduce false-negative la- bels, and conduct ablation experiments to verify the effectiveness.</p><p>• Experiments on three benchmark datasets demonstrate that AutoNER achieves the best performance when only using dictionaries with no additional human effort and is even compet- itive with the supervised benchmarks.</p><p>We release all code and data for future studies <ref type="bibr">1</ref> . Related open tools can serve as the NER module 1 https://github.com/shangjingbo1226/ AutoNER of various domain-specific systems in a plug-in- and-play manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview</head><p>Our goal, in this paper, is to learn a named entity tagger using, and only using dictionaries. Each dictionary entry consists of 1) the surface names of the entity, including a canonical name and a list of synonyms; and 2) the entity type. Considering the limited coverage of dictionaries, we extend ex- isting dictionaries by adding high-quality phrases as potential entities with unknown type. More de- tails on refining distant supervision for better NER performance will be presented in Sec. 4.</p><p>Given a raw corpus and a dictionary, we first generate entity labels (including unknown la- bels) by exact string matching, where conflicted matches are resolved by maximizing the total number of matched tokens ( <ref type="bibr" target="#b2">Etzioni et al., 2005;</ref><ref type="bibr" target="#b6">Hanisch et al., 2005;</ref><ref type="bibr" target="#b12">Lin et al., 2012;</ref><ref type="bibr" target="#b7">He, 2017)</ref>.</p><p>Based on the result of dictionary matching, each token falls into one of three categories: 1) it be- longs to an entity mention with one or more known types; 2) it belongs to an entity mention with un- known type; and 3) It is marked as non-entity.</p><p>Accordingly, we design and explore two neu- ral models, Fuzzy-LSTM-CRF with the modified IOBES scheme and AutoNER with the Tie or Break scheme, to learn named entity taggers based on such labels with unknown and multiple types. We will discuss the details in Sec. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Models</head><p>In this section, we introduce two prediction mod- els for the distantly supervised NER task, one un- der the traditional sequence labeling framework and another with a new labeling scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Fuzzy-LSTM-CRF with Modified IOBES</head><p>State-of-the-art named entity taggers follow the sequence labeling framework using IOB or IOBES scheme ( <ref type="bibr" target="#b18">Ratinov and Roth, 2009)</ref>, thus requiring a conditional random field (CRF) layer to capture the dependency between labels. How- ever, both the original scheme and the conven- tional CRF layer cannot handle multi-typed or unknown-typed tokens. Therefore, we propose the modified IOBES scheme and Fuzzy CRF layer ac- cordingly, as illustrated in <ref type="figure">Figure 1</ref>.</p><p>Modified IOBES. We define the labels accord- ing to the three token categories. 1) For a token</p><formula xml:id="formula_0">B- Diease I- Diease S- Chemical O … B- Diease I- Diease S- Chemical O … &lt;S&gt; Thus , indomethacin B- Diease I- Diease S- Chemical O … , by inhibition prostaglandin B- Diease I- Diease S- Chemical O … B- Diease I- Diease S- Chemical O … B- Diease I- Diease S- Chemical O … B- Diease I- Diease S- Chemical O … of synthesis diminish B- Diease I- Diease S- Chemical O … B- Diease I- Diease S- Chemical O … B- Diease I- Diease S- Chemical O … may … … … … max log( P P )</formula><p>Figure 1: The illustration of the Fuzzy CRF layer with modified IOBES tagging scheme. The named entity types are {Chemical, Disease}. "indomethacin" is a matched Chemical entity and "prostaglandin synthesis" is an unknown-typed high-quality phrase. Paths from Start to End marked as purple form all possible label sequences given the distant supervision.</p><p>marked as one or more types, it is labeled with all these types and one of {I, B, E, S} according to its positions in the matched entity mention. 2) For a token with unknown type, all five {I, O, B, E, S} tags are possible. Meanwhile, all available types are assigned. For example, when there are only two available types (e.g., Chemical and Disease), it has nine (i.e., 4 × 2 + 1) possible labels in total.</p><p>3) For a token that is annotated as non-entity, it is labeled as O.</p><p>As demonstrated in <ref type="figure">Fig. 1</ref>, based on the dic- tionary matching results, "indomethacin" is a singleton Chemical entity and "prostaglandin synthesis" is an unknown-typed high-quality phrase. Therefore, "indomethacin" is labeled as S-Chemical, while both "prostaglandin" and "synthesis" are labeled as O, B-Disease, I-Disease, . . ., and S-Chemical because the available entity types are {Chemical, Disease}. The non-entity tokens, such as "Thus" and "by", are labeled as O.</p><p>Fuzzy-LSTM-CRF. We revise the LSTM-CRF model ( <ref type="bibr" target="#b10">Lample et al., 2016)</ref> to the Fuzzy-LSTM- CRF model to support the modified IOBES labels.</p><p>Given a word sequence (X 1 , X 2 , . . . , X n ), it is first passed through a word-level BiL- STM (Hochreiter and Schmidhuber, 1997) (i.e., forward and backward LSTMs). After concatenat- ing the representations from both directions, the model makes independent tagging decisions for each output label. In this step, the model estimates the score P i,y j for the word X i being the label y j .</p><p>We follow previous works ( <ref type="bibr" target="#b14">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b10">Lample et al., 2016)</ref> to define the score of the predicted sequence, the score of the predicted sequence (y 1 , y 2 , . . . , y n ) is defined as:</p><formula xml:id="formula_1">s(X, y) = n i=0 Φ y i ,y i+1 + n i=1 P i,y i (1)</formula><p>where, Φ y i ,y i+1 is the transition probability from a label</p><formula xml:id="formula_2">y i to its next label y i+1 . Φ is a (k + 2) × (k + 2) matrix,</formula><p>where k is the number of distinct labels. Two additional labels start and end are used (only used in the CRF layer) to represent the beginning and end of a sequence, respectively. The conventional CRF layer maximizes the probability of the only valid label sequence. How- ever, in the modified IOBES scheme, one sentence may have multiple valid label sequences, as shown in <ref type="figure">Fig. 1</ref>. Therefore, we extend the conventional CRF layer to a fuzzy CRF model. Instead, it max- imizes the total probability of all possible label se- quences by enumerating both the IOBES tags and all matched entity types. Mathematically, we de- fine the optimization goal as Eq. 2.</p><formula xml:id="formula_3">p(y|X) = ˜ y∈Y possible e s(X,˜ y) ˜ y∈Y X e s(X,˜ y)<label>(2)</label></formula><p>where Y X means all the possible label sequences for sequence X, and Y possible contains all the pos- sible label sequences given the labels of modified IOBES scheme. Note that, when all labels and types are known and unique, the fuzzy CRF model is equivalent to the conventional CRF.</p><p>During the training process, we maximize the log-likelihood function of Eq. 2. For inference, we apply the Viterbi algorithm to maximize the score of Eq. 1 for each input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">AutoNER with "Tie or Break"</head><p>Identifying the nature of the distant supervision, we go beyond the sequence labeling framework and propose a new tagging scheme, Tie or Break. It focuses on the ties between adjacent to- kens, i.e., whether they are tied in the same entity mentions or broken into two parts. Accordingly, we design a novel neural model for this scheme.</p><formula xml:id="formula_4">e r a m i c u n i o d ␣ a n d ␣ 8 G B ␣ R c2,0 c2,1 c2,2 c2,3 c2,4 c2,5 c2,6 c2, c3,0 c3,1 c3,2 c3,3 c3,4 c3,5 c3,6 c3, c4,0 c4,2 c4,1 c4, c5,0 c5,1 c5,2 c5, c6,0</formula><p>ceramic unibody and ␣</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tie Break Unknown</head><p>Entity Type: AspectTerm Entity Type: None</p><formula xml:id="formula_5">c ␣ b y ␣ RAM ␣ 8GB ␣ c2, Break ␣ ␣ w i c6,0 c1,0 c3,6 t with h Break ␣ ␣ Entity Type: None Unknown c0, c1,2</formula><p>Figure 2: The illustration of AutoNER with Tie or Break tagging scheme. The named entity type is {AspectTerm}. "ceramic unibody" is a matched AspectTerm entity and "8GB RAM" is an unknown-typed high-quality phrase. Unknown labels will be skipped during the model training.</p><p>"Tie or Break" Tagging Scheme. Specifically, for every two adjacent tokens, the connection between them is labeled as (1) Tie, when the two tokens are matched to the same entity; <ref type="formula" target="#formula_3">(2)</ref> Unknown, if at least one of the tokens belongs to an unknown-typed high-quality phrase; (3) Break, otherwise. An example can be found in <ref type="figure">Fig. 2</ref>. The dis- tant supervision shows that "ceramic unibody" is a matched AspectTerm and "8GB RAM" is an unknown-typed high-quality phrase. Therefore, a Tie is labeled between "ceramic" and "unibody", while Unknown labels are put before "8GB", be- tween "8GB" and "RAM", and after "RAM".</p><p>Tokens between every two consecutive Break form a token span. Each token span is associated with all its matched types, the same as for the mod- ified IOBES scheme. For those token spans with- out any associated types, such as "with" in the ex- ample, we assign them the additional type None.</p><p>We believe this new scheme can better exploit the knowledge from dictionary according to the following two observations. First, even though the boundaries of an entity mention are mismatched by distant supervision, most of its inner ties are not affected. More interestingly, compared to multi-word entity mentions, matched unigram en- tity mentions are more likely to be false-positive labels. However, such false-positive labels will not introduce incorrect labels with the Tie or Break scheme, since either the unigram is a true entity mention or a false positive, it always brings two Break labels around.</p><p>AutoNER. In the Tie or Break scheme, en- tity spans and entity types are encoded into two folds. Therefore, we separate the entity span de- tection and entity type prediction into two steps.</p><p>For entity span detection, we build a binary classifier to distinguish Break from Tie, while Unknown positions will be skipped. Specifically, as shown in <ref type="figure">Fig. 2</ref>, for the prediction between i-th token and its previous token, we concatenate the output of the BiLSTM as a new feature vector, u i . u i is then fed into a sigmoid layer, which estimates the probability that there is a Break as</p><formula xml:id="formula_6">p(y i = Break|u i ) = σ(w T u i )</formula><p>where y i is the label between the i-th and its pre- vious tokens, σ is the sigmoid function, and w is the sigmoid layer's parameter. The entity span de- tection loss is then computed as follows.</p><formula xml:id="formula_7">L span = i|y i =Unknown l y i , p(y i = Break|u i )</formula><p>Here, l(·, ·) is the logistic loss. Note that those Unknown positions are skipped. After obtaining candidate entity spans, we fur- ther identify their entity types, including the None type for non-entity spans. As shown in <ref type="figure">Fig. 2</ref>, the output of the BiLSTM will be re-aligned to form a new feature vector, which is referred as v i for i-th span candidate. v i will be further fed into a softmax layer, which estimates the entity type dis- tribution as</p><formula xml:id="formula_8">p(t j |v i ) = e t T j v i t k ∈L e t T k v i</formula><p>where t j is an entity type and L is the set of all entity types including None.</p><p>Since one span can be labeled as multiple types, we mark the possible set of types for i-th entity span candidate as L i . Accordingly, we modify the cross-entropy loss as follows.</p><formula xml:id="formula_9">L type = H(ˆ p(·|v i , L i ), p(·|v i ))</formula><p>Here, H(p, q) is the cross entropy between p and q, andˆpandˆ andˆp(t j |v i , L i ) is the soft supervision distribu-tion. Specifically, it is defined as:</p><formula xml:id="formula_10">ˆ p(t j |v i , L i ) = δ(t j ∈ L i ) · e t T j v i t k ∈L δ(t k ∈ L i ) · e t T k v i where δ(t j ∈ L i )</formula><p>is the boolean function of check- ing whether the i-th span candidate is labeled as the type t j in the distant supervision.</p><p>It's worth mentioning that AutoNER has no CRF layer and Viterbi decoding, thus being more efficient than Fuzzy-LSTM-CRF for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Remarks on "Unknown" Entities</head><p>"Unknown" entity mentions are not the entities of other types, but the tokens that we are less confi- dent about their boundaries and/or cannot identify their types based on the distant supervision. For example, in <ref type="figure">Figure 1</ref>, "prostaglandin synthesis" is an "unknown" token span. The distant supervi- sion cannot decide whether it is a Chemical, a Disease, an entity of other types, two separate single-token entities, or (partially) not an entity. Therefore, in the FuzzyCRF model, we assign all possible labels for these tokens.</p><p>In our AutoNER model, these "unknown" posi- tions have undefined boundary and type losses, be- cause (1) they make the boundary labels unclear; and (2) they have no type labels. Therefore, they are skipped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Distant Supervision Refinement</head><p>In this section, we present two techniques to refine the distant supervision for better named entity tag- gers. Ablation experiments in Sec. 5.4 verify their effectiveness empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Corpus-Aware Dictionary Tailoring</head><p>In dictionary matching, blindly using the full dic- tionary may introduce false-positive labels, as there exist many entities beyond the scope of the given corpus but their aliases can be matched. For example, when the dictionary has a non-related character name "Wednesday Addams" 2 and its alias "Wednesday", many Wednesday's will be wrongly marked as persons. In an ideal case, the dictionary should cover, and only cover entities occurring in the given corpus to ensure a high pre- cision while retaining a reasonable coverage.</p><p>As an approximation, we tailor the original dic- tionary to a corpus-related subset by excluding en- tities whose canonical names never appear in the given corpus. The intuition behind is that to avoid ambiguities, people will likely mention the canon- ical name of the entity at least once. For example, in the biomedical domain, this is true for 88.12%, 95.07% of entity mentions on the BC5CDR and NCBI datasets respectively. We expect the NER model trained on such tailored dictionary will have a higher precision and a reasonable recall com- pared to that trained on the original dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Unknown-Typed High-Quality Phrases</head><p>Another issue of the distant supervision is about the false-negative labels. When a token span can- not be matched to any entity surface names in the dictionary, because of the limited coverage of dic- tionaries, it is still difficult to claim it as non-entity (i.e., negative labels) for sure. Specifically, some high-quality phrases out of the dictionary may also be potential entities.</p><p>We utilize the state-of-the-art distantly super- vised phrase mining method, AutoPhrase , with the corpus and dictionary in the given domain as input. AutoPhrase only re- quires unlabeled text and a dictionary of high- quality phrases. We obtain quality multi-word and single-word phrases by posing thresholds (e.g., 0.5 and 0.9 respectively). In practice, one can find more unlabeled texts from the same domain (e.g., PubMed papers and Amazon laptop reviews) and use the same domain-specific dictionary for the NER task. In our experiments, for the biomedical domain, we use the titles and abstracts of 686,568 PubMed papers (about 4%) uniformly sampled from the whole PubTator database as the train- ing corpus. For the laptop review domain, we use the Amazon laptop review dataset 3 , which is designed for the aspect-based sentiment analysis ( <ref type="bibr" target="#b25">Wang et al., 2011</ref>).</p><p>We treat out-of-dictionary phrases as poten- tial entities with "unknown" type and incorporate them as new dictionary entries. After this, only to- ken spans that cannot be matched in this extended dictionary will be labeled as non-entity. Being aware of these high-quality phrases, we expect the trained NER tagger should be more accurate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We conduct experiments on three benchmark datasets to evaluate and compare our proposed Fuzzy-LSTM-CRF and AutoNER with many other methods. We further investigate the effec- tiveness of our proposed refinements for the dis- tant supervision and the impact of the number of distantly supervised sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>Datasets are briefly summarized in <ref type="table" target="#tab_0">Table 1</ref>. More details as as follows.</p><p>• BC5CDR is from the most recent BioCreative V Chemical and Disease Mention Recognition task. It has 1,500 articles containing 15,935 Chemical and 12,852 Disease mentions.</p><p>• NCBI-Disease focuses on Disease Name Recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>It contains 793 abstracts and 6,881</head><p>Disease mentions.</p><p>• LaptopReview is from the SemEval 2014 Chal- lenge, Task 4 Subtask 1 ( <ref type="bibr" target="#b16">Pontiki et al., 2014</ref>) fo- cusing on laptop aspect term (e.g., "disk drive") Recognition. It consists of 3,845 review sen- tences and 3,012 AspectTerm mentions.</p><p>All datasets are publicly available. The first two datasets are already partitioned into three subsets: a training set, a development set, and a testing set.</p><p>For the LaptopReview dataset, we follow (Gian- nakopoulos et al., 2017) and randomly select 20% from the training set as the development set. Only raw texts are provided as the input of distantly su- pervised models, while the gold training set is used for supervised models.</p><p>Domain-Specific Dictionary. For the biomedi- cal datasets, the dictionary is a combination of both the MeSH database 4 and the CTD Chemical and Disease vocabularies 5 . The dictionary con- tains 322,882 Chemical and Disease entity surfaces. For the laptop review dataset, the dic- tionary has 13,457 computer terms crawled from a public website <ref type="bibr">6</ref> .</p><p>Metric. We use the micro-averaged F 1 score as the evaluation metric. Meanwhile, precision and recall are presented. The reported scores are the mean across five different runs.</p><p>Parameters and Model Training. Based on the analysis conducted in the development set, we conduct optimization with the stochastic gradient descent with momentum. We set the batch size and the momentum to 10 and 0.9. The learning rate is initially set to 0.05 and will be shrunk by 40% if there is no better development F 1 in the re- cent 5 rounds. Dropout of a ratio 0.5 is applied in our model. For a better stability, we use gradient clipping of 5.0. Furthermore, we employ the early stopping in the development set.</p><p>Pre-trained Word Embeddings.</p><p>For the biomedical datasets, we use the pre-trained 200- dimension word vectors 7 from ( <ref type="bibr" target="#b17">Pyysalo et al., 2013)</ref>, which are trained on the whole PubMed abstracts, all the full-text articles from PubMed Central (PMC), and English Wikipedia. For the laptop review dataset, we use the GloVe 100-dimension pre-trained word vectors 8 instead, which are trained on the Wikipedia and GigaWord.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Compared Methods</head><p>Dictionary Match is our proposed distant super- vision generation method. Specifically, we apply it to the testing set directly to obtain entity men- tions with exactly the same surface name as in the dictionary. The type is assigned through a major- ity voting. By comparing with it, we can check the improvements of neural models over the dis- tant supervision itself.</p><p>SwellShark, in the biomedical domain, is ar- guably the best distantly supervised model, es- pecially on the BC5CDR and NCBI-Disease datasets ( <ref type="bibr" target="#b4">Fries et al., 2017)</ref>. It needs no human an- notated data, however, it requires extra expert ef- fort for entity span detection on building POS tag- ger, designing effective regular expressions, and hand-tuning for special cases.</p><p>Distant-LSTM-CRF achieved the best perfor- mance on the LaptopReview dataset without an- notated training data using a distantly supervised <ref type="table">Table 2</ref>: [Biomedical Domain] NER Performance Comparison. The supervised benchmarks on the BC5CDR and NCBI-Disease datasets are LM-LSTM-CRF and LSTM-CRF respectively ( <ref type="bibr" target="#b26">Wang et al., 2018)</ref>. SwellShark has no annotated data, but for entity span extraction, it requires pre-trained POS taggers and extra human efforts of designing POS tag-based regular expressions and/or hand-tuning for special cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Human  LSTM-CRF model ( <ref type="bibr" target="#b5">Giannakopoulos et al., 2017)</ref>. Supervised benchmarks on each dataset are listed to check whether AutoNER can deliver com- petitive performance. On the BC5CDR and NCBI- Disease datasets, LM-LSTM-CRF (  and LSTM-CRF ( <ref type="bibr" target="#b10">Lample et al., 2016)</ref> achieve the state-of-the-art F 1 scores without ex- ternal resources, respectively ( <ref type="bibr" target="#b26">Wang et al., 2018</ref>).</p><p>On the LaptopReview dataset, we present the scores of the Winner in the SemEval2014 Chal- lenge Task 4 Subtask 1 (Pontiki et al., 2014).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">NER Performance Comparison</head><p>We present F 1 , precision, and recall scores on all datasets in <ref type="table" target="#tab_2">Table 2 and Table 3</ref>. From both ta- bles, one can find the AutoNER achieves the best performance when there is no extra human effort. Fuzzy-LSTM-CRF does have some improvements over the Dictionary Match, but it is always worse than AutoNER. Even though SwellShark is designed for the biomedical domain and utilizes much more ex- pert effort, AutoNER outperforms it in almost all cases. The only outlier happens on the NCBI- disease dataset when the entity span matcher in SwellShark is carefully tuned by experts for many special cases.</p><p>It is worth mentioning that AutoNER beats Distant-LSTM-CRF, which is the previous state- of-the-art distantly supervised model on the Lap- topReview dataset.</p><p>Moreover, AutoNER's performance is compet- itive to the supervised benchmarks. For exam- ple, on the BC5CDR dataset, its F 1 score is only 2.16% away from the supervised benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Distant Supervision Explorations</head><p>We investigate the effectiveness of the two tech- niques that we proposed in Sec. 4 via ablation ex- periments. As shown in <ref type="table" target="#tab_3">Table 4</ref>, using the tailored dictionary always achieves better F 1 scores than using the original dictionary. By using the tailored dictionary, the precision of the AutoNER model will be higher, while the recall will be retained similarly. For example, on the NCBI-Disease dataset, it significantly boosts the precision from 53.14% to 77.30% with an acceptable recall loss from 63.54% to 58.54%. Moreover, incorporating unknown-typed high-quality phrases in the dictio- nary enhances every score of AutoNER models significantly, especially the recall. These results match our expectations well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Test F 1 Scores vs. Size of Raw Corpus</head><p>Furthermore, we explore the change of test F 1 scores when we have different sizes of distantly supervised texts. We sample sentences uniformly random from the given raw corpus and then evalu- ate AutoNER models trained on the selected sen- tences. We also study what will happen when the gold training set is available. The curves can be found in <ref type="figure" target="#fig_0">Figure 3</ref>. The X-axis is the number of  distantly supervised training sentences while the Y-axis is the F 1 score on the testing set. When using distant supervision only, one can observe a significant growing trend of test F 1 score in the beginning, but later the increasing rate slows down when there are more and more raw texts.</p><p>When the gold training set is available, the dis- tant supervision is still helpful to AutoNER. In the beginning, AutoNER works worse than the super- vised benchmarks. Later, with enough distantly supervised sentences, AutoNER outperforms the supervised benchmarks. We think there are two possible reasons: (1) The distant supervision puts emphasis on those matchable entity mentions; and (2) The gold annotation may miss some good but matchable entity mentions. These may guide the training of AutoNER to a more generalized model, and thus have a higher test F 1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Comparison with Gold Supervision</head><p>To demonstrate the effectiveness of distant super- vision, we try to compare our method with gold annotations provided by human experts.</p><p>Specifically, we conduct experiments on the BC5CDR dataset by sampling different amounts of annotated articles for model training. As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, we found that our method outper- forms the supervised method by a large margin when less training examples are available. For ex- ample, when there are only 50 annotated articles available, the test F1 score drops substantially to 74.29%. To achieve a similar test F1 score (e.g.,  83.91%) as our AutoNER models (i.e., 84.8%), the supervised benchmark model requires at least 300 annotated articles. Such results indicate the effectiveness and usefulness of AutoNER on the scenario without sufficient human annotations.</p><p>Still, we observe that, when the supervised benchmark is trained with all annotations, it achieves the performance better than AutoNER. We conjugate that this is because AutoNER lacks more advanced techniques to handle distant super- vision, and we leave further improvements of Au- toNER to the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>The task of supervised named entity recognition (NER) is typically embodied as a sequence label- ing problem. Conditional random fields (CRF) models built upon human annotations and hand- crafted features are the standard ( <ref type="bibr" target="#b3">Finkel et al., 2005;</ref><ref type="bibr" target="#b21">Settles, 2004;</ref><ref type="bibr" target="#b11">Leaman and Gonzalez, 2008)</ref>. Recent advances in neural models have freed do-main experts from handcrafting features for NER tasks. ( <ref type="bibr" target="#b10">Lample et al., 2016;</ref><ref type="bibr" target="#b14">Ma and Hovy, 2016;</ref>. Such neural models are in- creasingly common in the domain-specific NER tasks ( <ref type="bibr" target="#b20">Sahu and Anand, 2016;</ref><ref type="bibr" target="#b1">Dernoncourt et al., 2017;</ref><ref type="bibr" target="#b26">Wang et al., 2018)</ref>. Semi-supervised meth- ods have been explored to further improve the ac- curacy by either augmenting labeled datasets with word embeddings or bootstrapping techniques in tasks like gene name recognition ( <ref type="bibr" target="#b9">Kuksa and Qi, 2010;</ref><ref type="bibr" target="#b23">Tang et al., 2014;</ref><ref type="bibr" target="#b24">Vlachos and Gasperin, 2006</ref>). Unlike these existing approaches, our study focuses on the distantly supervised setting without any expert-curated training data.</p><p>Distant supervision has attracted many atten- tions to alleviate human efforts. Originally, it was proposed to leverage knowledge bases to super- vise relation extraction tasks <ref type="bibr" target="#b0">(Craven et al., 1999;</ref><ref type="bibr" target="#b15">Mintz et al., 2009)</ref>. AutoPhrase has demonstrated powers in extracting high-quality phrases from domain-specific corpora like scientific papers and business reviews ) but it cannot categorize phrases into typed entities in a context- aware manner. We incorporate the high-quality phrases to enrich the domain-specific dictionary.</p><p>There are attempts on the distantly supervised NER task recently <ref type="bibr" target="#b19">(Ren et al., 2015;</ref><ref type="bibr" target="#b4">Fries et al., 2017;</ref><ref type="bibr" target="#b7">He, 2017;</ref><ref type="bibr" target="#b5">Giannakopoulos et al., 2017)</ref>. For example, <ref type="bibr">SwellShark (Fries et al., 2017)</ref>, specif- ically designed for biomedical NER, leverages a generative model to unify and model noise across different supervision sources for named entity typ- ing. However, it leaves the named entity span detection to a heuristic combination of dictionary matching and part-of-speech tag-based regular ex- pressions, which require extensive expert effort to cover many special cases. Other methods <ref type="bibr" target="#b19">(Ren et al., 2015;</ref><ref type="bibr" target="#b7">He, 2017</ref>) also utilize similar ap- proaches to extract entity span candidates before entity typing. Distant-LSTM-CRF ( <ref type="bibr" target="#b5">Giannakopoulos et al., 2017</ref>) has been proposed for the distantly supervised aspect term extraction, which can be viewed as an entity recognition task of a single type for business reviews. As shown in our experi- ments, our models can outperform Distant-LSTM- CRF significantly on the laptop review dataset.</p><p>To the best of our knowledge, AutoNER is the most effective model that can learn NER models by using, and only using dictionaries without any additional human effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this paper, we explore how to learn an effective NER model by using, and only using dictionar- ies. We design two neural architectures, Fuzzy- LSTM-CRF model with a modified IOBES tag- ging scheme and AutoNER with a new Tie or Break scheme. In experiments on three bench- mark datasets, AutoNER achieves the best F 1 scores without additional human efforts. Its per- formance is even competitive to the supervised benchmarks with full human annotation. In ad- dition, we discuss how to refine the distant super- vision for better NER performance, including in- corporating high-quality phrases mined from the corpus as well as tailoring dictionary according to the given corpus, and demonstrate their effective- ness in ablation experiments.</p><p>In future, we plan to further investigate the power and potentials of the AutoNER model with Tie or Break scheme in different languages and domains. Also, the proposed framework can be further extended to other sequence labeling tasks, such as noun phrase chunking. Moreover, going beyond the classical NER setting in this paper, it is interesting to further explore distant supervised methods for the nested and multiple typed entity recognitions in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: AutoNER: Test F 1 score vs. the number of distantly supervised sentences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: AutoNER: Test F 1 score vs. the number of human annotated articles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Dataset Overview.</head><label>1</label><figDesc></figDesc><table>Dataset 
BC5CDR 
NCBI-Disease 
LaptopReview 

Domain 
Biomedical 
Biomedical 
Technical Review 

Entity Types Disease, Chemical 
Disease 
AspectTerm 

Dictionary 
MeSH + CTD 
MeSH + CTD Computer Terms 

Raw Sent. # 
20,217 
7,286 
3,845 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>[Technical Review Domain] NER Perfor-
mance Comparison. The supervised benchmark refers 
to the challenge winner. 

Method 
LaptopReview 

Pre 
Rec 
F1 

Supervised Benchmark 84.80 66.51 74.55 

Distant-LSTM-CRF 
74.03 31.59 53.93 

Dictionary Match 
90.68 44.65 59.84 

Fuzzy-LSTM-CRF 
85.08 47.09 60.63 

AutoNER 
72.27 59.79 65.44 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 : Ablation Experiments for Dictionary Refinement. The dictionary for the LaptopReview dataset contains no alias, so the corpus-aware dictionary tailoring is not applicable.</head><label>4</label><figDesc></figDesc><table>Method 
BC5CDR 
NCBI-Disease 
LaptopReview 

Pre 
Rec 
F1 
Pre 
Rec 
F1 
Pre 
Rec 
F1 

AutoNER w/ Original Dict 
82.79 70.40 76.09 53.14 63.54 57.87 69.96 49.85 58.21 

AutoNER w/ Tailored Dict 
84.57 70.22 76.73 77.30 58.54 66.63 
Not Applicable 

AutoNER w/ Tailored Dict &amp; Phrases 88.96 81.00 
84.8 
79.42 71.98 75.52 72.27 59.79 65.44 

0 
5000 
10000 
15000 
20000 
# of Distantly Labeled Training Sentences 

0.70 

0.75 

0.80 

0.85 

Test F1 Scores 
AutoNER-Gold+DistantSupervision 
Supervised Benchmark 
AutoNER-DistantSupervision 

</table></figure>

			<note place="foot" n="2"> https://en.wikipedia.org/wiki/ Wednesday_Addams</note>

			<note place="foot" n="3"> http://times.cs.uiuc.edu/ ˜ wang296/ Data/</note>

			<note place="foot" n="4"> https://www.nlm.nih.gov/mesh/ download_mesh.html 5 http://ctdbase.org/downloads/</note>

			<note place="foot" n="6"> https://www.computerhope.com/jargon. htm 7 http://bio.nlplab.org/ 8 https://nlp.stanford.edu/projects/ glove/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Yu Zhang from Univer-sity of Illinois at Urbana-Champaign for provid-ing results of supervised benchmark methods on the BC5CDR and NCBI datasets.</p><p>Research was sponsored in part by U.S. Army Research Lab. under Cooperative Agreement No. W911NF-09-2-0053 (NSCTA), DARPA under Agreement No. W911NF-17-C-0099, National Science Foundation IIS 16-18481, IIS 17-04532, and IIS-17-41317, DTRA HD-TRA11810026, Google Ph.D. Fellowship and grant 1U54GM114838 awarded by NIGMS through funds provided by the trans-NIH Big Data to Knowledge (BD2K) initiative (www.bd2k.nih.gov). Any opinions, findings, and conclusions or recommendations expressed in this document are those of the author(s) and should not be interpreted as the views of any U.S. Gov-ernment. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Constructing biological knowledge bases by extracting information from text sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Kumlien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMB</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">1999</biblScope>
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">De-identification of patient notes with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozlem</forename><surname>Uzuner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="596" to="606" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised named-entity extraction from the web: An experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anamaria</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Daniel S Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="134" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Swellshark: A generative model for biomedical named entity recognition without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06360</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised aspect term extraction with b-lstm &amp; crf using automatically labelled datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanasios</forename><surname>Giannakopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudiu</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Hossmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Baeriswyl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</title>
		<meeting>the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="180" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Prominer: rule-based protein and gene entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hanisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Fundel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinz-Theodor</forename><surname>Mevissen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliane</forename><surname>Fluck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Autoentity: automated entity detection from massive text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>Computer Science of University of Illinois at Urbana-Champaign</orgName>
		</respStmt>
	</monogr>
<note type="report_type">M.S. Thesis for</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised bio-named entity recognition with word-codebook learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pavel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Kuksa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 SIAM International Conference on Data Mining</title>
		<meeting>the 2010 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Banner: an executable survey of advances in biomedical named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graciela</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biocomputing</title>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="652" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Entity linking at web scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Webscale Knowledge Extraction</title>
		<meeting>the Joint Workshop on Automatic Knowledge Base Construction and Webscale Knowledge Extraction</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="84" to="88" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Empower sequence labeling with task-aware neural language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haris</forename><surname>Papageorgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">2735</biblScope>
		</imprint>
	</monogr>
	<note>Ion Androutsopoulos, and Suresh Manandhar</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributional semantics resources for biomedical text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Moen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapio</forename><surname>Salakoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Symposium on Languages in Biology and Medicine</title>
		<meeting>the 5th International Symposium on Languages in Biology and Medicine<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="39" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Clustype: Effective entity recognition and typing by relation phrase-based clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>El-Kishky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangbo</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="995" to="1004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent neural network models for disease name recognition using domain invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunil</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Anand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2216" to="2225" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Biomedical named entity recognition using conditional random fields and rich feature sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international joint workshop on natural language processing in biomedicine and its applications</title>
		<meeting>the international joint workshop on natural language processing in biomedicine and its applications</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="104" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automated phrase mining from massive text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Evaluating word representation features in biomedical named entity recognition tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>BioMed research international</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bootstrapping and evaluating named entity recognition in the biomedical domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Gasperin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the HLTNAACL BioNLP Workshop on Linking Natural Language and Biology</title>
		<meeting>the HLTNAACL BioNLP Workshop on Linking Natural Language and Biology</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="138" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Latent aspect rating analysis without aspect keyword supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Cross-type biomedical named entity recognition with deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09851</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
