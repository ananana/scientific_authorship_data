<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Top-down Tree Structured Decoding with Syntactic Connections for Neural Machine Translation and Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jetic</forename><forename type="middle">G</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">TASC 9404</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
								<address>
									<addrLine>8888 University Drive</addrLine>
									<postCode>V5A 1S6</postCode>
									<settlement>Burnaby</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><forename type="middle">S</forename><surname>Shavarani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">TASC 9404</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
								<address>
									<addrLine>8888 University Drive</addrLine>
									<postCode>V5A 1S6</postCode>
									<settlement>Burnaby</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Sarkar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">TASC 9404</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
								<address>
									<addrLine>8888 University Drive</addrLine>
									<postCode>V5A 1S6</postCode>
									<settlement>Burnaby</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Top-down Tree Structured Decoding with Syntactic Connections for Neural Machine Translation and Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="401" to="413"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>401</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The addition of syntax-aware decoding in Neural Machine Translation (NMT) systems requires an effective tree-structured neural network , a syntax-aware attention model and a language generation model that is sensitive to sentence structure. We exploit a top-down tree-structured model called DRNN (Doubly-Recurrent Neural Networks) first proposed by Alvarez-Melis and Jaakola (2017) to create an NMT model called Seq2DRNN that combines a sequential encoder with tree-structured decoding augmented with a syntax-aware attention model. Unlike previous approaches to syntax-based NMT which use dependency parsing models our method uses constituency parsing which we argue provides useful information for translation. In addition, we use the syntactic structure of the sentence to add new connections to the tree-structured decoder neural network (Seq2DRNN+SynC). We compare our NMT model with sequential and state of the art syntax-based NMT models and show that our model produces more fluent translations with better reordering. Since our model is capable of doing translation and constituency parsing at the same time we also compare our parsing accuracy against other neural parsing models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation (NMT) models were initially proposed as extensions of sequential neu- ral language models ( <ref type="bibr" target="#b23">Sutskever et al., 2014;</ref><ref type="bibr">Cho et al., 2014;</ref><ref type="bibr" target="#b2">Bahdanau et al., 2015</ref>) or convolu- tions over n-grams in the decoder <ref type="bibr" target="#b11">(Kalchbrenner and Blunsom, 2013)</ref>. Early methods for dis- criminative training of machine translation models showed that the loss functions for translation were not sensitive to the production of certain impor- tant words such as verbs, without which the out- put sentence might be uninterpretable by humans. A good solution was to penalise such bad outputs using tree structures which get very low scores if important words like verbs are missing <ref type="bibr" target="#b4">(Chiang, 2005</ref>; <ref type="bibr" target="#b28">Zollmann and Venugopal, 2006</ref>; <ref type="bibr">Galley et al., 2006</ref>). To this end, there has been a push to incorporate some syntax into NMT mod- els: <ref type="bibr" target="#b21">Sennrich and Haddow (2016)</ref> incorporate POS tags and dependency information from the source side of a translation pair in NMT models. <ref type="bibr" target="#b22">Stahlberg et al. (2016)</ref> use source language syn- tax to guide the decoder of an NMT system to follow hierarchical structures (Hiero) rules <ref type="bibr" target="#b4">(Chiang, 2005</ref>). <ref type="bibr">Eriguchi et al. (2016)</ref> and <ref type="bibr" target="#b3">Bastings et al. (2017)</ref> use tree-structured encoders to exploit source language syntax. <ref type="bibr" target="#b0">Aharoni and Goldberg (2017)</ref> take the approach of serialising the parse trees to use in a sequential decoder. <ref type="bibr">Eriguchi et al. (2017)</ref> propose an NMT+RNNG model, which ex- plores the possibilities of using dependency syntax trees from the target language using StackLSTMs ( <ref type="bibr">Dyer et al., 2015</ref><ref type="bibr">Dyer et al., , 2016</ref> to aid a sequential de- coder. These approaches showed promising im- provements in translation quality but all the mod- els in previous work, even the model in <ref type="bibr">Eriguchi et al. (2017)</ref> which uses RNNG, are bottom-up tree structured decoders.</p><p>In contrast, we use a top-down tree-structured model called DRNN (Doubly-Recurrent Neural Networks) first proposed by <ref type="bibr" target="#b1">Alvarez-Melis and Jaakkola (2017)</ref> to model structural syntactic in- formation for NMT. We call our novel NMT model Seq2DRNN, using DRNNs as a tree- structured decoder combined with a sequential en- coder and a novel syntax-aware attention model.</p><p>All the previous work in syntax-aware NMT mentioned above has focused on dependency pars- ing as the syntactic model. In contrast, we wish to pursue phrase structure (aka constituency) based syntax-based NMT. We provide some analysis that shows that constituency information can help re- cover information in NMT decoding.</p><p>We perform extensive experiments comparing our model against other state-of-the-art sequence to sequence and syntax-aware NMT models and show that our model can improve translation qual- ity and reordering quality. The model performs translation and constituency parsing simultane- ously so we also compare our parsing accuracy to other neural parsing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model Description</head><p>In this paper, source sentence will be written as f = x 1 , x 2 , ..., x n , target sentence as e = y 1 , y 2 , ..., y m where y j is a word. Additionally, p k represents a non-terminal symbol (constituent, phrase) in the target sentence constituency tree.</p><p>[v; u] stands for concatenation of vectors v and u. W(x) is the word embedding of word x.</p><p>The design of our NMT system follows the encoder-decoder model (also known as a sequence to sequence model) proposed by <ref type="bibr">Cho et al. (2014)</ref> and <ref type="bibr" target="#b23">Sutskever et al. (2014)</ref>. Our system uses a standard bidirectional gated RNN (BiLSTM or bidirectional Long Short-Term Memory) ( <ref type="bibr" target="#b9">Huang et al., 2015)</ref> as the encoder and our proposed tree- structured RNN as the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sequence to Sequence NMT (Seq2Seq)</head><p>Neural machine translation models generally con- sist of an encoder, a decoder and an attention model ( <ref type="bibr" target="#b16">Luong et al., 2015;</ref>). The encoder is used to produce hidden representations of the source sentence, which is fed into the de- coder along with the attention information to pro- duce the translation output sequence.</p><p>A common approach is to use bidirectional LSTMs as encoder, produce forward hidden states − → h i and backward hidden states ← − h i , and the final representation h enc i is the concatenation of both:</p><formula xml:id="formula_0">− → h i = −−→ RNN enc ( − → h i−1 , W x (x i )) ← − h i = ←−− RNN enc ( ← − h i+1 , W x (x i )) h enc i = [ − → h i ; ← − h i ]<label>(1)</label></formula><p>The decoder takes the output of the encoder and generates a sequence in target language. The attention mechanism provides additional context vectors c j which is a weighted average contribu- tion of each h i source side encoding.</p><formula xml:id="formula_1">h dec j = RNN dec (h dec j−1 , [o j−1 ; c j ]) o j = softmax(Uh dec j + b)<label>(2)</label></formula><p>Here, U is the readout matrix and b is the bias vector. o j is the output word embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">NMT with a Tree-Structured Decoder (Seq2DRNN)</head><p>The output translation from a translation system should convey the same meaning as the input. This includes the correct word choices but also the right information structure. Sentence structure can be viewed as starting with an action or state (de- scribed via verbs or other predicates) and the en- tities or propositions involved in that activity or state (usually described via arguments to verbs). Thus certain words in the output translation, like verbs, are crucial to the understanding of the tar- get language sentence but only provide marginal value in n-gram matching evaluations like the BLEU score. Tree representations, produced via dependency parsing and constituency parsing, are useful because they are sensitive to this informa- tion structure. Our tree-structured decoder uses a neural network to generate trees (described in §2.2.1), which is incorporated into an NMT model (our novel encoder-decoder model is in §2.2.2) which translates and produces a parse tree. Our new Syntactic Connection method (SynC) is de- scribed in §2.2.4 which is combined with the Seq2DRNN model (Seq2DRNN+SynC) and the attention mechanism ( §2.2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Doubly-Recurrent Neural Network</head><p>The Doubly-Recurrent Neural Network model (Alvarez-Melis and Jaakkola, 2017) takes a vec- tor representation as input and generates a tree. <ref type="bibr" target="#b1">Alvarez-Melis and Jaakkola (2017)</ref> show that the DRNN model can effectively reconstruct trees but they do not use DRNNs within a full-scale NMT system. We also use DRNNs for phrase- structure (aka constituency) tree structures rather than dependency trees as in previous work. DRNN decoding proceeds top-down; the generation of nodes at depth d depend solely on the state of nodes at depth &lt; d. Unlike previous work in tree-structured decoding for NMT by <ref type="bibr">Dyer et al. (2016)</ref> and <ref type="bibr">Eriguchi et al. (2017)</ref>, the output sen- tence generation is not done in sequence, where the target word y j is generated after all y &lt;j are generated. DRNN first predicts the structure of the sentence and then expands each component to predict words. When generating y j , information regarding the structure of words from 1 to j − 1 and j + 1 to m can be used to aid prediction of y j . A DRNN consists of two recurrent neural net- work units, which separately process ancestral and fraternal information about nodes in the tree. As- suming a node is v, its immediate parent node is P (v) and its closest sibling on the left side (ap- pears in the target language sequence just before v) is S(v). The label of node v is z v . Then the an- cestral hidden representation h a and fraternal rep- resentation h f of a node are calculated with Equa- tion 3.</p><formula xml:id="formula_2">h a v = RNN a dec (h a P (v) , z P (v) ) h f v = RNN f dec (h f S(v) , z S(v) )<label>(3)</label></formula><p>h a v and h f v are then combined to produce the hid- den state of node v for prediction (predictive hid- den state h v , Equation 4), which is used to predict the labels of node v.</p><formula xml:id="formula_3">h v = tanh(U f h f v + U a h a v )<label>(4)</label></formula><p>During the label prediction, DRNN first makes topological decisions: whether (i) the current node is a leaf node (node with no children, α v ); then (ii) whether the current node has siblings on its right- hand side (γ v ). Both predictions are done using sigmoid activations:</p><formula xml:id="formula_4">o a v = σ(u a h v ) α v = 1 if o a v is activated.<label>(5)</label></formula><formula xml:id="formula_5">o f v = σ(u f h v ) γ v = 1 if o f v is activated.<label>(6)</label></formula><p>Then, label representation o v is predicted using α v and γ v , and the predictive hidden state h v :</p><formula xml:id="formula_6">o v = softmax(U o h v + α v u a + γ v u f ) (7)</formula><p>At inference time each node at the same depth is expanded independently, therefore the whole pro- cess can be parallelised. This parallelism advan- tage is not observed in any of the sequential de- coders that generate output sequence strictly from left to right nor from right to left ( §4 has more dis- cussion).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Parsing and Translating with DRNN</head><p>A DRNN is capable of producing a tree structure with labels given an input vector representation. If we train the DRNN to produce parse trees from the output of an encoder RNN, this system will be able to translate and parse at the same time.</p><p>(Alvarez-Melis and Jaakkola, 2017) in their paper provided a proof-of-concept NMT experiment us- ing dependency trees. Instead of an single RNN unit to process fraternal information they had mul- tiple for modelling the fraternal information on syntactic tree structure because dependency parse The model itself also disregards the sequential- ity of natural language, and lack attention mech- anisms to make it work in exchange for a strict top-bottom decoding procedure. We use constituency parse trees to represent sentences in the target language ( <ref type="figure">Figure 1</ref>) be- cause constituency or phrase-structure trees are more amenable to top-down derivation compared to dependency trees. It is also easier to model for DRNN, and presumably more capable at handling unknown words which is common in NMT sys- tems with limited vocabulary size.</p><p>Each node on the tree represents either a termi- nal symbol (a word) or a non-terminal symbol (a clause or phrase type). The sub-tree dominated by a non-terminal node is the clause or phrase identi- fied with this non-terminal node label.</p><p>A conventional bidirectional RNN (BiLSTM) encoder ( <ref type="bibr">Cho et al., 2014;</ref><ref type="bibr" target="#b23">Sutskever et al., 2014</ref>) is used to produce hidden states for the decoder (see <ref type="figure" target="#fig_0">Figure 2</ref>).</p><p>We use breadth-first search to implement the Seq2DRNN decoder. Two queues are used here: current queue which is the queue containing all of the nodes on the currently being processed depth, and next queue with nodes on the next depth (Al- gorithm 1 has all the details).</p><p>The decoding process starts from top to bottom, from root to its children, then to its grandchildren, and so on until the leaf nodes which are the output words.</p><p>In our implementation, sentence clauses (S nodes) are generated as the children of the root node to generalise over sentence types and in case there are multiple sentences in a single translation pair.</p><p>Initially, the current queue will only have one entry: the root node, which is initialised with the hidden representation of the source sentence.</p><p>Each node in the current queue is expanded in the following manner: first generate all of its sib- lings and add them to the current queue, and if any node happens to be non-terminal, generate its first child and add it to the next queue. After the current queue is empty, make next the new current queue and start working on nodes at the next depth.</p><p>For training, we use back-propagation through trees using the approach in <ref type="bibr" target="#b7">Goller and Küchler (1996)</ref>. In the forward pass, the source sentence is encoded into a hidden representation and fed into the decoder. The decoder generates the tree, pre- dicts the labels of every node from root to leaves. Then in the backward pass, gradients are calcu- lated and used to update the parameters. The loss calculation includes losses in topological predic- tions: o a v and o f v (Equations 5 and 6) and label pre- dictions: o v (Equation 7).</p><formula xml:id="formula_7">Loss(e) = v Loss label (o v , ˆ o v )+ α v Loss topo (o a v , ˆ o a v )+ α v Loss topo (o f v , ˆ o f v )<label>(8)</label></formula><p>Here α is a hyper-parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Attention Mechanism</head><p>Attention mechanisms usually work by adding an additional context vector during label prediction.</p><p>We use a variation of an existing attention mech- anism proposed by <ref type="bibr" target="#b16">Luong et al. (2015)</ref>. In our attention model, we produce a context vector c v for every node v by looking at all hidden states produced by the encoder h enc i , then calculating the weights and adding up the weighted hidden states.</p><formula xml:id="formula_8">weight v,i = V a tanh(W a h v + U a h enc i ) ∈ R (9) c v = n i=1ˆweight i=1ˆ i=1ˆweight v,i h enc i<label>(10)</label></formula><p>After the calculation in Equation 9, the weights are normalised with a softmax function before be-</p><formula xml:id="formula_9">Algorithm 1 Seq2DRNN Decoder 1: procedure DECODE(hiddenRep) 2:</formula><p>currentQueue ← Node from hiddenRep 3:</p><p>nextQueue ← empty 4: loop: 5: if currentQueue is not empty then 6:</p><p>node ← currentQueue.pop() 7:</p><p>Generate labels of node 8:</p><p>if node has siblings then 9:</p><p>currentQueue ← sibling(node) 10:</p><p>if node has children then 11:</p><p>nextQueue ← child(node) 12:</p><p>goto loop all nodes at current depth are generated move on to the next depth 13: if nextQueue is not empty then 14:</p><p>currentQueue ← nextQueue 15:</p><p>nextQueue ← empty 16:</p><p>goto loop. both queues should be empty now ing used to calculate the context vector. The atten- tion module allows the generation of labels to pay more attention to specific token representations of words in the input sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">SynC: Syntactic Connections for Lan- guage Generation (Seq2DRNN+SynC)</head><p>A conventional Seq2Seq model uses an RNN lan- guage model ( <ref type="bibr">Cho et al., 2014;</ref><ref type="bibr" target="#b23">Sutskever et al., 2014</ref>) conditioned on the input representation pro- duced by the encoder to generate the output one word at a time (Equation 11). The prediction of a word y j is directly conditioned on previously gen- erated words where c j is the context vector.</p><formula xml:id="formula_10">P(e) = j P(y j |y &lt;j , c j )<label>(11)</label></formula><p>The problem with this word-level language model is that it treats a sentence as a plain se- quence of symbols regardless of its syntactic con- struction. Sentences may contain multiple subor- dinate clauses and their boundaries are not well- modelled by sequential language models.</p><p>We propose a new method to connect the hid- den units in the Seq2DRNN decoder that pays at- tention to contextual tree relationships. The pre- diction of the representation of a word or a con- stituent z j (if a constituent then p j , if a word then y j ) is defined as follows:</p><formula xml:id="formula_11">P(z j | y &lt;j , c j ) = P(z j | y &lt;j , y k (∀k, z j ∈ p k ∨ precedes(y k , z j )) | c j )<label>(12)</label></formula><p>The generation of the representation of a word/constituent z j , which is part of the clause  that contain it (z j ∈ p k ), with clauses before which (precedes(p k , z j )), is conditioned on the follow- ing information: i) Word-level: previously gener- ated words y &lt;j ; ii) Ancestral Clause: the clauses that contain the current word p k , i.e. (∀kz j ∈ p k ); iii) Fraternal Clause: the clauses that precede the current clause p k , i.e. (∀k precedes(p k , z j )).</p><p>In practice, the generation of a node looks at the following representations:</p><p>1. Word-level: an RNN unit that produces the representation of previous words as a se- quence y &lt;j ;</p><p>2. Ancestral: treating the ancestors of the cur- rent node as a sequence (from root to the im- mediate parent), the representation of that se- quence: p k (∀k, z j ∈ p k );</p><p>3. Fraternal: treating the previous siblings of the current node as well as the previous sib- lings of its parent node and so on as a se- quence, the representation of that sequence:</p><formula xml:id="formula_12">p k (∀k, precedes(p k , z j )).</formula><p>SynC creates connections in the tree-structured decoder that pays attention to the structural con- text of generation of each terminal or non-terminal symbol in the phrase structure tree. For example in English, it is common for verb phrases to follow a noun phrase. But that noun phrase could itself be a subordinate clause with its own verb phrases. In this case, our goal is to explicitly model the fact that the previous phrase is a noun phrase instead of just the entire sequence of words.</p><p>SynC can be easily incorporated in the proposed Seq2DRNN model (Seq2DRNN+SynC). In addi- tion to the fraternal RNN unit that focuses on pre- ceding sibling nodes, and the ancestral DRNN unit that focus on parent nodes, a node would also look at its parent's previous sibling state (the hidden in Andrei when starving likes cheese, the prediction will be made knowing that the preceding clause is a noun phrase.</p><p>vector representation of preceding clauses from the very beginning of the sentence). When a non- terminal symbol is expanded into a sub-tree, it's first child will not have a previous sibling to pro- vide fraternal information (S(v) = Null, as in Equ 3). In this case, SynC establishes connection between its first child and its parent's fraternal in- formation provider for such fraternal RNN state (S(v) = S(P (v))).</p><formula xml:id="formula_13">h f v =        S(v) = Null, RNN f dec (h f S(v) , z S(v) ) S(v) = Null, S(v) := S(P (v)), RNN f dec (h f S(v) , z S(v) )<label>(13)</label></formula><p>An example is shown in <ref type="figure" target="#fig_3">Figure 4</ref>. In this case, a word-level language model will regard starving as the previous word, which is less helpful for the prediction of a verb phrase likes cheese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Training</head><p>Experiments in this paper utilise constituency trees on the target side, these trees are obtained by using the Stanford Lexical Parser ( <ref type="bibr" target="#b13">Klein and Manning, 2003a</ref>) which we chose for its speed and accuracy prior to training.</p><p>This procedure of pre-parsing data is not re- quired at test time, our NMT system would take a sentence as input and produces the translation in target language along with its constituency tree as output.</p><p>We use the German-English dataset from IWSLT2017 1 for our experiments, and tst2010- 2015 as the test set <ref type="table">(Table 1)</ref>.</p><p>To compare with other decoders that utilise target-side syntactic information, we also evaluate on three more datasets from News Commentary v8 using newstest2016 as testset <ref type="table" target="#tab_1">(Table 2)</ref>.</p><p>We replace all rarely occurring words with   UNK (Unknown) tokens. Only the top 50,000 most frequent words are kept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modelling details</head><p>The implementation of all models in this paper is done using DyNet (Neubig et al., 2017a) with Autobatching ( <ref type="bibr" target="#b18">Neubig et al., 2017b</ref> Note that this configuration is significantly smaller in both dimension and batch size than those presented in IWSLT2017 due to hardware limitations. All experiments are carried out on a single GTX 1080 Ti GPU with 11GB of VRAM. <ref type="table" target="#tab_2">Table 3 and Table 4</ref> has the BLEU ( <ref type="bibr" target="#b19">Papineni et al., 2002</ref>) and RIBES ( <ref type="bibr" target="#b10">Isozaki et al., 2010)</ref> scores. In our IWSLT2017 tests, both Seq2DRNN and Seq2DRNN+SynC produce better results than the Seq2Seq baseline model in terms of BLEU scores, while Seq2DRNN+SynC also produces better RIBES scores indicating better reordering of phrases in the output. The Seq2DRNN+SynC model performs better than the Seq2DRNN model. Both Seq2Seq and Seq2DRNN+SynC are able to produce results with lower perplexities than the baseline Seq2Seq model on the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>In our News Commentary v8 tests, the same relative performance from Seq2DRNN(SynC) can be observed. The Seq2DRNN+SynC model is also able to out-perform the Str2Tree model proposed by <ref type="bibr" target="#b0">Aharoni and Goldberg (2017)</ref> and NMT+RNNG by <ref type="bibr">Eriguchi et al. (2017)</ref> in most cases. Note that <ref type="bibr">Eriguchi et al. (2017)</ref> used de- pendency information instead of constituency in- formation as presented in our work and Aharoni and Goldberg (2017)'s work. <ref type="table" target="#tab_6">Table 5</ref> shows an example translation from all of the models we use in our experiments. Seq2Seq is able to translate with the correct vocabulary, but the sentences are often syntactically awkward. As the sentence length increases the syntactic fluency of Seq2Seq gets worse. Seq2DRNN is able to produce more syntactically fluent sentences since each lowest sub-clause contains typically ≤ 5 words. Seq2DRNN+SynC produces the best re- sults in this example: produces more syntactically fluent sentences, chooses the right words in the right place more frequently.</p><p>We also took several examples from our IWSLT17 experiment and blank out certain nouns by replacing them with unknown tokens ( <ref type="table">Table 6</ref>). Note that in our training set, most sentences do not have unknown tokens, and those that do only have at most 1. Our assumptions of the observed patterns in this case are: i) the proposed models are more capable at handling unknown tokens; ii) while Seq2DRNN is more capable at retaining the structure of the sentence, it cannot rely on a wider context to predict certain common phrases with noises in the source sentence; iii) the proposed Seq2DRNN+SynC model is more capable at han- dling unknown words both in the sense of being better at retaining sentence structure and handling noisy input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Attention Module</head><p>We visualise the attention weights of our Seq2DRNN+SynC model. Attention §2.2.3 com- putes a context vector for each node in the tree (a weighted sum of the source side vector represen- tations). For the translation pair in <ref type="figure" target="#fig_0">Figure 2</ref>, we show the attention weight of each pair of word and node (Equation 9) in <ref type="figure" target="#fig_5">Figure 5</ref>.</p><p>The addition of syntax nodes in the output en- ables the attention model to be used more effec- tively and is also valuable for visual inspection of syntactic nodes in the output mapping to the input.   Source wir wiederholten diesë ubung mit denselben studenten. was glauben sie passiert nun? nun verstanden sie den vorteil des prototyping. so wurde aus demselben, schlechten team eines unter den besten. sie produzierten die höchste konstruktion in der geringsten zeit. Literal we repeated this exercise with the same students. now what do you believe happened? now they understand the value of prototyping. so the same terrible team became one of the very best. they produced the tallest construction in the shortest time. Gold we did the exercise again with the same students. what do you think happened then? so now they understand the value of prototyping. so the same team went from being the very worst to being among the very best. they produced the tallest structures in the least amount of time. <ref type="bibr">Seq2Seq</ref> we repeated this with the same students. what happened you think differently? now, you know, the advantage of the design of the cycle. so, the same one of the team of the team among the best. it produced songs in the slightest building. <ref type="bibr">Seq2DRNN</ref> well repeat these queries with the same students. what do you think of this? now it understood the advantage of the interests. that's been made of the same thing of one thing. they produced the highest construction of the best time at the best time. Seq2DRNN+SynC we repeated this practice with the same students. what do you think happened? now, they understood the value of prototyping. it was being made of the same thing of one of the best ones. they produced the highest construction in the best time.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Parsing Quality</head><p>To evaluate the parsing quality, we follow the approach by <ref type="bibr" target="#b25">Vinyals et al. (2015)</ref> and train a DRNN(SynC) model to produce English to En- glish(Tree) translation. We use the same data and experiment settings that <ref type="bibr" target="#b25">Vinyals et al. (2015)</ref> used: the Wall Street Journal Penn Treebank English corpus with golden constituency structure, 256 for input/hidden dimension and 3 layers of RNN. We evaluate on section 23 of the aforementioned WSJ data using EVALB 2 . The results are presented in <ref type="table" target="#tab_8">Table 7</ref>.</p><p>Although falling short behind more specifically 2 https://nlp.cs.nyu.edu/evalb/ designed models like the RNNG by <ref type="bibr">Dyer et al. (2016)</ref>, our model is able to produce better results than the LSTM+AD model proposed by <ref type="bibr" target="#b25">Vinyals et al. (2015)</ref>, which is more comparable to ours since they are also using an NMT model to do con- stituency parsing. Since our work is more focusing on the translation aspect, optimising and designing a dedicated parser is slightly off-topic here. Nev- ertheless, it is worth noting that in 50.89% of the cases, Seq2DRNN+SynC was able to produce out- put that perfectly matches the reference. The same number for sentences with less than 40 words is 52.16%, while the F-measure increases to 90.5. This shows Seq2DRNN(SynC) when doing pars- ing can produce outputs of similar quality when handling longer sentences.</p><p>We also do evaluation on our translation results from the IWSLT dataset. Since translation re- sults do not come with reference parse trees, we parse the output of our decoder using the same parser we used in our other experiments: the Stan- ford Parser. Constituency parsing evaluation is done using Precision/Recall/F1-scores on the out- put constituent spans (unlabelled) and spans and labels (labelled). The results are presented in <ref type="table" target="#tab_9">Ta- ble 8 and Table 9</ref>. The parser we use gets F1 score of 87.04 on Penn Treebank English constituency 0 src es war zeit zum abendessen und wir hielten auss- chau nach einem restaurant.</p><p>die gute nachricht ist , dass die person , die das gesagt hat ann coulter war . S2S</p><p>and it was time for dinner , and we were looking for a restaurant.</p><p>the good news is that the person who said that was ann coulter . S2D it was the time for dinner , and we were looking for a restaurant.</p><p>the good news is that the person who said that was ann coulter . S2D+L it was time for dinner , and we were looking for a restaurant.</p><p>the good news is that the person who said that was ann coulter . 1 src es war zeit zum abendessen und wir hielten auss- chau nach einem UNK.</p><p>die gute UNK ist , dass die person , die das gesagt hat ann coulter war . S2S</p><p>and it was time for dinner , and we thought we were looking for a window.</p><p>the good news is that the person who said that the teacher was ann coulter . S2D it was the time for dinner , and we were looking for a UNK pilot.</p><p>the good motivator is that the person who said that was ann coulter . S2D+L it was time for dinner , and we were looking for a UNK.</p><p>the good news is that the person who said that was ann coulter . 2 src es war zeit zum UNK und wir hielten ausschau nach einem UNK.</p><p>die gute UNK ist , dass die UNK , die das gesagt hat ann coulter war . S2S</p><p>and it was time for the time time , and we thought we looked at a window search.</p><p>the good news is that the UNK that the UNK , which was ann coulter . S2D it was the second time , and we would look for a UNK pilot.</p><p>the good motivator is that the UNK group who said that was ann coulter. S2D+L it was a time for UNK , and we were looking for a UNK.</p><p>the good news is that the people who said that was ann coulter . 3 src es war UNK zum UNK und wir hielten ausschau nach einem UNK.</p><p>die gute UNK ist , dass die UNK , die das gesagt hat UNK war . S2S</p><p>UNK was a time time and UNK , looking for a UNK look for a look at a war.</p><p>UNK is not a cop 's needs to be ozzie good to good good , which is that the UNK UNK that said that it was said . S2D it was time of time and guerrilla . the good motivator is that the UNK planner that said this was a UNK video. S2D+L it was time for the tone and night to watch the connection .</p><p>the good news is that the people that said that was mandated . <ref type="table">Table 6</ref>: Unknown noun experiment samples. Substituted and correct nouns are marked in Bold, while incorrect elements are marked in underline. Examples shown are: no UNK; 1 UNK; 2 UNKs; 3 UNKs. When there are no unknown tokens, all three compared models are able to produce reasonably good if not identical translations. When there is only one UNK token, Seq2DRNN often does not use the context to predict an appropriate word or phrase. In contrast, both the Seq2Seq and Seq2DRNN+SynC were able to correctly predict that die gute UNK ist could be translated to the good news is. When there are 2 UNK tokens in the source sentence, Seq2Seq produces more incorrect predictions, Seq2DRNN makes some mistakes, while Seq2DRNN+SynC is able to get the most parts correct. Finally, when we replace 3 nouns, all models fail to some degree while Seq2Seq's output is the worst.    parse tree construction: the Seq2DRNN+SynC F1 score is comparable but lower than Seq2DRNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Recent research shows that modelling syntax is useful for various neural NLP tasks. <ref type="bibr">Dyer et al. (2015</ref><ref type="bibr">Dyer et al. ( , 2016</ref>; <ref type="bibr" target="#b25">Vinyals et al. (2015)</ref>; <ref type="bibr" target="#b15">Luong et al. (2016)</ref> have works on language modelling and parsing, <ref type="bibr" target="#b24">Tai et al. (2015)</ref> on semantic analysis, and <ref type="bibr" target="#b27">Zhang et al. (2016)</ref> on sentence completion, etc. <ref type="bibr">Eriguchi et al. (2017)</ref> showed that NMT model can benefit from neural syntactical parsing mod-els. <ref type="bibr">Choe and Charniak (2016)</ref> showed that a neu- ral parsing problem shares similarity to neural lan- guage modelling problem, which forms a build- ing block of an NMT system. We can then make the assumption that structural syntactic informa- tion utilised in neural parsing models should be able to aid NMT, which is shown to be true here. <ref type="bibr" target="#b27">Zhang et al. (2016)</ref> proposed TreeLSTM which is another structured neural decoder. TreeL- STM is not only structurally more complicated but also uses external classifiers. <ref type="bibr">Dong and Lapata (2016)</ref> also proposed a sequence-to-tree (Seq2Tree) model for question answering. Both of these models are not designed for NMT and lack a language model. While operate from top-to- bottom like Seq2DRNN(+SynC), TreeLSTM and Seq2Tree produce components that lack sequen- tial continuity which we have shown to be non- negligible for language generation.</p><p>Aharoni and Goldberg (2017), <ref type="bibr" target="#b26">Wu et al. (2017), and</ref><ref type="bibr">Eriguchi et al. (2017)</ref> experimented with NMT models that utilise target side structural syntax. <ref type="bibr" target="#b0">Aharoni and Goldberg (2017)</ref> treated constituency trees as sequential strings (linearised-tree) and trained a Seq2Seq model to produce such se- quences. <ref type="bibr" target="#b26">Wu et al. (2017)</ref> proposed SD-NMT, which models dependency syntax trees by adding a shift-reduce neural parser to a standard RNN decoder. <ref type="bibr">Eriguchi et al. (2017)</ref> in addition to <ref type="bibr" target="#b26">Wu et al. (2017)</ref>'s work, proposed NMT+RNNG which uses a modified RNNG generator <ref type="bibr">(Dyer et al., 2016)</ref> to process dependency instead of constituency information as originally proposed by <ref type="bibr">Dyer et al. (2016)</ref>, making it consequently a StackLSTM sequential decoder with additional RNN units so it is still a bottom-up tree-structured decoder rather than a top-down decoder like ours. Nevertheless, all of these research showed that target side syntax could improve NMT systems. We believe these models could also be augmented with SynC connections (with NMT+RNNG one has to instead use constituency information).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We propose an NMT model that utilises target side constituency syntax with a strictly top-down tree-structured decoder using Doubly-Recurrent Neural Networks (DRNN) incorporated into an encoder-decoder NMT model. We propose a new way of modelling language generation by estab- lishing additional clause-based syntactic connec- tions called SynC. Our experiments show that our proposed models can outperform a strong se- quence to sequence NMT baseline and several ri- val models and do parsing competitively.</p><p>In the future we hope to incorporate source side syntax into the model. We plan to explore the applications of SynC in NMT with more struc- tured attention mechanisms, and potentially a hy- brid phrase-based NMT systems with SynC, in which the model can benefit from SynC to be more extensible when handling larger lexicons. <ref type="bibr">Yoshua Bengio. 2014</ref>. Learning phrase representa- tions using RNN encoder-decoder for statistical ma- chine translation. In Empirical Methods in Natural Language Processing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Translation Samples (IWSLT2017 German-English)</head><p>The samples here are from our IWSLT2017 German-English testset. We compared the performances of all our proposed models as well as the baseline Seq2Seq model in <ref type="table" target="#tab_12">Table 10</ref>.</p><p>We provide an additional example of our attention module visualisation in <ref type="figure" target="#fig_6">Figure 6</ref> and for parser in <ref type="figure" target="#fig_7">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>1 m ist das höchste, was ich gesehen habe. Literal 1 m is the highest that i've seen. Reference thirty-nine inches is the tallest structure i've seen. Seq2Seq and the highest thing is i've seen. Seq2DRNN one is the highest thing i've seen at that. Seq2DRNN+SynC feet is the highest thing i've seen. Source ich weiß nicht . sie wollten in die zeit zurck , bevor es autos gab oder twitter oder amerika sucht den superstar. Literal i dont know. they want to go back in time, before there were automobiles or twitter or america looking for superstar. Reference i dont know. they want to go back before there were automobiles or twitter or american idol. Seq2Seq i don't know. they were in the days, when they were cars before cars or the earnings, or america, and the country. Seq2DRNN i don't want to know before time, they wanted to go back before the cars be- fore they were cars or americans. Seq2DRNN+SynC i don't know. they wanted to go back in time, they wanted to go back into the before, before there had cars or twitter visitors. The reason we include the literal translation is that sometimes the reference translation from the corpus can have additional components or be non-literal translations.  The sentence is randomly selected from our PennTreebank experiment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 1: Seq2DRNN on Constituency Tree Encoder Ich bin Doktor</figDesc><graphic url="image-32.png" coords="3,346.53,196.35,140.40,118.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: SynC example: the three types of information explicitly modelled when generating the current node/word likes in Andrei when starving likes cheese.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: SynC in action; when generating the word likes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Dataset</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Attention Module in Seq2DRNN+SynC. "ich" means "I", "bin" is "am", "doktor"'s literal translation is "doctor". Darker colour means higher weight (relevance score) as calculated in Equation 9. The values of each column sum up to 1. The attention weights in this example perfectly align with the appropriate clauses. Additional example is provided in the appendix.</figDesc><graphic url="image-107.png" coords="7,113.10,391.58,136.06,86.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Attention visualisation (Seq2DRNN+SynC). Darker colour means higher attention weight as defined in Equ 9. The sentence is randomly selected from our IWSLT experiment.</figDesc><graphic url="image-108.png" coords="12,117.35,445.68,362.83,272.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Parser attention visualisation (Seq2DRNN+SynC). Darker colour means higher attention weight as defined in Equ 9.</figDesc><graphic url="image-109.png" coords="13,72.00,258.26,453.55,290.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>News Commentary v8 Dataset information 

BLEU RIBES Perplx. 
Seq2Seq 
22.83 
81.5 
1.828 
Seq2DRNN 
23.53 
80.4 
1.644 
Seq2DRNN+SynC 25.36 
82.6 
1.750 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : IWSLT17 Experiment results</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>News Commentary v8 Experiment results. Seq2Seq and NMT+RNNG results are taken from Eriguchi et al. (2017), 

Str2Tree (string-to-linearised-tree) results (no RIBES scores) come from Aharoni and Goldberg (2017) All numbers reported 
here are of non-ensemble models. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Translation Sample. Gold is the reference, and Literal is produced by a bilingual German-English speaker.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Parser scores. Numbers from (Vinyals et al., 2015) 

are of non-ensemble models. 

Unlabelled 
Prec. Rec. 
F1 
Seq2DRNN 
96.87 96.93 96.90 
Seq2DRNN+SynC 96.43 95.89 96.16 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>IWSLT Translation result constituency unlabelled 

scores. Reference parse trees obtained using Stanford Parser. 

parsing (Klein and Manning, 2003b). 
The presence of SynC in the decoder influences 

Labelled 
Prec. Rec. 
F1 
Seq2DRNN 
91.63 91.69 91.66 
Seq2DRNN+SynC 90.73 90.22 90.48 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 9 :</head><label>9</label><figDesc>IWSLT Translation result constituency labelled scores. Reference parse trees obtained using Stanford Parser.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table>Translation Samples. Gold is the reference, and Literal is produced by a bilingual German-English speaker. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank the anonymous reviewers for their helpful remarks. The research was also partially supported by the Natural Sciences and Engineering Research Council of Canada grants NSERC RGPIN-2018-06437 and RGPAS-2018-522574 and a Department of National Defence (DND) and NSERC grant DGDND-2018-00025 to the third author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards string-to-tree neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tree-structured decoding with doubly-recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Melis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph convolutional encoders for syntax-aware neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A hierarchical phrase-based model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="263" to="270" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Describing multimedia content using attention-based encoder-decoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Multimedia Special Issue on Deep Learning for Multimedia Computing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<imprint>
			<publisher>Holger Schwenk,</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Küchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ICNN-96</title>
		<meeting>of the ICNN-96</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="347" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bidirectional LSTM-CRF models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic evaluation of translation quality for distant language pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;10</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="944" to="952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
	<note>ACL &apos;03. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
	<note>ACL &apos;03. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-task sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1701.03980</idno>
		<title level="m">Swabha Swayamdipta, and Pengcheng Yin. 2017a. Dynet: The dynamic neural network toolkit</title>
		<meeting><address><addrLine>Yusuke Oda, Matthew Richardson, Naomi Saphra</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On-the-fly operation batching in dynamic computation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno>abs/1705.07860</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bleu: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Products of random latent variable grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT &apos;10</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Linguistic input features improve neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Syntactically guided neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Waite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Computational Linguistics</title>
		<meeting>Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
	<note>NIPS&apos;15</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequence-to-dependency neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="698" to="707" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tree recurrent neural networks with application to language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Syntax augmented machine translation via chart parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Zollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Venugopal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Statistical Machine Translation</title>
		<meeting>the Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="138" to="141" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
