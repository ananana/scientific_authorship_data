<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Implicit Content-Introducing for Generative Short-Text Conversation Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyuan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Beijing Institute of Big Data Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Beijing Institute of Big Data Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Implicit Content-Introducing for Generative Short-Text Conversation Systems</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2190" to="2199"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The study on human-computer conversation systems is a hot research topic nowadays. One of the prevailing methods to build the system is using the gen-erative Sequence-to-Sequence (Seq2Seq) model through neural networks. However , the standard Seq2Seq model is prone to generate trivial responses. In this paper , we aim to generate a more meaningful and informative reply when answering a given question. We propose an implicit content-introducing method which incorporates additional information into the Se-q2Seq model in a flexible way. Specifically , we fuse the general decoding and the auxiliary cue word information through our proposed hierarchical gated fusion unit. Experiments on real-life data demonstrate that our model consistently outper-forms a set of competitive baselines in terms of BLEU scores and human evaluation .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>To establish a conversation system with adequate artificial intelligence is a long-cherished goal for researchers and practitioners. In particular, auto- matic conversation systems in open domains are attracting increasing attention due to its wide ap- plications, such as virtual assistants and chatbot- s. In open domains, researchers mainly focus on data-driven approaches, since the diversity and un- certainty make it impossible to prepare the inter- action logic and domain knowledge. Basically, there are two mainstream ways to build an open- domain conversation system: 1) to search pre- established database for candidate responses by * Corresponding author: ruiyan@pku.edu.cn query retrieval ( <ref type="bibr" target="#b5">Isbell et al., 2000</ref>; <ref type="bibr" target="#b20">Wang et al., 2013;</ref>, and 2) to generate a new, tailored utterance given the user- issued query <ref type="bibr" target="#b14">(Shang et al., 2015;</ref><ref type="bibr" target="#b18">Vinyals and Le, 2015;</ref><ref type="bibr" target="#b13">Serban et al., 2016;</ref>. In these studies, generation-based conversation systems have shown impressive po- tential. Especially, the Sequence-to-Sequence (Se- q2Seq) model <ref type="bibr" target="#b17">(Sutskever et al., 2014</ref>) based on neural networks has been extensively used in prac- tice; the idea is to encode a query as a vector and to decode the vector into a reply. Inspired by ( , we mainly focus on the generative short-text conversation without context informa- tion.</p><p>Despite this, the performance of Seq2Seq generation-based conversation systems is far from satisfactory because its generation process is not controllable; it responses to a query according to the pattern learned from the training corpus. As a result, the system is likely to generate an un- expected reply even with little semantics, e.g, "I don't know" and "Okay" due to the high frequency of these patterns in training data ( <ref type="bibr" target="#b7">Li et al., 2016a;</ref>. To address this issue, <ref type="bibr" target="#b7">Li et al. (2016a)</ref> proposed to increase diversity in the Se- q2Seq model so that more informative utterances have a chance to stand out.  provided a content-introducing approach that gen- erates a reply based on a predicted word. The word is usually enlightening and drives the gen- erated response to be more meaningful. However, this method is to some extent rigid; it requires the predicted word to explicitly occur in the generat- ed utterance. As shown in <ref type="table" target="#tab_0">Table 1</ref>, sometimes, it is better to generate a semantic related sentence based on the cue word rather than including it in the reply directly.</p><p>As for such content-introducing method, there are two aspects that need to be taken into consid-Query 你不觉得好丑吗(Don't you think it is ugly?) Cue Word 审美(Aesthetics) Reply 好恶心啊! (It's disgusting!) Query 先放个大招(Let me use my ultimate power.) Cue Word 技能(Skill) Reply 新技能？(New skill?) In this paper, we present an implicit content- introducing method for generative conversation systems, which incorporates cue words using our proposed hierarchical gated fusion unit (HGFU) in a flexible way. Our main contributions are as fol- lows:</p><p>• We propose the cue word GRU, another neu- ral cell, to deal with the auxiliary informa- tion. Compared with other gating methods, our cue word GRU is more flexible.</p><p>• We focus on the implicit content-introducing method during generation: the information of the cue word will be fused into the gen- eration process but not necessarily occur ex- plicitly. In this way, we change the "hard" content-introducing method into a new "soft" schema.</p><p>The rest of paper is organized as follows. We s- tart by introducing the technical background. In Section 3, we describe our proposed method. In Section 4, we illustrate the experimental setup and evaluations against a variety of baselines. Section 5 briefly reviews related work. Finally, we con- clude our paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Technical Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Seq2Seq Model and Attention Mechanism</head><p>Seq2Seq model was first introduced in statistical machine translation; the idea is to encode a source sentence as a vector by a recurrent neural network (RNN) and to decode the vector to a target sen- tence by another RNN. Now, the conversational generation is treated as a monolingual translation task ( <ref type="bibr" target="#b12">Ritter et al., 2011;</ref><ref type="bibr" target="#b14">Shang et al., 2015)</ref>. Given a query Q = (x 1 , ..., x n ), the encoder represents it as a context vector C and then the decoder gener- ates a response R = (y 1 , ..., y m ) word by word by maximizing the generation probability of R con- ditioned on Q. The objective function of Seq2Seq can be written as:</p><formula xml:id="formula_0">p(y 1 , ..., y m |x 1 , ..., x n ) =p(y 1 |C) T t=2 p(y t |C, y 1 , ..., y t−1 )<label>(1)</label></formula><p>To be specific, the encoder RNN calculates the context vector by:</p><formula xml:id="formula_1">h t = f (x t , h t−1 ); C = h T (2)</formula><p>where h t is the hidden state of encoder RNN at time t and f is a non-linear transformation which can be a long-short term memory unit (L- STM) <ref type="bibr" target="#b4">(Hochreiter and Schmidhuber, 1997</ref>) or a gated recurrent unit (GRU) ( . In this work, we implement f using GRU. The decoder RNN generates each reply word conditioned on the context vector C. The prob- ability distribution p t of candidate words at every time step t is calculated as:</p><formula xml:id="formula_2">s t = f (y t−1 , s t−1 , C); p t = softmax(s t , y t−1 )<label>(3)</label></formula><p>where s t is the hidden state of decoder RNN at time t and y t−1 is the generated word in the reply at time t − 1.</p><p>Attention mechanisms ( ) have been proved effective to improve the gener- ation quality. In Seq2Seq with attention, each y i corresponds to a context vector C i ; it is weighted average of all hidden states of the encoder. For- mally, C i is defined as C i = T j=1 α ij h j , where α ij is given by:</p><formula xml:id="formula_3">α ij = exp(e ij ) T k=1 exp(e ik ) ; e ij = η(s i−1 , h j ) (4)</formula><p>where η is usually implemented as a multi-layer perceptron (MLP) with tanh as an activation func- tion.  <ref type="figure">Figure 1</ref>: The architecture of our system. Based on the constructed corpus, we train our implicit content-introducing conversation system. Given a user-issued query, we first predict the cue word. Then, we incorporate the cue word into decoding process to generate a meaningful response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pointwise Mutual Information</head><p>Pointwise mutual information (PMI) <ref type="bibr" target="#b3">(Church and Hanks, 1990</ref>) is a measure of association ratio based on the information theoretic concept of mu- tual information. Given a pair of outcomes x and y belonging to discrete random variables X and Y, the PMI quantifies the discrepancy between the probability of their coincidence based on their joint distribution and their individual distributions. Mathematically:</p><formula xml:id="formula_4">PMI(x, y) = log p(x, y) p(x)p(y) = log p(x|y) p(x)<label>(5)</label></formula><p>This quantity is zero if x and y are independent, positive if they are positively correlated, and neg- ative if they are negatively correlated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Implicit Content-Introducing Conversation System</head><p>Figure 1 provides an overview of our system archi- tecture. We crawl conversational data from social media which are publicly available. After filtering and cleaning procedures, we establish the conver- sational parallel dataset, which consists of a large number of aligned query − reply pairs. Based on the entire set, we first predict the cue word for the given query in Subsection 3.1. Next, we pro- pose the new implicit content-introducing process, which explores when to incorporate the predicted cue word in Subsection 3.2 and how to apply such information in Subsection 3.3.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Cue Word Prediction</head><p>In computational linguistics, PMI has been used for finding collocations and associations between words. As mentioned in , it is an appropriate statistic for cue words prediction, which is also adopted in this paper to predict a cue word C w for the given query. Formally, given a query word w q and a reply word w r , the PMI is computed as:</p><formula xml:id="formula_5">PMI(w q , w r ) = log p(w q |w r ) p(w q )<label>(6)</label></formula><p>Then, we choose the cue word C w with highest PMI score against the query words w q1 , ..., w qn during the prediction, i.e., C w = argmax wr PMI(w q1 , ..., w qn , w r ), where</p><formula xml:id="formula_6">PMI(w q1 , ..., w qn , w r ) ≈ log i p(w qi |w r ) i p(w qi ) = i log p(w qi |w r ) p(w qi ) = i PMI(w qi , w r ) (7)</formula><p>The approximation is based on the indepen- dence assumptions of both the prior distribu- tion p(w qi ) and posterior distributions p(w qi |w r ). Even the two assumptions may not be true, we use them in a pragmatic way so that the word-level P- MI is additive for a whole utterance. PMI penal- izes a common word by dividing its prior proba- bility; hence, it prefers a word which is most "mu- tually informative" with the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Information Fusion Patterns</head><p>To implant the specific information in conversa- tion system, we consider two types of information fusion patterns, namely 1) Local information ini- tialization 2) Global information inception.</p><p>Local information initialization. In the local pattern, we fuse the cue word C w as the auxiliary  <ref type="figure">Figure 3</ref>: The structure of a HGFU. The bottom of two GRUs deal with corresponding input source, i.e., the last generated word y t−1 and the cue word C w . After that, fusion unit combines the output of two GRUs to compute current hidden state h t . .</p><p>information only in the beginning of decoding. We describe this kind of pattern by the blue arrowhead in <ref type="figure" target="#fig_0">Figure 2</ref>. Recurrent neural networks(RNNs) such as gated recurrent units (GRUs) have the a- bility to keep the information from the beginning to the end to some extent. Therefore, the cue word added on the first step of the neural networks can still influence the generation of the later steps. Global information inception. However, we observe that, although the network is capable of deciding what to keep in the cell state to affect the later generation, the influence of the added infor- mation in the beginning of decoding is becoming weaker and weaker over time. Therefore, to pro- vide the model a broader and more flexible space for learning, we propose a global information in- ception pattern, which fuses the cue word C w as the auxiliary information at every step of decod- ing. This process is presented by both the blue arrowhead and the green arrowheads in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hierarchical Gated Fusion Unit</head><p>In this subsection, we propose our Hierarchical Gated Fusion Unit (HGFU), which incorporates cue words into the generation process and relaxes the constraint from the "hard" content-introducing method into a new "soft" schema. <ref type="figure">Figure 3</ref> pro- vides an overview of the structure of a HGFU. As seen, the framework consists of three components: the standard GRU, the cue word GRU, and the fu- sion unit. Among them, standard GRU and cue word GRU take the last generated word y t−1 and cue word C w respectively as the decoder GRU's input; the fusion unit combines the hidden states of both GRUs to predict the next word y t . In the following, we will illustrate these components in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Standard GRU</head><p>We adopt the standard gated recurrent unit (GRU) with the attention mechanism at the decoder part. Let h t−1 be the last hidden state, y t−1 be the em- bedding of the last generated word, and C t be the current attention-based context. The current hid- den state of the general decoding, h y , is defined as:</p><formula xml:id="formula_7">r y = σ(W r y t−1 + U r h t−1 + U cr C t + b r ) z y = σ(W z y t−1 + U z h t−1 + U cz C t + b z ) h y = tanh(W h y t−1 + U h (r y • h t−1 ) + U ch C t + b h ) h y = (1 − z y ) • h t−1 + z y • h y<label>(8)</label></formula><p>where W 's ∈ R dim×E and U 's ∈ R dim×dim are weight matrices; b's ∈ R dim are bias terms; E denotes the word embedding dimensionality and dim denotes the number of hidden state units. This general decoding process is presented by the "Standard GRU" in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Cue word GRU</head><p>To generate more meaningful and informative replies, we introduce cue words as the additional information during generation. Naturally, the key point lies in how to incorporate such information. One of the prevailing methods is modifying the neural cell by various gating mechanisms. How- ever, these approaches are designed specially for a specific scenario, and not effective as expected when they are employed to other tasks. To tackle this issue, we propose the cue word GRU, another independent neural cell, to deal with the auxiliary information. Since this neural cell can be replaced easily by other units, it greatly improves the flexi- bility and reusability.</p><p>Given the last hidden state h t−1 , the additional cue word C w and the current attention-based con- text C t , the new hidden state of the auxiliary de-coding h w is computed by following equations:</p><formula xml:id="formula_8">r w = σ(W r C w + U r h t−1 + U cr C t + b r ) z w = σ(W z C w + U z h t−1 + U cz C t + b z ) h w = tanh(W h C w + U h (r w • h t−1 ) + U ch C t + b h ) h w = (1 − z w ) • h t−1 + z w • h w<label>(9)</label></formula><p>where W 's and U 's are weights and b's are bias terms like those in the standard GRU. Note that the standard GRU does not share parameter matrixes with the cue word GRU. The "Cue word GRU" in <ref type="figure">Figure 3</ref> describes the auxiliary decoding process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Fusion unit</head><p>To combine both the general decoding information and the auxiliary decoding information, we apply the fusion unit ( <ref type="bibr" target="#b0">Arevalo et al., 2017</ref>) integrating the hidden states of both standard GRU, i.e., h y , and the cue word GRU, i.e., h w , to compute the current hidden state h t . The equations are as fol- lows:</p><formula xml:id="formula_9">h y = tanh(W 1 h y ) h w = tanh(W 2 h w ) k = σ(W k [h y , h w ]) h t = k • h y + (1 − k) • h w θ = {W 1 , W 2 , W k } (10)</formula><p>with θ the parameters to be learned. From the e- quations above we can see that, the gate neuron k controls the contribution of the information calcu- lated from h y and h w to the overall output of the unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Training</head><p>When training on the aligned corpus, we random- ly sample a noun in the reply as the cue word. The objective function was the cross entropy error be- tween the generated word distribution p t and the actual word distribution y t in the training corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we compare our method with the- state-of-art response generation models based on a huge conversation resource. The objectives of our experiments are to 1) evaluate the effectiveness of our proposed HGFU model, and 2) explore how cue words affect the process of reply generation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>We evaluated our model on a massive Chi- nese dataset of human conversation crawled from the Baidu Tieba 1 forum. There are 500,000 query − reply pairs for training, 2,000 for val- idation, and another unseen 27,871 samples for testing. In total, we kept about 63,000 distinct words.</p><p>In our experiments, the encoder, the standard decoder and the cue word decoder have 1,000 hid- den units; the word embedding dimensionality is 610 which were initialized randomly and learned during training. We applied AdaDelta with a mini- batch size of 80 for optimization. These values were mostly chosen empirically. In order to pre- vent overfitting, early stopping was implemented using a held-out validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison Methods</head><p>In this paper, we conduct extensive experiments to compare our proposed method against sever- al representative baselines. All the methods ac- tually are implemented in two ways to utilize the cue word, which are local information initializa- tion and global information inception.</p><p>rGRU: Through a specially designed Recal- l gate ( <ref type="bibr" target="#b25">Xu et al., 2016)</ref>, domain knowledge was transformed into the extra global memory of a deep neural network.</p><p>SCGRU: In SCGRU ( <ref type="bibr" target="#b22">Wen et al., 2015b</ref>), an ad- ditional control cell was introduced to gate the dia-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>班主任还拍了我超级丑的照片已被笑死.(上 上 上镜 镜 镜)</head><p>Related Criterion Labels (Cue word) The teacher took a photo of me; it was really ugly and people laughed at me. (Photogenic) Reply1</p><p>谁的照片？Whose photo? Logic Consistency Unsuitable Reply2 什么时候拍的？When did he took the photo? Implicit Relevance Neutral Reply3</p><p>抱抱。Give you a hug. Implicit Relevance Neutral Reply4</p><p>我拍照也都是巨丑的！My photos are also ugly! -- Suitable <ref type="table">Table 2</ref>: An example query, corresponding cue word in bold and its candidate replies with human anno- tation. The query states that people laughed at the author's photo, it is unsuitable to ask the ownership of this photo in Reply1. Generally, Reply2 and Reply3 apply to this scenario, but they do not reflec- t semantic relevance with the cue word. Reply4 talks about the respondent's situation and related to "Photogenic", thus it is a suitable response.</p><p>logue act (DA) features during the generation pro- cess. SLGD: We implemented the Stochastic Language Generation in Dialogue (SLGD) method <ref type="bibr" target="#b21">(Wen et al., 2015a</ref>), which added ad- ditional features in each gate of the neural cell.</p><p>FGRU: To explore more fusion strategies, intu- itively, we fused the cue word and hidden states by vector concatenation during the decoding process.</p><p>Note that rGRU and SCGRU incorporate addi- tional information by gating mechanisms, while SLGD and FGRU fuse the information into each gate of the neural cell directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiment Evaluation</head><p>Objective metrics. To evaluate the performance of different methods for the conversation gener- ation task, we leverage BLEU ( <ref type="bibr" target="#b11">Papineni et al., 2002</ref>) as the automatic evaluation metric, which is originally designed for machine translation and evaluates the output by using n-gram matching between the output and the reference. Here, we use BLEU-1, BLEU-2 and BLEU-3 in our experi- ments.</p><p>Subjective metrics. Since automatic metric- s may not consistently agree with human percep- tion ( <ref type="bibr" target="#b16">Stent et al., 2005</ref>), human testing is essential to assess subjective quality. Hence, we randomly sampled 150 queries in the test set, then we invited five annotators to offer a judgment. For fairness, all of our human evaluation was conducted in a random, blind fashion, i.e., replies obtained from the five evaluated models are pooled and random- ly permuted for each annotator. Three levels are assigned to a reply with scores from 0 to 2: 0 =  Unsuitable reply, 2 = Suitable reply, and 1 = Neu- tral reply. To make the annotation task operable, the suit- ability of the generated reply is judged not only based on Grammar and Fluency, Logic Consis- tency and Semantic Relevance following <ref type="bibr" target="#b14">(Shang et al., 2015</ref>), but also Implicit Relevance, i.e., the generated reply should be semantically relevant to the predicted cue word, no matter the cue word explicitly appears in the reply or not. If any of the first three criteria is contradicted, the reply should be labeled as "Unsuitable". Only the replies con- forming to all requirements are labeled as "Suit- able". <ref type="table">Table 2</ref> shows an example of the annotation results of a query and its replies. The first reply is labeled as "Unsuitable" because of the logic con- sistency. Reply2 and Reply3 are not semantically related to the cue word, and is therefore annotated as "Neutral".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Overall Performance</head><p>The overall results against all baseline methods are listed in <ref type="table" target="#tab_5">Table 4</ref>. Our proposed HGFU mod- el in global schema obviously shows better per- formance than the baseline methods; it obtains the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English Tranlation Query 写的真心棒！(夸 夸 夸奖 奖 奖) What a nice written! (Appreciation) Reply 谢谢夸奖！么么哒！ Thanks for your appreciation! Love you! Query 还是无法淡定。(内 内 内心 心 心) Still cannot calm down. (Heart) Reply 内心是崩溃的吧。</head><p>Your heart must be broken. Query 我先去哭一会。(纸 纸 纸巾 巾 巾) I am going to cry for a while.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(Tissue) Reply 递纸巾！</head><p>Offer you a tissue! Query 当初你们不是说过他是诺维斯基吗？(说 说 说过 过 过) Didn't you say that he was N owitzki † ? (Say) Reply 说过吗？好像没有说过啊！？ Did I say it? I don't seem to say it!? <ref type="table">Table 3</ref>: The explicit introducing-content cases of our HGFU model. The predicted cue word in bold explicitly occurs in the generated reply. N owitzki † is a NBA basketball player.</p><p>highest BLEU scores as well as the highest human score.</p><p>In terms of automatic evaluations, the global- based methods perform much better than a set of local-based methods, which demonstrates the ef- fectiveness of global information inception. As mentioned above, the global schema provides the model a broader and more flexible space for learn- ing, which is benefit for information fusion. When it comes to human scores (For the sake of con- venience, we only conducted human evaluation in global schema), there are similar conclusions to BLEU results.</p><p>From <ref type="table" target="#tab_5">Table 4</ref>, we can see that the performance of rGRU is not as good as the other systems, while SCGRU outperforms the others in the local pattern and shows comparative performance in the glob- al schema. These two methods both augment the standard neural network with specially designed gate to control the cue word, but the results vary greatly. It is the limitation of gating mechanism- s that is lacking in adaptiveness. Besides, SLGD adding cue word term in each gate of the neural cell has the similar result as FGRU method, which concatenates cue word with hidden state. Basical- ly, our proposed HGFU has a significant improve- ment against the baseline systems. The most prob- able credits come from the cue word GRU: we ap- ply the extra GRU unit to control the auxiliary in- formation instead of fusion in the standard GRU, which is more flexible.</p><p>Till now, we have elaborated the overall per- formance of all methods. Next we will come to a closer look at some representative cases of our HGFU model for further analysis and discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis and Case Studies</head><p>Given a query and the cue word, our HGFU model generates a meaningful and informative response. In <ref type="table">Table 3</ref>, the predicted cue word occurs in the generated response and we treat this kind of gen- eration as the explicit introducing-content. How- ever, we do not strictly restrict tothis. As shown in <ref type="table" target="#tab_6">Table 5</ref>,our HGFUmodel also generates the replies without containing the cue word, while the re- sponsesare still somehow related to the cue word and the query. This reflects our expectation: the information of the cue word will be fused into the generation process but not necessarily occur ex- plicitly. It provesthe characteristics of our pro- posed new "soft" schema, whichare more flexible, extensible, and controllable.</p><p>We further analyze these explicit cases using a heat map as shown in <ref type="figure" target="#fig_1">Figure 4</ref>. We use various shades of blue to present the extent of correla- tion between the cue word and the generated re- ply. The darker the blue is, the higher correlation they have. For the added information in the reply (Here is exactly the cue word in darkblue), its po- sition and occurrence times are not fixed, which are autonomously controlled by our model. Besides, the rectangular pulse is also a signif- icant presentation of this correlation, which indi- cates how the k gate in fusion unit balance the in- fluence of h y and h w . When in the high level of the rectangular pulse, k "opens" the switch of h w to generate the current word; when in the low lev- el, the fusion unit mainly takes h y for generation. We observe that the switch corresponds with the heat map: the generated word is more correlated with the cue word when the switch is open.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chinese Sentence</head><p>English Tranlation Query 在微博看到这样一个评论真 的是完全无法反驳。(观 观 观点 点 点) I saw such a comment in the microblog which can- not be refused completely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(View) Reply 路人表示不服啊！ As an outsider, I am not convinced! Query 怎么突然就下雨了？(委 委 委屈 屈 屈)</head><p>Why is it raining suddenly? (Grievance) Reply 好伤心啊。 So sad.</p><formula xml:id="formula_10">Query 泰 民 这 张 也 是 做 了 很 久 桌 面。(屏 屏 屏保 保 保)</formula><p>This photo of T aemin † was also taken as a desktop for a long while. (Screenshot) Reply 锁屏吗？ As the lockscreen? Query 混脸熟求勾搭！(小 小 小新 新 新)</p><p>Make acquaintance and seek chances for further re- lations! (Freshman) Reply 同新人！求认识。 I am also the new! Nice to meet you. 5 Related work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Conversation Systems</head><p>Automatic human-computer conversation has at- tractedincreasing attention over the past few years. At the very beginning, people start the research using hand-crafted rules and templates ( <ref type="bibr" target="#b19">Walker et al., 2001;</ref><ref type="bibr" target="#b9">Misu and Kawahara, 2007;</ref><ref type="bibr" target="#b23">Williams et al., 2013</ref>). These approaches require no da- ta or little data for trainingbuthuge manual ef- fort to build the model, which is very time- consuming. For now, buildinga conversation sys- temmainly falls into two categories: retrieval- based and generation-based. As information re- trieval techniques are developing fast, <ref type="bibr" target="#b6">Leuski et al. (2009)</ref> build systems to select the most suit- able response from the query-reply pairs using a statistical language model in cross-lingual in- formation retrieval.  propose a retrieval-based conversation system with the deep learning-to-respond schema through a deep neural network framework driven by web data. Recently, generation-based conversation system- s have shownimpressive potential. <ref type="bibr" target="#b14">Shang et al. (2015)</ref> generate replies for short-text conversation by Seq2Seq-basedneural networks with local and global attentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Content Introducing</head><p>In  <ref type="formula" target="#formula_0">(2016)</ref> incorpo- rate topic information into Seq2Seq framework to generate informative and interesting responses. To provide informative clues for content introducing, <ref type="bibr" target="#b8">Li et al. (2016b)</ref> detect entities from previous ut- terances and search for more related entities in a large knowledge graph. A very recent study similar to ours is , where the predicted word explicitly occurs in the generated utterance. Unlike the existing work, we explore an implicit content-introducing method for neural conversation systems, which utilizes the addition- al cue word in a "soft" manner to generate a more meaningful response given a user-issued query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we explore an implicit content- introducing method for generative short-text con- versation system. Given a user-issued query, our proposed HGFU incorporates an additional cue word in a "soft" manner to generate a more mean- ingful response. The HGFU model consists of three components: the standard GRU, the cue word GRU and the fusion unit. The standard GRU operates a general decoding process, and the cue word GRU imitates this process but treats the pre- dicted cue word as the current input. As for the fu- sion unit, it combines both the hidden states of the standard GRU and the cue word GRU to generate the current output word. The experimental results demonstrate the effectiveness of our approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The information fusion patterns. The local information initialization is presented by the blue arrowhead and the global information inception includes both the blue arrowhead and the green arrowhead.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Heat map and the k gate openness. Bottom: The correlation between the generated reply words and the cue word. Top: The openness of k gate in fusion unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>vertical domains, Wen et al. (2015b) apply an additional control cell to gate the dialogue ac- t (DA) features during the generation process to ensure the generated repliesexpressthe intended meaning. Also, the Stochastic Language Gener- ation in Dialogue method (Wen et al., 2015a) adds additional features in each gate of the neural cel- l. Xu et al. (2016) introduce a new trainable gate to recall the global domain memory to enhance the ability of modeling the sequence semantics. Dif- ferent from the above work, our paper addresses the problem of content introducing in the open- domain generative conversation systems. In open domains, Xing et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>The content-introducing conversation ex-
amples. 

eration. 1) How to add the additional cue words 
during the generation process? One of the pre-
vailing methods is modifying the neural cell with 
various gating mechanisms (Wen et al., 2015a,b; 
Xu et al., 2016). However, we need careful oper-
ation to ensure the neuron works as expected. 2) 
How to display the cue words in replies? As men-
tioned above, the explicit content-introducing ap-
proach in (Mou et al., 2016) does not fit well with 
all situations. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Performance of evaluated methods.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>The implicit introducing-content cases of our HGFU model. The cue word in bold is not 
contained in the reply, while the response is still related to the cue word. T aemin  † is a Korean singer. 

</table></figure>

			<note place="foot" n="1"> http://tieba.baidu.com</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Gated multimodal units for information fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Arevalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Montes-Y Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">A</forename><surname>González</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01992</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>ab- s/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">Ward</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cobot in lambdamoo: A social statistics agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">Lee</forename><surname>Isbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Kormann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth National Conference on Artificial Intelligence</title>
		<meeting>the Seventeenth National Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="36" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Building effective question answering characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Leuski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronakkumar</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Traum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue</title>
		<meeting>the 7th SIGdial Workshop on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="18" to="27" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stalematebreaker: A proactive content-introducing approach to automatic human-computer conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07" />
			<biblScope unit="page" from="2845" to="2851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Speechbased interactive information guidance system using question-answering technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruhisa</forename><surname>Misu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">145</biblScope>
		</imprint>
	</monogr>
	<note>IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-11" />
			<biblScope unit="page" from="3349" to="3358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data-driven response generation in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="583" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3776" to="3783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural responding machine for short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1577" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Two are better than one: An ensemble of retrieval-and generation-based dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.07149</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evaluating evaluation methods for generation in the presence of variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Marge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Singhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Text Processing and Computational Linguistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="341" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A neural conversational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning Workshop of the 32nd International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Quantitative and qualitative evaluation of darpa communicator spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marilyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><forename type="middle">E</forename><surname>Passonneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 39th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="515" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A dataset for research on short-text conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="945" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stochastic language generation in dialogue using recurrent neural networks with convolutional sentence reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongho</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="275" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantically conditioned lstm-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1711" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepak Ramachandran, and Alan Black</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Raux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGDIAL 2013 Conference</title>
		<meeting>the SIGDIAL 2013 Conference</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="404" to="413" />
		</imprint>
	</monogr>
	<note>The dialog state tracking challenge</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Topic augmented neural response generation with a joint attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
		<idno>arX- iv:1606.08340</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Incorporating loosestructured knowledge into lstm with recall gate for conversation modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingquan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1605.05110</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to respond with deep neural networks for retrievalbased human-computer conversation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
