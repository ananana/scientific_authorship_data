<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CORE: Context-Aware Open Relation Extraction with Factorization Machines</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
							<email>petroni@dis.uniroma1.it</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><forename type="middle">Del</forename><surname>Corro</surname></persName>
							<email>delcorro@mpi-inf.mpg.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
							<email>rgemulla@uni-mannheim.de</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics Saarbrücken</orgName>
								<orgName type="institution">Sapienza University of Rome Rome</orgName>
								<address>
									<country>Italy, Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Mannheim Mannheim</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CORE: Context-Aware Open Relation Extraction with Factorization Machines</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose CORE, a novel matrix fac-torization model that leverages contextual information for open relation extraction. Our model is based on factorization machines and integrates facts from various sources, such as knowledge bases or open information extractors, as well as the context in which these facts have been observed. We argue that integrating contex-tual information-such as metadata about extraction sources, lexical context, or type information-significantly improves prediction performance. Open information extractors, for example, may produce extractions that are unspecific or ambiguous when taken out of context. Our experimental study on a large real-world dataset indicates that CORE has significantly better prediction performance than state-of-the-art approaches when contextual information is available.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Open relation extraction (open RE) is the task of extracting new facts for a potentially unbounded set of relations from various sources such as knowledge bases or natural language text. The task is closely related to targeted information ex- traction (IE), which aims to populate a knowledge base (KB) with new facts for the KB's relations, such as wasBornIn(Sepp Herberger, Mannheim). Existing methods either reason within the KB it- self ( <ref type="bibr" target="#b18">Franz et al., 2009;</ref><ref type="bibr" target="#b25">Nickel et al., 2011;</ref><ref type="bibr" target="#b15">Drumond et al., 2012)</ref> or leverage large text corpora to learn patterns that are indicative of KB rela- tions ( <ref type="bibr" target="#b24">Mintz et al., 2009;</ref><ref type="bibr" target="#b34">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b23">Min et al., 2013</ref>). In both cases, targeted IE meth- ods are inherently limited to an (often small) set of predefined relations, i.e., they are not "open".</p><p>The open RE task is also related to open infor- mation extraction (open IE) ( <ref type="bibr" target="#b10">Banko et al., 2007;</ref><ref type="bibr" target="#b13">Del Corro and Gemulla, 2013)</ref>, which extracts large amounts of surface relations and their ar- guments from natural language text; e.g., "critiz- ices"("Dante", "Catholic Church"). <ref type="bibr" target="#b5">1</ref> Although open IE is a domain-independent approach, the ex- tracted surface relations are purely syntactic and often ambiguous or noisy. Moreover, open IE methods usually do not "predict" facts that have not been explicitly observed in the input data. Open RE combines the above tasks by predicting new facts for an open set of relations. The key challenge in open RE is to reason jointly over the universal schema consisting of KB relations and surface relations ( <ref type="bibr" target="#b31">Riedel et al., 2013)</ref>.</p><p>A number of matrix or tensor factorization mod- els have recently been proposed in the context of relation extraction ( <ref type="bibr" target="#b31">Riedel et al., 2013;</ref><ref type="bibr" target="#b20">Huang et al., 2014;</ref>). These models use the available data to learn la- tent semantic representations of entities (or entity pairs) and relations in a domain-independent way; the latent representations are subsequently used to predict new facts. Existing models often focus on either targeted IE or open RE. Targeted mod- els are used for within-KB reasoning; they rely on the closed-world assumption and often do not scale with the number of relations. Open RE mod- els use the open-world assumption, which is more suitable for the open RE task because the avail- able data is often highly incomplete. In this paper, we propose CORE, a novel open RE factorization model that incorporates and exploits contextual in- formation to improve prediction performance.</p><p>Consider for example the sentence "Tom Peloso joined Modest Mouse to record their fifth studio album". Open IE systems may extract the sur- face fact "join"(TP, MM) from this sentence. Note that surface relation "join" is unspecific; in this case, it refers to becoming a member of a music band (as opposed to, say, an employee of a com- pany). Most existing open RE systems use the extracted surface fact for further reasoning, but they ignore the context from which the fact was extracted. We argue in this paper that exploiting contextual information is beneficial for open RE.</p><p>For our example, we may use standard NLP tools like a named entity recognizer to detect that TP is a person and MM an organization. These coarse- grained types give us hints about the domain and range of the "join" relation for the surface fact, al- though the actual meaning of "join" still remains opaque. Now imagine that the above sentence was extracted from a newspaper article published in the music section. This information can help to infer that "join" indeed refers to joining a band. Other contextual information, such as the words "record" and "album" that occur in the sentence, further strengthen this interpretation. A context- aware open RE system should leverage such in- formation to accurately predict facts like "is band member of"(TP, MM) and "plays with"(TP, MM).</p><p>Note that the prediction of the fact "is band member of"(TP, MM) is facilitated if we make use of a KB that knows that TP is a musician and MM is a music band. If TP and/or MM are not present in the knowledge base, however, such a reason- ing does not apply. In our work, we consider both linked entities (in-KB) and non-linked entity men- tions (out of-KB). Since KB are often incomplete, this open approach to handle named entities allows us to extract facts for all entities, even if they do not appear in the KB.</p><p>In this paper, we propose CORE, a flexible open RE model that leverages contextual infor- mation. CORE is inspired by the combined fac- torization and entity model (FE) of <ref type="bibr" target="#b31">Riedel et al. (2013)</ref>. As FE, CORE associates latent semantic representations with entities, relations, and argu- ments. In contrast to FE, CORE uses factorization machines <ref type="bibr" target="#b30">(Rendle, 2012)</ref> as its underlying frame- work, which allows us to incorporate context in a flexible way. CORE is able to leverage and inte- grate arbitrary contextual information associated with the input facts into its open RE factoriza- tion model. To support reasoning under the open- world assumption, we propose an efficient method for parameter estimation in factorization machines based on Bayesian personalized ranking <ref type="bibr" target="#b28">(Rendle et al., 2009)</ref>.</p><p>We conducted an experimental study on a real- world dataset using contextual information along the lines mentioned above. Our model is exten- sible, i.e., additional contextual information can be integrated when available. Even with limited amount of contextual information used in our ex- periments, our CORE model provided higher pre- diction performance than previous models. Our findings validate the usefulness of contextual in- formation for the open RE task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There is a large body of related work on relation extraction; we restrict attention to methods that are most similar to our work.</p><p>Targeted IE. Targeted IE methods aim to ex- tract from natural-language text new instances of a set of predefined relations, usually taken from a KB. Most existing methods make use of distant supervision, i.e., they start with a set of seed in- stances (pairs of entities) for the relations of inter- est, search for these seed instances in text, learn a relation extractor from the so-obtained training data, and optionally iterate ( <ref type="bibr" target="#b24">Mintz et al., 2009;</ref><ref type="bibr" target="#b34">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b23">Min et al., 2013)</ref>. Open RE models are more general then targeted IE meth- ods in that they additionally reason about surface relations that do not correspond to KB relations. For this reason, <ref type="bibr" target="#b31">Riedel et al. (2013)</ref> argued and experimentally validated that open RE models can outperform targeted IE methods.</p><p>Open IE. In contrast to targeted IE, the goal of open IE is to extract all (or most) relations ex- pressed in natural-language text, whether or not these relations are defined in a KB ( <ref type="bibr" target="#b10">Banko et al., 2007;</ref><ref type="bibr" target="#b16">Fader et al., 2011;</ref><ref type="bibr" target="#b13">Del Corro and Gemulla, 2013</ref>). The facts obtained by open IE meth- ods are often not disambiguated, i.e., the enti- ties and/or the relation are not linked to a knowl- edge base; e.g., "criticizes"("Dante", "Catholic Church"). The goal of our work is to reason about extracted open-IE facts and their contextual infor- mation. Our method is oblivious to the actual open IE method being used.</p><p>Relation clustering. One way to reason about KB and surface relations is to cluster the relations: whenever two relations appear in the same clus- ter, they are treated as synonymous ( <ref type="bibr" target="#b19">Hasegawa et al., 2004;</ref><ref type="bibr" target="#b33">Shinyama and Sekine, 2006;</ref><ref type="bibr" target="#b37">Yao et al., 2011;</ref><ref type="bibr" target="#b35">Takamatsu et al., 2011;</ref><ref type="bibr" target="#b22">Min et al., 2012;</ref><ref type="bibr" target="#b9">Akbik et al., 2012;</ref><ref type="bibr" target="#b12">de Lacalle and Lapata, 2013</ref>). For example, if "criticizes" and "hates" are clustered together, then we may pre- dict "hates"("Dante", "Catholic Church") from the above fact (which is actually not true). The general problem with relation clustering is its "black and white" approach to relations: either two relations are the same or they are different. This assumption generally does not hold for the surface relations extracted by open IE systems ( <ref type="bibr" target="#b31">Riedel et al., 2013)</ref>; examples of other types of relationships between relations include implication or mutual exclusion.</p><p>Tensor factorization. Matrix or tensor factor- ization approaches try to address the above prob- lem: instead of clustering relations, they directly predict facts. Both matrix and tensor models learn and make use of semantic representations of rela- tions and their arguments. The semantic represen- tations ideally captures all the information present in the data; it does not, however, establish a direct relationship (such as synonymy) between different KB or surface relations.</p><p>Tensor factorization models conceptually model the input data as a subject×relation×object tensor, in which non-zero values correspond to input facts. The tensor is factored to construct a new tensor in which predicted facts take large non-zero values. Examples of such tensor fac- torization models are TripleRank ( <ref type="bibr" target="#b18">Franz et al., 2009</ref>), RESCAL <ref type="bibr" target="#b25">(Nickel et al., 2011;</ref>, or PITF ( <ref type="bibr" target="#b15">Drumond et al., 2012)</ref>. Tensor factorization models are generally well-suited to reason within a KB because they are able to pre- dict relations between arbitrary pairs of subjects and objects. In the context of open RE, however, these methods suffer from limited scalability with the number of relations as well as from their large prediction space ( ).</p><p>Matrix factorization. The key difference be- tween matrix and tensor factorization models is that the former restrict the prediction space, i.e., these models generally cannot predict arbitrary facts. Similar to distant supervision approaches, matrix factorization models focus on predicting facts for which some direct evidence exists. In more detail, most methods restrict the prediction space to the set of facts for which the subject and the object share at least some relation in the input data. For this reason, matrix factorization models are not suited for in-KB reasoning; an individual pair of entities usually does not occur in more than one KB relation. In the open RE context, how- ever, input relations are semantically related so that many subject-object pairs belong to multiple relations. The key advantage of matrix methods is (1) that this restriction allows them to use addi- tional features-such as features for each subject- object pair-and (2) that they scale much better with the number of relations. Examples of such matrix factorization models include ( <ref type="bibr" target="#b36">Tresp et al., 2009;</ref><ref type="bibr" target="#b21">Jiang et al., 2012;</ref><ref type="bibr" target="#b17">Fan et al., 2014;</ref><ref type="bibr" target="#b20">Huang et al., 2014</ref>).  have also shown that a combination of matrix and tensor factoriza- tion models can be fruitful. Closest to our work is the "universal schema" matrix factorization ap- proach of <ref type="bibr" target="#b31">Riedel et al. (2013)</ref>, which combines a latent features model, a neighborhood model and an entity model but does not incorporate context. Our CORE model follows the universal schema idea, but uses a more general factorization model, which includes the information captured by the la- tent features and entity model (but not the neigh- borhood model), and incorporates contextual in- formation.</p><p>Using contextual information. It is well known that contextual information can improve IE meth- ods. Information such as bag-of-words, part-of- speech tags, entity types, or parse trees have been integrated into many existing systems ( <ref type="bibr" target="#b24">Mintz et al., 2009;</ref><ref type="bibr" target="#b38">Zhang et al., 2012;</ref><ref type="bibr" target="#b35">Takamatsu et al., 2011;</ref><ref type="bibr" target="#b39">Zhou et al., 2007;</ref><ref type="bibr" target="#b12">de Lacalle and Lapata, 2013;</ref><ref type="bibr" target="#b9">Akbik et al., 2012)</ref>. Our work differs in that we integrate contextual information into an open RE system. To do so, we leverage factoriza- tion machines <ref type="bibr" target="#b29">(Rendle et al., 2011;</ref><ref type="bibr" target="#b30">Rendle, 2012)</ref>, which have been successfully applied to exploit contextual information in the context of recom- mender systems. We show how to model open RE data and context with factorization machines and provide a method for parameter estimation under the open-world assumption.  De- note by R the set of all observed relations, by E the set of all observed entities, and by T ⊆ E × E the set of all observed entity pairs, which we refer to as tuples. A fact takes form r(t) and is com- posed of a relation r ∈ R and a tuple t ∈ T ; e.g., "join"(TP, MM). Note that there may be multiple observations for a fact. Finally, denote by C the set of all contextual variables; each observation is associated with a set c ⊆ C of context variables. In this paper we restrict attention to categorical context variables; our model can potentially han- dle continuous context as in <ref type="bibr" target="#b30">(Rendle, 2012)</ref>.</p><formula xml:id="formula_0">1 0 0 1 0 0 0.5 0.5 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0.5 0.5 1 0 1 0 1 0 0 0 1 0 0 0.5 0.5 0 0 1 0.6 0.4 0 0 1 0 0 1 0 0 0.5 0.5 1 0 1 0 "born in"(x,</formula><formula xml:id="formula_1">Problem definition.</formula><p>The open RE task is to pro- duce a ranked list of tuples T r ⊆ T for each rela- tion r ∈ R; the list is restricted to new tuples, i.e., tuples t ∈ T for which r(t) has not been observed in the input. The rank of each tuple reflects the model's prediction of the likelihood that the corre- sponding fact is indeed true. A good model thus ranks correct facts higher than incorrect ones.</p><p>Modeling facts. Denote by V = R ∪ T ∪ E ∪ C the set of all observed relations, tuples, entities, and contextual variables. For ease of exposition, we refer to the elements of V as variables. We model the input data in terms of a matrix in which each row corresponds to a fact (i.e., not an obser- vation) and each column to a variable. We group columns according to the type of the variables; e.g, there are relation columns, tuple columns, entity columns, and a group of columns for each type of contextual information. The matrix is populated such that in each row the values of each column group sum up to unity, i.e., we normalize values within column groups. In particular, we set to 1 the values of the variable of the relation and the tuple of the corresponding fact. We set to 0.5 the variables corresponding to the two entities referred to by the fact. An example is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Here the first row, for instance, corresponds to the fact "born in" <ref type="bibr">(Caesar, Rome)</ref>. Note that we model tuples and entities separately: the entity variables expose which arguments belong to the fact, the tu- ple variables expose their order.</p><p>Modeling context. As described above, we model the data in terms of a matrix in which rows corresponds to facts (instead of observations). The reasoning behind this approach is as follows. First, we may see a fact in multiple observations; our goal is to leverage all the available context. Sec- ond, facts but not observations are the target of our predictions. Finally, we are interested in predict- ing new facts, i.e., facts that we have not seen in the input data. For these facts, there is no corre- sponding observation so that we cannot directly obtain contextual information. To address these points, our model aggregates the context of rele- vant observations for each fact; this approach al- lows us to provide comprehensive contextual in- formation for both observed and unobserved facts. We group contextual information by the type of information: examples include metadata about the extraction sources (e.g., from an article on music), types of the entities of a tuple (e.g., (person, lo- cation)), or the bag-of-words in the sentence from which an extraction has been obtained. We ag- gregate the contextual information for each tuple t ∈ T ; this tuple-level approach allows us to pro- vide contextual information for unobserved facts. In more detail, we count in how many observations each contextual variable has been associated with the tuple, and then normalize the count values to 1 within each group of columns. The so-obtained values can be interpreted as the relative frequen- cies with which each contextual variable is associ- ated with the tuple. The contextual information as-sociated with each fact is given by the aggregated, normalized context of its tuple. <ref type="figure" target="#fig_0">Fig. 1</ref> shows context information arranged in two groups: tuple types and tuple topics. We cap- ture information such as that the tuple (Caesar, Rome) has only been seen in articles on history or that tuple <ref type="bibr">(Fermi, Rome)</ref> </p><note type="other">is mentioned in both physics and history articles (slightly more often in the former). Since context is associated with tu- ples, facts 2 and 4 on (Fermi, Sapienza) share con- textual information. This form of context sharing (as well as entity sharing) allows us to propagate information about tuples across various relations. Factorization model. CORE employs a matrix factorization model based on factorization ma- chines and the open-world assumption to capture latent semantic information about the individual variables. In particular, we associate with each variable v ∈ V a bias term b v ∈ R and a latent feature vector f v ∈ R d , where the dimensionality d of the latent feature space is a</note><p>hyperparameter of our model. Denote by X the set of rows in the in- put matrix, which we henceforth refer to as train- ing points. For each training point x ∈ X, denote by x v the value of variable v ∈ V in the corre- sponding row of the matrix. Our model associates with training point x ∈ X a score s(x) computed as follows:</p><formula xml:id="formula_2">s(x) = v∈V xvbv + v 1 ∈V v 2 ∈V \{v 1 } xv 1 xv 2 f T v 1 f v 2 (1)</formula><p>Here the bias terms models the contribution of each individual variable to the final score, whereas the latent feature vectors model the contribution of all pairwise interactions between variables. Note that only bias terms and feature vectors corre- sponding to non-zero entries in x affect the score and that x is often sparse. Since we can compute s(x) in time linear to both the number of nonzero entries in x and the dimensionality d <ref type="bibr" target="#b30">(Rendle, 2012)</ref>, score computation is fast. As discussed below, we (roughly) estimate bias terms and fea- ture vectors such that observed facts achieve high scores. We may thus think of each feature vector as a low-dimensional representation of the global information contained in the corresponding vari- able.</p><p>Prediction. Given estimates for bias terms and latent feature vectors, we rank unobserved facts as follows. Fix a relation r ∈ R and a tuple t ∈ T such that r(t) has not been observed. As indicated above, our model overcomes the key problem that there is no observation, and thus no context, for r(t) by context aggregation and sharing. In par- ticular, we create an test pointˆxpointˆ pointˆx for tuple r(t) in a way similar to creating data points, i.e., we set the relation, tuple, and entity variables accordingly and add the aggregated, normalized context of t. Once test pointˆxpointˆ pointˆx has been created, we can predict its score s(ˆ x) using Eq. (1). We then rank each un- observed tuple by its so-obtained score, i.e., tuples with higher scores are ranked higher. The resulting ranking constitutes the list T r of predicted facts for relation r.</p><p>Bayesian personalized ranking. The parame- ters of our model are given by Θ = { b v , f v | v ∈ V }. In approaches based on the closed-world as- sumption, Θ is estimated by minimizing the error between model predictions and target values (e.g., 1 for true facts, 0 for false facts). In our setting of open RE, all our observations are positive, i.e., we do not have negative training data. One way to handle the absence of negative training data is to associate a target value of 0 to all unobserved facts. This closed-world approach essentially as- sumes that all unobserved facts are false, which may not be a suitable assumption for the sparsely observed relations of open RE. Following <ref type="bibr" target="#b31">Riedel et al. (2013)</ref>, we adopt the open-world assump- tion instead, i.e., we treat each unobserved facts as unknown. Since factorization machines origi- nally require explicit target values (e.g., feedback in recommender systems), we need to adapt pa- rameter estimation to the open-world setting.</p><p>In more detail, we employ a variant of the Bayesian personalized ranking (BPR) optimiza- tion criterion <ref type="bibr" target="#b28">(Rendle et al., 2009</ref>). We asso- ciate with each training point x a set of negative samples</p><formula xml:id="formula_3">X − x . Each negative sample x − ∈ X − x</formula><p>is an unobserved fact with its associated context (constructed as described in the prediction section above). Generally, the negative samples x − should be chosen such that they are "less likely" to be true than fact x. We maximize the following optimiza- tion criterion:</p><formula xml:id="formula_4">1 |X| x∈X   x − ∈X − x ln σ(δ(x, x − )) |X − x | − λΘx 2  <label>(2)</label></formula><p>where σ(x) = 1 1+e −x denotes the logistic func- tion, δ(x, x − ) = s(x) − s(x − ) denotes the differ-ence of scores, and Θ x = { b v , f v | x v = 0 } the subset of the model parameters relevant for train- ing point x. Here we use L2 regularization con- trolled by a single hyperparameter λ. In essence, the BPR criterion aims to maximize the average "difference" ln σ(δ(x, x − )) between the score of fact x and each of its negative samples x − , av- eraged over all facts. In other words, we aim to score x higher than each x − . (Note that under the closed-world assumption, we would instead con- sider x − as being false.) For a more in-depth dis- cussion of BPR, see <ref type="bibr" target="#b28">(Rendle et al., 2009)</ref>. Sampling negative evidence. To make BPR ef- fective, the set of negative samples needs to be chosen carefully. A naive approach is to take the set of all unobserved facts between each relation r ∈ R and each tuple t ∈ T (or E × E) as the set X − x . The reasoning is that, after all, we ex- pect "random" unobserved facts to be less likely to be true than observed facts. This naive approach is problematic, however, because the set of nega- tive samples is independent of x and thus not suf- ficiently informative (i.e., it contains many irrele- vant samples).</p><p>To overcome this problem, the negative sample set needs to be related to x in some way. Since we ultimately use our model to rank tuples for each relation individually, we consider as negative evi- dence for x only unobserved facts from the same relation ( <ref type="bibr" target="#b31">Riedel et al., 2013)</ref>. In more detail, we (conceptually) build a negative sample set X − r for each relation r ∈ R. We include into X − r all facts r(t)-again, along with their context-such that t ∈ T is an observed tuple but r(t) is an unob- served fact. Thus the subject-object pair t of enti- ties is not observed with relation r in the input data (but with some other relation). The set of negative samples associated with each training point x is defined by the relation r of the fact contained in x, that is X − x = X − r . Note that we do not actually construct the negative sample sets; see below.</p><p>Parameter estimation. We maximize Eq. (2) using stochastic gradient ascent. This allows us to avoid constructing the sets X − x , which are of- ten infeasibly large, and worked well in our ex- periments. In particular, in each stochastic gra- dient step, we randomly sample a training point x ∈ X, and subsequently randomly sample a neg- ative sample x − ∈ X − x . This sampling procedure can be implemented very efficiently. We then per-  form the following ascent step with learning rate η:</p><formula xml:id="formula_5">Θ ← Θ + η Θ ln σ(d(x, x − )) − λΘ x 2</formula><p>One can show that the stochastic gradient used in the formula above is an unbiased estimate of the gradient of Eq. <ref type="formula" target="#formula_4">(2)</ref>. To speed up parameter es- timation, we use a parallel lock-free version of stochastic gradient ascent as in <ref type="bibr" target="#b27">Recht et al. (2011)</ref>. This allows our model to handle (reasonably) large datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conducted an experimental study on real- world data to compare our CORE model with other state-of-the-art approaches. <ref type="bibr" target="#b1">2</ref> Our experi- mental study closely follows the one of <ref type="bibr" target="#b31">Riedel et al. (2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Dataset. We made use of the dataset of <ref type="bibr" target="#b31">Riedel et al. (2013)</ref>, but extended it with contextual in- formation. The dataset consisted of 2.5M sur- face facts extracted from the New York Times cor- pus <ref type="bibr" target="#b32">(Sandhaus, 2008)</ref>, as well as 16k facts from Freebase. Surface facts have been obtained by using a named-entity recognizer, which addition- ally labeled each named entity mention with its coarse-grained type (i.e., person, organization, lo- cation, miscellaneous). For each pair of entities found within a sentence, the shortest dependency path between these pairs was taken as surface rela- tion. The entity mentions in each surface fact were linked to Freebase using a simple string matching   <ref type="table">Table 2</ref>: True facts and MAP 100 # (in parentheses) in the top-100 evaluation-set tuples for Freebase rela- tions. We consider as context the article metadata (m), the tuple types (t) and the bag-of-words (w). Best value per relation in bold (unique winner) or italic (multiple winners). Average weighs are # column values.</p><formula xml:id="formula_6">6 (0.09) 3 (0.13) 6 (0.15) roadcast/area served 5 0 (0.0) 4 (0.71) 4 (0.73) 4 (0.65) 4 (0.66) 4 (0.66) 5 (0.64) 5 (0.72) person/religion 5 2 (0.0) 3 (0.21) 2 (0.22) 1 (0.2) 3 (0.22) 3 (0.25) 2 (0.21) 3 (0.21) composer/compositions 3 2 (0.1) 2 (0.34) 2 (0.35) 2 (0.34) 2 (0.35) 1 (0.33) 2 (0.22) 2 (0</formula><p>method. If no match was found, the entity men- tion was kept as is. There were around 2.2M tu- ples (distinct entity pairs) in this dataset, out of which 580k were fully linked to Freebase. For each of these tuples, the dataset additionally in- cluded all of the corresponding facts from Free- base. Using the metadata 3 of each New York Times article, we enriched each surface fact by the following contextual information: news desk (e.g., sports desk, foreign desk), descriptors (e.g., finances, elections), online section (e.g., sports, business), section (e.g., a, d), publication year, and bag-of-words of the sentence from which the sur- face fact has been extracted.</p><p>Training data. From the raw dataset described above, we filtered out all surface relations with less than 10 instances, and all tuples with less than two instances, as in <ref type="bibr" target="#b31">Riedel et al. (2013)</ref>. Tab. 1 summarizes statistics of the resulting dataset. Here we considered a fact or tuple as linked if both of its entities were linked to Freebase, as partially- linked if only one of its entities was linked, and as non-linked otherwise. In contrast to previous work ( <ref type="bibr" target="#b31">Riedel et al., 2013;</ref>), we retain partially-linked and non-linked facts in our dataset.</p><p>3 Further information can be found at htps:// catalog.ldc.upenn.edu/LDC2008T19. Evaluation set. Open RE models produce pre- dictions for all relations and all tuples. To keep the experimental study feasible and comparable to previous studies, we use the full training data but evaluate each model's predictions on only the sub- sample of 10k tuples (≈ 6% of all tuples) of <ref type="bibr" target="#b31">Riedel et al. (2013)</ref>. The subsample consisted of 20% linked, 40% partially-linked and 40% non-linked tuples. For each (surface) relation and method, we predicted the top-100 new facts (not in training) for the tuples in the subsample.</p><p>Considered methods. We compared various forms of our CORE model with PITF and the matrix factorization model NFE. Our study fo- cused on these two factorization models because they outperformed other models (including non- factorization models) in previous studies ( <ref type="bibr" target="#b31">Riedel et al., 2013;</ref>). All models were trained with the full training data described above.</p><p>PITF ( <ref type="bibr" target="#b15">Drumond et al., 2012)</ref>. PITF is a recent tensor factorization method designed for within- KB reasoning. PITF is based on factorization ma- chines so that we used our scalable CORE imple- mentation for training the model. NFE ( <ref type="bibr" target="#b31">Riedel et al., 2013)</ref>. NFE is the full model proposed in the "universal schema" work of <ref type="bibr" target="#b31">Riedel et al. (2013</ref>  <ref type="table">Table 3</ref>: True facts and MAP 100 # (in parentheses) in the top-100 evaluation-set tuples for surface relations. We consider as context the article metadata (m), the tuple types (t) and the bag-of-words (w). Best value per relation in bold (unique winner) or italic (multiple winners). Average weighs are # column values. model (N), a matrix factorization model (F), and an entity model (E). The F and E models together are similar (but not equal) to our CORE model without context. The NFE model outperformed tensor models ( ) as well as clus- tering methods and distantly supervised methods in the experimental study of <ref type="bibr" target="#b31">Riedel et al. (2013)</ref> for open RE tasks. We use the original source code of <ref type="bibr" target="#b31">Riedel et al. (2013)</ref> for training.</p><p>CORE. We include multiple variants of our model in the experimental study, each differing by the amount of context being used. We con- sider as context the article metadata (m), the tu- ple types (t) and the bag-of-words (w). Each tuple type is a pair of subject-object types of (e.g. (per- son, location)). The basic CORE model uses rela- tions, tuples and entities as variables. We addition- ally consider the CORE+t, CORE+w, CORE+mt, and CORE+mtw models, where the suffix indi- cates which contextual information has been in- cluded. The total number of variables in the re- sulting models varied between 300k (CORE) to 350k (CORE+mtw). We used a modified version of libfm for training. <ref type="bibr" target="#b6">4</ref> Our version adds support for BPR and parallelizes the training algorithm.</p><p>Methodology. To evaluate the prediction perfor- mance of each method, we followed <ref type="bibr" target="#b31">Riedel et al. (2013)</ref>. We considered a collection of 19 Freebase relations (Tab. 2) and 10 surface relations (Tab. 3) and restrict predictions to tuples in the evaluation set.</p><p>Evaluation metrics. For each relation and method, we computed the top-100 evaluation set predictions and labeled them manually. We used 4 http://www.libfm.org as evaluation metrics the mean average precision defined as:</p><formula xml:id="formula_7">MAP 100 # = 100 k=1 I k · P @k min{100, #}<label>(3)</label></formula><p>where indicator I k takes value 1 if the k-th pre- diction is true and 0 otherwise, and # denotes the number of true tuples for the relation in the top- 100 predictions of all models. The denominator is included to account for the fact that the eval- uation set may include less than 100 true facts. MAP 100 # reflects how many true facts are found by each method as well as their ranking. If all # facts are found and ranked top, then MAP 100 # = 1. Note that our definition of MAP 100 # differs slightly from <ref type="bibr" target="#b31">Riedel et al. (2013)</ref>; our metric is more ro- bust because it is based on completely labeled evaluation data. To compare the prediction per- formance of each system across multiple rela- tions, we averaged MAP 100 # values, in both an un- weighted and a weighted (by #) fashion.</p><p>Parameters. For all systems, we used d = 100 latent factors, λ = 0.01 for all variables, a constant learning rate of η = 0.05, and ran 1000 epochs of stochastic gradient ascent. These choices correspond to the ones of <ref type="bibr" target="#b31">Riedel et al. (2013)</ref>; no further tuning was performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results.</head><p>Prediction performance. The results of our ex- perimental study are summarized in Tab. 2 (Free- base relations) and Tab. 3 (surface relations). As mentioned before, all reported numbers are with respect to our evaluation set. Each entry shows the number of true facts in the top-100 predictions and, in parentheses, the MAP 100 # value. The # col- <ref type="figure">Figure 2</ref>: Some facts predicted by our model for the Freebase relation author(x,y) and the surface relation "scientist at"(x,y). Most similar relations also reported, using cosine similarity between the correspond- ing latent feature vectors as distance.</p><p>umn list the total number of true facts found by at least one method. The last two lines show the ag- gregated MAP 100 # scores.</p><p>We start our discussion with the results for Free- base relations (Tab. 2). First note that the PITF model generally did not perform well; as dis- cussed before, tensor factorization models such as PITF suffer from a large prediction space and cannot incorporate tuple-level information. NFE and CORE, both matrix factorization models, per- formed better and were on par with each other. This indicates that our use of factorization ma- chines does not affect performance in the ab- sence of context; after all, both methods essen- tially make use of the same amount of informa- tion. The key advantage of our model over NFE is that we can incorporate contextual informa- tion. Our results indicate that using such informa- tion indeed improves prediction performance. The CORE+mtw model performed best overall; it in- creased the average MAP 100 # by four points (six points weighted) compared to the best context- unware model. Note that for some relations, in- cluding only subsets of the contextual informa- tion produced better results than using all contex- tual information (e.g., film/directed by). We thus conjecture that extending our model by variable- specific regularization terms may be beneficial.</p><p>Tab. 3 summarizes our results for surface rela- tions. In general, the relative performance of the models agreed with the one on Freebase relations. One difference is that using bag-of-word context significantly boosted prediction performance. One reason for this boost is that related surface rela- tions often share semantically related words (e.g., "professor at" and "scientist at") and may occur in similar sentences (e.g., mentioning "university", "research", ...).</p><p>Anecdotal results. <ref type="figure">Fig. 2</ref> shows the top test-set predictions of CORE+mtw for the author and "sci- entist at" relations. In both cases, we also list re- lations that have a similar semantic representation in our model (highest cosine similarity). Note that semantic similarity of relations is one aspect of our model; predictions incorporate other aspects such as context (i.e., two "similar" relations in different contexts are treated differently).</p><p>Training time. We used a machine with 16- cores Intel Xeon processor and 128GB of mem- ory. Training CORE took roughly one hour, NFE roughly six hours (single core only), and training CORE+mtw took roughly 20 hours. Our imple- mentation can handle reasonably large data, but an investigation of faster, more scalable training methods appears worthwhile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed CORE, a matrix factorization model for open RE that incorporates contextual informa- tion. Our model is based on factorization ma- chines and the open-world assumption, integrates various forms of contextual information, and is ex- tensible. Our experimental study suggests that ex- ploiting context can significantly improve predic- tion performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example for representing a context-aware open RE problem with CORE</figDesc><graphic url="image-5.png" coords="4,431.11,70.40,63.87,111.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Dataset statistics. 

</table></figure>

			<note place="foot" n="1"> We mark (non-disambiguated) mentions of entities and relations in quotation marks throughout this paper.</note>

			<note place="foot" n="3"> The CORE Model Input data. We model the input data as a set of observations of the form (r, t, c), where r refer to a KB or surface relation, t refer to a subject-object pair of entities (or entity mentions) and c to contextual information. An observation obtained from the example of the introduction may be (&quot;join&quot;, (TP, MM), { types:(person,org),</note>

			<note place="foot" n="2"> Source code, datasets, and supporting material are available at http://dws.informatik.uni-mannheim. de/en/resources/software/core/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston</forename><surname>Groom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Gump</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><forename type="middle">D M</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">White Hotel)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Life Itself) 4 (Edmund White, Skinned Alive) 5 (Peter Manso, Brando: The Biography)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Rosenblatt</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The Lion&apos;s Pride) 7 (Richard Taruskin, Stravinsky and …) y&quot;(x,y) 0.97 &quot;book by</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Renehan</surname><genName>Jr</genName></persName>
		</author>
		<imprint>
			<pubPlace>x,y</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">91 &quot;who wrote&quot;(x,y) 0.89 &quot; &apos;s poem</title>
		<imprint>
			<pubPlace>x,y</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">R M</forename><surname>Riordan Roett ; Dr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberts</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
		<respStmt>
			<orgName>Johns Hopkins University ; University of Missouri) 3 (Linda Mayes, Yale University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">T</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cardiff Business School)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
		<respStmt>
			<orgName>Russell Ross, University of Iowa</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Richter</surname></persName>
		</author>
		<imprint>
			<pubPlace>Kingsborough College</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Washington University) … &quot;scientist at</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Weidenbaum</surname></persName>
		</author>
		<imprint>
			<pubPlace>x,y</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of relations and discriminative extraction patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larysa</forename><surname>Visengeriyeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priska</forename><surname>Herger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Computational Linguistics</title>
		<meeting>the 24th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Holmer Hemsen, and Alexander Löser</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Open information extraction from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Soderl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Broadhead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the 20th International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Typed tensor decomposition of knowledge bases for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised relation extraction with general domain knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Oier Lopez De Lacalle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><surname>Del Corro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Clausie: clause-based open information extraction</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on World Wide Web</title>
		<meeting>the 22nd International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predicting rdf triples in incomplete knowledge bases with tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Drumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Schmidtthieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual ACM Symposium on Applied Computing (SAC)</title>
		<meeting>the 27th Annual ACM Symposium on Applied Computing (SAC)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Identifying relations for open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Miao Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">Fang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Triplerank: Ranking semantic web data by tensor decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antje</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergej</forename><surname>Sizov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Staab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Semantic Web Conference (ISWC)</title>
		<meeting>the 8th International Semantic Web Conference (ISWC)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discovering relations among named entities from large corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics (ACL)</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A scalable approach for statistical learning in semantic graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achim</forename><surname>Rettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="22" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Link prediction in multirelational graphs using additive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 International Workshop on Semantic Technologies meet Recommender Systems &amp; Big Data (SeRSy)</title>
		<meeting>the 2012 International Workshop on Semantic Technologies meet Recommender Systems &amp; Big Data (SeRSy)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ensemble semantics for large-scale unsupervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Bonan Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinyew</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>EMNLP-CoNLL</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with an incomplete knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonan</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (HLT-NAACL)</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (HLT-NAACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP)</title>
		<meeting>the Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML)</title>
		<meeting>the 28th international conference on machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Factorizing yago: scalable machine learning for linked data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on World Wide Web</title>
		<meeting>the 21st International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hogwild: A lock-free approach to parallelizing stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the 25th Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bpr: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Steffen Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeno</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<meeting>the 25th Conference on Uncertainty in Artificial Intelligence (UAI)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast context-aware recommendations with factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeno</forename><surname>Steffen Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th international ACM conference on Research and development in Information Retrieval (SIGIR)</title>
		<meeting>the 34th international ACM conference on Research and development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Factorization machines with libfm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (HLTNAACL)</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (HLTNAACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sandhaus</surname></persName>
		</author>
		<title level="m">The New York Times Annotated Corpus. Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Preemptive information extraction using unrestricted relation discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Shinyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (HLT-NAACL)</title>
		<meeting>the 2006 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (HLT-NAACL)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>EMNLPCoNLL</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization leveraging contexts for unsupervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shingo</forename><surname>Takamatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Issei</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="87" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Materializing and querying learned knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achim</forename><surname>Bundschus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rettinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 International Workshop on Inductive Reasoning and Machine Learning for the Semantic Web (IRMLeS)</title>
		<meeting>the 2009 International Workshop on Inductive Reasoning and Machine Learning for the Semantic Web (IRMLeS)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Structured relation discovery using generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ontological smoothing for relation extraction with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the 26th Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tree kernel-based relation extraction with context-sensitive structured parse tree information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoming</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>EMNLP-CoNLL</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
