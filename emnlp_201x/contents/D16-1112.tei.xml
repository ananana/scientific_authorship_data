<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Headline Generation on Abstract Meaning Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sho</forename><surname>Takase</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Sciences</orgName>
								<orgName type="institution" key="instit1">Tohoku University</orgName>
								<orgName type="institution" key="instit2">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit3">NTT Corporation ‡</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Sciences</orgName>
								<orgName type="institution" key="instit1">Tohoku University</orgName>
								<orgName type="institution" key="instit2">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit3">NTT Corporation ‡</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
							<email>{takase, okazaki}@ecei.tohoku.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Sciences</orgName>
								<orgName type="institution" key="instit1">Tohoku University</orgName>
								<orgName type="institution" key="instit2">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit3">NTT Corporation ‡</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Sciences</orgName>
								<orgName type="institution" key="instit1">Tohoku University</orgName>
								<orgName type="institution" key="instit2">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit3">NTT Corporation ‡</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Sciences</orgName>
								<orgName type="institution" key="instit1">Tohoku University</orgName>
								<orgName type="institution" key="instit2">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit3">NTT Corporation ‡</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Headline Generation on Abstract Meaning Representation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1054" to="1059"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural network-based encoder-decoder models are among recent attractive methodologies for tackling natural language generation tasks. This paper investigates the usefulness of structural syntactic and semantic information additionally incorporated in a baseline neural attention-based model. We encode results obtained from an abstract meaning representation (AMR) parser using a modified version of Tree-LSTM. Our proposed attention-based AMR encoder-decoder model improves headline generation benchmarks compared with the baseline neural attention-based model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural network-based encoder-decoder models are cutting-edge methodologies for tackling natural lan- guage generation (NLG) tasks, i.e., machine trans- lation ( <ref type="bibr" target="#b2">Cho et al., 2014</ref>), image captioning ( <ref type="bibr" target="#b14">Vinyals et al., 2015)</ref>, video description ( <ref type="bibr" target="#b13">Venugopalan et al., 2015)</ref>, and headline generation ( <ref type="bibr" target="#b11">Rush et al., 2015)</ref>.</p><p>This paper also shares a similar goal and moti- vation to previous work: improving the encoder- decoder models for natural language generation. There are several directions for enhancement. This paper respects the fact that NLP researchers have expended an enormous amount of effort to develop fundamental NLP techniques such as POS tagging, dependency parsing, named entity recognition, and semantic role labeling. Intuitively, this structural, syntactic, and semantic information underlying in- put text has the potential for improving the quality of NLG tasks. However, to the best of our knowledge, there is no clear evidence that syntactic and seman- tic information can enhance the recently developed encoder-decoder models in NLG tasks.</p><p>To answer this research question, this paper pro- poses and evaluates a headline generation method based on an encoder-decoder architecture on Ab- stract Meaning Representation (AMR). The method is essentially an extension of attention-based sum- marization (ABS) ( <ref type="bibr" target="#b11">Rush et al., 2015)</ref>. Our pro- posed method encodes results obtained from an AMR parser by using a modified version of Tree- LSTM encoder <ref type="bibr" target="#b12">(Tai et al., 2015)</ref> as additional in- formation of the baseline ABS model. Conceptu- ally, the reason for using AMR for headline gen- eration is that information presented in AMR, such as predicate-argument structures and named entities, can be effective clues when producing shorter sum- maries (headlines) from original longer sentences. We expect that the quality of headlines will improve with this reasonable combination (ABS and AMR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Attention-based summarization (ABS)</head><p>ABS proposed in <ref type="bibr" target="#b11">Rush et al. (2015)</ref> has achieved state-of-the-art performance on the benchmark data of headline generation including the DUC-2004 dataset <ref type="bibr" target="#b10">(Over et al., 2007)</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the model structure of ABS. The model predicts a word sequence (summary) based on the combination of the neural network language model and an input sen- tence encoder.</p><p>Let V be a vocabulary. x i is the i-th indicator vector corresponding to the i-th word in the input sentence. Suppose we have M words of an input sentence. X represents an input sentence, which &lt;s&gt; canadian prime … year &lt;s&gt; canada … nato</p><formula xml:id="formula_0">F x 1 F x 3 F x 2 F x M Ey iC+1 Ey i E 0 y i E 0 y iC+1 nnlm enc y i+1</formula><p>input sentence headline is represented as a sequence of indicator vectors, whose length is M . That is, x i ∈ {0, 1} |V | , and</p><formula xml:id="formula_1">X = (x 1 , . . . , x M ). Similarly, let Y represent a sequence of indicator vectors Y = (y 1 , . . . , y L ), whose length is L. Here, we assume L &lt; M . Y C,i</formula><p>is a short notation of the list of vectors, which consists of the sub-sequence in Y from y i−C+1 to y i . We assume a one-hot vector for a special start symbol, such as "⟨S⟩", when i &lt; 1. Then, ABS outputs a summaryˆYsummaryˆ summaryˆY given an input sentence X as follows:</p><formula xml:id="formula_2">ˆ Y = arg max Y { log p(Y |X) } ,<label>(1)</label></formula><formula xml:id="formula_3">log p(Y |X) ≈ L−1 ∑ i=0 log p(y i+1 |X, Y C,i ),<label>(2)</label></formula><formula xml:id="formula_4">p(y i+1 |X, Y C,i ) ∝ exp ( nnlm(Y C,i ) + enc(X, Y C,i ) ) ,<label>(3)</label></formula><p>where nnlm(Y C,i ) is a feed-forward neural network language model proposed in ( <ref type="bibr" target="#b1">Bengio et al., 2003)</ref>, and enc(X, Y C,i ) is an input sentence encoder with attention mechanism. This paper uses D and H as denoting sizes (di- mensions) of vectors for word embedding and hid- den layer, respectively. Let E ∈ R D×|V | be an embedding matrix of output words. Moreover, let U ∈ R H×(CD) and O ∈ R |V |×H be weight matri- ces of hidden and output layers, respectively <ref type="bibr">1</ref> . Using the above notations, nnlm(Y C,i ) in Equation 3 can be written as follows:</p><formula xml:id="formula_5">nnlm(Y C,i ) = Oh, h = tanh(U ˜ y c ),<label>(4)</label></formula><p>where˜ywhere˜ where˜y c is a concatenation of output embed- ding vectors from i − C + 1 to i, that is, ˜ y c = (Ey i−C+1 · · · Ey i ). Therefore, ˜ y c is a (CD) di- mensional vector.</p><p>Next, F ∈ R D×|V | and E ′ ∈ R D×|V | denote embedding matrices of input and output words, re- spectively. O ′ ∈ R |V |×D is a weight matrix for the output layer. P ∈ R D×(CD) is a weight matrix for mapping embedding of C output words onto embed- ding of input words. ˜ X is a matrix form of a list of input embeddings, namely,</p><formula xml:id="formula_6">˜ X = [ ˜ x 1 , . . . , ˜ x M ] , where˜xwhere˜ where˜x i = F x i . Then, enc(X, Y C,i</formula><p>) is defined as the following equations:</p><formula xml:id="formula_7">enc(X, Y C,i ) = O ′ ¯ Xp,<label>(5)</label></formula><formula xml:id="formula_8">p ∝ exp( ˜ X T P ˜ y ′ c ),<label>(6)</label></formula><p>where˜ywhere˜ where˜y ′ c is a concatenation of output embedding vectors from i − C + 1 to i similar tõ y c , that is,</p><formula xml:id="formula_9">˜ y ′ c = (E ′ y i−C+1 · · · E ′ y i ). Moreover, ¯ X is a matrix form of a list of averaged input word em- beddings within window size Q, namely, ¯ X = [ ¯ x 1 , . . . , ¯ x M ], where ¯ x i = ∑ i+Q q=i−Q 1 Q ˜ x q . Equation 6</formula><p>is generally referred to as the atten- tion model, which is introduced to encode a rela- tionship between input words and the previous C output words. For example, if the previous C output words are assumed to align to x i , then the surround- ing Q words (x i−Q , . . . , x i+Q ) are highly weighted by Equation 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>Our assumption here is that syntactic and semantic features of an input sentence can greatly help for generating a headline. For example, the meanings of subjects, predicates, and objects in a generated headline should correspond to the ones appearing in an input sentence. Thus, we incorporate syntactic and semantic features into the framework of head- line generation. This paper uses an AMR as a case study of the additional features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">AMR</head><p>An AMR is a rooted, directed, acyclic graph that encodes the meaning of a sentence. Nodes in an AMR graph represent 'concepts', and directed edges represent a relationship between nodes. Concepts  AMR of the input sentence headline encAMR <ref type="figure">Figure 2</ref>: Model structure of our proposed attention- based AMR encoder; it outputs a headline using ABS and encoded AMR with attention.</p><p>consist of English words, PropBank event predi- cates, and special labels such as "person". For edges, AMR has approximately 100 relations (Ba- narescu et al., 2013) including semantic roles based on the PropBank annotations in OntoNotes ( <ref type="bibr" target="#b6">Hovy et al., 2006</ref>). To acquire AMRs for input sentences, we use the state-of-the-art transition-based AMR parser ( <ref type="bibr" target="#b15">Wang et al., 2015</ref>). <ref type="figure">Figure 2</ref> shows a brief sketch of the model struc- ture of our attention-based AMR encoder model. We utilize a variant of child-sum Tree-LSTM originally proposed in <ref type="bibr" target="#b12">(Tai et al., 2015)</ref> to encode syntactic and semantic information obtained from output of the AMR parser into certain fixed-length embedding vectors. To simplify the computation, we transform a DAG structure of AMR parser output to a tree structure, which we refer to as "tree-converted AMR structure". This transformation can be performed by separating multiple head nodes, which often appear for representing coreferential concepts, to a corre- sponding number of out-edges to head nodes. Then, we straightforwardly modify Tree-LSTM to also en- code edge labels since AMR provides both node and edge labels, and original Tree-LSTM only encodes node labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attention-Based AMR Encoder</head><p>Let n j and e j be N and E dimensional em- beddings for labels assigned to the j-th node, and the out-edge directed to its parent node 2 . W in , W f n , W on , W un ∈ R D×N are weight matrices <ref type="bibr">2</ref> We prepare a special edge embedding for a root node.</p><note type="other">for node embeddings n j 3 . Similarly, W ie , W f e ,</note><p>W oe , W ue ∈ R D×E are weight matrices for edge embeddings e j . W ih , W f h , W oh , W uh ∈ R D×D are weight matrices for output vectors connected from child nodes. B(j) represents a set of nodes, which have a direct edge to the j-th node in our tree- converted AMR structure. Then, we define embed- ding a j obtained at node j in tree-converted AMR structure via Tree-LSTM as follows:</p><formula xml:id="formula_10">˜ h j = ∑ k∈B(j) a k ,<label>(7)</label></formula><formula xml:id="formula_11">i j = σ ( W in n j + W ie e j + W ih˜hih˜ ih˜h j ) ,<label>(8)</label></formula><formula xml:id="formula_12">f jk = σ ( W f n n j + W f e e j + W f h a k ) ,<label>(9)</label></formula><formula xml:id="formula_13">o j = σ ( W on n j + W oe e j + W oh˜hoh˜ oh˜h j ) ,<label>(10)</label></formula><formula xml:id="formula_14">u j = tanh ( W un n j + W ue e j + W uh˜huh˜ uh˜h j ) ,<label>(11)</label></formula><formula xml:id="formula_15">c j = i j ⊙ u j ∑ k∈B(j) f jk ⊙ c k ,<label>(12)</label></formula><formula xml:id="formula_16">a j = o j ⊙ tanh(c j ).<label>(13)</label></formula><p>Let J represent the number of nodes in tree- converted AMR structure obtained from a given in- put sentence. We introduce A ∈ R D×J as a matrix form of a list of hidden states a j for all j, namely, A = [a 1 , . . . , a J ]. Let O ′′ ∈ R |V |×D be a weight matrix for the output layer. Let S ∈ R D×(CD) be a weight matrix for mapping the context embedding of C output words onto embeddings obtained from nodes. Then, we define the attention-based AMR encoder 'encAMR(A, Y C,i )' as follows:</p><formula xml:id="formula_17">encAMR(A, Y C,i ) = O ′′ As,<label>(14)</label></formula><formula xml:id="formula_18">s ∝ exp(A T S ˜ y ′ c ).<label>(15)</label></formula><p>Finally, we combine our attention-based AMR en- coder shown in Equation 14 as an additional term of Equation 3 to build our headline generation system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To demonstrate the effectiveness of our proposed method, we conducted experiments on benchmark data of the abstractive headline generation task de- scribed in <ref type="bibr" target="#b11">Rush et al. (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DUC-2004</head><p>Gigaword test data used Gigaword in ( <ref type="bibr" target="#b11">Rush et al., 2015)</ref> Our sampled test data For a fair comparison, we followed their evalu- ation setting. The training data was obtained from the first sentence and the headline of a document in the annotated Gigaword corpus ( <ref type="bibr" target="#b9">Napoles et al., 2012)</ref>  <ref type="bibr">4</ref> . The development data is DUC-2003 data, and test data are both <ref type="bibr">DUC-2004</ref><ref type="bibr" target="#b10">(Over et al., 2007</ref> and sentence-headline pairs obtained from the an- notated Gigaword corpus as well as training data <ref type="bibr">5</ref> . All of the generated headlines were evaluated by ROUGE <ref type="bibr" target="#b7">(Lin, 2004)</ref>  <ref type="bibr">6</ref> . For evaluation on DUC- 2004, we removed strings after 75-characters for each generated headline as described in the DUC- 2004 evaluation. For evaluation on Gigaword, we forced the system outputs to be at most 8 words as in <ref type="bibr" target="#b11">Rush et al. (2015)</ref> since the average length of headline in Gigaword is 8.3 words. For the pre- processing for all data, all letters were converted to lower case, all digits were replaced with '#', and words appearing less than five times with 'UNK'. Note that, for further evaluation, we prepared 2,000 sentence-headline pairs randomly sampled from the test data section of the Gigaword corpus as our ad- ditional test data.</p><formula xml:id="formula_19">Method R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L ABS (</formula><p>In our experiments, we refer to the baseline neural attention-based abstractive summarization method described in <ref type="bibr" target="#b11">Rush et al. (2015)</ref> as "ABS", and our proposed method of incorporating AMR structural information by a neural encoder to the baseline method described in Section 3 as "ABS+AMR". Additionally, we also evaluated the performance of the AMR encoder without the attention mechanism, which we refer to as "ABS+AMR(w/o attn)", to investigate the contribution of the attention mech- anism on the AMR encoder. For the parameter es- timation (training), we used stochastic gradient de- scent to learn parameters. We tried several val- ues for the initial learning rate, and selected the value that achieved the best performance for each method. We decayed the learning rate by half if the log-likelihood on the validation set did not improve for an epoch. Hyper-parameters we selected were D = 200, H = 400, N = 200, E = 50, C = 5, and Q = 2. We re-normalized the embedding after each epoch ( <ref type="bibr" target="#b5">Hinton et al., 2012</ref>).</p><p>For ABS+AMR, we used the two-step training scheme to accelerate the training speed. The first phase learns the parameters of the ABS. The second phase trains the parameters of the AMR encoder by using 1 million training pairs while the parameters of the baseline ABS were fixed and unchanged to prevent overfitting. <ref type="table">Table 1</ref> shows the recall of ROUGE (Lin, 2004) on each dataset. ABS (re-run) represents the perfor- mance of ABS re-trained by the distributed scripts <ref type="bibr">7</ref> . We can see that the proposed method, ABS+AMR, outperforms the baseline ABS on all datasets. In particular, ABS+AMR achieved statistically signif- icant gain from ABS (re-run) for ROUGE-1 and ROUGE-2 on DUC-2004. However in contrast, we observed that the improvements on Gigaword (the same test data as <ref type="bibr" target="#b11">Rush et al. (2015)</ref>) seem to be lim- ited compared with the DUC-2004 dataset. We as- sume that this limited gain is caused largely by the quality of AMR parsing results. This means that the I(1): crown prince abdallah ibn abdel aziz left saturday at the head of saudi arabia 's delegation to the islamic summit in islamabad , the official news agency spa reported . G: saudi crown prince leaves for islamic summit A: crown prince leaves for islamic summit in saudi arabia P: saudi crown prince leaves for islamic summit in riyadh I(2): a massive gothic revival building once christened the lunatic asylum west of the &lt;unk&gt; was auctioned off for $ #.# million -lrb- euro# .# million -rrb-.  Gigaword test data provided by <ref type="bibr" target="#b11">Rush et al. (2015)</ref> is already pre-processed. Therefore, the quality of the AMR parsing results seems relatively worse on this pre-processed data since, for example, many low-occurrence words in the data were already re- placed with "UNK". To provide evidence of this as- sumption, we also evaluated the performance on our randomly selected 2,000 sentence-headline test data also taken from the test data section of the annotated Gigaword corpus. "Gigaword (randomly sampled)" in <ref type="table">Table 1</ref> shows the results of this setting. We found the statistical difference between ABS(re-run) and ABS+AMR on ROUGE-1 and ROUGE-2.</p><p>We can also observe that ABS+AMR achieved the best ROUGE-1 scores on all of the test data. Ac- cording to this fact, ABS+AMR tends to success- fully yield semantically important words. In other words, embeddings encoded through the AMR en- coder are useful for capturing important concepts in input sentences. <ref type="figure" target="#fig_3">Figure 3</ref> supports this observa- tion. For example, ABS+AMR successfully added the correct modifier 'saudi' to "crown prince" in the first example. Moreover, ABS+AMR generated a consistent subject in the third example.</p><p>The comparison between ABS+AMR(w/o attn) and ABS+AMR (with attention) suggests that the attention mechanism is necessary for AMR encod- ing. In other words, the encoder without the atten- tion mechanism tends to be overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Recently, the Recurrent Neural Network (RNN) and its variant have been applied successfully to various NLP tasks. For headline generation tasks, <ref type="bibr" target="#b3">Chopra et al. (2016)</ref> exploited the RNN decoder (and its variant) with the attention mechanism instead of the method of <ref type="bibr" target="#b11">Rush et al. (2015)</ref>: the combination of the feed-forward neural network language model and attention-based sentence encoder.  also adapted the RNN encoder-decoder with attention for headline generation tasks. Moreover, they made some efforts such as hierarchical atten- tion to improve the performance. In addition to us- ing a variant of RNN,  pro- posed a method to handle infrequent words in nat- ural language generation. Note that these recent developments do not conflict with our method us- ing the AMR encoder. This is because the AMR encoder can be straightforwardly incorporated into their methods as we have done in this paper, incor- porating the AMR encoder into the baseline. We be- lieve that our AMR encoder can possibly further im- prove the performance of their methods. We will test that hypothesis in future study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper mainly discussed the usefulness of in- corporating structural syntactic and semantic infor- mation into novel attention-based encoder-decoder models on headline generation tasks. We selected abstract meaning representation (AMR) as syntac- tic and semantic information, and proposed an attention-based AMR encoder-decoder model. The experimental results of headline generation bench- mark data showed that our attention-based AMR encoder-decoder model successfully improved stan- dard automatic evaluation measures of headline gen- eration tasks, ROUGE-1, ROUGE-2, and ROUGE- L. We believe that our results provide empirical ev- idence that syntactic and semantic information ob- tained from an automatic parser can help to improve the neural encoder-decoder approach in NLG tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Model structure of 'attention-based summarization (ABS)'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>"</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>G: massive ##th century us mental hospital fetches $ #.# million at auction A: west african art sells for $ #.# million in P: west african art auctioned off for $ #.# million I(3): brooklyn , the new bastion of cool for many new yorkers , is poised to go mainstream chic . G: high-end retailers are scouting sites in brooklyn A: new yorkers are poised to go mainstream with chic P: new york city is poised to go mainstream chic</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of generated headlines on Gigaword. I: input, G: true headline, A: ABS (re-run), and P: ABS+AMR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Rush et al., 2015) 26.55 7.06 22.05 30.88 12.22 27.</figDesc><table>77 
-
-
-
ABS (re-run) 
28.05 7.38 23.15 31.26 12.46 28.25 
32.93 13.43 29.80 
ABS+AMR 
 *  28.80  *  7.83  *  23.62 31.64  *  12.94 28.54  *  33.43  *  13.93 30.20 
ABS+AMR(w/o attn) 
28.28 7.21 23.12 30.89 12.40 27.94 
31.32 12.83 28.46 

Table 1: Results of methods on each dataset. We marked  *  on the ABS+AMR results if we observed 
statistical difference (p &lt; 0.05) between ABS (re-run) and ABS+AMR on the t-test. (R-1: ROUGE-1, R-2: 
ROUGE-2, R-L: ROUGE-L) 

</table></figure>

			<note place="foot" n="1"> Following Rush et al. (2015), we omit bias terms throughout the paper for readability, though each weight matrix also has a bias term.</note>

			<note place="foot" n="3"> As with Equation 4, all the bias terms are omitted, though each weight matrix has one.</note>

			<note place="foot" n="4"> Training data can be obtained by using the script distributed by the authors of Rush et al. (2015). 5 Gigaword test data can be obtained from https:// github.com/harvardnlp/sent-summary 6 We used the ROUGE-1.5.5 script with option &quot;−n2 −m −b75 −d&quot;, and computed the average of each ROUGE score.</note>

			<note place="foot" n="7"> https://github.com/facebook/NAMAS</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their insight-ful comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Abstract Meaning Representation for Sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Neural Probabilistic Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Abstractive Sentence Summarization with Attentive Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2016)</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pointing the Unknown Words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improving Neural Networks by Preventing Co-adaptation of Feature Detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>abs/1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">OntoNotes: The 90% Solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2006)</title>
		<meeting>the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2006)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ROUGE: A Package for Automatic Evaluation of Summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out: Proceedings of the Association for Computational Linguistics Workshop</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Annotated Gigaword</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction (AKBC-WEKEX)</title>
		<meeting>the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction (AKBC-WEKEX)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">DUC in Context. Information Processing and Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1506" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Neural Attention Model for Abstractive Sentence Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP 2015)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP 2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2015)</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Translating Videos to Natural Language Using Deep Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2015)</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1494" to="1504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Show and Tell: A Neural Image Caption Generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition (CVPR 2015)</title>
		<meeting>the Computer Vision and Pattern Recognition (CVPR 2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Transition-based Algorithm for AMR Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2015)</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="366" to="375" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
