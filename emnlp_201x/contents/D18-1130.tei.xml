<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Entity Tracking Improves Cloze-style Reading Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luong</forename><surname>Hoang</surname></persName>
							<email>lhoanger@gmail.com, {swiseman,srush}@seas.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University Cambridge</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University Cambridge</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University Cambridge</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Entity Tracking Improves Cloze-style Reading Comprehension</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1049" to="1055"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1049</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Reading comprehension tasks test the ability of models to process long-term context and remember salient information. Recent work has shown that relatively simple neural methods such as the Attention Sum-Reader can perform well on these tasks; however, these systems still significantly trail human performance. Analysis suggests that many of the remaining hard instances are related to the inability to track entity-references throughout documents. This work focuses on these hard entity tracking cases with two extensions: (1) additional entity features, and (2) training with a multi-task tracking objective. We show that these simple modifications improve performance both independently and in combination , and we outperform the previous state of the art on the LAMBADA dataset, particularly on difficult entity examples.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There has been tremendous interest over the past several years in Cloze-style <ref type="bibr" target="#b21">(Taylor, 1953)</ref> reading comprehension tasks, datasets, and models <ref type="bibr" target="#b11">(Hermann et al., 2015;</ref><ref type="bibr" target="#b12">Hill et al., 2016;</ref><ref type="bibr" target="#b13">Kadlec et al., 2016;</ref><ref type="bibr" target="#b6">Dhingra et al., 2016;</ref><ref type="bibr" target="#b4">Cui et al., 2016</ref>). Many of these systems apply neural models to learn to predict answers based on contextual matching, and have inspired other work in long-form generation and question answering. The extent and limits of these successes have also been a topic of in- terest ( <ref type="bibr" target="#b2">Chu et al., 2017)</ref>. Re- cent analysis by <ref type="bibr" target="#b2">Chu et al. (2017)</ref> suggests that a significant portion of the errors made by standard models, especially on the LAMBADA dataset <ref type="bibr" target="#b17">(Paperno et al., 2016)</ref>, derive from the inability to cor- rectly track entities or speakers, or a failure to han- dle various forms of reference.</p><p>This work targets these shortcomings by de- signing a model and training scheme targeted to- wards entity tracking. Specifically we introduce <ref type="bibr">Figure 1:</ref> A LAMBADA example where the final word "julie" (with reference chain in brackets) is the answer, y, to be predicted from the preceding context x. A system must know the two speak- ers and the current dialogue turn, simple context matching is not sufficient. Here, our model's pre- dictions before and after adding multi-task objec- tive are shown.</p><p>two simple changes to a stripped down model: (1) simple, entity-focused features, and (2) two multi-task objectives that target entity tracking. Our ablation analysis shows that both indepen- dently improve entity tracking, which is the pri- mary source of overall model's improvement. To- gether they lead to state-of-the-art performance on LAMBADA dataset and near state-of-the-art on CBT dataset ( <ref type="bibr" target="#b12">Hill et al., 2016)</ref>, even with a rel- atively simple model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>Cloze-style reading comprehension uses a passage of word tokens x = x 1:n (the context), with one token x j masked; the task is to fill in the masked word y, which was originally at position j. These datasets aim to present a benchmark challenge re- quiring some understanding of the context to se- lect the correct word. This task is a prerequi- site for problems like long-form generation and document-based question answering.</p><p>A number of datasets in this style exist with dif-ferent focus. Here we considered the LAMBADA dataset and the named entity portion of the Chil- dren's Book Test dataset (CBT-NE). LAMBADA uses novels where examples consist of 4-5 sen- tences and the last word to be predicted is masked, x n . The dataset is constructed carefully to focus on examples where humans needed the context to predict the masked word. CBT-NE examples, on the other hand, include 21 sentences where the masked word is a named entity extracted from the last sentence, with j ≤ n, and is constructed in a more automated way. We show an example from LAMBADA in <ref type="figure">Figure 1</ref>. In CBT, as well as the similar CNN/Daily Mail dataset ( <ref type="bibr" target="#b11">Hermann et al., 2015</ref>), the answer y is always contained in x whereas in LAMBADA it may not be. <ref type="bibr" target="#b2">Chu et al. (2017)</ref> showed, however, that training only on ex- amples where y is in x leads to improved overall performance, and we adopt this approach as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>The first popular neural network reading comprehension models were the Attentive Reader and its variant Impatient Reader ( <ref type="bibr" target="#b11">Hermann et al., 2015)</ref>. Both were the first to use bidirec- tional LSTMs to encode the context paragraph and the query separately. The Stanford Reader ( ) is a simpler version with fewer lay- ers for inference. These models use an encoder to map each context token x i to a vector u i . Fol- lowing the terminology of , ex- plicit reference models calculate a similarity mea- sure s i = s(u i , q) between each context vector u i and a query vector q derived for the masked word. These similarity scores are projected to an attention distribution α = softmax({s i }) over the context positions in 1, . . . , n, which are taken to be candidate answers. The Attention Sum Reader ( <ref type="bibr" target="#b13">Kadlec et al., 2016</ref>) is a further simplified version. It computes u i and q with separate bidirectional GRU ( <ref type="bibr" target="#b3">Chung et al., 2014</ref>) networks, and s i with a dot-product. It is trained to minimize:</p><formula xml:id="formula_0">L 0 (θ) = − ln p(y | x, q) = − ln i:x i =y p(x i | q) = − ln i:x i =y α i ,</formula><p>where θ is the set of all parameters associated with the model, and y is the correct answer. At test time, a pointer sum attention mechanism is used to predict the word type with the highest aggre- gate attention as the answer. The Gated Attention Reader ( <ref type="bibr" target="#b6">Dhingra et al., 2016</ref>) leverages the same mechanism for prediction and introduces an atten- tion gate to modulate the joint context-query infor- mation over multiple hops.</p><p>The Recurrent Entity Networks ( <ref type="bibr" target="#b10">Henaff et al., 2016</ref>) uses a custom gated recurrent module, Dy- namic Memory, to learn and update entity repre- sentations as new examples are received. Their gate function is combined of (1) a similarity mea- sure between the input and the hidden states, and (2) a set of trainable "key" vectors which could learn any attribute of an entity such as its loca- tion or other entities it is interacting with in the current context. The Query Reduction Networks ( <ref type="bibr" target="#b19">Seo et al., 2016</ref>) is also a gated recurrent network which tracks state in a paragraph and uses a hid- den query vector to keep pointing to the answer at each step. The query is successively transformed with each new sentence to a reduced state that's easier to answer given the new information.</p><p>Model In this work, we were particularly inter- ested in the shortcomings of simple models and exploring whether or how much entity tracking could help, since <ref type="bibr" target="#b2">Chu et al. (2017)</ref> has pointed out this weakness. As a result, we adapt a simplified Attention Sum (AttSum) reader throughout all ex- periments. Our version uses only a single bidirec- tional GRU for both u i and q. This GRU is of size 2d, using the first d states for the context and sec- ond d for the query. Formally, let </p><formula xml:id="formula_1">u i = → h i,↑ || ← h i,↑ .</formula><p>For datasets using the last word, the query is con-</p><formula xml:id="formula_2">structed as q = → h n,↓ || ← h 1,↓ .</formula><p>When the masked word can be anywhere, the query is constructed</p><formula xml:id="formula_3">as q = → h j−1,↓ || ← h j+1,↓ .</formula><p>Our main contribution is the extension of this simple model to incorporate entity tracking. Other authors have explored extending neural reading comprehension models with linguistic features, particularly <ref type="bibr" target="#b7">Dhingra et al. (2017)</ref> who use a modi- fied GRU with knowledge such as coreference re- lations and hypernymy. In <ref type="bibr" target="#b5">Dhingra et al. (2018)</ref>, the most recent coreferent antecedent for each to- ken is incorporated into the update equations of the GRU unit to bias the reader towards coreferent recency. In this work, we instead use a much sim- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Sentence Index, POS Tag, NER Tag 2 Is among last 3 PERSON words in x 3 Is a PERSON word in the last sentence 4 Is a PERSON word identical to previous PERSON word 5 Is a PERSON word identical to next PERSON word 6 Quoted-speech Index 7 Speaker</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning to Track Entities</head><p>Analysis on reading comprehension has indicated that neural models are strong at matching local context information but weaker at following enti- ties through the discourse ( <ref type="bibr" target="#b2">Chu et al., 2017</ref>). We consider two straightforward ways for extending the Attention Sum baseline to better track entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method 1: Features</head><p>We introduce a short-list of features in <ref type="table" target="#tab_0">Table 1</ref> to augment the representa- tion of each word in x. These features are meant to help the system to identify and use the rela- tionships between words in the passage. 1 Features 2-5 apply only to words tagged PERSON by the NER tagger. Features 6-7 apply only to words be- tween opening and closing quotation marks. Fea- ture 6 indicates the index of the quote in the doc- ument, and Feature 7 gives the assumed speaker of the quote using some simple rules; we provide the rules in the Supplementary Material. Though most of these features are novel, they are moti- vated by recent analysis ( <ref type="bibr" target="#b23">Wang et al., 2015;</ref>.</p><p>All features are incorporated into a word's rep- resentation by embedding each discrete feature into a vector of the same size as the original word embedding, adding the vectors as well as a bias, and applying a tanh nonlinearity.</p><p>Method 2: Multitasking We additionally en- courage the neural model to keep track of enti- ties by multitasking with simple auxiliary entity- tracking tasks. Examples such as <ref type="figure">Figure 1</ref> suggest that keeping track of which entities are currently in 1 POS tags are produced with the NLTK library ( <ref type="bibr">Bird et al., 2009)</ref>, and NER tags with the Stanford NER tag- ger ( <ref type="bibr" target="#b9">Finkel et al., 2005</ref>). We additionally found it useful to tag animate words as PERSONs on the CBT-NE data, using the animate word list of <ref type="bibr">Bergsma and Lin (2006).</ref> scope is useful for answering reading comprehen- sion questions. There, amy and julie are convers- ing, and being able to track that amy is the speaker of the final quote helps to rule her out as a candi- date answer. We consider two tasks:</p><p>For Task 1 (L 1 ) we train the same model to pre- dict repeated named entities. For all named enti- ties x j such that there is a x i = x j with i &lt; j, we attempt to mask and predict the word type x j . This is done by introducing another Cloze predic- tion, but now setting the target y = x j , reduc- ing the context to preceding words x 1:j−1 with u i = → h i , and the query q = → h j−1 . (Note that unlike above, both of these only use the forward states of the GRU). We use a bilinear similarity score s i = q T Q u i , for this prediction where Q is a learned transformation in R 2d×2d . This task is inspired by the antecedent ranking task in corefer- ence <ref type="bibr" target="#b26">(Wiseman et al., 2015</ref><ref type="bibr" target="#b25">(Wiseman et al., , 2016</ref>.</p><p>For Task 2 (L 2 ) we train to predict the order in- dex in which a named entity has been introduced. For example, in <ref type="figure">Figure 1</ref>, julie would be 1, amy would be 2, marsh would be 3, etc. The hope here is that learning to predict when entities reap- pear will help the model track their reoccurences. For the blue labeled julie, the model would aim to predict 1, even though it appears later in the con- text. This task is inspired by the One-Hot Pointer Reader of  on the Who-did- What dataset ( <ref type="bibr" target="#b16">Onishi et al., 2016)</ref>. Formally, let- ting idx(x j ) be the predicted index for x j , we min- imize:</p><formula xml:id="formula_4">L 2 (θ) = − ln p( idx(x j ) = idx(x j ) | x 1:j−1 ) = − ln softmax(W → h j ) idx(x j ) ,</formula><p>where W ∈ R |E|×2d and E is the set of entity word types in the document. Note that this is a simpler computation, requiring only O(|E| × n) predictions per x, whereas L 1 requires O(n 2 ). The full model minimizes a multi-task loss:</p><formula xml:id="formula_5">L 0 (θ) + γ 1 L 1 (θ) + γ 2 L 2 (θ).</formula><p>Using L 1 and L 2 simultaneously did not lead to improved perfor- mance however, and so either γ 1 , γ 2 is always 0. We believe that this is because, while the learn- ing objectives for L 1 and L 2 are mathematically different, they are both designed to similarly track the entities mentioned so far in the document and thus do not provide complementary information to each other.</p><p>We found it useful to have two hyperparameters per auxiliary task governing the number of distinct named entity word types and tokens used in defin- ing the losses L 1 and L 2 . In particular, per doc- ument these hyperparameters control in a top-to- bottom order the number of distinct named entity word types we attempt to predict, as well as the number of tokens of each type considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Methods This section highlights several aspects of our methodology; full hyperparameters are given in the Supplementary Material. For the training sets, we exclude examples where the an- swer is not in the context. The validation and test sets are not modified however and the model with the highest accuracy on the validation set is chosen for testing. For both tasks, the context words are mapped to learned embeddings; importantly, we initialize the first 100 dimensions with the 100- dimensional GLOVE embeddings ( <ref type="bibr" target="#b18">Pennington et al., 2014</ref>). Named entity words are anonymized, as is done in the CNN/Daily Mail corpus <ref type="bibr" target="#b11">(Hermann et al., 2015)</ref> and in some of the experiments of . The model is regularized with dropout ( <ref type="bibr" target="#b20">Srivastava et al., 2014</ref>) and opti- mized with ADAM ( <ref type="bibr" target="#b14">Kingma and Ba, 2014</ref>). For all experiments we performed a random search over hyperparameter values <ref type="bibr">(Bergstra and Bengio, 2012)</ref>, and report the results of the models that per- formed best on the validation set. Our implemen- tation is available at https://github.com/ harvardnlp/readcomp. <ref type="table" target="#tab_2">Table 2</ref> shows the full results of our best models on the LAMBADA and CBT-NE datasets, and compares them to recent, best-performing results in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>For both tasks the inclusion of either en- tity features or multi-task objectives leads to large statistically significant increases in valida- tion and test score, according to the McNemar test (α = 0.05) with continuity correction <ref type="bibr" target="#b8">(Dietterich, 1998</ref>). Without features, AttSum + L 2 achieves the best test results, whereas with fea- tures AttSum-Feat + L 1 performs best on CBT- NE. The results on LAMBADA indicate that en- tity tracking is a very important overlooked as- pect of the task. Interestingly, with features in- cluded, AttSum-Feat + L 2 appears to hurt test performance on LAMBADA and leaves CBT- NE performance essentially unchanged, amount- ing to a negative result for L 2 . On the other hand, the effect of AttSum-Feat + L 1 is pro-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LAMBADA</head><p>Val Test GA Reader ( <ref type="bibr" target="#b2">Chu et al., 2017</ref>) - 49.00 MAGE (48) ( <ref type="bibr" target="#b7">Dhingra et al., 2017)</ref> 51.10 51.60 MAGE (64) ( <ref type="bibr" target="#b7">Dhingra et al., 2017)</ref> 52.10 51.10 GA + C-GRU ( <ref type="bibr" target="#b5">Dhingra et al., 2018</ref>   <ref type="bibr" target="#b8">(Dietterich, 1998)</ref>.</p><p>nounced on CBT-NE, and while our simple mod- els do not increase the state-of-the-art test perfor- mance on CBT-NE, they outperform "attention- over-attention" in addition to reranking <ref type="bibr" target="#b4">(Cui et al., 2016)</ref>, and is outperformed only by architectures supporting "multiple-hop" inference over the doc- ument ( <ref type="bibr" target="#b6">Dhingra et al., 2016</ref>). Our best model on CBT-NE test set, AttSum-Feat + L 1 , is very close to the current state-of-the-art result. On the vali- dation sets for both LAMBADA and CBT-NE, the improvements from adding features to AttSum + L i are statistically significant (for full results refer to our supplementary material). On LAMBADA, the L 1 multi-tasked model is a 3.5-point increase on the state of the art.</p><p>Our method also employs fewer parameters than other richer models such as the GA Reader in ( <ref type="bibr" target="#b6">Dhingra et al., 2016)</ref>. More specifically, in terms of number of parameters, our models are very similar to a 1-hop GA Reader. In contrast, all published experiments of the latter use 3 hops where each hop requires 2 separate Bi-GRUs, one  to model the document and one for the query. This constitutes the largest difference in model size be- tween the two approaches. <ref type="table" target="#tab_4">Table 3</ref> considers the performance of the differ- ent models based on a segmentation of the data. Here we consider examples where: (1) Entity - if the answer is a named entity; (2) Speaker -if the answer is a named entity and the speaker of quote; (3) Quote -if the answer is found within a quoted speech. Note that Speaker and Quote cat- egories, while mutually exclusive, are subsets of the overall Entity category. We see that both the additional features and multi-task objectives in- dependently result in a clear improvement in all categories, but that the gains are particularly pro- nounced for named entities and specifically for Speaker and Quote examples. Here we see siz- able increases in performance, particularly in the Speaker category. We see larger increases in the more dialog heavy LAMBADA task.</p><p>As a qualitative example of the improvement af- forded by multi-task training, in <ref type="figure">Figure 1</ref> we show the different predictions made by our model with and without L 1 (colored as blue and red, respec- tively). Note that amy and julie are both entities that have been repeated twice in the passage. In addition to the final answer, our model with the L 1 loss was also able to predict these entities (at the colored locations) given preceding words. Fur- ther qualitative analysis reveals that these augmen- tations improved the model's ability to eliminate non-entity choices from predictions. Some exam- ples are shown in <ref type="figure" target="#fig_1">Figure 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This work demonstrates that learning to track en- tities with features and multi-task learning signif- icantly increases the performance of a baseline reading comprehension system, particularly on the difficult LAMBADA dataset. This result indicates that higher-level word relationships may not be modeled by simple neural systems, but can be in- corporated with minor additional extensions. This work hints that it is difficult for vanilla models to learn long-distance entity relations, and that these may need to be encoded directly through features or possibly with better pre-trained representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>→</head><label></label><figDesc>h i and ← h i (both in R 2d ) represent the forward and backward states of a bidirectional GRU run over x, and let → h i,↑ and → h i,↓ be the first and second d states respectively, and define || as the concatenation operator. The context vectors are constructed as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: LAMBADA examples where AttSum incorrectly predicts a non-entity answer whereas AttSum-Feat and AttSum + L i choose correctly.</figDesc><graphic url="image-3.png" coords="5,313.74,149.89,205.34,81.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Word-level features used in AttSum-Feat 
model. 

pler set of features and compare to this and several 
other models as baseline approaches. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Validation &amp; Test results on all datasets. 
AttSum* are our models, including variants with 
features and multi-task loss. Others indicate previ-
ous best published results. All improvements over 
AttSum are statistically significant (α = 0.05) ac-
cording to the McNemar test with continuity cor-
rection </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Ablation results on validation sets, see text 
for definitions of the numeric columns and mod-
els. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">text with the natural language toolkit</title>
		<imprint>
			<publisher>Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A thorough examination of the cnn/daily mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Broad context language modeling as reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="52" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Attention-overattention neural networks for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04423</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>William W Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05922</idno>
		<title level="m">Neural models for reasoning over multiple mentions using coreference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Gated-attention readers for text comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>William W Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01549</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>William W Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02620</idno>
		<title level="m">Linguistic knowledge as memory for recurrent neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Approximate statistical tests for comparing supervised classification learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas G Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1895" to="1923" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Tracking the world state with recurrent entity networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03969</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01547</idno>
		<title level="m">Text understanding with the attention sum reader network</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">DIM Reader: Dual Interaction Model for Machine Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Degen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer International Publishing</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Who did what: A large-scale person-centered cloze dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Onishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05457</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Kevin Gimpel, and David McAllester</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The LAMBADA dataset: Word prediction requiring a broad discourse context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Query-reduction networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04582</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">cloze procedure: a new tool for measuring readability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journalism Bulletin</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="415" to="433" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Natural language comprehension with the epireader</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02270</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Machine comprehension with syntax, frames, and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="700" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Emergent predication structure in hidden state vectors of neural readers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Onishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="26" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning global features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="994" to="1004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning anaphoricity and antecedent ranking features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1416" to="1426" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
