<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Surprisingly Easy Hard-Attention for Sequence to Sequence Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Shankar</surname></persName>
							<email>shiv_shankar@iitb.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">IIT Bombay</orgName>
								<orgName type="department" key="dep2">IIT Bombay</orgName>
								<orgName type="department" key="dep3">IIT Bombay</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhant</forename><surname>Garg</surname></persName>
							<email>sidgarg@cs.wisc.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">IIT Bombay</orgName>
								<orgName type="department" key="dep2">IIT Bombay</orgName>
								<orgName type="department" key="dep3">IIT Bombay</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
							<email>sunita@iitb.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">IIT Bombay</orgName>
								<orgName type="department" key="dep2">IIT Bombay</orgName>
								<orgName type="department" key="dep3">IIT Bombay</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Surprisingly Easy Hard-Attention for Sequence to Sequence Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="640" to="645"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>640</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we show that a simple beam approximation of the joint distribution between attention and output is an easy, accurate, and efficient attention mechanism for sequence to sequence learning. The method combines the advantage of sharp focus in hard attention and the implementation ease of soft attention. On five translation and two morphological inflection tasks we show effortless and consistent gains in BLEU compared to existing attention mechanisms.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In structured input-output models as used in tasks like translation and image captioning, the attention variable decides which part of the input aligns to the current output. Many attention mechanisms have been proposed ( <ref type="bibr">Xu et al., 2015;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b10">Luong et al., 2015;</ref><ref type="bibr" target="#b11">Martins and Astudillo, 2016)</ref> but the de facto standard is a soft attention mechanism that first assigns attention weights to input encoder states, then computes an attention weighted 'soft' aligned input state, which finally derives the output distribution. This method is end to end differentiable and easy to implement.</p><p>Another less popular variant is hard attention that aligns each output to exactly one input state but requires intricate training to teach the network to choose that state. When successfully trained, hard attention is often found to be more accurate ( <ref type="bibr">Xu et al., 2015;</ref><ref type="bibr">Zaremba and Sutskever, 2015)</ref>. In NLP, a recent success has been in a monotonic hard attention setting in morphological inflection tasks ( <ref type="bibr">Yu et al., 2016;</ref><ref type="bibr" target="#b0">Aharoni and Goldberg, 2017)</ref>. For general seq2seq learning, methods like Sparse- Max ( <ref type="bibr" target="#b11">Martins and Astudillo, 2016)</ref> and local atten- tion ( <ref type="bibr" target="#b10">Luong et al., 2015)</ref> were proposed to bridge the gap between soft and hard attention. * Both authors contributed equally to this work In this paper we propose a surprisingly simpler alternative based on the original joint distribution between output and attention, of which existing soft and hard attention mechanisms are approximations. The joint model couples input states individually to the output like in hard attention, but it combines the advantage of end-to-end trainability of soft atten- tion. When the number of input states is large, we propose to use a simple approximation of the full joint distribution called Beam-joint. This approxi- mation is also easily trainable and does not suffer from the high variance of Monte-Carlo sampling gradients of hard attention.</p><p>We evaluated our model on five translation tasks and increased BLEU by 0.8 to 1.7 over soft atten- tion, which in turn was better than hard and the recent <ref type="bibr">Sparsemax (Martins and Astudillo, 2016)</ref> attention. More importantly, the training process was as easy as soft attention. For further support, we also evaluate on two morphological inflection tasks and got gains over soft and hard attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>For sequence to sequence (seq2seq) learning the encoder-decoder model is the standard and we re- view it here. We then review related work on atten- tion mechanisms on these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Attention-based Encoder Decoder Model</head><p>Let x 1 , . . . , x m denote the tokens in the input sequence that have been transformed by an encoder network to state vectors x 1 , . . . , x m , which we jointly denote as x 1...m . Let y 1 , . . . , y n denote the output tokens in the target sequence. The Encoder-Decoder (ED) network factorizes Pr(y 1 , . . . , y n |x 1...m ) as n t=1 Pr(y t |x 1...m , s t ) where s t is a decoder state summarizing y 1 , . . . y t−1 . For each t, a hidden attention variable a t is used to denote which part of x 1...m aligns with y t . Let P (a t = j|x 1...m , s t ) denote the probability that encoder state x j is relevant for output y t . Typically this is estimated using a softmax function over attention scores computed from x j and decoder state s t as follows.</p><formula xml:id="formula_0">P (a t = j|x 1...m , s t ) = e A θ (x j ,st) m r=1 e A θ (xr,st)<label>(1)</label></formula><p>where A θ (., .) is the attention unit that scores each input state x j as per the decoder state s t . There- after, in the popular soft-attention mechanism, the attention weighted sum of the input states is used to model log likelihood for each y t as</p><formula xml:id="formula_1">log Pr(y t |x 1...m ) = log Pr(y t | a P t (a)x a ) (2)</formula><p>where P t (a t = j) is the short form for P (a t = j|x 1...m , s t ). Also, here and in the rest of the paper we drop s t from P (y t ) and P t (a) for ease of nota- tion. The weighted sum a P t (a)x a is called an input context c t which is fed to the decoder RNN along with y t for computing the next state s t+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Related Work</head><p>We next review existing attention types.</p><p>Soft Attention is the attention method described in the previous section and is the current stan- dard for seq2seq learning <ref type="bibr">(Xu Chen, 2018;</ref><ref type="bibr" target="#b8">Koehn, 2017)</ref>. It was proposed for translation in <ref type="bibr" target="#b1">(Bahdanau et al., 2014</ref>) and refined further in ( <ref type="bibr" target="#b10">Luong et al., 2015</ref>). As shown in Eq 2, here each output is derived from an attention averaged input. This diffuses the coupling between the input and out- put. The advantage of soft attention is end to end differentiability, and fast training and inference.</p><p>Hard Attention was proposed in its current form in ( <ref type="bibr">Xu et al., 2015</ref>) and attends to exactly one input state for an output <ref type="bibr">1</ref> . During training, log-likelihood is an expectation over sampled attentions:</p><formula xml:id="formula_2">log P t (y t |x 1...m ) = M l=1 log P t (y t |xã|x˜|xã l ) (3)</formula><p>whereãwhere˜whereã 1 , . . . , ˜ a M are sampled from the multino- mial P t (a). Because of the sampling, the gradi- ent has to be computed by Monte Carlo gradi- ent/REINFORCE <ref type="bibr">(Williams, 1992)</ref> and is subject to high variance. Many tricks are required to train hard attention and there is little standardization across implementations. <ref type="bibr">Xu et al (2015)</ref> use a combination of REINFORCE and soft attention. <ref type="bibr">Zaremba et al(2015)</ref> uses curriculum learning that starts as soft-attention and gradually becomes dis- crete. Ling&amp; Rush (2017) aggregates multiple sam- ples during training, and a single sampled attention while testing. However, once trained well the sharp focus on memory provided by hard-attention has been found to yield superior performance ( <ref type="bibr">Xu et al., 2015;</ref><ref type="bibr" target="#b15">Shankar and Sarawagi, 2018)</ref>.</p><p>Sparse/Local Attention Many attempts have been made to bridge the gap between soft and hard attention. <ref type="bibr" target="#b10">Luong et al (2015)</ref> proposes local atten- tion that averages a window of input. This has been refined later to include syntax ( <ref type="bibr" target="#b4">Chen et al., 2017;</ref><ref type="bibr" target="#b14">Sennrich and Haddow, 2016;</ref><ref type="bibr" target="#b5">Chen et al., 2018)</ref>. Another idea is to replace the softmax in soft atten- tion with sparsity inducing operators <ref type="bibr" target="#b11">(Martins and Astudillo, 2016;</ref><ref type="bibr" target="#b13">Niculae and Blondel, 2017)</ref>. How- ever, all sparse/local attention methods continue to compute P (y) from an attention weighted sum of inputs (Eq: 2) unlike hard attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Joint Attention-Output Models</head><p>We start from an explicit joint representation of the uncertainty of the attention and output variables.</p><formula xml:id="formula_3">log P t (y t |x 1...m ) = log a P t (a)P t (y t |x a ) (4)</formula><p>The joint model directly couples individual input states to the output, and thus is a type of hard atten- tion. Also, by taking an expectation, instead of a single hard attention, it enjoys differentiability as in soft-attention. We call this the full-joint method.</p><p>Unfortunately, either when the vocabulary or the number of encoder states (m) is large, full-joint is not practical. Existing hard and soft attentions can be viewed as its approximations that either marginalize early or hard select attention. We show a surprisingly simple alternative approximation that provides hard attention without its training com- plexity. Our method called Beam-joint determin- istically selects the top-k highest attention values and approximates the full joint log probability as</p><formula xml:id="formula_4">log P t (y t |x 1...m ) ≈ log a∈TopK(Pt(a)) P t (a)P t (y t |x a ) (5)</formula><p>Thus, in beam-joint, we first compute the multi- nomial attention distribution in O(m) time using Eq 1, select the Top-K input positions from the multinomial, next with hard attention on each posi- tion compute K output softmax, and finally com- pute the attention weighted output mixture distribu- tion. The number of output softmax is K times in normal soft-attention but the actual running time overhead is only 20-30% for translation tasks. We used the default pass-through TopK operator (which is not differentiable) and optimize the beam- approximation directly. We also experimented with a version which smoothly shifts from soft-attention to beam-attention, but found that training the beam- approximation directly leads to best results.</p><p>We show empirically that this very simple scheme is surprisingly effective compared to exist- ing hard and soft attention over several translation tasks. Unlike sampling and variational methods that require careful tuning and exotic tricks during training, this simple scheme trains as easily as soft- attention, without significant increase in training time because even K = 6 works well enough.</p><p>Another reason why our 'sum of probabilities' form performs better could be the softmax barrier effect highlighted in <ref type="figure">(Yang et al., 2018)</ref>. The au- thors argue that the richness of natural language cannot be captured in normal softmax due to the low rank constraint it imposes on input-to-output matrix. They improve performance using a Mixture of Softmax model. Our beam-joint also is a mixture of softmax and possibly achieves higher rank than a single softmax. However their mixture requires learning multiple softmax matrices, whereas ours are due to varying attention and we do not learn any extra parameters than soft attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We compare attention models on two NLP tasks: machine translation and morphological inflection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Machine translation</head><p>We experiment on five language pairs from three datasets: IWSLT15 English↔Vietnamese (Cet- tolo et al., 2015) which contains 133k train, 1.5k validation(tst2012) and 1.2k test(tst2013) sentence pairs respectively; IWSLT14 German↔English ( <ref type="bibr" target="#b3">Cettolo et al., 2014</ref>) which contains 160k train, 7.2k validation and 6.7k test sentence pairs respec- tively ; Workshop on Asian Translation 2017 Japanese→English ( <ref type="bibr" target="#b12">Nakazawa et al., 2016</ref>) which contains 2M train, 1.8k validation and 1.8k test sentence pairs respectively. We use a 2 layer bi- directional encoder and a 2 layer unidirectional de- coder with 512 hidden LSTM units and 0.2 dropout rate with vanilla SGD optimizer. We base our im- plementation 2 on the NMT code 3 in Tensorflow. We did no special hyper-parameter tuning and used standard-softmax tuned parameters on a batch size of 64.</p><p>Comparing attention models We compare beam-joint (default K = 6) with standard soft and hard attention. To further dissect the reasons behind beam-joint's gains, we compare beam-joint with a sampling based approximation of full-joint called Sample-Joint that replaces the TopK in Eq 5 with K attention weighted samples. We train sample- joint as well as hard-attention with REINFORCE with 6-samples. Also to ascertain that our gains are not explained by sparsity alone, we compare with <ref type="bibr">Sparsemax (Martins and Astudillo, 2016)</ref>.</p><p>In <ref type="table">Table 1</ref> we show perplexity and BLEU with three beam sizes (B). Beam-joint significantly out- performs all other variants, including the standard soft attention by 0.8 to 1.7 BLEU points. The per- plexity shows even a more impressive drop in all five datasets. Also we observe training times for beam-joint to be only 20-30% higher than soft- attention, establishing that beam-joint is both prac- tical and more accurate.</p><p>Sample-joint is much worse than beam-joint. Apart from the problem of high variance of gra- dients in the reinforce step, another problem is that sampling repeats states whereas TopK in beam- joint gets distinct states. Hard attention too faces training issues and performs worse than soft atten- tion, explaining why it is not commonly used in NMT. Sample-joint is better than Hard attention, further highlighting the merits of the joint distri- bution. Sparsemax is competitive but marginally worse than soft attention. This is concordant with the recent experiments of (Niculae and Blondel, 2017).</p><p>Comparison with Full Joint Next we evaluate the impact of our beam-joint approximation against full-joint and soft attention. Full-joint cannot scale to large vocabularies, therefore we only compare on En-Vi with a batch size of 32. <ref type="figure">Figure 1a</ref> shows final BLEU of these methods as well as BLEU against increasing training steps. Beam-joint both converges faster and to a higher score than soft-  Next, in <ref type="figure">Figure 1b</ref>, we compare beam-joint (solid lines) and soft attention (dotted lines) for convergence rates on three other datasets. For each dataset beam-joint trains faster with a consistent improvement of more than 1 BLEU.</p><p>Effect of K in Beam-joint We show the effect of K used in TopK of beam-joint in <ref type="figure">Figure 2</ref> on the En-Vi and De-En tasks. On En-Vi BLEU increases from 16.0 to 25.7 to 26.5 as K increases from 1 to 2 to 3; and then saturates quickly. Similar behavior is observed in the other dataset. This shows that small K values like 6 suffice for translation.</p><p>We further evaluate whether the performance gain of beam-joint is due to the softmax barrier alone in <ref type="table" target="#tab_2">Table 2</ref>. We used our models trained with K=6, and deployed them for test-time greedy de- coding with K set to 1. Since the output now has only a single softmax component, this model faces the same bottleneck as soft-attention. One can ob- serve that as expected these results are worse than beam-joint with K=6, however they still exceed soft-attention by a significant margin, demonstrat- ing that the performance gain is not solely due to the effect of ensembling or softmax-barrier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Morphological Inflection</head><p>To demonstrate the use of this approach beyond translation, we next consider two morphological   <ref type="bibr" target="#b6">Durrett and DeNero, 2013)</ref>'s dataset containing 8 inflection forms for German Nouns (de-N) and 27 forms for German Verbs (de-V). The number of training words is 2364 and 1627 respectively while the validation and test words are 200 each. We train a one layer encoder and decoder with 128 hidden LSTM units each with a dropout rate of 0.2 using Adam( <ref type="bibr" target="#b7">Kingma and Ba, 2014</ref>) and measure 0/1 accuracy for soft, hard and full-joint attention models. Due to limited input length and vocabulary, we were able to run directly the full-joint model. We also ran the 100 units wide two layer LSTM with hard-monotonic atten- tion provided by <ref type="bibr" target="#b0">(Aharoni and Goldberg, 2017)</ref> labeled Hard-Mono 4 . The table below shows that even for this task full-joint scores over existing at- tention models <ref type="bibr">5</ref> . The generic full-joint attention provides slight gains even over the task specific hard-monotonic attention. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Soft</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper we showed a simple yet effective ap- proximation of the joint attention-output distribu- tion in sequence to sequence learning. Our joint model consistently provides higher accuracy with- out significant running time overheads in five trans- lation and two morphological inflection tasks. An interesting direction for future work is to extend beam-joint to multi-head attention architectures as in ( <ref type="bibr">Vaswani et al., 2017;</ref><ref type="bibr">Xu Chen, 2018</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Test BLEU in various settings (Beam=1). Best viewed in color</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Comparing soft attention with Beam-</head><label>2</label><figDesc></figDesc><table>Joint 
</table></figure>

			<note place="foot" n="1"> Note, attention on a single input encoder state does not imply attention on a single input token because RNNs or selfattention capture the context around the token.</note>

			<note place="foot" n="2"> https://github.com/sid7954/beam-joint-attention 3 https://github.com/tensorflow/nmt</note>

			<note place="foot" n="4"> https://github.com/roeeaharoni/morphologicalreinflection 5 Our numbers are lower than earlier reported because ours use a single model whereas (Aharoni and Goldberg, 2017) and others report from an ensemble of five models.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Morphological inflection generation with hard monotonic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2004" to="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The iwslt 2015 evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roldano</forename><surname>Cattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWSLT 2015, International Workshop on Spoken Language Translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Report on the 11th iwslt evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>iwslt</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improved neural machine translation with a syntax-aware encoder and decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Syntax-directed attention for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<idno>abs/1711.04231</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Supervised learning of complete morphological paradigms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter</title>
		<meeting>the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<idno>abs/1709.07809</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Coarse-tofine attention models for document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on New Frontiers in Summarization, NFiS at EMNLP 2017</title>
		<meeting>the Workshop on New Frontiers in Summarization, NFiS at EMNLP 2017<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From softmax to sparsemax: A sparse model of attention and multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramón Fernández</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Astudillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Aspec: Asian scientific paper excerpt corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Yaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyotaka</forename><surname>Uchimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A regularized framework for sparse and structured neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Linguistic input features improve neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Labeled memory networks for online model adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
