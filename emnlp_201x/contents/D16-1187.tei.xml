<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deceptive Review Spam Detection via Exploiting Task Relatedness and Unlabeled Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Hai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
							<email>peilin.zpl@alipay.com §</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cheng</surname></persName>
							<email>pcheng1@ntu.edu.sg *</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Li</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangxia</forename><surname>Li</surname></persName>
							<email>gxli@xidian.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">¶</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Infocomm Research</orgName>
								<orgName type="department" key="dep2">A*STAR</orgName>
								<orgName type="institution">SCSE</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country>Singapore, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab, Shenzhen</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country>Singapore, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">SCST</orgName>
								<orgName type="institution" key="instit2">Xidian University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deceptive Review Spam Detection via Exploiting Task Relatedness and Unlabeled Data</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1817" to="1826"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Existing work on detecting deceptive reviews primarily focuses on feature engineering and applies off-the-shelf supervised classification algorithms to the problem. Then, one real challenge would be to manually recognize plentiful ground truth spam review data for model building, which is rather difficult and often requires domain expertise in practice. In this paper, we propose to exploit the related-ness of multiple review spam detection tasks and readily available unlabeled data to address the scarcity of labeled opinion spam data. We first develop a multi-task learning method based on logistic regression (MTL-LR), which can boost the learning for a task by sharing the knowledge contained in the training signals of other related tasks. To leverage the unlabeled data, we introduce a graph Lapla-cian regularizer into each base model. We then propose a novel semi-supervised multi-task learning method via Laplacian regular-ized logistic regression (SMTL-LLR) to further improve the review spam detection performance. We also develop a stochastic alternating method to cope with the optimization for SMTL-LLR. Experimental results on real-world review data demonstrate the benefit of SMTL-LLR over several well-established baseline methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nowadays, more and more individuals and organi- zations have become accustomed to consulting user- generated reviews before making purchases or on- line bookings. Considering great commercial ben- efits, merchants, however, have tried to hire peo- ple to write undeserving positive reviews to promote their own products or services, and meanwhile to post malicious negative reviews to defame those of their competitors. The fictitious reviews and opin- ions, which are deliberately created in order to pro- mote or demote targeted entities, are known as de- ceptive opinion spam (Jindal and <ref type="bibr" target="#b11">Liu, 2008;</ref><ref type="bibr" target="#b22">Ott et al., 2011</ref>).</p><p>By formulating deceptive opinion spam detection as a classification problem, existing work primarily focuses on extracting different types of features and applies off-the-shelf supervised classification algo- rithms to the problem ( <ref type="bibr" target="#b11">Jindal and Liu, 2008;</ref><ref type="bibr" target="#b22">Ott et al., 2011;</ref><ref type="bibr" target="#b8">Feng et al., 2012;</ref><ref type="bibr" target="#b5">Chen and Chen, 2015)</ref>. Then, one weakness of previous work lies in the de- mand of manually recognizing a large amount of ground truth review spam data for model training. Unlike other forms of spamming activities, such as email or web spam, deceptive opinion spam, which has been deliberately written to sound authentic, is more difficult to be recognized by manual read. In an experiment, three undergraduate students were (randomly) invited to identify spam reviews from nonspam ones in hotel domain. As shown in <ref type="table" target="#tab_0">Table 1</ref>, their average accuracy is merely 57.3% <ref type="bibr" target="#b22">(Ott et al., 2011)</ref>. Then, given a limited set of labeled review data for a domain, e.g., hotel, it is almost impossible to build a robust classification model for detecting deceptive spam reviews in reality.</p><p>In this work, we deal with the problem of de- tecting a textual review as spam or not, i.e., non- spam. We consider each deceptive review spam de- tection problem within each domain, e.g., detecting</p><p>Judge-1 Judge-2 Judge-3 Accuracy 61.9% 56.9% 53.1% F-spam 48.7% 30.3% 43.6% F-nonspam 69.7%</p><p>68.8% 59.9% spam hotel/restuarnt reviews from hotel/restaurnat domain, to be a different task. Previous studies have empirically shown that learning multiple re- lated tasks simultaneously can significantly improve performance relative to learning each task indepen- dently, especially when only a few labeled data per task are available <ref type="bibr" target="#b4">(Caruana, 1997;</ref><ref type="bibr" target="#b1">Bakker and Heskes, 2003;</ref><ref type="bibr" target="#b0">Argyriou et al., 2006</ref>). Thus, given the limited labeled review data for each domain, we for- mulate the review spam detection tasks for multi- ple domains, e.g., hotel, restaurant, and so on, as a multi-task learning problem. We develop a multi-task learning method via lo- gistic regression (MTL-LR) to address the problem.</p><p>One key advantage of the method is that it allows to boost the learning for one review spam detection task by leveraging the knowledge contained in the training signals of other related tasks. Then, there is often a large quantity of review data freely avail- able online. In order to leverage the unlabeled data, we introduce a graph Laplacian regularizer into each base logistic regression model. We extend MTL- LR, and propose a novel semi-supervised multi-task learning model via Laplacian regularized logistic re- gression (SMTL-LLR) to further boost the review spam detection performance under the multi-task learning setting. Moreover, to cope with the opti- mization problem for SMTL-LLR, we also develop a stochastic alternating optimization method, which is computationally efficient.</p><p>To the best of our knowledge, this is the first work that generalizes opinion spam detection from in- dependent single-task learning to symmetric multi- task learning setting. By symmetric, we mean that the setting seeks to improve the performance of all learning tasks simultaneously. In this sense, it is dif- ferent from transfer learning <ref type="bibr" target="#b24">(Pan and Yang, 2010)</ref>, where the objective is to improve the performance of a target task using information from source tasks.</p><p>Under this new setting, we can exploit the com- monality shared by related review spam detection tasks as well as readily available unlabeled data, and then alleviate the scarcity of labeled spam review data. Experimental results on real-world review data demonstrate the superiority of SMTL-LLR over sev- eral representative baseline methods.</p><p>The rest of this paper is organized as follows. Sec- tion 2 presents related work. Section 3 introduces the proposed methods and stochastic alternating op- timization algorithm. Then, in Section 4, we present the experimental results in detail, and conclude this paper in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Previous work typically formulates deceptive opin- ion spam detection as a classification problem, and then presents different types of features to train su- pervised classification algorithms for the problem. <ref type="bibr" target="#b11">Jindal and Liu (2008)</ref> first studied opinion spam de- tection problem. They built the ground truth review data set by treating the duplicate reviews in a given corpus as spam reviews and the rest as nonspam re- views. They presented review, product, and reviewer related features, and then trained logistic regression (LR) model on the features for finding fake review spam. <ref type="bibr" target="#b22">Ott et al. (2011)</ref> created the ground truth re- view data via a crowd-sourcing service called Ama- zon Mechanical Turk 1 . They presented three dif- ferent types of features for opinion spam detection, i.e., genre identification features, psycholinguistic deception features, and standard n-gram text fea- tures. They found that the supervised support vec- tor machines (SVM) trained on the textual n-gram features can achieve good performance. <ref type="bibr" target="#b8">Feng et al. (2012)</ref> presented syntactic stylometry features and trained SVM model for deception detection, while <ref type="bibr" target="#b5">Chen and Chen (2015)</ref> built the SVM classifier on a diversity of features, such as content and thread features, for opinion spam detection in web forum. In addition, <ref type="bibr" target="#b14">Li et al. (2014)</ref> employed a feature- based additive model to explore the general rule for deceptive opinion spam detection. Generally, in or- der to build robust supervised review spam detection models, we have to manually recognize large-scale ground truth spam data. But this could be very ex-pensive, and often requires domain expertise.</p><p>Though a large amount of unlabeled review data are freely available online, very limited work has been done on developing semi-supervised methods for review spam detection. <ref type="bibr" target="#b13">Li et al. (2011)</ref> used a two-view co-training method <ref type="bibr" target="#b2">(Blum and Mitchell, 1998</ref>) for semi-supervised learning to identify fake review spam. One limitation of the work is that it needs additional reviewer information when build- ing model. Given a corpus of textual reviews, the reviewer related view may not be always available in reality. Moreover, the co-training method is not intrinsically geared to learning from the unlabeled review data, instead, simply makes use of the un- labeled reviews within a fully supervised learning framework, negating the semi-supervised learning benefit. For some particular scenarios, the available training data could be only a partially labeled set of positive examples, e.g., spam reviews, and a large set of unlabeled reviews. Positive unlabeled learn- ing (PU) <ref type="bibr" target="#b6">(De Comite et al., 1999;</ref><ref type="bibr" target="#b16">Liu et al., 2002</ref>) may be then used for deceptive review spam detec- tion ( <ref type="bibr" target="#b9">Hernandez et al., 2013</ref>). However, this clearly contrasts with our problem, where our training data contains a complete labeled set of positive (spam) and negative (nonspam) reviews besides the unla- beled set of review data.</p><p>In addition, instead of detecting spam reviews di- rectly, considerable efforts have been made to recog- nize review spammers, i.e., online users who have written spam reviews. <ref type="bibr" target="#b15">Lim et al. (2010)</ref> studied different types of spamming behavioral indicators, and then used a regression method on the indicators for finding review spammers. <ref type="bibr" target="#b26">Wang et al. (2012)</ref> investigated the relationships among reviewers, re- views, and stores, and developed a social review graph based method to identify online store spam- mers. <ref type="bibr">Mukherjee et al. (2013)</ref> developed an author spamicity Bayesian model to exploit the observed behavioral footprints for spammer detection. In re- ality, a group of online users may work together to create spam reviews. <ref type="bibr" target="#b20">Mukherjee et al. (2012)</ref> de- veloped a group spam ranking algorithm to detect spammer groups.</p><p>Multi-task learning is a learning paradigm that seeks to boost generalization performance by learn- ing a task together with other tasks at the same time while using a shared representation <ref type="bibr" target="#b4">(Caruana, 1997</ref>).</p><p>Most majority of existing work on multi-task learn- ing does not infer actual task relations from train- ing data automatically, instead, they typically make the assumptions that the relations are existent or are given as prior knowledge <ref type="bibr" target="#b25">(Thrun and O'Sullivan, 1996;</ref><ref type="bibr" target="#b1">Bakker and Heskes, 2003;</ref><ref type="bibr" target="#b7">Evgeniou and Pontil, 2004;</ref><ref type="bibr" target="#b0">Argyriou et al., 2006</ref>; <ref type="bibr" target="#b17">Liu et al., 2009)</ref>. To better fit the multi-task learning model to real- world data, <ref type="bibr" target="#b27">Zhang and Yeung (2010)</ref> proposed a convex regularization formulation named multi-task relation learning (MTRL), which can learn real rela- tionships between tasks under a multi-task learning framework.</p><p>In this work, we focus on detecting online decep- tive review spam. We formulate review spam detec- tion for multiple domains (e.g., hotel and restaurant) as a multi-task learning problem. Following the con- vex framework of MTRL, we first develop a multi- task learning method via logistic regression (MTL- LR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We employ logistic regression as base classifi- cation model, because: 1) It is a robust model that does not have configuration parameters to tune; 2) It can be straightforwardly extended, and be efficiently trained using convex optimization techniques (Hoi et al., 2006; Minka, 2003); and 3) It has been shown</head><p>effective for large-scale text classification and fake review detection problems ( <ref type="bibr" target="#b10">Hoi et al., 2006;</ref><ref type="bibr" target="#b11">Jindal and Liu, 2008)</ref>. Then, to leverage the large volume of unlabeled review data, we extend the base logis- tic regression model, and incorporate a graph Lapla- cian regularizer into it. We thus develop a new semi- supervised multi-task learning paradigm via Lapla- cian regularized logistic regression, which is able to further boost the performance for review spam de- tection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-task Learning via Logistic Regression</head><p>Given m review domains D 1 , . . . , D m , we ac- cordingly have m review spam detection tasks T 1 , . . . , T m , which share a common feature space with d dimensions. For the task T i in the domain D i , there is a small labeled set of l i review examples</p><formula xml:id="formula_0">L i = {(x i 1 , y i 1 ), . . . , (x i l i , y i l i</formula><p>)}, where x i j ∈ R d is the vectorial representation of the review j in the la- beled set L i , and y i j ∈ {+1, −1} refers to the spam (+1) or nonspam (−1) label of the review. When there is only one review spam detection task, for example, T i , we can use logistic regression (LR) model to learn a supervised classifier based on the labeled set L i . The objective function of LR for single-task learning is</p><formula xml:id="formula_1">P i LR (w i ) = 1 l i l i ∑ j=1 ln(1 + exp(−y i j w ⊤ i x i j )) + λ 2 ∥w i ∥ 2 ,</formula><p>where w i ∈ R d , λ &gt; 0 refers to regularization pa- rameter.</p><p>Once the model is learned from solving the opti- mization problem, given a test review instance x j ′ of the task T i , we can employ the model to predict it as spam, i.e., ˆ y j ′ = 1, with probability</p><formula xml:id="formula_2">P rob(ˆ y j ′ = 1) = 1 1 + exp(−w ⊤ i x i j ′ )</formula><p>. Now we have m review spam detection tasks for multiple domains, and we would learn m supervised classification models simultaneously. To achieve this, we introduce a covariance matrix Ω to represent the correlations among the m review spam detection tasks, where Ω ij refers to the relation/covariance be- tween a pair of tasks T i and T j . Since Ω is a task covariance matrix, we require it to satisfy the con- straint Ω ≽ 0, i.e., positive semidefinite. We also re- strict T r(Ω) = 1 without of loss of generality, since for any covariance matrix T r(Σ) ̸ = 1, we can use Σ T r(Σ) as Ω. If the covariance matrix is given as prior knowledge, then we introduce a supervised multi- task learning (MTL) framework via logistic regres- sion as follows</p><formula xml:id="formula_3">P Ω M T L (W) = m ∑ i=1 1 l i l i ∑ j=1 ln(1 + exp(−y i j w T i x i j )) + λ 2 T r(WW T ) + β 2 T r(WΩ −1 W T ),</formula><p>where W = (w 1 , . . . , w m ), and β &gt; 0 is a regular- ization parameter. Under this multi-task learning setting, the first term refers to the sum of all the average empirical loss, the second term refers to the regularizer used to avoid over-fitting, and the last term is introduced to leverage the shared knowledge from multiple learn- ing tasks according to their relationships.</p><p>In reality, the covariance matrix may be not pro- vided a priori. We then present the following multi- task learning model, which can learn the model pa- rameters W and Ω automatically from training re- view data</p><formula xml:id="formula_4">P M T L (W, Ω) = m ∑ i=1 1 l i l i ∑ j=1 ln(1 + exp(−y i j w T i x i j )) + λ 2 T r(WW T ) + β 2 T r(WΩ −1 W T ) s.t. Ω ≽ 0, T r(Ω) = 1,</formula><p>If we have only one review spam detection task, i.e., m = 1, then it is straightforward to verify that the above multi-task learning formulation would be re- duced to single-task objective function of logistic re- gression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semi-supervised Multi-task Learning via Laplacian Regularized Logistic Regression</head><p>Generally, for a given review domain D i , there is a large set of unlabeled reviews U i = {x i l i +1 , . . . , x i n i } in addition to the labeled review set L i . Then, for each review spam detection task T i , we constracut a weighted neighborhood graph G i = (V i , E i ) based on both labeled and unlabeled review sets L i and U i . V refers to the set of data points, each of which stands for a review example x i j (j : 1, . . . , n i ) from either L i or U i . E refers to the set of weighted edges. Specifically, if a review example/point x i j is among the K nearest neighbors of the review point x i k , we put an edge linking the two examples, and vice versa. We also assign an ad- jacent weight score s i jk to the edge, which represents the similarity or closeness between the two reviews. Once the neighborhood graph G i has been built for each task, a Laplacian regularizer can be then con- strcted on the graph to extend the regular logistic re- gression model. Considering the similarity matrix S i that corre- sponds to the graph G i for the task T i , it is expected that a good model would also minimize the follow-ing objective</p><formula xml:id="formula_5">∑ jk s i jk (w ⊤ i x i j − w ⊤ i x i k ) 2 ,</formula><p>This objective implies that w ⊤ i x i j should be close to w ⊤ i x i k if the similarity s i jk is large. The objective can be simplified as</p><formula xml:id="formula_6">∑ jk s i jk (w ⊤ i x i j − w ⊤ i x i k ) 2 = Tr(w ⊤ i X i (D i − S i )X ⊤ i w i ) = Tr(w ⊤ i X i L i X ⊤ i w i ),</formula><p>where</p><formula xml:id="formula_7">D i = diag(D i jj ) is a diagonal matrix, D i jj = ∑ k s i jk</formula><p>, and L i = D i − S i refers to the graph Lapla- cian matrix.</p><p>Then, given both labeled review set L i and un- labeled set U i for the task T i , we extend the ba- sic logistic regression by incorporating the graph Laplacian regularizer into its learning framework, and develop a new semi-supervised Laplacian regu- larized logistic regression (LLR) model. The objec- tive function of LLR for semi-supervised single-task learning is given below</p><formula xml:id="formula_8">P i LLR (w i ) = 1 l i l i ∑ j=1 ln(1 + exp(−y i j w ⊤ i x i j )) + λ 2 ∥w i ∥ 2 + γ 2 Tr(w ⊤ i X i L i X ⊤ i w i ),</formula><p>where λ &gt; 0 and γ &gt; 0 are regularization parame- ters.</p><p>The semi-supervised formulation of LLR bal- ances several desires. The first term is used to min- imize the loss of the model on the labeled review data, the second term is used to minimize the com- plexity of the model, and the last term refers to the Laplacian regularizer, which is introduced to make the prediction of the model smooth on the whole re- view data set.</p><p>Next, based on the objective function of the above LLR model, we extend the supervised multi-task learning framework, and propose a novel semi- supervised multi-task learning paradigm via Lapla- cian regularized logistic regression (SMTL-LLR) as follows</p><formula xml:id="formula_9">P SM T L (W, Ω) = m ∑ i=1 1 l i l i ∑ j=1 ln(1 + exp(−y i j w T i x i j )) + λ 2 T r(WW ⊤ ) + β 2 T r(WΩ −1 W ⊤ ) + γ 2 m ∑ i=1 1 n i T r(w T i X i L i X T i w i ) s.t., Ω ≽ 0, T r(Ω) = 1.</formula><p>Under this new semi-supervised unified frame- work, our proposed SMTL-LLR model can lever- age the large amount of unlabeled review data in ad- dition to the labeled ones to learn multiple review spam detection models simultaneously, and then, what is learned for one task can help other related tasks be learned better. In contrast, previous single- task learning based review spam detection models, which are trained independently, and are typically built on a limited set of labeled review data, cannot benefit from this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Stochastic Alternating Method</head><p>There are two parameters W and Ω in the objective function of the proposed SMTL-LLR model. It is not easy to optimize the objective function against the two parameters at the same time. We then de- velop a stochastic alternating method to cope with the optimization problem for SMTL-LLR, i.e., alter- natively updating one parameter by fixing the other. In particular, we initialize W with the values ran- domly chosen from <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>, and initialize Ω as a di- agonal matrix, where Ω ii = 1 m . For each iteration, the key update steps for the two parameters are de- scribed as follows</p><formula xml:id="formula_10">• Step 1: Update W while Ω is fixed. W ← argmin W P SM T L (W, Ω)</formula><p>• Step 2: Update Ω while W is fixed.</p><formula xml:id="formula_11">Ω ← argmin Ω P SM T L (W, Ω)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Updating W While Fixing Ω</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For</head><p>Step 1 of the alternating optimization method, we introduce a stochastic gradient descent method to efficiently update the parameter W, while Ω is fixed. Formally, given a learning task T i , we ran- domly choose a subset or mini-batch of reviews A i b = {(x i j , y i j )|j ∈ [l i ]} from the labeled set L i in a particular iteration, where [l i ] denotes {1, . . . , l i } and |A i b | = r ≪ l i . Based on the subset of labeled reviews A i b , we can construct an unbiased estimate of the objective function</p><formula xml:id="formula_12">P SM T L (W, Ω, {A i b } m i=1 ) = m ∑ i=1 1 r ∑ j∈A i b ln(1 + exp(−y i j w T i x i j )) + λ 2 T r(WW T ) + β 2 T r(WΩ −1 W T ) + γ 2 m ∑ i=1 1 n i T r(w T i X i L i X T i w i )</formula><p>We can then obtain an unbiased stochastic gradi- ent of the objective</p><formula xml:id="formula_13">∇ W P SM T L (W, Ω, {A i b } m i=1 ) = [g 1 b , . . . , g m b ] + λW + βWΩ −1 +[γ 1 n 1 X 1 L 1 X T 1 w 1 , . . . , γ 1 n m X m L m X T m w m ],</formula><p>where</p><formula xml:id="formula_14">g i b = 1 r ∑ j∈A i b −y i j x i j 1 + exp(y i j w T i x i j )</formula><p>. Next, the model parameter W can be updated via stochastic gradient descent method</p><formula xml:id="formula_15">W t+ 1 2 = W t − η t ∇ W P SM T L (W, Ω, {A i b } m i=1 )</formula><p>where η t &gt; 0 refers to learning rate in iteration t.</p><p>Note that, after each update step for the parame- ter W, we perform a scaling process by forcing the solution</p><formula xml:id="formula_16">∥W t+1 ∥ F ≤ √ 2m ln(2)/λ,</formula><p>and then have the following update rule</p><formula xml:id="formula_17">W t+1 = min(1, √ 2m ln(2)/λ ∥W t+ 1 2 ∥ F )W t+ 1 2 .</formula><p>We provide a straightforward theoretical analysis, which shows an upper bound of the norm of the op- tima solution W * , and explains why we perform the above scaling step. Using the fact that</p><formula xml:id="formula_18">P SM T L (W * ) ≤ P SM T L (0), we thus have λ 2 ∥W * ∥ 2 F ≤ P SM T L (W * ) ≤ P SM T L (0) = m ln(2).</formula><p>The fist inequality is guaranteed by</p><formula xml:id="formula_19">ln(1 + exp(−y i j w ⊤ i x i j )) &gt; 0, T r(WΩ −1 W ⊤ ) ≥ 0,</formula><p>and</p><formula xml:id="formula_20">Tr(w ⊤ i X i L i X ⊤ i w i ) ≥ 0. 3.3.2 Updating Ω While Fixing W</formula><p>The second step of the stochastic alternating method is equivalent to solving the following opti- mization problem</p><formula xml:id="formula_21">min Ω T r(W Ω −1 W ⊤ ) s.t., Ω ≽ 0, T r(Ω) = 1.</formula><p>This convex formulation enjoys the following closed-form solution <ref type="bibr" target="#b27">(Zhang and Yeung, 2010</ref>)</p><formula xml:id="formula_22">Ω = (W ⊤ W) 1 2 T r((W ⊤ W) 1 2 )</formula><p>.</p><p>It is obviously observed that Ω models the correla- tions between each pair of the tasks or the models. Algorithm 1 summarizes the stochastic alternat- ing optimization method for SMTL-LLR. Given la- beled and unlabeled review data for multiple review domains, we run the algorithm for P alternating loops. Within each loop p, we update the model pa- rameter W for T iterations via stochastic gradient descent method, where B is number of mini-batches; after that, we update the task covariance matrix Ω once based on new W. The procedure is performed iteratively until it is converged. Then, multiple op- timized review spam detection models and task co- variance matrix would be learned finally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Stochastic Alternating Method</head><p>Input: Labeled and unlabeled review data for multiple tasks Initial learning rate η 0 , hyper-parameter δ Regularization parameters λ, β, γ Initialization: Initialize W with values randomly chosen from <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> Initialize Ω = diag(1/m, . . . , 1/m) for p = 1, . . . , P do W 1 = W for t = 1, . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. , T do</head><p>Learning rate η t = η0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1+η0δt</head><p>Randomly shuffle reviews in the training set</p><formula xml:id="formula_23">for b = 1, . . . , B do Compute ∇ W P SM T L (W, Ω, {A i b } m i=1 ) Update W t+ 1 2 = W t −η t ∇ W P SM T L (W, Ω, {A i b } m i=1 ) W t+1 = min(1, √ 2m ln(2)/λ ∥ W t+ 1 2 ∥F ) W t+ 1 2 end for end for Update W = W T +1 Update Ω = (W ⊤ W) 1 2 T r((W ⊤ W) 1 2 )</formula><p>end for Output: W and Ω In addition, we also rely on the stochastic alter- nating method to optimize the proposed MTL-LR method. Differently, we need to remove all the terms related to unlabeled data, i.e., discarding the Lapla- cian regularization term from the objective function and gradient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate the proposed multi- task learning methods MTL-LR and SMTL-LLR for review spam detection, and demonstrate the im- proved effectiveness of the methods over other well- established baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Sets</head><p>Due to big challenge in manually recognizing de- ceptive reviews, there are limited benchmark opin- ion spam data in this field. We used three ground truth data sets from the review domains, doctor 2 , 2 https://www.ratemds.com hotel 3 , and restaurant 4 , respectively, to evaluate the proposed methods, which were created by following the similar rules used in <ref type="bibr" target="#b22">(Ott et al., 2011</ref>). Then, for each ground truth review data set, we randomly col- lected a large number of unlabeled reviews (10,000), which were written about the same entities or do- main. <ref type="table">Table 2</ref> shows some data statistics, where the last column computes the ratio of labeled reviews to unlabeled ones.</p><note type="other">Spam/Nonspam Unlabeled Ratio Doctor 200/200 10,000 4.0% Hotel 300/300 10,000 6.0% Restaurant 200/200</note><p>10,000 4.0% <ref type="table">Table 2</ref>: Some statistics of review data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>We followed previous work <ref type="bibr" target="#b18">(Mihalcea and Strapparava, 2009;</ref><ref type="bibr" target="#b22">Ott et al., 2011)</ref>, and leveraged text un- igram and bigram term-frequency features to train our models for review spam detection. This problem setting is quite useful, for example, when user be- havior data are sparse or even not available in prac- tical applications. Supervised classification models, such as logis- tic regression (LR) and support vector machines (SVM), have been used to identify fake review spam ( <ref type="bibr" target="#b11">Jindal and Liu, 2008;</ref><ref type="bibr" target="#b22">Ott et al., 2011</ref>). We compared our methods with the two models. Semi- supervised positive-unlabeled (PU) learning was employed for review spam detection, then we chose one representative PU learning method ( <ref type="bibr" target="#b16">Liu et al., 2002</ref>) to evaluate our models. We did not compare our methods with the two-view co-training method, which was used for fake review detection <ref type="bibr" target="#b13">(Li et al., 2011</ref>), because the reviewer view data are not available in the ground truth review sets. Instead, we selected a well-known semi-supervised trans- ductive SVM (TSVM) <ref type="bibr" target="#b12">(Joachims, 1999</ref>) to evaluate our models. Different from the proposed methods, we trained each of above baselines in a single do- main, because they are single-task learning methods. Moreover, we also compared our methods with one well-established multi-task learning baseline MTRL ( <ref type="bibr" target="#b27">Zhang and Yeung, 2010)</ref>, which has not been used for review spam detection problem.</p><p>It is important to specify appropriate values for the parameters in the proposed methods. In our setting, we used the learning rates η t that asymp- totically decrease with iteration numbers <ref type="bibr">(Bottou, 2012)</ref>. Following previous work <ref type="bibr" target="#b22">(Ott et al., 2011;</ref><ref type="bibr" target="#b5">Chen and Chen, 2015)</ref>, we conducted five-fold cross-validation experiments, and determined the values of the regularization and hyper parameters via a grid-search method. <ref type="table" target="#tab_2">Table 3</ref> reports the spam and nonspam review detec- tion accuracy of our methods SMTL-LLR and MTL- LR against all other baseline methods. In terms of 5% significance level, the differences between SMTL-LLR and the baseline methods are consid- ered to be statistically significant.  Under symmetric multi-task learning setting, our methods SMTL-LLR and MTL-LR outperform all other baselines for identifying spam reviews from nonspam ones. MTL-LR achieves the average ac- curacy of 85.2% across the three domains, which is 3.1% and 3.4% better than LR and SVM trained in the single task learning setting, and 1.2% higher than MTRL. Training with a large quantity of unlabeled review data in addition to labeled ones, SMTL-LLR improves the performance of MTL-LR, and achieves the best average accuracy of 87.2% across the do- mains, which is 3.2% better than that of MTRL, and is 4.3% better than TSVM, a semi-supervised sin- gle task learning model. PU gives the worst perfor- mance, because learning only with partially labeled positive review data (spam) and unlabeled data may not generalize as well as other methods. <ref type="figure" target="#fig_0">Figure 1</ref> plots SMTL-LLR accuracy versus unla- beled data sizes from 0 to 10,000, where 0 corre- sponds to using only labeled data to build the model, i.e., MTL-LR. Note that we first randomly sampled 2,000 unlabeled reviews to build the first set, and then created the second set by appending another randomly selected set of 2,000 reviews to the pre- vious one. We repeated the process until all the un- labeled review data sets were created. We observed that learning from unlabeled reviews does help to boost the performance of MTL-LR, which was trained with labeled data alone. The performance of SMTL-LLR improves when training with more and more unlabeled review data. This is because the useful patterns learned from unlabeled data perhaps supports SMTL-LLR to generalize bet- ter. But continuing to learn from much more unla- beled reviews may even harm the performance. One explanation is that appending more unlabeled data may also incur noisy information to learning pro- cess. Interestingly, the performance of SMTL-LLR keeps increasing on the doctor domain, when train- ing with more and more unlabeled reviews up to 10,000. From above observations, we conclude that an elaborately selected set of high-quality unlabeled review data may help SMTL-LLR to learn better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance versus Unlabeled Data Size</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Task Correlation</head><p>Based on the covariance matrix (Ω) learned from the review spam detection tasks, we obtained the corre- lation between each pair of tasks for doctor, hotel, and restaurant domains, as shown in <ref type="table" target="#tab_3">Table 4</ref>. The re- view spam detection tasks are highly correlated with each other for hotel and restaurant domains (0.772). This is reasonable due to the large amount of com- monality shared between the two domains. We can see that the tasks are also positively correlated be- tween hotel and doctor, as well as between doctor and restaurant domains.   <ref type="table" target="#tab_4">Table 5</ref> lists top weighted shared text features among the review spam detection tasks for doctor, hotel, and restaurant domains. Generally, review spam- mers demonstrate similar motivations when creat- ing deceptive review spam, i.e., promoting their own products/services or defaming those of their com- petitors. Though different aspects or entities can be commented on across different domains, we find that many features or expressions are indeed shared among the three review domains. As we know, deceptive reviewers normally write up reviews for making money, thus they prefer choosing exagger- ated language in their lies, no matter which domains they are working with. As shown in the first row for spam category, they tend to exaggerate their sen- timents using the words like "definitely", "sure", "highly", and so on. In contrast, truthful reviewers contribute reviews for sharing their true feelings or personal anecdotes. They are willing to write up detailed factual expe- riences, for example, about the doctors they visited or delicious foods they enjoyed. Their reviews thus tend to contain language patterns in past tense, such as "went", "did", and "took" shown in the second row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Shared Text Features among Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have coped with the problem of detecting de- ceptive review spam. Given the limited labeled re- view data for individual domains, we formulated it as a multi-task learning problem. We first de- veloped a multi-task learning method via logistic regression (MTL-LR), which allows to boost the Labels Features Spam staff, friendly,comfortable, really, right, experience, best, way, amazing, check, away, staff friendly, definitely, sure, highly recommend Nonspam good, just, like, went, did, people, excellent, took, wonderful, things, day, fantastic, know, going, nice learning for one task by sharing the knowledge con- tained in the training signals of other related tasks.</p><p>To leverage the unlabeled data, we introduced a graph Laplacian regularizer into each base model, and proposed a semi-supervised multi-task learning model via Laplacian regularized logistic regression (SMTL-LLR). Moreover, to deal with the optimiza- tion problem, we developed a stochastic alternating method. Experimental results on real-world review data demonstrated the superiority of SMTL-LLR over several well-established baseline methods.</p><p>For future work, we plan to create much more ground truth review data from other review domains and different applications like forums or microblogs, and further test our proposed models for deceptive opinion spam detection. We also plan to incorporate our model into a practical opinion mining system, in this way, more reliable opinion and sentiment anal- ysis results can be then expected.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Accuracy versus Unlabeled Data Size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Performance of human judges for review spam 
detection in hotel domain (Ott et al., 2011), where F-
spam/F-nonspam means F-score for spam/nonspam label. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Spam and nonspam review detection results in 
the doctor, hotel, and restaurant review domains. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Task correlations. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Top weighted shared text features for 
spam/nonspam category across the three review domains. 

</table></figure>

			<note place="foot" n="1"> https://www.mturk.com</note>

			<note place="foot" n="3"> https://www.tripadvisor.com 4 http://www.yelp.com</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-task feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodoros</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems</title>
		<meeting>the Twentieth Annual Conference on Neural Information Processing Systems<address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Task clustering and gating for bayesian multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="83" to="99" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh annual conference on Computational learning theory</title>
		<meeting>the eleventh annual conference on Computational learning theory</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent tricks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="421" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Opinion spam detection in web forum: A real case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Hsi</forename><surname>Yu-Ren Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web<address><addrLine>Republic and Canton of Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="173" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Positive and Unlabeled Examples Help Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>De Comite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Gilleron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Letouzey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Algorithmic Learning Theory</title>
		<meeting>the Tenth International Conference on Algorithmic Learning Theory<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="219" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Regularized multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodoros</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Syntactic stylometry for deception detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritwik</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="171" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using PU-learning to detect deceptive opinion spam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Montes-Y-Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</title>
		<meeting>the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis<address><addrLine>Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large-scale text categorization by batch mode active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on World Wide Web</title>
		<meeting>the 15th International Conference on World Wide Web<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="633" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Opinion spam and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Web Search and Data Mining</title>
		<meeting>the International Conference on Web Search and Data Mining<address><addrLine>Palo Alto, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="219" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth International Conference on Machine Learning</title>
		<meeting>the Sixteenth International Conference on Machine Learning<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to identify review spam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI Proceedings-International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">2488</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards a general rule for identifying deceptive opinion spam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1566" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detecting product review spammers using rating behaviors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet-An</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hady Wirawan</forename><surname>Lauw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 19th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="939" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Partially supervised classification of text documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wee Sun Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth International Conference on Machine Learning</title>
		<meeting>the Nineteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="387" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-task feature learning via efficient l 2, 1-norm minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Shuiwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence</title>
		<meeting>the twenty-fifth conference on uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="339" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The lie detector: Explorations in the automatic recognition of deceptive language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACLIJCNLP 2009 Conference Short Papers</title>
		<meeting>the ACLIJCNLP 2009 Conference Short Papers<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="309" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A comparison of numerical optimizers for logistic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">P</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CMU Technical Report</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spotting fake reviewer groups in consumer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalie</forename><surname>Glance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on World Wide Web</title>
		<meeting>the 21st International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spotting opinion spammers using behavioral footprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meichun</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malu</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riddhiman</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="632" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Finding deceptive opinion spam by any stretch of the imagination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">T</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th</title>
		<meeting>the 49th</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="309" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Discovering structure in multiple learning tasks: The TC algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>O&amp;apos;sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Machine Learning</title>
		<editor>L. Saitta</editor>
		<meeting>the 13th International Conference on Machine Learning<address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgen Kaufmann</publisher>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Identify online store review spammers via social review graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno>61:1-61:21</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2012-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A convex formulation for learning task relationships in multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence<address><addrLine>Catalina Island, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="733" to="442" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
