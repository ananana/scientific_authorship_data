<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Language as a Latent Variable: Discrete Generative Models for Sentence Compression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Google Deepmind</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Language as a Latent Variable: Discrete Generative Models for Sentence Compression</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="319" to="328"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this work we explore deep generative models of text in which the latent representation of a document is itself drawn from a discrete language model distribution. We formulate a variational auto-encoder for inference in this model and apply it to the task of compressing sentences. In this application the generative model first draws a latent summary sentence from a background language model, and then subsequently draws the observed sentence conditioned on this latent summary. In our empirical evaluation we show that generative formulations of both abstractive and extractive compression yield state-of-the-art results when trained on a large amount of supervised data. Further, we explore semi-supervised compression scenarios where we show that it is possible to achieve performance competitive with previously proposed supervised models while training on a fraction of the supervised data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The recurrent sequence-to-sequence paradigm for natural language generation <ref type="bibr">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr">Sutskever et al., 2014</ref>) has achieved re- markable recent success and is now the approach of choice for applications such as machine transla- tion ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>, caption generation ( <ref type="bibr" target="#b11">Xu et al., 2015</ref>) and speech recognition ( <ref type="bibr" target="#b0">Chorowski et al., 2015)</ref>. While these models have developed so- phisticated conditioning mechanisms, e.g. attention, fundamentally they are discriminative models trained only to approximate the conditional output distribu- tion of strings. In this paper we explore modelling the joint distribution of string pairs using a deep genera- tive model and employing a discrete variational auto- encoder (VAE) for inference ( <ref type="bibr" target="#b6">Kingma and Welling, 2014;</ref><ref type="bibr" target="#b6">Rezende et al., 2014;</ref><ref type="bibr" target="#b8">Mnih and Gregor, 2014</ref>). We evaluate our generative approach on the task of sentence compression. This approach provides both alternative supervised objective functions and the opportunity to perform semi-supervised learning by exploiting the VAEs ability to marginalise the latent compressed text for unlabelled data.</p><p>Auto-encoders ( <ref type="bibr" target="#b10">Rumelhart et al., 1985)</ref> are a typi- cal neural network architecture for learning compact data representations, with the general aim of perform- ing dimensionality reduction on embeddings <ref type="bibr" target="#b4">(Hinton and Salakhutdinov, 2006</ref>). In this paper, rather than seeking to embed inputs as points in a vector space, we describe them with explicit natural language sen- tences. This approach is a natural fit for summarisa- tion tasks such as sentence compression. According to this, we propose a generative auto-encoding sen- tence compression (ASC) model, where we intro- duce a latent language model to provide the variable- length compact summary. The objective is to perform Bayesian inference for the posterior distribution of summaries conditioned on the observed utterances. Hence, in the framework of VAE, we construct an in- ference network as the variational approximation of the posterior, which generates compression samples to optimise the variational lower bound.</p><p>The most common family of variational auto- encoders relies on the reparameterisation trick, which is not applicable for our discrete latent language model. Instead, we employ the REINFORCE al- gorithm <ref type="bibr" target="#b8">(Mnih et al., 2014;</ref><ref type="bibr" target="#b8">Mnih and Gregor, 2014)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reconstruction (Soft Attention)</head><p>Figure 1: Auto-encoding Sentence Compression Model to mitigate the problem of high variance during sampling-based variational inference. Nevertheless, when directly applying the RNN encoder-decoder to model the variational distribution it is very difficult to generate reasonable compression samples in the early stages of training, since each hidden state of the se- quence would have |V | possible words to be sampled from. To combat this we employ pointer networks ( <ref type="bibr" target="#b0">Vinyals et al., 2015)</ref> to construct the variational dis- tribution. This biases the latent space to sequences composed of words only appearing in the source sentence (i.e. the size of softmax output for each state becomes the length of current source sentence), which amounts to applying an extractive compression model for the variational approximation.</p><p>In order to further boost the performance on sen- tence compression, we employ a supervised forced- attention sentence compression model (FSC) trained on labelled data to teach the ASC model to generate compression sentences. The FSC model shares the pointer network of the ASC model and combines a softmax output layer over the whole vo- cabulary. Therefore, while training on the sentence- compression pairs, it is able to balance copying a word from the source sentence with generating it from the background distribution. More importantly, by jointly training on the labelled and unlabelled datasets, this shared pointer network enables the model to work in a semi-supervised scenario. In this case, the FSC teaches the ASC to generate rea- sonable samples, while the pointer network trained on a large unlabelled data set helps the FSC model to perform better abstractive summarisation.</p><p>In Section 6, we evaluate the proposed model by jointly training the generative (ASC) and discrimina- tive (FSC) models on the standard Gigaword sentence compression task with varying amounts of labelled and unlabelled data. The results demonstrate that by introducing a latent language variable we are able to match the previous benchmakers with small amount of the supervised data. When we employ our mixed discriminative and generative objective with all of the supervised data the model significantly outperforms all previously published results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Auto-Encoding Sentence Compression</head><p>In this section, we introduce the auto-encoding sen- tence compression model <ref type="figure">(Figure 1</ref>) 1 in the frame- work of variational auto-encoders. The ASC model consists of four recurrent neural networks -an en- coder, a compressor, a decoder and a language model. Let s be the source sentence, and c be the compres- sion sentence. The compression model (encoder- compressor) is the inference network q φ (c|s) that takes source sentences s as inputs and generates extractive compressions c. The reconstruction model (compressor-decoder) is the generative net- work p θ (s|c) that reconstructs source sentences s based on the latent compressions c. Hence, the for- ward pass starts from the encoder to the compressor and ends at the decoder. As the prior distribution, a language model p(c) is pre-trained to regularise the latent compressions so that the samples drawn from the compression model are likely to be reasonable natural language sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Compression</head><p>For the compression model (encoder-compressor), q φ (c|s), we employ a pointer network consisting of a bidirectional LSTM encoder that processes the source sentences, and an LSTM compressor that generates compressed sentences by attending to the encoded source words.</p><p>Let s i be the words in the source sentences, h e i be the corresponding state outputs of the encoder. h e i are the concatenated hidden states from each direction:</p><formula xml:id="formula_0">h e i = f−→ enc ( h e i−1 , s i )||f←− enc ( h e i+1 , s i )<label>(1)</label></formula><p>Further, let c j be the words in the compressed sen- tences, h c j be the state outputs of the compressor. We construct the predictive distribution by attending to the words in the source sentences:</p><formula xml:id="formula_1">h c j =f com (h c j−1 , c j−1 ) (2) u j (i) =w T 3 tanh(W 1 h c j +W 2 h e i ) (3) q φ (c j |c 1:j−1 , s) = softmax(u j )<label>(4)</label></formula><p>where c 0 is the start symbol for each compressed sentence and h c 0 is initialised by the source sentence vector of h e |s| . In this case, all the words c j sampled from q φ (c j |c 1:j−1 , s) are the subset of the words appeared in the source sentence (i.e. c j ∈ s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Reconstruction</head><p>For the reconstruction model (compressor-decoder) p θ (s|c), we apply a soft attention sequence-to- sequence model to generate the source sentence s based on the compression samples c ∼ q φ (c|s).</p><p>Let s k be the words in the reconstructed sentences and h d k be the corresponding state outputs of the decoder:</p><formula xml:id="formula_2">h d k = f dec (h d k−1 , s k−1 )<label>(5)</label></formula><p>In this model, we directly use the recurrent cell of the compressor to encode the compression samples 2 :</p><formula xml:id="formula_3">ˆ h c j =f com ( ˆ h c j−1 , c j )<label>(6)</label></formula><p>where the state outputsˆhoutputsˆ outputsˆh c j corresponding to the word inputs c j are different from the outputs h c j in the compression model, since we block the information from the source sentences. We also introduce a start symbol s 0 for the reconstructed sentence and h d 0 is initialised by the last state outputˆhoutputˆ outputˆh c |c| . The soft attention model is defined as:</p><formula xml:id="formula_4">v k (j) =w T 6 tanh(W 4 h d k + W 5 ˆ h c j ) (7) γ k (j) = softmax(v k (j))<label>(8)</label></formula><formula xml:id="formula_5">d k = |c| j γ k (j) ˆ h c j (v k (j))<label>(9)</label></formula><p>We then construct the predictive probability distribu- tion over reconstructed words using a softmax:</p><formula xml:id="formula_6">p θ (s k |s 1:k−1 , c) = softmax(W 7 d k )<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Inference</head><p>In the ASC model there are two sets of parameters, φ and θ, that need to be updated during inference. Due to the non-differentiability of the model, the repa- rameterisation trick of the VAE is not applicable in this case. Thus, we use the REINFORCE algorithm ( <ref type="bibr" target="#b8">Mnih et al., 2014;</ref><ref type="bibr" target="#b8">Mnih and Gregor, 2014</ref>) to reduce the variance of the gradient estimator. The variational lower bound of the ASC model is:</p><formula xml:id="formula_7">L =E q φ (c|s) [log p θ (s|c)] − D KL [q φ (c|s)||p(c)] log q φ (c|s) q φ (c|s) p θ (s|c)p(c)dc = log p(s) (11)</formula><p>Therefore, by optimising the lower bound (Eq. 11), the model balances the selection of keywords for the summaries and the efficacy of the composed com- pressions, corresponding to the reconstruction error and KL divergence respectively. In practise, the pre-trained language model prior p(c) prefers short sentences for compressions. As one of the drawbacks of VAEs, the KL divergence term in the lower bound pushes every sample drawn</p><formula xml:id="formula_8">s 1 s 2 s 3 s 4 h 1 e c 1 c 2 c 3 c 1 c 2 c 0 h 1 c h 2 e h 3 e h 4 e h 2 c h 3 c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head><p>Compresser</p><formula xml:id="formula_9">h 0 c α β 1 2 3 α α 1 β β 2 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compression (Combined Pointer Networks)</head><p>Selected from V Thus acting to regularise the posterior, but also to restrict the learning of the encoder. If the estimator keeps sampling short compressions during inference, the LSTM decoder would gradually rely on the con- texts from the decoded words instead of the informa- tion provided by the compressions, which does not yield the best performance on sentence compression. Here, we introduce a co-efficient λ to scale the learning signal of the KL divergence:</p><formula xml:id="formula_10">L=E q φ (c|s) [log p θ (s|c)]−λD KL [q φ (c|s)||p(c)] (12)</formula><p>Although we are not optimising the exact variational lower bound, the ultimate goal of learning an effec- tive compression model is mostly up to the recon- struction error. In Section 6, we empirically apply λ = 0.1 for all the experiments on ASC model. In- terestingly, λ controls the compression rate of the sentences which can be a good point to be explored in future work.</p><p>During the inference, we have different strategies for updating the parameters of φ and θ. For the pa- rameters θ in the reconstruction model, we directly update them by the gradients:</p><formula xml:id="formula_11">∂L ∂θ = E q φ (c|s) [ ∂ log p θ (s|c) ∂θ ] ≈ 1 M m ∂ log p θ (s|c (m) ) ∂θ (13)</formula><p>where we draw M samples c (m) ∼ q φ (c|s) indepen- dently for computing the stochastic gradients.</p><p>For the parameters φ in the compression model, we firstly define the learning signal,</p><formula xml:id="formula_12">l(s, c) = log p θ (s|c) − λ(log q φ (c|s) − log p(c)).</formula><p>Then, we update the parameters φ by:</p><formula xml:id="formula_13">∂L ∂φ = E q φ (c|s) [l(s, c) ∂ log q φ (c|s) ∂φ ] ≈ 1 M m [l(s, c (m) ) ∂ log q φ (c (m) |s) ∂φ ]<label>(14)</label></formula><p>However, this gradient estimator has a big variance because the learning signal l(s, c (m) ) relies on the samples from q φ (c|s). Therefore, following the RE- INFORCE algorithm, we introduce two baselines b and b(s), the centred learning signal and input- dependent baseline respectively, to help reduce the variance.</p><p>Here, we build an MLP to implement the input- dependent baseline b(s). During training, we learn the two baselines by minimising the expectation:</p><formula xml:id="formula_14">E q φ (c|s) [(l(s, c) − b − b(s)) 2 ].<label>(15)</label></formula><p>Hence, the gradients w.r.t. φ are derived as,</p><formula xml:id="formula_15">∂L ∂φ ≈ 1 M m (l(s, c (m) )−b−b(s)) ∂ log q φ (c (m) |s) ∂φ<label>(16)</label></formula><p>which is basically a likelihood-ratio estimator.</p><p>high-variance for the gradient estimator. Here, we introduce our supervised forced-attention sentence compression (FSC) model to teach the compression model to generate coherent compressed sentences.</p><p>Neither directly replicating the pointer network of ASC model, nor using a typical sequence-to- sequence model, the FSC model employs a force- attention strategy <ref type="figure" target="#fig_0">(Figure 2</ref>) that encourages the com- pressor to select words appearing in the source sen- tence but keeps the original full output vocabulary V . The force-attention strategy is basically a com- bined pointer network that chooses whether to select a word from the source sentence s or to predict a word from V at each recurrent state. Hence, the combined pointer network learns to copy the source words while predicting the word sequences of com- pressions. By sharing the pointer networks between the ASC and FSC model, the biased estimator obtains further positive biases by training on a small set of labelled source-compression pairs.</p><p>Here, the FSC model makes use of the compres- sion model (Eq. 1 to 4) in the ASC model,</p><formula xml:id="formula_16">α j = softmax(u j ),<label>(17)</label></formula><p>where α j (i), i ∈ (1, . . . , |s|) denotes the probability of selecting s i as the prediction for c j .</p><p>On the basis of the pointer network, we further introduce the probability of predicting c j that is se- lected from the full vocabulary,</p><formula xml:id="formula_17">β j = softmax(W h c j ),<label>(18)</label></formula><p>where β j (w), w ∈ (1, . . . , |V |) denotes the probabil- ity of selecting the wth from V as the prediction for c j . To combine these two probabilities in the RNN, we define a selection factor t for each state output, which computes the semantic similarities between the current state and the attention vector,</p><formula xml:id="formula_18">η j = |s| i α j (i)h e i<label>(19)</label></formula><formula xml:id="formula_19">t j = σ(η T j M h c j ).<label>(20)</label></formula><p>Hence, the probability distribution over compressed words is defined as,</p><formula xml:id="formula_20">p(c j |c 1:j−1 , s)= t j α j (i) + (1 − t j )β j (c j ), c j = s i (1 − t j )β j (c j ), c j ∈ s<label>(21)</label></formula><p>Essentially, the FSC model is the extended compres- sion model of ASC by incorporating the pointer net- work with a softmax output layer over the full vocab- ulary. So we employ φ to denote the parameters of the FSC model p φ (c|s), which covers the parameters of the variational distribution q φ (c|s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Semi-supervised Training</head><p>As the auto-encoding sentence compression (ASC) model grants the ability to make use of an unla- belled dataset, we explore a semi-supervised train- ing framework for the ASC and FSC models. In this scenario we have a labelled dataset that contains source-compression parallel sentences, (s, c) ∈ L,</p><p>and an unlabelled dataset that contains only source sentences s ∈ U. The FSC model is trained on L so that we are able to learn the compression model by maximising the log-probability,</p><formula xml:id="formula_21">F = (c,s)∈L log p φ (c|s).<label>(22)</label></formula><p>While the ASC model is trained on U, where we maximise the modified variational lower bound,</p><formula xml:id="formula_22">L= s∈U (E q φ (c|s) [log p θ (s|c)]−λD KL [q φ (c|s)||p(c)]).<label>(23)</label></formula><p>The joint objective function of the semi-supervised learning is,</p><formula xml:id="formula_23">J= s∈U (E q φ (c|s) [log p θ (s|c)]−λD KL [q φ (c|s)||p(c)]) + (c,s)∈L log p φ (c|s).<label>(24)</label></formula><p>Hence, the pointer network is trained on both un- labelled data, U, and labelled data, L, by a mixed criterion of REINFORCE and cross-entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>As one of the typical sequence-to-sequence tasks, sentence-level summarisation has been explored by a series of discriminative encoder-decoder neural mod- els. <ref type="bibr">Filippova et al. (2015)</ref> carries out extractive summarisation via deletion with LSTMs, while Rush et al. (2015) applies a convolutional encoder and an attentional feed-forward decoder to generate abstrac- tive summarises, which provides the benchmark for the Gigaword dataset. <ref type="bibr" target="#b9">Nallapati et al. (2016)</ref> fur- ther improves the performance by exploring multi- ple variants of RNN encoder-decoder models. The recent works <ref type="bibr" target="#b4">Gulcehre et al. (2016)</ref>, <ref type="bibr" target="#b9">Nallapati et al. (2016)</ref> and <ref type="bibr" target="#b4">Gu et al. (2016)</ref> also apply the similar idea of combining pointer networks and softmax output. However, different from all these discriminative mod- els above, we explore generative models for sentence compression. Instead of training the discriminative model on a big labelled dataset, our original intuition of introducing a combined pointer networks is to bridge the unsupervised generative model (ASC) and supervised model (FSC) so that we could utilise a large additional dataset, either labelled or unlabelled, to boost the compression performance. <ref type="bibr" target="#b1">Dai and Le (2015)</ref> also explored semi-supervised sequence learn- ing, but in a pure deterministic model focused on learning better vector representations.</p><p>Recently variational auto-encoders have been ap- plied in a variety of fields as deep generative mod- els. In computer vision <ref type="bibr" target="#b6">Kingma and Welling (2014)</ref>  <ref type="formula" target="#formula_0">(2016)</ref> proposes a generative model that explicitly extracts syntactic relationships among words and phrases which further supports the argument that generative models can be a statistically efficient method for learning neural networks from small data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Dataset &amp; Setup</head><p>We evaluate the proposed models on the standard Gi- gaword 3 sentence compression dataset. This dataset was generated by pairing the headline of each article with its first sentence to create a source-compression pair. <ref type="bibr" target="#b10">Rush et al. (2015)</ref> provided scripts 4 to filter out outliers, resulting in roughly 3.8M training pairs, a 400K validation set, and a 400K test set. In the following experiments all models are trained on the training set with different data sizes <ref type="bibr">5</ref> and tested on a 2K subset, which is identical to the test set used by <ref type="bibr" target="#b10">Rush et al. (2015)</ref> and <ref type="bibr" target="#b9">Nallapati et al. (2016)</ref>. We decode the sentences by k = 5 Beam search and test with full-length Rouge score.</p><p>For the ASC and FSC models, we use 256 for the dimension of both hidden units and lookup tables. In the ASC model, we apply a 3-layer bidirectional RNN with skip connections as the encoder, a 3-layer RNN pointer network with skip connections as the compressor, and a 1-layer vanilla RNN with soft at- tention as the decoder. The language model prior is trained on the article sentences of the full training set using a 3-layer vanilla RNN with 0.5 dropout. To lower the computational cost, we apply different vo- cabulary sizes for encoder and compressor <ref type="bibr">(119,506 and 68,897</ref>) which corresponds to the settings of <ref type="bibr" target="#b10">Rush et al. (2015)</ref>. Specifically, the vocabulary of the decoder is filtered by taking the most frequent 10,000 words from the vocabulary of the encoder, where the rest of the words are tagged as '&lt;unk&gt;'. In further consideration of efficiency, we use only one sample for the gradient estimator. We optimise the model by <ref type="bibr">Adam (Kingma and Ba, 2015</ref>) with a 0.0002 learning rate and 64 sentences per batch. The model converges in 5 epochs. Except for the pre- trained language model, we do not use dropout or embedding initialisation for ASC and FSC models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Extractive Summarisation</head><p>The first set of experiments evaluate the models on extractive summarisation. Here, we denote the joint  The ASC+FSC 2 model employs the full unlabelled dataset in addition to the existing labelled dataset, which is the true semi-supervised setting. <ref type="table" target="#tab_2">Table 1</ref> presents the test Rouge score on extractive compression. We can see that the ASC+FSC 1 model achieves significant improvements on F-1 scores when compared to the supervised FSC model only trained on labelled data. Moreover, fixing the labelled data size, the ASC+FSC 2 model achieves better per- formance by using additional unlabelled data than the ASC+FSC 1 model, which means the semi-supervised learning works in this scenario. Interestingly, learn- ing on the unlabelled data largely increases the preci- sions (though the recalls do not benefit from it) which leads to significant improvements on the F-1 Rouge scores. And surprisingly, the extractive ASC+FSC 1 model trained on full labelled data outperforms the abstractive NABS ( <ref type="bibr" target="#b10">Rush et al., 2015</ref>) baseline model (in <ref type="table" target="#tab_6">Table 4</ref>).</p><formula xml:id="formula_24">Model Training Data Recall Precision F-1 Labelled Unlabelled R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L FSC</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Abstractive Summarisation</head><p>The second set of experiments evaluate performance on abstractive summarisation <ref type="table" target="#tab_4">(Table 2)</ref>. Consistently, we see that adding the generative objective to the discriminative model (ASC+FSC 1 ) results in a sig- nificant boost on all the Rouge scores, while em- ploying extra unlabelled data increase performance further (ASC+FSC 2 ). This validates the effectiveness of transferring the knowledge learned on unlabelled data to the supervised abstractive summarisation.</p><p>In <ref type="figure" target="#fig_2">Figure 3</ref>, we present the validation perplexity to compare the abilities of the three models to learn the compression languages. The ASC+FSC 1 (red) employs the same dataset for unlabelled and labelled training, while the ASC+FSC 2 (black) employs the full unlabelled dataset. Here, the joint ASC+FSC 1 model obtains better perplexities than the single dis- criminative FSC model, but there is not much dif- ference between ASC+FSC 1 and ASC+FSC 2 when the size of the labelled dataset grows. From the per- spective of language modelling, the generative ASC model indeed helps the discriminative model learn to generate good summary sentences. <ref type="table" target="#tab_5">Table 3</ref> displays the validation perplexities of the benchmark models, where the joint ASC+FSC 1 model trained on the full labelled and unlabelled datasets performs the best on modelling compression languages. <ref type="table" target="#tab_6">Table 4</ref> compares the test Rouge score on ab- stractive summarisation. Encouragingly, the semi- supervised model ASC+FSC 2 outperforms the base- line model NABS when trained on 500K supervised pairs, which is only about an eighth of the super- vised data. In <ref type="bibr" target="#b9">Nallapati et al. (2016)</ref>, the authors exploit the full limits of discriminative RNN encoder- decoder models by incorporating a sampled soft- max, expanded vocabulary, additional lexical fea- tures, and combined pointer networks 6 , which yields the best performance listed in <ref type="table" target="#tab_6">Table 4</ref>. However, when all the data is employed with the mixed ob-    jective ASC+FSC 1 model, the result is significantly better than this previous state-of-the-art. As the semi- supervised ASC+FSC 2 model can be trained on un- limited unlabelled data, there is still significant space left for further performance improvements. <ref type="table">Table 5</ref> presents the examples of the compression sentences decoded by the joint model ASC+FSC 1 and the FSC model trained on the full dataset.</p><formula xml:id="formula_25">Model Training Data Recall Precision F-1 Labelled Unlabelled R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L FSC</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>From the perspective of generative models, a sig- nificant contribution of our work is a process for reducing variance for discrete sampling-based vari- ational inference. The first step is to introduce two baselines in the control variates method due to the fact that the reparameterisation trick is not applica- ble for discrete latent variables. However it is the second step of using a pointer network as the biased estimator that makes the key contribution. This re- sults in a much smaller state space, bounded by the length of the source sentence (mostly between 20 and 50 tokens), compared to the full vocabulary. The final step is to apply the FSC model to transfer the knowledge learned from the supervised data to the pointer network. This further reduces the sampling variance by acting as a sort of bootstrap or constraint on the unsupervised latent space which could encode almost anything but which thus becomes biased to- wards matching the supervised distribution. By using these variance reduction methods, the ASC model is able to carry out effective variational inference for the latent language model so that it learns to summarise the sentences from the large unlabelled training data.</p><p>In a different vein, according to the reinforce- ment learning interpretation of sequence level train- ing ( <ref type="bibr" target="#b9">Ranzato et al., 2016</ref>), the compression model of the ASC model acts as an agent which iteratively generates words (takes actions) to compose the com-pression sentence and the reconstruction model acts as the reward function evaluating the quality of the compressed sentence which is provided as a reward signal. <ref type="bibr" target="#b9">Ranzato et al. (2016)</ref> presents a thorough empirical evaluation on three different NLP tasks by using additional sequence-level reward <ref type="bibr">(BLEU and Rouge-2)</ref> to train the models. In the context of this paper, we apply a variational lower bound (mixed re- construction error and KL divergence regularisation) instead of the explicit Rouge score. Thus the ASC model is granted the ability to explore unlimited unla- belled data resources. In addition we introduce a su- pervised FSC model to teach the compression model to generate stable sequences instead of starting with a random policy. In this case, the pointer network that bridges the supervised and unsupervised model is trained by a mixed criterion of REINFORCE and cross-entropy in an incremental learning framework. Eventually, according to the experimental results, the joint ASC and FSC model is able to learn a robust compression model by exploring both labelled and unlabelled data, which outperforms the other sin- gle discriminative compression models that are only trained by cross-entropy reward signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper we have introduced a generative model for jointly modelling pairs of sequences and evalu- ated its efficacy on the task of sentence compression. The variational auto-encoding framework provided an effective inference algorithm for this approach and also allowed us to explore combinations of dis- criminative (FSC) and generative (ASC) compression models. The evaluation results show that supervised training of the combination of these models improves upon the state-of-the-art performance for the Giga- word compression dataset. When we train the su- pervised FSC model on a small amount of labelled data and the unsupervised ASC model on a large set of unlabelled data the combined model is able to outperform previously reported benchmarks trained on a great deal more supervised data. These results demonstrate that we are able to model language as a discrete latent variable in a variational auto-encoding framework and that the resultant generative model is able to effectively exploit both supervised and unsu- pervised data in sequence-to-sequence tasks.</p><p>src the sri lankan government on wednesday announced the closure of government schools with immediate effect as a military campaign against tamil separatists escalated in the north of the country . ref sri lanka closes schools as war escalates asca sri lanka closes government schools asce sri lankan government closure schools escalated fsca sri lankan government closure with tamil rebels closure src factory orders for manufactured goods rose #.# percent in septem- ber , the commerce department said here thursday . ref us september factory orders up #.# percent asca us factory orders up #.# percent in september asce factory orders rose #.# percent in september fsca factory orders #.# percent in september src hong kong signed a breakthrough air services agreement with the united states on friday that will allow us airlines to carry freight to asian destinations via the territory . ref hong kong us sign breakthrough aviation pact asca us hong kong sign air services agreement asce hong kong signed air services agreement with united states fsca hong kong signed air services pact with united states src a swedish un soldier in bosnia was shot and killed by a stray bul- let on tuesday in an incident authorities are calling an accident , military officials in stockholm said tuesday . ref swedish un soldier in bosnia killed by stray bullet asca swedish un soldier killed in bosnia asce swedish un soldier shot and killed fsca swedish soldier shot and killed in bosnia src tea scores on the fourth day of the second test between australia and pakistan here monday . ref australia vs pakistan tea scorecard asca australia v pakistan tea scores asce australia tea scores fsca tea scores on #th day of #nd test src india won the toss and chose to bat on the opening day in the opening test against west indies at the antigua recreation ground on friday . ref india win toss and elect to bat in first test asca india win toss and bat against west indies asce india won toss on opening day against west indies fsca india chose to bat on opening day against west indies src a powerful bomb exploded outside a navy base near the sri lankan capital colombo tuesday , seriously wounding at least one person , military officials said . ref bomb attack outside srilanka navy base asca bomb explodes outside sri lanka navy base asce bomb outside sri lankan navy base wounding one fsca bomb exploded outside sri lankan navy base src press freedom in algeria remains at risk despite the release on wednesday of prominent newspaper editor mohamed &lt;unk&gt; after a two-year prison sentence , human rights organizations said . ref algerian press freedom at risk despite editor 's release &lt;unk&gt; picture asca algeria press freedom remains at risk asce algeria press freedom remains at risk fsca press freedom in algeria at risk <ref type="table">Table 5</ref>: Examples of the compression sentences. src and ref are the source and reference sentences provided in the test set. asc a and asc e are the abstrac- tive and extractive compression sentences decoded by the joint model ASC+FSC 1 , and fsc a denotes the abstractive compression obtained by the FSC model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Forced Attention Sentence Compression Model from the variational distribution towards the prior. Thus acting to regularise the posterior, but also to restrict the learning of the encoder. If the estimator keeps sampling short compressions during inference, the LSTM decoder would gradually rely on the contexts from the decoded words instead of the information provided by the compressions, which does not yield the best performance on sentence compression. Here, we introduce a co-efficient λ to scale the learning signal of the KL divergence: L=E q φ (c|s) [log p θ (s|c)]−λD KL [q φ (c|s)||p(c)] (12)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, Rezende et al. (2014), and Gregor et al. (2015) have demonstrated strong performance on the task of im- age generation and Eslami et al. (2016) proposed variable-sized variational auto-encoders to identify multiple objects in images. While in natural language processing, there are variants of VAEs on modelling documents (Miao et al., 2016), sentences (Bowman et al., 2015) and discovery of relations (Marcheg- giani and Titov, 2016). Apart from the typical initi- ations of VAEs, there are also a series of works that employs generative models for supervised learning tasks. For instance, Ba et al. (2015) learns visual attention for multiple objects by optimising a varia- tional lower bound, Kingma et al. (2014) implements a semi-supervised framework for image classification and Miao et al. (2016) applies a conditional varia- tional approximation in the task of factoid question answering. Dyer et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Perplexity on validation dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>s 0 s 1 s 2 s 1 s 3 s 2 s 4 s 3 h 1 d h 2 d h 3 d h 4 d Decoder s 1 s 2 s 3 s 4 h 1 e c 1 c 2 c 3 c 1 c 2 c 0 h 1 c h 2 e h 3 e h 4 e h 2 c h 3 c Encoder Compressor h 0 c Compression2 c ^ h 3 c ^ h 1 c</head><label></label><figDesc></figDesc><table>(Pointer Networks) 

3 

α 

2 

α 

1 

α 

^ 
h </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Extractive Summarisation Performance. (1) The extractive summaries of these models are decoded 
by the pointer network (i.e the shared component of the ASC and FSC models). (2) R-1, R-2 and R-L 
represent the Rouge-1, Rouge-2 and Rouge-L score respectively. 

models by ASC+FSC 1 and ASC+FSC 2 where ASC 
is trained on unlabelled data and FSC is trained on 
labelled data. The ASC+FSC 1 model employs equiv-
alent sized labelled and unlabelled datasets, where 
the article sentences of the unlabelled data are the 
same article sentences in the labelled data, so there 
is no additional unlabelled data applied in this case. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Abstractive Summarisation Performance. The abstractive summaries of these models are decoded by 
the combined pointer network (i.e. the shared pointer network together with the softmax output layer over the 
full vocabulary). 

Model 
Labelled Data Perplexity 

Bag-of-Word (BoW) 
3.8M 
43.6 
Convolutional (TDNN) 
3.8M 
35.9 
Attention-Based (NABS) 
3.8M 
27.1 
(Rush et al., 2015) 

Forced-Attention (FSC) 
3.8M 
18.6 
Auto-encoding (ASC+FSC1) 
3.8M 
16.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison on validation perplexity. BoW, 
TDNN and NABS are the baseline neural compres-
sion models with different encoders in Rush et al. 
(2015) 

Model 
Labelled Data R-1 
R-2 R-L 
(Rush et al., 2015) 
3.8M 
29.78 11.89 26.97 
(Nallapati et al., 2016) 
3.8M 
33.17 16.02 30.98 
ASC + FSC2 
500K 
30.14 12.05 27.99 
ASC + FSC2 
1M 
31.09 12.79 28.97 
ASC + FSC1 
3.8M 
34.17 15.94 31.92 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 : Comparison on test Rouge scores</head><label>4</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> The language model, layer connections and decoder soft attentions are omitted in Figure 1 for clarity.</note>

			<note place="foot" n="2"> The recurrent parameters of the compressor are not updated by the gradients from the reconstruction model.</note>

			<note place="foot" n="3"> Forced-attention Sentence Compression In neural variational inference, the effectiveness of training largely depends on the quality of the inference network gradient estimator. Although we introduce a biased estimator by using pointer networks, it is still very difficult for the compression model to generate reasonable natural language sentences at the early stage of learning, which results in</note>

			<note place="foot" n="3"> https://catalog.ldc.upenn.edu/LDC2012T21 4 https://github.com/facebook/NAMAS 5 The hyperparameters where tuned on the validation set to maximise the perplexity of the summaries rather than the reconstructed source sentences.</note>

			<note place="foot" n="6"> The idea of the combined pointer networks is similar to the FSC model, but the implementations are slightly different.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Bahdanau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06349.[Chorowskietal.2015</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Proceedings of NIPS</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3061" to="3069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sentence compression by deletion with lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08575.[Filippovaetal.2015</idno>
	</analytic>
	<monogr>
		<title level="m">Katja Filippova, Enrique Alfonseca, Carlos A Colmenares, Lukasz Kaiser, and Oriol Vinyals</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Proceedings of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06393</idno>
		<idno>arXiv:1603.08148</idno>
	</analytic>
	<monogr>
		<title level="m">Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio. 2016. Pointing the unknown words</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Reducing the dimensionality of data with neural networks</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Kalchbrenner and Blunsom2013] Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discrete-state variational autoencoders for joint discovery and factorization of relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ba ; Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welling ; Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS. [Marcheggiani and Titov2016] Diego Marcheggiani and Ivan Titov</title>
		<meeting>NIPS. [Marcheggiani and Titov2016] Diego Marcheggiani and Ivan Titov</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Semi-supervised learning with deep generative models. Transactions of the Association for Computational Linguistics, 4.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural variational inference for text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Volodymyr Mnih, Nicolas Heess, and Alex Graves. 2014. Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor2014] Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Proceedings of NIPS</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Nallapati</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06023</idno>
	</analytic>
	<monogr>
		<title level="m">Shakir Mohamed, and Daan Wierstra</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Proceedings of ICML</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J Williams ; Alexander M</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS. [Vinyals et al.2015] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In Proceedings of NIPS</title>
		<meeting>NIPS. [Vinyals et al.2015] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In NIPS</meeting>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="page" from="2674" to="2682" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
	<note>Proceedings of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<title level="m">Show, attend and tell: Neural image caption generation with visual attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
