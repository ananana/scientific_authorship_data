<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Effective Use of Context in Noisy Entity Linking</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mueller</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit2">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
							<email>gdurrett@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit2">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Effective Use of Context in Noisy Entity Linking</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1024" to="1029"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1024</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>To disambiguate between closely related concepts , entity linking systems need to effectively distill cues from a mention&apos;s textual context. We investigate several techniques for using these cues in the task of noisy entity linking on short texts. Our starting point is a state-of-the-art attention-based model from prior work; while this model&apos;s attention typically identifies context that is topically relevant, it fails to identify some of the most indicative context words, especially those exhibiting lexical overlap with the true title. Augmenting the model with convolutional networks over characters still leaves it largely unable to pick up on these cues compared to sparse features that target them directly, indicating that automatically learning how to identify relevant character-level context features is a hard problem. Armed with these sparse features, our final system 1 outperforms past work on the WikilinksNED test set by 2.8% absolute.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Effectively using an entity mention's context to disambiguate it is the crux of the entity linking task: in isolation, the mention Richard Wright could refer to three possible entities in Wikipedia's knowledge base corresponding to an artist, a mu- sician, or an author. Previous work in this area has distilled context information by exploiting tf- idf features <ref type="bibr" target="#b5">(Cucerzan, 2007;</ref><ref type="bibr" target="#b20">Milne and Witten, 2008;</ref><ref type="bibr" target="#b22">Ratinov et al., 2011</ref>), global link coher- ence ( <ref type="bibr" target="#b14">Hoffart et al.;</ref><ref type="bibr" target="#b24">Sil and Florian, 2016)</ref>, cues from coreference ( <ref type="bibr" target="#b2">Cheng and Roth, 2013;</ref><ref type="bibr" target="#b13">Hajishirzi et al., 2013;</ref><ref type="bibr" target="#b7">Durrett and Klein, 2014</ref>), con- volutional neural networks ( <ref type="bibr" target="#b27">Sun et al.;</ref><ref type="bibr">FrancisLandau et al., 2016)</ref>, or more sophisticated neural architectures ( <ref type="bibr" target="#b12">Gupta et al., 2017;</ref><ref type="bibr" target="#b25">Sil et al., 2018</ref>).</p><p>These approaches typically focus on aggregat- ing information from a mix of sources, including long-range information from the textual context or other linked entities. While this approach is suit- able for entity linking settings such as newswire <ref type="bibr" target="#b0">(Bentivogli, 2010)</ref> and <ref type="bibr">Wikipedia (Ratinov et al., 2011</ref>), we cannot always rely on this information in other settings like Twitter ( <ref type="bibr" target="#b11">Guo et al., 2013;</ref><ref type="bibr" target="#b9">Fang and Chang, 2014;</ref><ref type="bibr" target="#b15">Huang et al., 2014;</ref><ref type="bibr" target="#b6">Dredze et al., 2016</ref>), Snapchat ( <ref type="bibr" target="#b21">Moon et al., 2018</ref>), other web platforms ( <ref type="bibr" target="#b8">Eshel et al., 2017)</ref>, or dialogue sys- tems ( <ref type="bibr" target="#b1">Bowden et al., 2018)</ref>. We need models that can make effective use of limited context windows in noisy settings.</p><p>In this work, we investigate this problem of ef- fectively using context in the setting of the Wik- ilinksNED dataset from <ref type="bibr" target="#b8">Eshel et al. (2017)</ref>. The examples in this dataset, which consists of 3.2 mil- lion entity disambiguation examples derived from Wikilinks ( <ref type="bibr" target="#b26">Singh et al., 2012</ref>), have at most 20 words of context on either side and usually no other mentions of the entity being disambiguated. We build off a state-of-the-art attentive LSTM model from prior work ( <ref type="bibr" target="#b8">Eshel et al., 2017)</ref> and show that despite its good performance, it fails to resolve some examples that human readers would find trivial. For example, disambiguating the iden- tity of the song Down in <ref type="figure">Figure 1</ref> is easy if we can recognize the nearby string Jay Sean in the con- text, but the model sometimes fails to do this.</p><p>We explore the performance of a standard at- tention mechanism as well as two modifications. First, we inject character information into the model through character-level CNNs; these give the model a deeper ability to recognize character correspondences between the context and entity ti- tle. However, these convolutional filters struggle to learn useful features in this noisy context and ultimately do not help performance. By contrast, sparse features explicitly targeting these overlaps </p><formula xml:id="formula_0">[l · et, l et, (l et) 2 ] [r · et, r et, (r et) 2 ]</formula><p>s t e t l r <ref type="figure">Figure 1</ref>: Neural entity linking model. Down has three possible link targets: in the attentive variants of our model, each target computes attention weights over GRUs that consume the left and right context. These representations are passed into a layer that compares them to the entity's embedding, yielding a final score which is normalized over all possible link targets.</p><p>appear to be more successful. We investigate the relation between our model's attention and what the sparse features learn. Our final model, using these features, achieves an accuracy of 75.8% on this dataset, substantially outperforming our base- line model as well as results from prior work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Basic Model</head><p>The WikilinksNED dataset consists of entity men- tions in context scraped from the web, with gold annotation derived from the fact that those mentions originally appeared with hyperlinks to Wikipedia. We denote the mention text (i.e., an- chor text of the hyperlink) by m, and denote the left and right context of the mention by c l and c r respectively; these are at most 20 words. For this dataset, we can assume that the possible linked ti- tles for a mention have been seen in training, and the main task is instead to disambiguate between them and identify the gold title t * . We therefore follow prior work ( <ref type="bibr" target="#b8">Eshel et al., 2017</ref>) and take as candidates all gold entities in the training set whose mention was m rather than relying on a sep- arate candidate generation scheme. Our model places a distribution over titles P (t|m, c l , c r ), where t takes values in the set of candidate Wikipedia titles for that mention. This model, depicted in <ref type="figure">Figure 1</ref>, roughly follows that of <ref type="bibr" target="#b8">Eshel et al. (2017)</ref>, with some key differences, as we discuss in the rest of this section.</p><p>Embedding contexts Given an example of the form (m, c l , c r ), our model first uses a GRU layer ( <ref type="bibr" target="#b4">Cho et al., 2014</ref>) over each context to convert c l and c r into continuous vector representations l and r, respectively. Our word embeddings are trained over Wikipedia as described in the following para- graph.</p><p>Embedding entities We follow the method of <ref type="bibr" target="#b8">Eshel et al. (2017)</ref> for generating entity embed- dings, using word2vecf ( <ref type="bibr" target="#b17">Levy and Goldberg, 2014</ref>) to jointly train word and entity embeddings simultaneously using Wikipedia article text. Each title t is associated in turn with each content word w in the article, yielding a set of (w, t) pairs that are consumed by the training procedure. This yields a set of title embeddings e t which we can treat as distributed representations of entities.</p><p>Entity-context comparison We systematically compare the representations for l, r, and e t as fol- lows:</p><formula xml:id="formula_1">[l · e t , r · e t , l − e t , r − e t , (l − e t ) 2 , (r − e t ) 2 ]</formula><p>where · denotes the conventional dot product and the other comparisons are elementwise. These fea- tures form the input to a final feedforward layer which produces a real-valued score s t for the given title. Repeating this computation for each title, our model's distribution is P (t|m, c l , c r ) = softmax t (s t ).</p><p>Training Because our model involves substan- tial computation for each possible title, we want to limit the set of titles considered during train- ing. For each example we consider, we construct a set T containing the gold title and 4 negative "dis- tractor" titles from the candidate set. Unlike Es- hel et al. (2017), we structure training as a multi- class decision among these titles rather than a bi- nary prediction problem over each title as gold or not. We run our model over the candidates t ∈ T to produce the distribution P (t|m, c l , c r ) and train to maximize the log probablity log P (t * |m, c l , c r ) of the gold title.</p><p>Results The model set forth in this section is the basis for the remaining models in this paper; we call it the GRU model as that is the only context en- coding mechanism it uses. As shown in <ref type="table">Table 1</ref>, this GRU model gets a score of 73.4 on the Wik- ilinksNED development set. In the next section, we explore techniques for using the context in a more sophisticated way to improve further on this result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Accuracy on Test (%) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Exploiting Context Cues</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Attention</head><p>One way to improve over the basic GRU model is to use attention over the context based on the ti- tle under consideration. The attention we use is a modified version of the dot product attention (Lu- ong et al., 2015) used by <ref type="bibr" target="#b8">Eshel et al. (2017)</ref>, al- lowing the model to weight the importance of the outputs of the GRU at each time step. Each con- text (left and right) has its own attention weights. For a given side of context and candidate t, the at- tention first computes a transformation of the en- tity embedding e t as follows: q t = tanh(W e t ). This allows the model to learn an attention query q t distinct from the candidate embedding e t . The model then computes attention probabilities α i for each GRU output o i , normalized over the entire sequence of GRU outputs (of length n):</p><formula xml:id="formula_2">α i = softmax i (q t · o i )</formula><p>The resulting probability distribution is used to take a weighted sum of GRU outputs to get a rep- resentation a:</p><formula xml:id="formula_3">a = n i α i o i</formula><p>We compute a l and a r independently and symmet- rically for the left and right context. These vectors are then fed forward through the model as the fi- nal continuous representation of the left or right context, l or r respectively.</p><p>Results In <ref type="table">Table 1</ref>, we see that our model with attention (GRU+ATTN) outperforms our basic GRU model by around 1% absolute. It also outperforms the roughly similar model of <ref type="bibr" target="#b8">Eshel et al. (2017)</ref> on the test set: this gain is due to a combination of factors including the improved training procedure and some small modeling changes. 2 However, our attention scheme is not without its shortcomings, as we now discuss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Shortcomings of Attention</head><p>One common and frustrating error our model makes is failing to correctly disambiguate men- tions whose contexts share similar words or character overlap with the gold entity's actual Wikipedia title. In these instances, the model fails to attend correctly to words that we, as human readers, would most likely see as disambiguating terms. For instance, in this example's left context:</p><p>...known also for the B.P. Koirala In- stitute of Health Sciences, one of the biggest government hospital. The in- digenous people of Dharan are Limbu ... the model fails to identify people as a critical term for disambiguation. This failure is partially due to the model's sole reliance on distributed represen- tations: the embedding for people and the title em- bedding for Limbu People need to somehow con- tain enough common information for the model to associate these, identify people as an important token, and use it to disambiguate between candi- date titles such as Limbu People, Limbu Language, and Limbu Alphabet. Moreover, with such noisy, unstructured context, it is difficult for the model to learn to rely on other grammatical or semantic cues (such as are indicating that the title is prob- ably a plural noun, which alphabet and language are not).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Character CNNs</head><p>One way to address these issues in the model is to exploit more fine-grained character-level infor- mation. This circumvents the need to separately learn a distributed correspondence between terms with lexical overlap, and is especially useful when these terms may be unknown words; for exam- ple, a year mentioned within a context is often unknown and therefore assigned an UNK embed- ding, even if that year matches exactly with a year in the gold candidate's title. One solution to this is to allow our model to consult character-level information, which past models have done successfully for named en- tity recognition ( <ref type="bibr" target="#b3">Chiu and Nichols, 2015;</ref><ref type="bibr" target="#b16">Lample et al., 2016;</ref><ref type="bibr" target="#b19">Ma and Hovy, 2016</ref>), text classifica- tion ( <ref type="bibr" target="#b28">Zhang et al., 2015)</ref>, and POS tagging <ref type="bibr" target="#b23">(Santos and Zadrozny, 2014</ref>). We use convolutional neural networks (CNNs) to distill character repre- sentations of words into vectors that we concate- nate with our word representations. We addition- ally use character CNNs over entity titles and con- catenate these representations with the title em- beddings e t , to allow the model to learn to char- acterize similarities between contexts and entity titles. Our CNNs use window sizes of 6 and 100 filters each; these values were selected through hy- perparameter tuning on the development set. <ref type="table">Table 1</ref> shows the impact of incorporating char- acter CNNs (GRU+ATTN+CNN). Surprisingly, these have a mild negative impact on perfor- mance. One possible explanation of this is that it causes the model to split its attention between se- mantically important and lexically similar context terms. Consider the following example:</p><p>really think Final Fight could be a lot of a fun as a vigilante justice movie with a high quotient of hand-to-hand fight se- quences. Think The Warriors</p><p>The gold title is The Warrior (film) and the base model correctly places 90% of its attention weight on the word movie when calculating attention for this title. However, the character-level CNN model only places 60% of its attention weight on it, distributing its attention values more evenly across the rest of the words. Such cases are fre- quent: the average highest weight given by at- tention in GRU+ATTN+CNN is about 6% lower than the average highest attention weight given by GRU+ATTN. The CNNs seem to have generally decreased the model's confidence in what context clues are key for disambiguation, leading to lower performance. We will return to more analysis of this in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Lexical Feature Set</head><p>To determine whether character level overlap be- tween the entity title and context is useful, we take Entity Title: Limbu People</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Known also for the B.P. Koirala Institute of Health Sciences, one of the biggest government hospital. The indigenous people of Dharan are Limbu</head><p>Left Context: a more direct approach to incorporating that infor- mation into our model and build a set of sparse features that directly target it. <ref type="figure" target="#fig_0">Figure 2</ref> shows an example of how our features are computed. We fire features on each word in the context that is either an exact match or a sub- string of a word in the candidate title; people is the relevant token here. We conjoin that match information with whether the word is in the left or right context along with the bucketed offset of the word from the mention. This feature set is then appended to the vector comparison features to form the input to the model's feedforward layer (see <ref type="figure">Figure 1)</ref>. <ref type="table">Table 1</ref> shows the results of stacking these features on top of our model with attention (GRU+ATTN+FEATS). We see our highest devel- opment set performance and correspondingly high test performance from this model. This indicates that character-level information is useful for dis- ambiguation, but character CNNs as we incorpo- rated them are not able to distill it as effectively as sparse features can. Our model augmented with these sparse features achieves state-of-the-art re- sults on the test set.</p><formula xml:id="formula_4">[</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Attention and "Obvious" Terms</head><p>Now that we have identified features which seem useful for this entity linking problem, we can ask how the tokens attended by our attention mecha- nism compare to those singled out by the features.  <ref type="figure">Figure 3</ref>: Examples of our models putting high atten- tion weight into irrelevant context words, not acknowl- edging the relevance of disambiguating terms that share lexical overlap with the correct title. We display the weight given to the top 4 attended words above each word for two of our models.</p><p>that contains one of our lexical features, out of all examples where such a feature exists anywhere. The reported probability mass is the total attention mass that the model puts into words that associated with lexical features, averaged over all examples where such features exist. We see that the model frequently fails to exploit this information, and moreover the addition of CNNs does not strongly improve this. <ref type="figure">Figure 3</ref> shows examples of this behavior. In the first example, rather than identifying cheese as a salient term, both models instead focus more heavily on milk and like. Similarly, in the second example, the model fails to recognize the impor- tance of robot in the context.</p><p>One possible reason that CNNs don't help more is that the sparse features only trigger on a sub- set of examples. Because the CNNs process ev- ery example, they may not see enough examples of lexical overlap to pick up on it, and instead try to augment what the word embedding model is already doing with subword information, which ends up being unstable for this task. Naturally, words with these overlap characteristics are not al- ways the most disambiguating term. However, in light of noisy contexts, when the standard repre- sentation of context fails to be sufficient for allow- ing the model to disambiguate, we want the model to be able to leverage this character level informa- tion to help it make intuitive decisions, which the CNN fails to do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we observed that in noisy entity link- ing settings on short texts, neural models relying  on attention do not always pick up on the cor- rect context clues, even when those clues exhibit very obvious surface overlap with the correct en- tity title. These models can perform better when augmented with sparse features explicitly target- ing this kind of lexical overlap: our system us- ing these features achieves state-of-the-art disam- biguation accuracy on the WikilinksNED dataset. By contrast, automatically learning learning fine- grained character-level features with CNNs in this context is hard. More exploration is needed to bet- ter understand what inductive biases are necessary for an entity linking system to make maximally ef- fective use of the information available to it.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example of feature generation from an example. Here, because the word people occurs in the title and in the left context 4 words away from the mention, the indicator feature [Pos=4,Match=ExactWord,Context=Left] fires in the feature set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 containsvery strong stinky cheese from sheep's milk. Like Stilton… Delta Robot I'm aiming high, for a little robot on my desk, of an articulated -or a delta… -GRU+ATTN</head><label>2</label><figDesc>statistics regarding the attention values of our GRU+ATTN and GRU+ATTN+CNN model on a subset of ex- amples that both models got wrong. We define accuracy as the percentage of examples in which the model gives the highest attention to a word Stilton Cheese Roquefort is a</figDesc><table>-GRU+ATTN+CNN 

Feat=[Pos=6-10,Match=ExactWord,Context=Left] 

Feat=[Pos=5,Match=ExactWord,Context=Left] 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Our models' attention "accuracy": how often 
each model's maximally-attended word also triggered 
a feature to fire. Prob Mass indicates the average sum 
of attention scores over feature-triggering words. All 
values are computed over a sample of 10,000 examples 
that each model got wrong. 

</table></figure>

			<note place="foot" n="2"> Note that in Eshel et al. (2017), the authors point out that their dataset has a high percentage of errors (35% of the errors made by their model are spurious), meaning that the skyline on this task is likely not higher than 90%.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by NSF Grant IIS-1814522, a Bloomberg Data Science Grant, and an equipment grant from NVIDIA. The au-thors acknowledge the Texas Advanced Comput-ing Center (TACC) at The University of Texas at Austin for providing HPC resources used to con-duct this research. Thanks as well to the anony-mous reviewers for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Extending English ACE 2005 Corpus Annotation with Ground-truth Links to Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SlugNERDS: A Named Entity Recognition Tool for Open Domain Dialogue Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">K</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shereen</forename><surname>Oraby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amita</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Relational Inference for Wikification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Named Entity Recognition with Bidirectional LSTM-CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Fethi Bougares, Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-Scale Named Entity Disambiguation Based on Wikipedia Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silviu Cucerzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Twitter at the Grammys: A Social Media Corpus for Entity Linking and Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Deyoung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media</title>
		<meeting>The Fourth International Workshop on Natural Language Processing for Social Media</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Joint Model for Entity Analysis: Coreference, Typing, and Linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Named Entity Disambiguation for Noisy Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Eshel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Radinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Entity Linking on Microblogs with Spatial and Temporal Signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Capturing Semantic Similarity for Entity Linking with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Francis-Landau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">To Link or Not to Link? A Study on Endto-End Tweet Entity Linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Kiciman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Entity Linking via Joint Encoding of Types, Descriptions, and Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint Coreference Resolution and Named-Entity Linking with Multi-Pass Sieves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Zilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust Disambiguation of Named Entities in Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Amir</forename><surname>Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilaria</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>Fürstenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilyana</forename><surname>Taneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Collective Tweet Wikification based on Semi-supervised Graph Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural Architectures for Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DependencyBased Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attentionbased Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end Sequence Labeling via Bi-directional LSTM-CNNsCRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to Link with Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multimodal Named Entity Disambiguation for Noisy Social Media Posts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwhan</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Neves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>and Vitor Carvalho</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Local and Global Algorithms for Disambiguation to Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Doug Downey, and Mike Anderson</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning Character-level Representations for Part-ofSpeech Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dos</forename><surname>Cicero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">One for all: Towards language independent named entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avirup</forename><surname>Sil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural cross-lingual entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avirup</forename><surname>Sil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gourab</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Wikilinks: A Large-scale Cross-Document Coreference Corpus Labeled via Links to Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling Mention, Context and Entity with Neural Networks for Entity Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhou</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Character-level Convolutional Networks for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
