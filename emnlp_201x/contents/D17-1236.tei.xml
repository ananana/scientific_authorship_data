<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bootstrapping incremental dialogue systems from minimal data: the generalisation power of dialogue grammars</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Eshghi</surname></persName>
							<email>a.eshghi@hw.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Interaction Lab Heriot-Watt University</orgName>
								<orgName type="laboratory" key="lab2">Interaction Lab Heriot-Watt University</orgName>
								<orgName type="laboratory" key="lab3">Interaction Lab Heriot-Watt University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Shalyminov</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Interaction Lab Heriot-Watt University</orgName>
								<orgName type="laboratory" key="lab2">Interaction Lab Heriot-Watt University</orgName>
								<orgName type="laboratory" key="lab3">Interaction Lab Heriot-Watt University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
							<email>o.lemon@hw.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Interaction Lab Heriot-Watt University</orgName>
								<orgName type="laboratory" key="lab2">Interaction Lab Heriot-Watt University</orgName>
								<orgName type="laboratory" key="lab3">Interaction Lab Heriot-Watt University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bootstrapping incremental dialogue systems from minimal data: the generalisation power of dialogue grammars</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2220" to="2230"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We investigate an end-to-end method for automatically inducing task-based dialogue systems from small amounts of unannotated dialogue data. It combines an incremental semantic grammar-Dynamic Syntax and Type Theory with Records (DS-TTR)-with Reinforcement Learning (RL), where language generation and dialogue management are a joint decision problem. The systems thus produced are incremental: dialogues are processed word-byword , shown previously to be essential in supporting natural, spontaneous dialogue. We hypothesised that the rich linguistic knowledge within the grammar should enable a combinatorially large number of dialogue variations to be processed, even when trained on very few dialogues. Our experiments show that our model can process 74% of the Facebook AI bAbI dataset even when trained on only 0.13% of the data (5 dialogues). It can in addition process 65% of bAbI+, a corpus 1 we created by systematically adding incremental dialogue phenomena such as restarts and self-corrections to bAbI. We compare our model with a state-of-the-art retrieval model, memn2n (Bordes et al., 2017). We find that, in terms of semantic accuracy, memn2n shows very poor robust-ness to the bAbI+ transformations even when trained on the full bAbI dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There are currently several key problems for the practical data-driven (rather than hand-crafted) development of task-oriented dialogue systems, <ref type="bibr">1</ref> Dataset available at https://bit.ly/babi_plus among them: (1) large amounts of dialogue data are needed, i.e. thousands of examples in a do- main; (2) this data is usually required to be anno- tated with task-specific semantic/pragmatic infor- mation for the domain (e.g. various dialogue act schemes); and (3) the resulting systems are gen- erally turn-based, and so do not support natural spontaneous dialogue which is processed incre- mentally, word-by-word, with many characteristic phenomena that arise from this incrementality.</p><p>In overcoming issue (2), a recent advance made in research on (non-task) chat dialogues has been the development of so-called "end-to-end" sys- tems, in which all components are trained from textual dialogue examples, e.g. ( <ref type="bibr" target="#b27">Sordoni et al., 2015;</ref><ref type="bibr" target="#b28">Vinyals and Le, 2015)</ref>. However, as <ref type="bibr" target="#b1">Bordes and Weston (2017)</ref> argued, these end-to-end meth- ods may not transfer well to task-based settings (where the user is trying to achieve a domain goal, such as booking a flight or finding a restaurant, re- sulting in an API call). <ref type="bibr" target="#b1">Bordes and Weston (2017)</ref> then presented an end-to-end method using Mem- ory Networks (memn2ns), which achieves 100% performance on a test-set of 1000 dialogues, af- ter being trained on 1000 dialogues. This method processes dialogues turn-by-turn, and so does not have the advantages of more natural incremen- tal systems <ref type="bibr" target="#b0">(Aist et al., 2007;</ref><ref type="bibr" target="#b26">Skantze and Hjalmarsson, 2010)</ref>; nor does it really perform lan- guage generation, rather it's based on a retrieval model that selects from a set of candidate system responses seen in the data.</p><p>This paper investigates an approach to these challenges -dubbed babble -using an incremental, semantic parser and generator for dialogue <ref type="bibr" target="#b6">Eshghi, 2015)</ref>, based around the Dynamic Syntax grammar formalism (DS, <ref type="bibr" target="#b17">Kempson et al. (2001)</ref>; <ref type="bibr" target="#b2">Cann et al. (2005)</ref>).</p><p>Our advance in this paper, for end-to-end sys- tems, is therefore twofold: (a) the babble method overcomes the requirement for large amounts of dialogue data (i.e. 1000s of dialogues in a do- main); (b) resulting systems are word-by-word in- cremental, in both parsing, generation and dia- logue management. We show that using only 5 ex- ample dialogues from the bAbI, Task 1 dataset (i.e. 0.13% of the training data used by <ref type="bibr" target="#b1">(Bordes et al., 2017)</ref>) babble can automatically induce dialogue systems which process 74% of the bAbI testset in an incremental manner. We then introduce an extended incremental version of the bAbI dataset, which we call bAbI+ (see section 4.1), which adds some characteristic incremental phenomena -such as mid-utterance self-corrections -to the bAbI dialogues (this new dataset is freely avail- able). Using this, we demonstrate that the bab- ble system can in addition generalise to, and pro- cess 65% of the bAbI+ dataset, still when trained only on 5 dialogues from bAbI. We compare this method to <ref type="bibr" target="#b1">(Bordes et al., 2017)</ref>'s memn2n, which, in terms of semantic accuracy (reflected in how well api-calls are predicted at the end of bAbI Task 1), shows very poor robustness to the bAbI+ transformations, even when it is trained on the full bAbI dataset.</p><p>This overall method is portable to other task- based domains. Furthermore, as we use a seman- tic parser, the semantic/contextual representations of the dialogue can be used directly for large-scale inference, required in more complex tasks (e.g. in- teractive QA and search).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Dimensions of Pragmatic Synonymy</head><p>There are two important dimensions along which dialogues can vary, but nevertheless, lead to iden- tical contexts: interactional, and lexical. Inter- actional synonymy is analogous to syntactic syn- onymy -when two distinct sentences are parsed to identical logical forms -except that it occurs not only at the level of a single sentence, but at the di- alogue or discourse level. <ref type="figure" target="#fig_1">Fig. 1</ref> shows examples of interactional variants that lead to very similar final contexts, in this case, that the user wants to buy an LG phone. These dialogues can be said to be pragmatically synonymous for this domain. Ar- guably, a good computational model of dialogue processing, and interactional dynamics should be able to capture this synonymy.</p><p>Lexical synonymy relations, on the other hand, hold among utterances, or dialogues, when differ- ent words (or sequences of words) express mean- ings that are sufficiently similar in a particular do- main. What is striking about lexical synonymy re- lations is that unlike syntactic/interactional ones, they can often break down when one moves to an- other domain: lexical synonymy relations are do- main specific.</p><p>Eshghi &amp; Lemon (2014) developed a method similar in spirit to <ref type="bibr" target="#b18">Kwiatkowski et al. (2013)</ref> for capturing lexical synonymy relations by creating clusters of semantic representations based on ob- servations that they give rise to similar or identi- cal extra-linguistic actions observed within a do- main (e.g. a data-base query, a flight booking, or any API call). Distributional methods could also be used for this purpose (see e.g. <ref type="bibr" target="#b19">Lewis &amp; Steedman (2013)</ref>). In general, this kind of clustering is achieved when the domain-general semantics re- sulting from semantic parsing is grounded in a par- ticular domain.</p><p>We note that while interactional synonymy re- lations in dialogue should be accounted for by se- mantic grammars or formal models of dialogue structure (such as DS-TTR ( <ref type="bibr" target="#b8">Eshghi et al., 2012)</ref>, or KoS (Ginzburg, 2012)), lexical synonymy rela- tions have to be learned from data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Why a grammar-based approach?</head><p>Recent end-to-end data-driven machine learn- ing approaches treat dialogue as a sequence-to- sequence generation problem, and train their mod- els from large datasets e.g. ( <ref type="bibr" target="#b27">Sordoni et al., 2015;</ref><ref type="bibr">Wen et al., 2016b,a;</ref><ref type="bibr" target="#b28">Vinyals and Le, 2015)</ref>. The systems resulting from these types of approach are in principle able to handle variations/patterns that they have encountered (sufficiently often) in the training data, but not beyond.</p><p>This large-data constraint is problematic for de- velopers but is also strange when we consider the structural knowledge that we have about language and dialogue that can be encoded in grammars and computational models of interaction. Indeed, it is often stated that for humans to learn how to perform adequately in a domain, one example is enough from which to learn (e.g. <ref type="bibr" target="#b20">Li et. al (2006)</ref>).</p><p>Furthermore, as these systems do not parse to logical forms, they do not allow for explicit infer- ence, which further limits their application.</p><p>We therefore develop a method combining learning from data with an incremental seman- tic grammar of dialogue that is able to generalise from small number of observations in a domain -   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Inducing Dialogue Systems</head><p>Our overall method involves incrementally pars- ing dialogues, and encoding the resulting seman- tics as state vectors in a Markov Decision Pro- cess (MDP), which is then used for Reinforcement Learning (RL) of word-level actions for system output (i.e. a combined incremental DM and NLG module for the resulting dialogue system).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dynamic Syntax and Type Theory with</head><p>Records (DS-TTR)</p><p>Dynamic Syntax (DS) is an action-based, word- by-word incremental and semantic grammar for- malism ( <ref type="bibr" target="#b17">Kempson et al., 2001;</ref><ref type="bibr" target="#b2">Cann et al., 2005</ref>), especially suited to the highly fragmentary and context-dependent nature of dialogue. In DS, words are conditional actions -semantic updates; and dialogue is modelled as the interactive and in- cremental construction of contextual and semantic representations ( ) -see <ref type="figure" target="#fig_3">Fig. 2</ref>. The contextual representations afforded by DS are of the fine-grained semantic content that is jointly negotiated/agreed upon by the interlocu- tors, as a result of processing questions and an- swers, clarification interaction, acceptances, self- /other-corrections, restarts, and other characteris- tic incremental phenomena in dialogue -see 3 for a sketch of how self-corrections and restarts are pro- cessed via a backtrack and search mechanism over the parse search graph (see Hough (2011); Hough and Purver (2014);  for details of the model, and how this parse search graph is effectively the context of the conversation). Gen- eration/linearisation in DS is defined using trial- and-error parsing (see Section 3.2, with the pro- vision of a generation goal, viz. the semantics of the utterance to be generated. Generation thus proceeds, just as with parsing, on a word-by-word basis (see ; Hough (2015) for details). The upshot of this is that using DS, we can not only track the semantic content of some current turn as it is being constructed (parsed or generated) word-by-word, but also the context of the conversation as whole, with the latter also en- coding the grounded/agreed content of the con- versation (see e.g.  <ref type="formula">(2010)</ref> for details). Crucially for our model below, the inherent incrementality of DS-TTR together with the word-level, as well as cross-turn, parsing constraints it provides, en- ables the word-by-word exploration of the space of grammatical dialogues, and the semantic and contextual representations that result from them.</p><p>Type Theory with Records (TTR) is an exten- sion of standard type theory shown to be useful in semantics and dialogue modelling <ref type="bibr" target="#b3">(Cooper, 2005;</ref><ref type="bibr" target="#b12">Ginzburg, 2012)</ref>. To accommodate dialogue pro- cessing, and allow for richer representations of the dialogue context recent work has integrated DS and the TTR framework to replace the logical for- malism in which meanings are expressed ( <ref type="bibr" target="#b23">Purver et al., 2010</ref><ref type="bibr" target="#b8">Eshghi et al., 2012</ref>). In TTR, logical forms are specified as record types (RTs), sequences of fields of the form [ l : T ] contain- ing a label l and a type T . RTs can be witnessed (i.e. judged as true) by records of that type, where a record is a sequence of label-value pairs <ref type="bibr">[ l = v ]</ref>,</p><formula xml:id="formula_0">and [ l = v ] is of type [ l :</formula><p>T ] just in case v is of type T (see <ref type="figure" target="#fig_3">Fig. 2</ref> for example record types). Importantly for us here, the standard subtype re- lation ⊑ can be defined for record types:</p><formula xml:id="formula_1">R 1 ⊑ R 2 if for all fields [ l : T 2 ] in R 2 , R 1 contains [ l : T 1 ]</formula><p>where T 1 ⊑ T 2 . A record type can thus be in- definitely extended, and is therefore always under- specified by definition. This allows for incremen- tally growing meanings to be expressed in a natu- ral way as more words are parsed or generated in</p><formula xml:id="formula_2">[ event : e s p1 =today(event) : t ] →                  event =arrive : e s p1 =today(event) : t p2 =pres(event) : t x =robin : e p3 =sub j(event,x) : t                  →                            event =arrive : e s p1 =today(event) : t p2 =pres(event)</formula><p>: t x =robin : e p3 =sub j(event,x) : t x1</p><p>:</p><formula xml:id="formula_3">e p3 = f rom(event,x1) : t                            →                            event =arrive : e s p1 =today(event) : t p2 =pres(event)</formula><p>: turn. In addition, as will become clear below, this subtype checking operation is the key mechanism used in our system below for feature checking.</p><formula xml:id="formula_4">t x =robin : e p =sub j(event,x) : t x1 =S weden : e p3 = f rom(event,x1) : t                            "A:</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overall Method: babble</head><p>In this section we describe our method for combin- ing incremental dialogue parsing with Reinforce- ment Learning for Dialogue Management (DM) and Natural Language Generation (NLG) where these are treated as a joint decision/optimisation problem.</p><p>We start with two resources: a) a DS-TTR parser DS (either learned from data ( <ref type="bibr" target="#b7">Eshghi et al., 2013a)</ref>, or constructed by hand), for incremental language processing, but also, more generally, for tracking the context of the dialogue using Eshghi et al.'s model of feedback ( <ref type="bibr" target="#b6">Eshghi, 2015;</ref>); b) a set D of tran- scribed successful dialogues in the target domain.</p><p>We perform the following steps overall to in- duce a fully incremental dialogue system from D:</p><p>1. Automatically induce the MDP state space, S , and the dialogue goal, G D , from D;</p><p>2. Automatically define the state encoding func- tion F : C → S ; where s ∈ S is a (binary) state vector, designed to extract from the cur- rent context of the dialogue, the semantic fea- tures observed in the example dialogues D; and c ∈ C is a DS context, viz. a pair of TTR Record Types: ⟨c p , c g ⟩, where c p is the con- tent of the current, PENDING clause as it is being constructed, but not necessarily fully grounded yet; and c g is the content already jointly built and GROUNDED by the inter- locutors (loosely following the DGB model of (Ginzburg, 2012)).</p><p>3. Define the MDP action set as the DS lexicon L (i.e. actions are words);</p><p>4. Define the reward function R as reaching G D , while minimising dialogue length.</p><p>We then solve the generated MDP using Reinforcement Learning, with a standard Q- learning method, implemented using BURLAP <ref type="bibr" target="#b21">(McGlashan, 2016)</ref>: train a policy π : S → L, where L is the DS Lexicon, and S the state space induced using F. The system is trained in inter- action with a (semantic) simulated user, also au- tomatically built from the dialogue data and de- scribed in the next section.</p><p>The state encoding function, F As shown in <ref type="figure" target="#fig_2">figure 4</ref> the MDP state is a binary vector of size 2 × |Φ|, i.e. twice the number of the RT fea- tures. The first half of the state vector contains the grounded features (i.e. agreed by the participants) ϕ i , while the second half contains the current se- mantics being incrementally built in the current di- alogue utterance. Formally:</p><formula xml:id="formula_5">s = ⟨F 1 (c p ), . . . , F m (c p ), F 1 (c g ), . . . , F m (c g )⟩;</formula><p>where F i (c) = 1 if c ⊑ ϕ i , and 0 otherwise. (Recall that ⊑ is the RT subtype relation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Semantic User Simulation</head><p>The simulator is in charge of two key tasks dur- ing training: (1) generating user turns in the right dialogue contexts; and (2) word-by-word monitor- ing of the utterance so far generated by the sys-Grounded Semantics</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Current Turn Semantics</head><p>Dialogue so far</p><formula xml:id="formula_6">                           x2 : e e2 =like : es x1 =US R : e p2 =pres(e2)</formula><p>: t p5 =sub j(e2,x1) : t p4 =ob j(e2,x2) : t p11 =phone(x2) : t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>                                                                         x2</head><p>: e e2 =like : es x1 =US R : e p2 =pres(e2)</p><p>: t p5 =sub j(e2,x1) : t p4 =ob j(e2,x2)</p><p>: t p11 =phone(x2) : t x3</p><p>: e p10 =by(x2,x3) : t p9 =brand(x3)</p><p>: t p10 =question(x3) : t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>                                             </head><p>SYS: What would you like? USR: a phone SYS: by which brand?</p><p>RT Feature:</p><formula xml:id="formula_7">[ x10 : e p15 =brand(x10) : t ][ e3 =like : es p2 =pres(e3) : t ]          x10 : e x8 : e p14 =by(x8,x10) : t                   e3 =like : es x5 =usr : e p7 =sub j(e3,x5) : t                   x8 : e e3 =like : es p6 =ob j(e3,x8) : t          F 1 ↓ F 2 ↓ F 3 ↓ F 4 ↓ F 5 ↓ State: ⟨ Current Turn: 1, 1, 1, 1, 1, ⟩ Grounded: 0, 1, 0, 1, 1</formula><p>Figure 4: Semantics to MDP state encoding with RT features tem during exploration (i.e. babbling grammati- cal word sequences) by the system. To exploit (and evaluate) the full generalisation properties of the DS dialogue model, both <ref type="formula">(1)</ref> and <ref type="formula">(2)</ref> use the full machinery of the DS parser, as well the state encoding function F, described above. They are thus performed based on the semantic context of the dialogue so far, as tracked by DS (rather than, e.g. being based on string or template matching). Since this includes not just the semantic features of the current turn, but also of the history of the conversation, our simulator respects the turn or- derings encountered in the data, i.e. it is sensitive to the order in which information is gathered from the user. The rules required for (1) &amp; (2) are extracted automatically from the raw dialogue data, D, us- ing DS and F. The dialogues in D are parsed and encoded using F incrementally. For (1), all the states that trigger the user into action, s i = F(c) -where c is a DS context -immediately prior to any user turn are recorded, and mapped to what the user ends up saying in those contexts -for more than one training dialogue there may be more than one candidate (in the same context/state). The rules thus extracted will be of the form: s trig → {u 1 , . . . , u n }, where u i are user turns. Now note that the s i 's prior to the user turns also immediately follow system turns. And thus to per- form (2), i.e. to monitor the system's behaviour during training, we only need to check further that the current state resulting from processing a word generated by the system, subsumes -is extendible to -one of the s i . We perform this through a sim- ple bitmask operation (recall that the states are bi- nary). The simulation can thus semantically iden- tify erroneous/out-of-domain actions (words) by the system. It would then terminate the learning episode and penalise the system immediately, aid- ing speed of training significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We have so far induced two prototype dialogue systems, one in an 'electronics shopping' domain (see <ref type="bibr" target="#b16">Kalatzis et al. (2016)</ref> and <ref type="figure" target="#fig_1">Fig. 1</ref>) and another in a 'restaurant-search' domain, showing that fully incremental dialogue systems can be automati- cally induced from small amounts of unannotated dialogue transcripts ( <ref type="bibr" target="#b16">Kalatzis et al., 2016;</ref> -in this case both systems were boot- strapped from a single successful example dia- logue. We are in the process of evaluating these systems with real users.</p><p>In this paper, however, our focus is not on build- ing dialogue systems per se, but on: (1) study- ing and quantifying the interactional and struc- tural generalisation power of the DS-TTR gram- mar formalism (see Section 2), and that of sym- bolic, grammar-based approaches to language pro- cessing more generally. We focus here on spe- cific dialogue phenomena, such as mid-sentence self-corrections, hesitations, and restarts (see be- low); (2) doing the same for Bordes and We- ston's (2017) state-of-the-art, bottom up response retrieval model, without use of linguistic knowl- edge of any form; and (3) comparing (1) and <ref type="bibr">(2)</ref>.</p><p>In order to test and quantify the interactional and structural generalisation power/robustness of the two models, babble and memn2n, we need contrasting dialogue data-sets that control for in- teractional vs. lexical variations in the input dia- logues. Furthermore, to make our results compa- rable to the existing approach of Bordes and We- ston (2017), we need to use the same dataset that they have used. We therefore use Facebook AI Research's bAbI dialogue tasks dataset ( <ref type="bibr" target="#b1">Bordes et al., 2017</ref>). These are goal-oriented dialogues in the domain of restaurant search. Here we tackle Task 1, where in each dialogue the system asks the user about their preferences for the properties of a restaurant, and each dialogue results in an API call which contains values of each slot obtained. Other than the explicit API call notation, there are no an- notations in the data whatsoever.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The bAbI+ dataset</head><p>While containing some lexical variation, the origi- nal bAbI dialogues significantly lack interactional variation vital for natural real-life dialogue. In order to obtain such variation while holding lex- ical variation constant, we created the bAbI+ dataset by systematically transforming the bAbI dialogues. bAbI+ is an extension of the original bAbI Task 1 dialogues with everyday incremental dialogue phenomena (hesitations, restarts, and corrections -see below). While the original bAbI tasks 2-7 increase the user's goal complexity, modifications introduced in bAbI+ can be thought of as orthog- onal to this: we instead increase the complexity of surface forms of dialogue utterances, while keep- ing every other aspect of the task fixed.</p><p>The variations introduced in bAbI+ are: 1. Hesitations, e.g. as in "we will be uhm eight"; 2. Restarts, e.g. "can you make a restau- rant uhm yeah can you make a restaurant reservation for four people with french cuisine in a moderate price range"; and 3. Corrections affect- ing task-specific information -both short-distance ones correcting one token, e.g. "with french oh no spanish food", and long-distance NP/PP-level corrections, e.g. "with french food uhm sorry with spanish food".</p><p>The phenomena above are mixed in probabilis- tically from the fixed sets of templates to the origi- nal data 2 . The modifications affect a total of 11336 2 See</p><p>https://github.com/ishalyminov/babi_ tools utterances in the 3998 dialogues. Around 21% of user turns contain corrections, 40% hesitations, and 5% restarts (they are not mutually exclusive, so that an utterance can contain up to 3 modifi- cations). Our modifications, with respect to cor- rections in particular, are more conservative than those observed in real-world data: <ref type="bibr" target="#b14">Hough (2015)</ref> reports that self-corrections appear in 20% of all turns of natural conversations from the British Na- tional Corpus, and in 40% of turns in the Map Task, a corpus of human-human goal-oriented di- alogues. Here's part of an example dialogue in the bAbI+ corpus:</p><p>sys: hello what can I help you with today? usr: I'd like to book a uhm yeah I'd like to book a table in a expensive price range sys: I'm on it. Any preference on a type of cuisine? usr: with indian food no sorry with spanish food please</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Memory Network setup</head><p>In all the experiments we describe below, we fol- low Bordes and Weston's setup by using a memn2n (we took an open source Tensorflow implementa- tion for bAbI QA tasks and modified it 3 accord- ing to their setup -see details below). In or- der to adapt the data for the memn2n, we trans- form the dialogues into &lt;story, question, answer&gt; triplets. The number of triplets for a single dia- logue is equal to the number of the system's turns, and in each triplet, the answer is the current sys- tem's turn, the question is the user's turn preced- ing it, and the story is a list of all the previous turns among both sides. The memn2n hyperparameters are set as follows: 1 hop, and 128 as the size of embeddings; we train it for 100 epochs with a learning rate of 0.01 and a batch size of 8 -in this we follow the best bAbI Task 1 setup reported by <ref type="bibr" target="#b1">(Bordes et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Testing the DS-TTR parser</head><p>Dynamic Syntax (DS) lexicons are learnable from data ( <ref type="bibr">Eshghi et al., 2013a,b)</ref>. But since the lexicon was induced from a corpus of child-directed utter- ances in this prior work, there were some construc- tions as well as individual words that it did not include <ref type="bibr">4</ref> . One of the authors therefore extended this induced grammar manually to cover the bAbI dataset, which, despite not being very diverse, contains a wide range of complex grammatical constructions, such as long sequences of preposi- tional phrases, adjuncts, short answers to yes/no and wh-questions, appositions of NPs, causative verbs etc.</p><p>We parsed all dialogues in the bAbI train and test sets, as well as on the bAbI+ corpus word-by- word, including both user and system utterances, in context. The grammar parses 100% of the dia- logues, i.e. it does not fail on any word in any of the dialogues. We assess the semantic accuracy of the parser on bAbI &amp; bAbI+ using the dialogue- final api-calls in section 4.5 below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experiment 1: Generalisation from small data</head><p>We have now set out all we need to perform the first experiment. Our aim here is to assess the generalisation power that results from the gram- mar and our state encoding method (section 3.1) -we dub our overall model babble -and compare this to the state of the art results of <ref type="bibr" target="#b1">Bordes et al. (2017)</ref>. The method in <ref type="bibr" target="#b1">Bordes et al. (2017)</ref> is not generative, rather it is based on retrieval of sys- tem responses, based on the history of the dialogue up to that point. Therefore, for direct comparison, and for simplicity of exposition, we do the same here: we apply the method described for creating a user simulation (section 3.2.1), this time for the system side, resulting in a 'system simulation'. We then use this to predict a system response, by pars- ing and encoding the containing test dialogue up to the point immediately prior to the system turn. This results in a triggering state, s trig , which is then used as the key to look up the system's re- sponse from the rules constructed as per section 3.2.1. The returned response is then parsed word- by-word as normal, and this same process con- tinues for the rest of the dialogue. This method uses the full machinery of DS-TTR &amp; our state- encoding method -the babble model -and will thus reflect the generalisation properties that we are interested in.</p><p>Cross-Validation Since we are here interested in data efficiency and generalisation we use all the bAbI and bAbI+ data -the train, dev, and test sets -as follows: we train Bordes &amp; Weston's memn2n and babble from 1-5 examples selected at random from the longest dialogues in bAbI -note bAbI+ data is never used for training in these experi- ments. This process is repeated across 10 folds.</p><p>The models are then tested on sets of 1000 ex- amples selected at random, in each fold. Both the training and test sets constructed in this way are kept constant in each fold across the babble &amp; memn2n models. The test sets are selected either exclusively from bAbI or exclusively from bAbI+. <ref type="table">Table 1</ref> shows per utterance accuracies for the bab- ble &amp; memn2n models. Per utterance accuracy is the percentage of all system turns in the test di- alogues that were correctly predicted. The table shows that babble can generalise to a remarkable 74% of bAbI and 65% of bAbI+ with only 5 input dialogues from bAbI. It also shows that memn2ns can also generalise remarkably well. Although as discussed below, this result is misleading on its own as the memn2ns are very poor at generating the final api-calls correctly on both the bAbI &amp; bAbI+ data, and are thus making too many seman- tic mistakes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Results: Predicting system turns</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experiment 2: Semantic Accuracy</head><p>The results from Experiment 1 on their own can be misleading, as correct prediction of system re- sponses does not in general tell us enough about how well the models are interpreting the dia- logues, or whether they are doing this with a suf- ficient level of granularity. To assess this, in this second experiment, we measure the semantic ac- curacy of each model by looking exclusively at how accurately they predict the final api-calls in the bAbI &amp; bAbI+ datasets. For the memn2n model, we follow the same overall procedure as in the previous experiment: train on bAbI data, and test on bAbI+.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Results: Prediction of api-calls</head><p>BABBLE Mere successful parsing of all the di- alogues in the bAbI and bAbI+ datasets as shown above doesn't mean that the semantic representa- tions compiled for the dialogues were in fact cor- rect. To measure the semantic accuracy of the DS- TTR parser we programmatically checked that the correct slot values -those in the api-call anno- tations -were in fact present in the semantic rep- resentations produced by the parser for each dia- logue (see <ref type="figure" target="#fig_3">Fig. 2</ref> for example semantic representa- tions). We further checked that there is no other in- correct slot value present in these representations.</p><p>The results showed that the parser has 100% se-# of training dialogues: mantic accuracy on both bAbI and bAbI+ <ref type="bibr">5</ref> . This result is not surprising, given that DS-TTR is a general model of incremental language process- ing, including phenomena such as self-corrections and restarts (see <ref type="bibr" target="#b14">Hough (2015)</ref> for details of the model).</p><p>MEMN2N Given just 1 to 5 training instances from bAbI as in the previous experiment, the mean api-call prediction accuracy of the memn2n model is nearly 0 on both bAbI and bAbI+. This is not at all unexpected, since prediction of the api-calls is a generative process, unlike the prediction of system turns which can be done on a retrieval/look-up basis alone. For this, the model needs to observe the different word sequences that might determine each parameter (slot) value, and observe them with sufficient frequency and vari- ation. This is unlike a semantic parser like DS- TTR, that produces semantic representations for the dialogues as a result of the structural, linguis- tic knowledge that it embodies. Nevertheless, we were also interested in the general semantic robustness of the memn2n model, to the transformations in bAbI+, i.e. how well does the memn2n model interpret bAbI+ dialogues, when trained on the full bAbI dataset? Does it then learn to generalise to (process) the bAbI+ di- alogues with sufficient semantic accuracy? <ref type="table">Table 2</ref> shows that we can fully replicate the re- sults reported in <ref type="bibr" target="#b1">Bordes et al. (2017)</ref>: the memn2n model can predict the api-calls with 100% ac- curacy, when trained on the bAbI train-set and tested on the bAbI test-set. But when this same model is tested on bAbI+, the accuracy drops to a 5 A helpful reviewer points out that the DS-TTR setup is a carefully tuned rule-based system, thus perhaps rendering these results trivial. But we note that the results here are not due to ad-hoc constructions of rules/lexicons, but due to the generality of the grammar model, and its attendant incremen- tal, left-to-right properties; and that the same parser can be used in other domains. Furthermore, the ability to process self-corrections, restarts, etc. "comes for free", without the need to add or posit new machinery testing configuration accuracy memn2n on bAbI 100 memn2n on bAbI+ 28 <ref type="table">Table 2</ref>: api-call prediction accuracies (%) for the memn2n model trained on the bAbI trainset very poor 28%, making any dialogue system built using this model unusable in the face of natural, spontaneous dialogue data. This is further dis- cussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">babble</head><p>The method described above has the following advantages over previous approaches to dialogue system development: -incremental (and thus more natural) language understanding, dialogue management, and gener- ation;</p><p>-"end-to-end" method for task-based systems: no Dialogue Act annotations are required (i.e. re- duced development time and effort);</p><p>-a complete dialogue system for a new task can be automatically induced, using only 'raw' data - i.e. successful dialogue transcripts;</p><p>-the MDP state and action spaces are automat- ically induced, rather than having to be designed by hand (as in prior work);</p><p>-wide-coverage, task-based dialogue systems can be built from much smaller amounts of data as shown in section 4 .</p><p>This final point bears further examination. As an empirically adequate model of incremental lan- guage processing in dialogue, the DS-TTR gram- mar is required to capture interactional variants such as question-answer pairs, over-and under- answering, self-and other-corrections, clarifica- tion, split-utterances, and ellipsis more generally. As we showed in section 4, even if most of these structures are not present in the training exam-ple(s), the resulting trained system is able to han- dle them, thus resulting in a very significant gen- eralisation around the original data.</p><p>We also note that since we were in this instance interested in a direct comparison with memn2ns over the bAbI &amp; bAbI+ datasets, we didn't ex- ploit the power of Reinforcement Learning and exploration as we described above -as we have done before with other systems ( <ref type="bibr" target="#b16">Kalatzis et al., 2016)</ref>. Therefore the generalisation results we report above for babble follow entirely from the knowledge present within the grammar as a com- putational model of dialogue processing and con- textual update, rather than this having been learned from data. Applying the full RL method described above would have meant that the system would actually discover many interactional and syntac- tic variations that are not present in bAbI, nor in bAbI+.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">memn2n</head><p>Even when trained on very few training instances, the memn2n model was able to predict system re- sponses remarkably well. But results from Exper- iment 2 above showed that this was misleading: the memn2ns were making a drastic number of se- mantic mistakes when interpreting the dialogues, both in the bAbI and bAbI+ datasets. Even when trained on the full bAbI data-set, the model per- formed badly on bAbI+ in terms of semantic ac- curacy. We diagnose these results as follows: Problem complexity: The first thing to notice is that in bAbI dialogue Task 1, the responses are highly predictable and stay constant regardless of the actual task details (slot values) up to the point of the final api-calls; and further, that the prediction of api-calls is a generative process, unlike the prediction of the system turns, which is retrieval-based. This, in our view, explains the very large difference in memn2n performance across the two prediction tasks.</p><p>Model robustness to the bAbI+ transforma- tions:. The variations introduced in bAbI+ are repetitions of both content and non-content words, as well additional incorrect slot values. The model was working in the same setup as babble, there- fore none of those variations could be treated as unknown tokens for either system. Although in the case of memn2n, some of the mixed-in words never appeared in the training data, and bAbI+ ut- terances were augmented significantly with those words -so it was interesting to see how such un- trained embeddings would affect the latent mem- ory representations inside memn2n. The resulting performance suggests that there was no signifi- cant impact on memn2n from these variations as far as the predicting system responses was con- cerned. But the incorrect slot values introduced in self-corrections affect the system's task com- pletion performance significantly, only appearing at the point of api-call predictions.</p><p>We note also that none of our experiments in this paper involved training memn2n on bAbI+ data. There is a very interesting question here: is the memn2n model in principle able to learn to pro- cess the bAbI+ structures if it is in fact trained on it? And how much bAbI+ data would it require to do so? These issues are address in detail in .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Our main advances are in a) training end-to-end dialogue systems from small amounts of data, b) incremental processing for wider coverage of more natural everyday dialogues (e.g. containing self-repairs).</p><p>We compared our grammar-based approach to dialogue processing (DS-TTR) with a state- of-the-art, end-to-end response retrieval model (memn2ns) ( <ref type="bibr" target="#b1">Bordes et al., 2017)</ref>, when training on small amounts of dialogue data.</p><p>Our experiments show that our model can pro- cess 74% of the Facebook AI bAbI dataset even when trained on only 0.13% of the data (5 dia- logues). It can in addition process 65% of bAbI+, a corpus we created by systematically adding in- cremental dialogue phenomena such as restarts and self-corrections to bAbI. We find on the other hand that the memn2n model is not robust to the structures we introduced in bAbI+, even when trained on the full bAbI dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Interactional</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Some Interactional Variations in a Shopping Domain in fact even from just a few examples of successful dialogues-to a large range of interactional and syntactic variations, including everyday natural incremental phenomena.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 ,</head><label>4</label><figDesc>and see Eshghi et al. (2015); Purver et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Incremental parsing using DS-TTR</figDesc><graphic url="image-1.png" coords="4,72.00,190.40,453.55,57.38" type="bitmap" /></figure>

			<note place="foot" n="3"> See https://github.com/ishalyminov/memn2n 4 We are currently looking into applying Eshghi et al.&apos;s (2013a) model to induce DS grammars from larger semantic corpora such as the Groningen Meaning Bank, leading to much more wide-coverage lexicons</note>

			<note place="foot" n="6"> https://sites.google.com/site/ hwinteractionlab/babble</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research is supported by the EPSRC, un-der grant number EP/M01553X/1 (BABBLE project 6 ).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Incremental dialogue system faster than and preferred to its nonincremental counterpart</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Aist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Campana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">Gomez</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Stoness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tanenhaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the Congitive Science Society</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning end-to-end goal-oriented dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=S1Bb3D5gg" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The Dynamics of Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronnie</forename><surname>Cann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Kempson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lutz</forename><surname>Marten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Elsevier</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Records and record types in semantic theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Logic and Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="112" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Feedback in conversation as incremental semantic update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eshghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Howes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gregoromichelaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Purver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Computational Semantics (IWCS 2015). Association for Computational Linguisitics</title>
		<meeting>the 11th International Conference on Computational Semantics (IWCS 2015). Association for Computational Linguisitics<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Dylan: Parser for dynamic syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eshghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Purver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hough</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>Queen Mary University of London</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DS-TTR: An incremental, semantic, contextual parser for dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Eshghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Semdial 2015 (goDial), the 19th workshop on the semantics and pragmatics of dialogue</title>
		<meeting>Semdial 2015 (goDial), the 19th workshop on the semantics and pragmatics of dialogue</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incremental grammar induction from childdirected dialogue utterances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Eshghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Annual Workshop on Cognitive Modeling and Computational Linguistics (CMCL)</title>
		<meeting>the 4th Annual Workshop on Cognitive Modeling and Computational Linguistics (CMCL)<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="94" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conversational interactions: Capturing dialogue dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Eshghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Kempson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Gregoromichelaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">From Quantification to Conversation: Festschrift for Robin Cooper on the occasion of his 65th birthday</title>
		<editor>S. Larsson and L. Borin</editor>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>College Publications</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="325" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How domaingeneral can we be? Learning incremental dialogue systems without dialogue acts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Eshghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Semdial 2014 (DialWatt)</title>
		<meeting>Semdial 2014 (DialWatt)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Probabilistic grammar induction in an incremental semantic framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Eshghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yo</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CSLP, Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Interactional Dynamics and the Emergence of Language Games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Eshghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Shalyminov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ESSLLI 2017 workshop on Formal approaches to the Dynamics of Linguistic Interaction</title>
		<meeting>the ESSLLI 2017 workshop on Formal approaches to the Dynamics of Linguistic Interaction<address><addrLine>Barcelona</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The Interactive Stance: Meaning for Conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ginzburg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Incremental semantics driven natural language generation with self-repairing capability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Natural Language Processing (RANLP). Hissar</title>
		<meeting><address><addrLine>Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="79" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Modelling Incremental SelfRepair Processing in Dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hough</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Queen Mary University of London</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Probabilistic type theory for incremental dialogue processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W14-1410" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS)</title>
		<meeting>the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS)<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="80" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bootstrapping incremental dialogue systems: using linguistic knowledge to learn from minimal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kalatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Eshghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS 2016 workshop on Learning Methods for Dialogue</title>
		<meeting>the NIPS 2016 workshop on Learning Methods for Dialogue<address><addrLine>Barcelona</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Dynamic Syntax: The Flow of Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Kempson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilfried</forename><surname>Meyer-Viol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dov</forename><surname>Gabbay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Blackwell</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scaling semantic parsers with on-thefly ontology matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><forename type="middle">E</forename><surname>Tom Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Combined distributional and logical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="179" to="192" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<idno type="doi">10.1109/TPAMI.2006.79</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2006.79" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="594" to="611" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">BURLAP: BrownUMBC Reinforcement Learning and Planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mcglashan</surname></persName>
		</author>
		<ptr target="http://burlap.cs.brown.edu/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Incremental semantic construction in a dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Eshghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Computational Semantics</title>
		<editor>J. Bos and S. Pulman</editor>
		<meeting>the 9th International Conference on Computational Semantics<address><addrLine>Oxford, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="365" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">You&apos;s: Context, speech acts and grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Gregoromichelaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilfried</forename><surname>Meyer-Viol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronnie</forename><surname>Cann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aspects of Semantics and Pragmatics of Dialogue. SemDial 2010, 14th Workshop on the Semantics and Pragmatics of Dialogue. Polish Society for Cognitive Science, Pozna´nPozna´n</title>
		<editor>P. Łupkowski and M. Purver</editor>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
	<note>Splitting the &apos;I&apos;s and crossing the</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dialogue and compound contributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Gregoromichelaki</surname></persName>
		</author>
		<ptr target="http://www.cambridge.org/us/academic/subjects/engineering/communications-and-signal-processing/natural-language-generation-interactive-systems" />
	</analytic>
	<monogr>
		<title level="m">Natural Language Generation in Interactive Systems</title>
		<editor>S. Bangalore and A. Stent, editors</editor>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="63" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Challenging Neural Dialogue Models with Natural Data: Memory Networks Fail on Incremental Phenomena</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Shalyminov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Eshghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Workshop on the Semantics and Pragmatics of Dialogue</title>
		<meeting>the 21st Workshop on the Semantics and Pragmatics of Dialogue</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards incremental speech generation in dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Skantze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Hjalmarsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGDIAL 2010 Conference</title>
		<meeting>the SIGDIAL 2010 Conference<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A neural network approach to context-sensitive generation of conversational responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno>arXiv (1506.06714</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<title level="m">A neural conversational model. arXiv (1506.05869v3)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-domain neural network language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gaši´cgaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Mrkši´cmrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the 2016 Conference on North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A networkbased end-to-end trainable task-oriented dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Mrkši´cmrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Gaši´cgaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<idno>arXiv preprint: 1604.04562</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
