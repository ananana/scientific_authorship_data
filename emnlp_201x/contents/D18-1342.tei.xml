<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Breaking the Beam Search Curse: A Study of (Re-)Scoring Methods and Stopping Criteria for Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Oregon State University Corvallis</orgName>
								<address>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Oregon State University Corvallis</orgName>
								<address>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Research Sunnyvale</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Oregon State University Corvallis</orgName>
								<address>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Research Sunnyvale</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Breaking the Beam Search Curse: A Study of (Re-)Scoring Methods and Stopping Criteria for Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3054" to="3059"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3054</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Beam search is widely used in neural machine translation, and usually improves translation quality compared to greedy search. It has been widely observed that, however, beam sizes larger than 5 hurt translation quality. We explain why this happens, and propose several methods to address this problem. Furthermore , we discuss the optimal stopping criteria for these methods. Results show that our hyperparameter-free methods outperform the widely-used hyperparameter-free heuristic of length normalization by +2.0 BLEU, and achieve the best results among all methods on Chinese-to-English translation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, neural machine translation (NMT) has surpassed traditional phrase-based or syntax- based machine translation, becoming the new state of the art in MT ( <ref type="bibr" target="#b4">Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b16">Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2014)</ref>. While NMT training is typically done in a "local" fashion which does not employ any search (bar notable exceptions such as <ref type="bibr" target="#b12">Ranzato et al. (2016)</ref>, <ref type="bibr" target="#b14">Shen et al. (2016)</ref>, and <ref type="bibr" target="#b18">Wiseman and Rush (2016)</ref>), the decoding phase of all NMT systems universally adopts beam search, a widely used heuristic, to improve translation quality.</p><p>Unlike phrase-based MT systems which enjoy the benefits of very large beam sizes (in the or- der of 100-500) ( <ref type="bibr" target="#b6">Koehn et al., 2007)</ref> , most NMT systems choose tiny beam sizes up to 5; for exam- ple, Google's GNMT (  and Face- book's ConvS2S ( <ref type="bibr" target="#b1">Gehring et al., 2017</ref>) use beam sizes 3 and 5, respectively. Intuitively, the larger the beam size is, the more candidates it explores, and the better the translation quality should be. While this definitely holds for phrase-based MT systems, surprisingly, it is not the case for NMT: many researchers observe that translation qual- ity degrades with beam sizes beyond 5 or 10 ( <ref type="bibr" target="#b17">Tu et al., 2017;</ref><ref type="bibr" target="#b7">Koehn and Knowles, 2017)</ref>. We call this phenomenon the "beam search curse", which is listed as one of the six biggest challenges for NMT ( <ref type="bibr" target="#b7">Koehn and Knowles, 2017)</ref>.</p><p>However, there has not been enough attention on this problem. <ref type="bibr" target="#b3">Huang et al. (2017)</ref> hint that length ratio is the problem, but do not explain why larger beam sizes cause shorter lengths and worse BLEU. <ref type="bibr" target="#b10">Ott et al. (2018)</ref> attribute it to two kinds of "uncertainties" in the training data, namely the copying of source sentence and the non-literal translations. However, the first problem is only found in European language datasets and the sec- ond problem occurs in all datasets but does not seem to bother pre-neural MT systems. Therefore, their explanations are not satisfactory.</p><p>On the other hand, previous work adopts several heuristics to address this problem, but with vari- ous limitations. For example, RNNSearch <ref type="bibr" target="#b0">(Bahdanau et al., 2014</ref>) and ConvS2S use length nor- malization, which (we will show in Sec. 6) seems to somewhat alleviate the problem, but far from being perfect. Meanwhile,  and <ref type="bibr" target="#b3">Huang et al. (2017)</ref> use word-reward, but their re- ward is a hyper-parameter to be tuned on dev set.</p><p>Our contributions are as follows:</p><p>• We explain why the beam search curse exists, supported by empirical evidence (Sec. 3).</p><p>• We review existing rescoring methods, and then propose ours to break the beam search curse (Sec. 4). We show that our hyperparameter-free methods outperfrom the previous hyperparameter-free method (length normalization) by +2.0 BLEU (Sec. 6).</p><p>• We also discuss the stopping criteria for our rescoring methods (Sec. 5). Experiments</p><p>show that with optimal stopping alone, the translation quality of the length normaliza- tion method improves by +0.9 BLEU.</p><p>After we finish our paper, we became aware of a parallel work <ref type="bibr" target="#b9">(Murray and Chiang, 2018</ref>) that also reveals the same root cause we found for the beam search curse: the length ratio problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries: NMT and Beam Search</head><p>We briefly review the encoder-decoder architec- ture with attention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>). An RNN encoder takes an input sequence x = (x 1 , ..., x m ), and produces a sequence of hidden states. For each time step, the RNN de- coder will predict the probability of next output word given the source sequence and the previously generated prefix. Therefore, when doing greedy search, at time step i, the decoder will choose the word with highest probability as y i . The decoder will continue generating until it emits &lt;/eos&gt;. In the end, the generated hypothesis is y = (y 1 , ..., y n ) with y n = &lt;/eos&gt;, with model score</p><formula xml:id="formula_0">S (x, y) = |y| i=1 log p(y i | x, y 1..{i−1} ) (1)</formula><p>As greedy search only explores a single path, we always use beam search to improve search quality. Let b denote the beam size, then at step i the beam B i is an ordered list of size b:</p><formula xml:id="formula_1">B 0 = [&lt;s&gt;, p(&lt;s&gt; | x)] B i = b top{{y • y i , s·p(y i |x, y) | y , s ∈ B i−1 }</formula><p>In the most naive case, after reaching the maxi- mum length (a hard limit), we get N possible can- didate sequences {y 1 , ..., y N }. The default strat- egy chooses the one with highest model score. We will discuss more sophistcated ways of stopping and choosing candidates in later sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Beam Search Curse</head><p>The most popular translation quality metric, BLEU ( <ref type="bibr" target="#b11">Papineni et al., 2002</ref>), is defined as:</p><formula xml:id="formula_2">BLEU = bp · exp(1/4 4 n=1 log p n ) (2) where bp = min{e 1−1/lr , 1}<label>(3)</label></formula><p>where lr = |y|/|y * |</p><p>Here p n are the n-gram precisions, and |y| and |y * | denote the hypothesis and reference lengths, while bp is the brevity penalty (penalizing short brevity penalty bp = min{e 1−1/lr , 1} <ref type="figure">Figure 1</ref>: As beam size increases beyond 3, BLEU score on the dev set gradually drops. All terms are cal- culated by multi-bleu.pl. translations) and lr is the length ratio ( <ref type="bibr" target="#b15">Shi et al., 2016;</ref><ref type="bibr" target="#b7">Koehn and Knowles, 2017)</ref>, respectively. With beam size increasing, |y| decrases, which causes the length ratio to drop, as shown in <ref type="figure">Fig. 1</ref>. Then the brevity penalty term, as a function of the length ratio, decreases even more severely. Since bp is a key factor in BLEU, this explains why the beam search curse happens. <ref type="bibr">1</ref> The reason why |y| decreases as beam size in- creases is actually twofold:</p><p>1. As beam size increases, the more candidates it could explore. Therefore, it becomes eas- ier for the search algorithm to find the &lt;/eos&gt; symbol. <ref type="figure" target="#fig_0">Fig. 2</ref> shows that the &lt;/eos&gt; indices decrease steadily with larger beams. 2. Then, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, shorter candidates have clear advantages w.r.t. model score.</p><p>Hence, as beam size increases, the search algo- rithm will generate shorter candidates, and then prefer even shorter ones among them. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Rescoring Methods</head><p>We first review existing methods to counter the length problem and then propose new ones to ad- dress their limitations. In particular, we propose to predict the target length from the source sentence, in order to choose a hypothesis with proper length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Previous Rescoring Methods</head><p>RNNSearch ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>) first intro- duces the length normalization method, whose score is simply the average model score:</p><formula xml:id="formula_4">ˆ S length norm (x, y) = S (x, y)/|y|<label>(5)</label></formula><p>This is the most widely used rescoring method since it is hyperparameter-free. GNMT ( ) incorporates length and coverage penalty into the length normalization method, while also adding two hyperparameters to adjust their influences. (please check out their pa- per for exact formulas).</p><p>Baidu NMT (He et al., 2016) borrows the Word Reward method from pre-neural MT, which gives a reward r to every word generated, where r is a hyperparameter tuned on the dev set:</p><formula xml:id="formula_5">ˆ S WR (x, y) = S (x, y) + r · |y|<label>(6)</label></formula><p>Based on the above, <ref type="bibr" target="#b3">Huang et al. (2017)</ref> propose a variant called Bounded Word-Reward which only rewards up to an "optimal" length. This length is calculated using a fixed "generation ra- tio" gr , which is the ratio between target and source sequence length, namely the average num- ber of target words generated for each source word. It gives reward r to each word up to a bounded length L(x, y) = min{|y|, gr · |x|}:</p><formula xml:id="formula_6">ˆ S BWR (x, y) = S (x, y) + r · L(x, y) (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Rescoring with Length Prediction</head><p>To remove the fixed generation ratio gr from Bounded Word-Reward, we use a 2-layer MLP, which takes the mean of source hidden states as input, to predict the generation ratio gr * (x). Then we replace the fixed ratio gr with it, and get our predicted length L pred (x) = gr * (x) · |x|.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Bounded Word-Reward</head><p>With predicted length, the new predicted bound and final score would be:</p><formula xml:id="formula_7">L * (x, y) = min{|y|, L pred (x)} (8) ˆ S BWR * (x, y) = S (x, y) + r · L * (x, y)<label>(9)</label></formula><p>While the predicted length is more accurate, there is still a hyperparameter r (word reward), so we design two methods below to remove it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Bounded Adaptive-Reward</head><p>We propose Bounded Adaptive-Reward to auto- matically calculate proper reward based on the current beam. With beam size b, the reward for time step t is the average negative log-probability of the words in the current beam.</p><formula xml:id="formula_8">r t = −(1/b) b i=1 log p(word i )<label>(10)</label></formula><p>Its score is very similar to <ref type="formula">(7)</ref>:</p><formula xml:id="formula_9">ˆ S AdaR (x, y) = S (x, y) + L * t=1 r t<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">BP-Norm</head><p>Inspired by the BLEU score definition, we propose BP-Norm method as follows:</p><formula xml:id="formula_10">ˆ S bp (x, y) = log bp + S (x, y)/|y| (12)</formula><p>bp is the same brevity penalty term as in <ref type="formula" target="#formula_2">(3)</ref>. Here, we regard our predicted length as the reference length. The beauty of this method appears when we drop the logarithmic symbol in <ref type="formula">(12)</ref>:</p><formula xml:id="formula_11">exp( ˆ S bp (x, y)) = bp · |y| i=1 p(y i |...) 1/|y| = bp ·exp 1 |y| |y| i=1 log p(y i |...)</formula><p>which is in the same form of BLEU score (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Stopping Criteria</head><p>Besides rescoring methods, the stopping criteria (when to stop beam search) is also important, for both efficiency and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Conventional Stopping Criteria</head><p>By default, OpenNMT-py ( <ref type="bibr" target="#b5">Klein et al., 2017</ref>) stops when the topmost beam candidate stops, be- cause there will not be any future candidates with higher model scores. However, this is not the case for other rescoring methods; e.g., the score of length normalization (5) could still increase. Another popular stopping criteria, used by RNNSearch ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>), stops the beam search when exactly b finished candidates have been found. Neither method is optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Optimal Stopping Criteria</head><p>For Bounded Word-Reward, <ref type="bibr" target="#b3">Huang et al. (2017)</ref> introduces a provably-optimal stopping criterion that could stop both early and optimally. We also introduce an optimal stopping criterion for BP- Norm. Each time we generate a finished candi- date, we update our best scorê S . Then, for the topmost beam candidate of time step t, we have:</p><formula xml:id="formula_12">ˆ S bp = S t,0 t + min{1 − L pred t , 0} ≤ S t,0 R (13)</formula><p>where R is the maximum generation length. Since S t,0 will drop after time step t, if</p><formula xml:id="formula_13">S t,0</formula><p>R ≤ ˆ S , we reach optimality. This stopping criterion could also be applied to length normalization (5).</p><p>Meawhile, for Bounded Adaptive-Reward, we can have a similar optimal stopping criterion: If the score of topmost beam candidate at time step t &gt; L pred is lower thanˆSthanˆ thanˆS , we reach optimality.</p><p>Proof. The first part ofˆSofˆ ofˆS AdaR in (11) will decrease after time step t, while the second part stays the same when t &gt; L pred . So the score in the future will monotonically decrease.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>Our experiments are on Chinese-to-English trans- lation task, based on the OpenNMT-py codebase. <ref type="bibr">4</ref> We train our model on 2M sentences, and ap- ply BPE (Sennrich et al., 2015) on both sides, which reduces Chinese and English vocabulary sizes down to 18k and 10k respectively. We then exclude pairs with more than 50 source or target tokens. We validate on NIST 06 and test on NIST 08 (newswire portions only for both). We report case-insensitive, 4 reference BLEU scores. We use 2-layers bidirectional LSTMs for the encoder. We train the model for 15 epochs, and choose the one with lowest perplexity on the dev set. Batch size is 64; both word embedding and hidden state sizes 500; and dropout 0.3. The total parameter size is 28.5M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Parameter Tuning and Results</head><p>We compare all rescoring methods mentioned above. For the length normalization method, we also show its results with optimal stopping.</p><p>For Bounded Word-Reward method with and without our predicted length, we choose the best r on the dev set seperately. The length normal-  ization used by  has two hyper- parameters, namely α for length penalty and β for coverage penalty. We jointly tune them on the dev set, and choose the best config. (α=0.3, β=0.3). <ref type="figure" target="#fig_2">Figure 4</ref> show our results on the dev set. We see that our proposed methods get the best per- formance on the dev set, and continue growing as beam size increases. We also observe that op- timal stopping boosts the performance of length normalization method by around +0.9 BLEU. In our experiments, we regard our predicted length as the maximum generation length in (13). We further observe from <ref type="figure" target="#fig_4">Fig. 5</ref> that our methods keep the length ratio close to 1, and greatly improve the quality on longer input sentences, which are noto- riously hard for NMT <ref type="bibr" target="#b14">(Shen et al., 2016)</ref>. <ref type="table" target="#tab_2">Table 1</ref> collects our results on both dev and test sets. Without loss of generality, we show results with both small and large beam sizes, which aver- age over b=14,15,16 and b=39,40,41, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Discussion</head><p>From <ref type="table" target="#tab_2">Table 1</ref>, we could observe that with our length prediction model, Bounded word-reward method gains consistent improvement. On the other hand, results from length normalization method show that optimal stopping technique <ref type="bibr">Small beam (b = 14, 15, 16)</ref> dev test gains significant improvement by around +0.9 BLEU. While with both, our proposed methods beat all previous methods, and gain improvement over hyperparameter-free baseline (i.e. length nor- malization) by +2.0 BLEU. Among our proposed methods, Bounded word- reward has the reward r as an hyper-parameter, while the other two methods get rid of that. Among them, we recommend the BP-Norm method, because it is the simplest method, and yet works equally well with others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We first explain why the beam search curse exists and then formalize all previous rescoring methods. Beyond that, we also propose several new methods to address this problem. Results from the Chinese- English task show that our hyperparameter-free methods beat the hyperparameter-free baseline (length normalization) by +2.0 BLEU.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Searching algorithm with larger beams generates &lt;/eos&gt; earlier. We use the average first, second and third &lt;/eos&gt; positions on the dev set as an example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Candidate lengths vs. model score. This scatter plot is generated from 242 finished candidates when translated from one source sequence with beam size 80.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The BLEU scores and length ratios (lr = |y|/|y * |) of various rescoring methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: BLEU scores and length ratios on the dev set over various input sentence lengths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Average BLEU scores and length ratios over small and large beams. indicates our methods.</figDesc><table></table></figure>

			<note place="foot" n="1"> The length ratio is not just about BLEU: if the hypothesis length is only 75% of reference length, something that should have been translated must be missing; i.e., bad adequacy. Indeed, Murray and Chiang (2018) confirm the same phenomenon with METEOR. 2 Pre-neural SMT models, being probabilistic, also favor short translations (and derivations), which is addressed by word (and phrase) reward. The crucial difference between SMT and NMT is that the former stops when covering the whole input, while the latter stops on emitting &lt;/eos&gt;.</note>

			<note place="foot" n="3"> Murray and Chiang (2018) attribute the fact that beam search prefers shorter candidates to the label bias problem (Lafferty et al., 2001) due to NMT&apos;s local normalization.</note>

			<note place="foot" n="4"> https://github.com/OpenNMT/OpenNMT-py</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Kenton Lee suggested the length prediction idea. This work was partially supported by DARPA N66001-17-2-4030, and NSF IIS-1817231 and IIS-1656051. We thank the anonymous reviewers for suggestions and Juneki Hong for proofreading.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>References Dzmitry Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improved neural machine translation with smt features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="151" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">When to finish? optimal beam search for neural text generation (modulo beam size)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In EMNLP</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">413</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">OpenNMT: Open-Source Toolkit for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions</title>
		<meeting>the 45th annual meeting of the ACL on interactive poster and demonstration sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Six challenges for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Knowles</surname></persName>
		</author>
		<idno>abs/1706.03872</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Correcting length bias in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WMT</title>
		<meeting>WMT</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Analyzing uncertainty in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00047</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Philadephia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Minimum risk training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Why neural translations are the right length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2278" to="2282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural machine translation with reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3097" to="3103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence learning as beam-search optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
