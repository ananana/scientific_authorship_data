<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Typed Tensor Decomposition of Knowledge Bases for Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<postCode>14850</postCode>
									<settlement>Ithaca</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<postCode>98052</postCode>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Typed Tensor Decomposition of Knowledge Bases for Relation Extraction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1568" to="1579"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>While relation extraction has traditionally been viewed as a task relying solely on textual data, recent work has shown that by taking as input existing facts in the form of entity-relation triples from both knowledge bases and textual data, the performance of relation extraction can be improved significantly. Following this new paradigm, we propose a tensor decomposition approach for knowledge base embedding that is highly scalable, and is especially suitable for relation extraction. By leveraging relational domain knowledge about entity type information, our learning algorithm is significantly faster than previous approaches and is better able to discover new relations missing from the database. In addition, when applied to a relation extraction task, our approach alone is comparable to several existing systems, and improves the weighted mean average precision of a state-of-the-art method by 10 points when used as a subcomponent.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Identifying the relationship between entities from free text, relation extraction is a key task for ac- quiring new facts to increase the coverage of a structured knowledge base. Given a pre-defined database schema, traditional relation extraction approaches focus on learning a classifier using tex- tual data alone, such as patterns between the oc- currences of two entities in documents, to deter- mine whether the entities have a particular rela- tion. Other than using the existing known facts to label the text corpora in a distant supervision setting ( <ref type="bibr" target="#b6">Bunescu and Mooney, 2007;</ref><ref type="bibr" target="#b21">Mintz et al., *</ref> Work conducted while interning at Microsoft Research.</p><p>2009; <ref type="bibr" target="#b25">Riedel et al., 2010;</ref><ref type="bibr" target="#b27">Ritter et al., 2013)</ref>, an existing knowledge base is typically not involved in the process of relation extraction.</p><p>However, this paradigm has started to shift re- cently, as researchers showed that by taking exist- ing facts of a knowledge base as an integral part of relation extraction, the model can leverage richer information and thus yields better performance. For instance, <ref type="bibr" target="#b26">Riedel et al. (2013)</ref> borrowed the idea of collective filtering and constructed a ma- trix where each row is a pair of entities and each column is a particular relation. For a true entity- relation triple (e 1 , r, e 2 ), either from the text cor- pus or from the knowledge base, the correspond- ing entry in the matrix is 1. A previously unknown fact (i.e., triple) can be discovered through ma- trix decomposition. This approach can be viewed as creating vector representations of each relation and candidate pair of entities. Because each entity does not have its own representation, relationships of any unpaired entities cannot be discovered. Al- ternatively, <ref type="bibr" target="#b34">Weston et al. (2013)</ref> created two types of embedding -one based on textual similarity and the other based on knowledge base, where the lat- ter maps each entity and relation to the same d- dimensional vector space using a model proposed by <ref type="bibr" target="#b4">Bordes et al. (2013a)</ref>. They also showed that combining these two models results in a signif- icant improvement over the model trained using only textual data.</p><p>To make such an integrated strategy work, it is important to capture all existing entities and rela- tions, as well as the known facts, from both tex- tual data and large databases. In this paper, we propose a new knowledge base embedding model, TRESCAL, that is highly efficient and scalable, with relation extraction as our target application. Our work is built on top of RESCAL ( <ref type="bibr" target="#b22">Nickel et al., 2011</ref>), which is a tensor decomposition method that has proven its scalability by factoring YAGO ( <ref type="bibr" target="#b3">Biega et al., 2013</ref>) with 3 million entities and 41 million triples ( <ref type="bibr" target="#b23">Nickel et al., 2012</ref>). We improve the tensor decomposition model with two technical innovations. First, we exclude the triples that do not satisfy the relational constraints (e.g., both arguments of the relation spouse-of need to be person entities) from the loss, which is done by selecting sub-matrices of each slice of the ten- sor during training. Second, we introduce a math- ematical technique that significantly reduces the computational complexity in both time and space when the loss function contains a regularization term. As a consequence, our method is more than four times faster than RESCAL, and is also more accurate in discovering unseen triples.</p><p>Our contributions are twofold. First, compared to other knowledge base embedding methods de- veloped more recently, it is much more efficient to train our model. As will be seen in Sec. 5, when applied to a large knowledge base created using NELL <ref type="bibr">(Carlson et al., 2010</ref>) that has 1.8M entity-relation triples, our method finishes training in 4 to 5 hours, while an alternative method (Bor- des et al., 2013a) needs almost 3 days. Moreover, the prediction accuracy of our model is competi- tive to others, if not higher. Second, to validate its value to relation extraction, we apply TRESCAL to extracting relations from a free text corpus along with a knowledge base, using the data provided in ( <ref type="bibr" target="#b26">Riedel et al., 2013</ref>). We show that TRESCAL is complementary to existing systems and signif- icantly improves their performance when using it as a subcomponent. For instance, this strategy im- proves the weighted mean average precision of the best approach in ( <ref type="bibr" target="#b26">Riedel et al., 2013</ref>) by 10 points (47% to 57%).</p><p>The remainder of this paper is organized as fol- lows. We survey most related work in Sec. 2 and provide the technical background of our approach in Sec. 3. Our approach is detailed in Sec. 4, fol- lowed by the experimental validation in Sec. 5. Fi- nally, Sec. 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our approach of creating knowledge base em- bedding is based on tensor decomposition, which is a well-developed mathematical tool for data analysis. Existing tensor decomposition models can be categorized into two main families: the CP and Tucker decompositions. The CP (CAN- DECOMP/PARAFAC) decomposition <ref type="bibr" target="#b15">(Kruskal, 1977;</ref><ref type="bibr" target="#b13">Kiers, 2000</ref>) approximates a tensor by a sum of rank-one tensors, while the Tucker decompo- sition <ref type="bibr" target="#b33">(Tucker, 1966)</ref>, also known as high-order SVD <ref type="bibr" target="#b10">(De Lathauwer et al., 2000</ref>), factorizes a ten- sor into a core tensor multiplied by a matrix along each dimension. A highly scalable distributional algorithm using the Map-Reduce architecture has been proposed recently for computing CP ( <ref type="bibr" target="#b12">Kang et al., 2012</ref>), but not for the Tucker decomposition, probably due to its inherently more complicated model form.</p><p>Matrix and tensor decomposition methods have been applied to modeling multi-relational data. For instance, <ref type="bibr" target="#b30">Speer et al. (2008)</ref> aimed to cre- ate vectors of latent components for representing concepts in a common sense knowledge base us- ing SVD. <ref type="bibr" target="#b11">Franz et al. (2009)</ref> proposed TripleRank to model the subject-predicate-object RDF triples in a tensor, and then applied the CP decomposition to identify hidden triples. Fol- lowing the same tensor encoding, <ref type="bibr" target="#b22">Nickel et al. (2011)</ref> proposed RESCAL, a restricted form of Tucker decomposition for discovering previously unknown triples in a knowledge base, and later demonstrated its scalability by applying it to YAGO, which was encoded in a 3M × 3M × 38 tensor with 41M triples ( <ref type="bibr" target="#b23">Nickel et al., 2012)</ref>.</p><p>Methods that revise the objective function based on additional domain information have been proposed, such as MrWTD, a multi-relational weighted tensor decomposition method ( <ref type="bibr" target="#b17">London et al., 2013)</ref>, coupled matrix and tensor fac- torization ( <ref type="bibr" target="#b24">Papalexakis et al., 2014)</ref>, and col- lective matrix factorization <ref type="bibr" target="#b28">(Singh and Gordon, 2008)</ref>. Alternatively, instead of optimizing for the least-squares reconduction loss, a non-parametric Bayesian approach for 3-way tensor decomposi- tion for modeling relational data has also been pro- posed <ref type="bibr" target="#b32">(Sutskever et al., 2009)</ref>. Despite the exis- tence of a wide variety of tensor decomposition models, most methods do not scale well and have only been tested on datasets that are much smaller than the size of real-world knowledge bases.</p><p>Multi-relational data can be modeled by neural- network methods as well. For instance, <ref type="bibr" target="#b5">Bordes et al. (2013b)</ref> proposed the Semantic Matching En- ergy model (SME), which aims to have the same d-dimensional vector representations for both en- tities and relations. Given the vectors of entities e 1 , e 2 and relation r. They first learn the latent representations of (e 1 , r) and (e 2 , r). The score of (e 1 , r, e 2 ) is determined by the inner product of the vectors of (e 1 , r) and (e 2 , r). Later, they proposed a more scalable method called translat- ing embeddings (TransE) ( <ref type="bibr" target="#b4">Bordes et al., 2013a</ref>). While both entities and relations are still repre- sented by vectors, the score of (e 1 , r, e 2 ) becomes the negative dissimilarity measure of the corre- sponding vectors −−e i + r k − e j , motivated by the work in ( <ref type="bibr" target="#b20">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b19">Mikolov et al., 2013a</ref>). Alternatively, <ref type="bibr" target="#b29">Socher et al. (2013)</ref> pro- posed a Neural Tensor Network (NTN) that repre- sents entities in d-dimensional vectors created sep- arately by averaging pre-trained word vectors, and then learns a d × d × m tensor describing the inter- actions between these latent components in each of the m relations. All these methods optimize for loss functions that are more directly related to the true objective -the prediction accuracy of cor- rect entity-relation triples, compared to the mean- squared reconstruction error in our method. Nev- ertheless, they typically require much longer train- ing time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>In this section, we first describe how entity- relation triples are encoded in a tensor. We then introduce the recently proposed tensor decompo- sition method, RESCAL (Nickel et al., 2011) and explain how it adopts an alternating least-squares method, ASALSAN ( <ref type="bibr" target="#b0">Bader et al., 2007)</ref>, to com- pute the factorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoding Binary Relations in a Tensor</head><p>Suppose we are given a knowledge base with n entities and m relation types, and the facts in the knowledge base are denoted as a set of entity-relation triples T = {(e i , r k , e j )}, where i, j ∈ {1, 2, · · · n} and k ∈ {1, 2, · · · m}. A triple (e i , r k , e j ) simply means that the i-th en- tity and the j-th entity have the k-th relation. Following ( <ref type="bibr" target="#b11">Franz et al., 2009</ref>), these triples can naturally be encoded in a 3-way tensor X ∈ {0, 1} n×n×m , such that X i,j,k = 1 if and only if the triple (e i , r k , e j ) ∈ T 1 . The tensor can be viewed as consisting of m slices, where each slice is an n×n square matrix, denoting the interactions of the entities of a particular relation type. In the remainder of this paper, we will use X k to refer to the k-th slice of the tensor X . <ref type="figure">Fig. 1</ref>   <ref type="figure">Figure 1</ref>: A tensor encoding of m binary relation types and n entities. A slice X k denotes the entities having the k-th relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RESCAL</head><p>In order to identify latent components in a ten- sor for collective learning, <ref type="bibr" target="#b22">Nickel et al. (2011)</ref> proposed RESCAL, which is a tensor decomposi- tion approach specifically designed for the multi- relational data described in Sec. 3.1. Given a ten- sor X n×n×m , RESCAL aims to have a rank-r ap- proximation, where each slice X k is factorized as</p><formula xml:id="formula_0">X k ≈ AR k A T .</formula><p>(1)</p><p>A is an n × r matrix, where the i-th row denotes the r latent components of the i-th entity. R k is an asymmetric r × r matrix that describes the inter- actions of the latent components according to the k-th relation. Notice that while R k differs in each slice, A remains the same. A and R k are derived by minimizing the loss function below.</p><formula xml:id="formula_1">min A,R k f (A, R k ) + λ · g(A, R k ),<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">f (A, R k ) = 1 2 k X k − AR k A T 2 F</formula><p>is the mean-squared reconstruction error and</p><formula xml:id="formula_3">g(A, R k ) = 1 2 A 2 F + k R k 2 F is the regu- larization term.</formula><p>RESCAL is a special form of Tucker decom- position <ref type="bibr" target="#b33">(Tucker, 1966</ref>) operating on a 3-way ten- sor. Its model form (Eq. <ref type="formula" target="#formula_15">(1)</ref>) can also be regarded as a relaxed form of DEDICOM ( <ref type="bibr" target="#b0">Bader et al., 2007)</ref>, which derives the low-rank approximation as:</p><formula xml:id="formula_4">X k ≈ AD k RD k A T .</formula><p>To compare RESCAL to other tensor decomposition methods, interested readers can refer to ( <ref type="bibr" target="#b14">Kolda and Bader, 2009</ref>).</p><p>The optimization problem in Eq. <ref type="formula" target="#formula_1">(2)</ref> can be solved using the efficient alternating least-squares (ALS) method. This approach alternatively fixes R k to solve for A and then fixes A to solve R k . The whole procedure stops until f (A,R k ) X 2 F con- verges to some small threshold or the maximum number of iterations has been reached.</p><p>By finding the solutions where the gradients are 0, we can derive the update rules of A and R k as below.</p><formula xml:id="formula_5">A← k X k AR T k +X T k AR k k B k +C k +λI −1 , where B k = R k A T AR T k and C k = R T k A T AR k . vec(R k ) ← Z T Z + λI −1 Z T vec(X k ), (3)</formula><p>where vec(R k ) is the vectorization of R k , Z = A ⊗ A and the operator ⊗ is the Kronecker prod- uct.</p><p>Complexity Analysis Following the analysis in ( <ref type="bibr" target="#b23">Nickel et al., 2012</ref>), we assume that each X k is a sparse matrix, and let p be the number of non-zero entries 2 . The complexity of computing X k AR T k and X T k AR k is O(pr + nr 2 ). Evaluating B k and C k requires O(nr 2 ) and the matrix inversion re- quires O(r 3 ). Therefore, the complexity of updat- ing A is O(pr+nr 2 ) assuming n r. The updat- ing rule of R k involves inverting an r 2 × r 2 ma- trix. Therefore, directly computing the inversion requires time complexity O(r 6 ) and space com- plexity O(r 4 ). Although <ref type="bibr" target="#b23">Nickel et al. (2012)</ref> con- sidered using QR decomposition to simplify the updates, it is still time consuming with the time complexity O(r 6 + pr 2 ). Therefore, the total time complexity is O(r 6 +pr 2 ) and the step of updating R k is the bottleneck in the optimization process. We will describe how to reduce the time complex- ity of this step to O(nr 2 + pr) in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach</head><p>We describe how we leverage the relational do- main knowledge in this section. By removing the incompatible entity-relation triples from the loss function, training can be done much more effi- ciently and results in a model with higher pre- diction accuracy. In addition, we also introduce a mathematical technique to reduce the compu- tational complexity of the tensor decomposition methods when taking into account the regulariza- tion term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Applying Relational Domain Knowledge</head><p>In the domain of knowledge bases, the notion of entity types is the side information that commonly exists and dictates whether some entities can be legitimate arguments of a given predicate. For instance, suppose the relation of interest is born- in, which denotes the birth location of a person. When asked whether an incompatible pair of en- tities, such as two person entities like Abraham Lincoln and John Henry, having this rela- tion, we can immediately reject the possibility. Al- though the type information and the constraints are readily available, it is overlooked in the pre- vious work on matrix and tensor decomposition models for knowledge bases ( <ref type="bibr" target="#b26">Riedel et al., 2013;</ref><ref type="bibr" target="#b23">Nickel et al., 2012</ref>). Ignoring the type information has two implications. Incompatible entity-relation triples still participate in the loss function of the optimization problem, which incurs unnecessary computation. Moreover, by choosing values for these incompatible entries we introduce errors in training the model that can reduce the quality of the model.</p><p>Based on this observation, we propose Typed- RESCAL, or TRESCAL, which leverages the en- tity type information to improve both the effi- ciency of model training and the quality of the model in term of prediction accuracy. We em- ploy a direct and simple approach by excluding the triples of the incompatible entity types from the loss in Eq. (2). For each relation, let L k and R k be the set of entities with a compatible type to the k-th relation. That is, (e i , r k , e j ) is a feasible triple if and only if e i ∈ L k and e j ∈ R k . For no- tational convenience, we use A k l , A kr to denote the sub-matrices of A that consists of rows asso- ciated with L k and R k , respectively. Analogously, let X k lr be the sub-matrix of X k that consists of only the entity pairs compatible to the k-th rela- tion. The rows and columns of X k lr map to the en- tities in A k l and A kr , respectively. In other words, entries of X k but not in X k lr do not satisfy the type constraint and are ignored from the computation.</p><formula xml:id="formula_6">~ ~   χ k A A T Rk A kl A kr T χ k lr e Lk e Rk</formula><p>Figure 2: The construction of TRESCAL. Suppose the k-th relation is born-in. L k is then a set of person entities and R k is a set of location entities.</p><p>Only the sub-matrix corresponds to the compati- ble entity pairs (i.e., X k lr ) and the sub-matrices of the associated entities (i.e., A k l and A T kr ) will be included in the loss. TRESCAL solves the following optimization problem:</p><formula xml:id="formula_7">min A,R k f (A, R k ) + λ · g(A, R k ),<label>(4)</label></formula><p>where</p><formula xml:id="formula_8">f (A, R k ) = 1 2 k X k lr − A k l R k A T kr 2 F and g(A, R k ) = 1 2 A 2 F + k R k 2 F .</formula><p>Similarly, A and R k can be solved using the alternating least-squares method. The update rule of A is</p><formula xml:id="formula_9">A ← k X k lr A kr R T k + X T k lr A k l R k × k B kr + C k l + λI −1 , where B kr = R k A T kr A kr R T k and C k l = R T k A T k l A k l R k . The update of R k becomes: vec(R k ) ← A T kr A kr ⊗ A T k l A k l + λI −1 × vec(A k l T X k lr A kr ),<label>(5)</label></formula><p>Complexity Analysis Let ¯ n be the average number of entities with a compatible type to a relation. Follow a similar derivation in Sec. 3.2, the time complexity of updating A is O(pr + ¯ nr 2 ) and the time complexity of updating R k remains to be O(r 6 + pr 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Handling Regularization Efficiently</head><p>Examining the update rules of both RESCAL and TRESCAL, we can see that the most time- consuming part is the matrix inversions. For RESCAL, this is the term (Z T Z+λI) −1 in Eq. <ref type="formula" target="#formula_15">(3)</ref>, where Z = A ⊗ A. Nickel et al. (2011) made the observation that if λ = 0, the matrix inversion can be calculated by</p><formula xml:id="formula_10">(Z T Z) −1 = (A T A) −1 A ⊗ (A T A) −1 A.</formula><p>Then</p><note type="other">, it only involves an inversion of an r × r ma- trix, namely A T A. However, if λ &gt; 0, directly calculating Eq. (3) requires to invert an r 2 × r 2 matrix and thus becomes a bottleneck in solving Eq. (2).</note><p>To reduce the computational complexity of the update rules of R k , we compute the inver- sion Z T Z + λI −1 by applying singular value decomposition (SVD) to A, such that A = UΣV T , where U and V are orthogonal matrices and Σ is a diagonal matrix. Then by using proper- ties of the Kronecker product we have:</p><formula xml:id="formula_11">Z T Z + λI −1 = λI + VΣ 2 V T ⊗ VΣ 2 V T −1 = λI + (V ⊗ V)(Σ 2 ⊗ Σ 2 )(V ⊗ V) T −1 = (V ⊗ V) λI + Σ 2 ⊗ Σ 2 −1 (V ⊗ V) T .</formula><p>The last equality holds because V ⊗ V is also an orthogonal matrix. We leave the de- tailed derivations in Appendix A. Notice that λI + Σ 2 ⊗ Σ 2 −1 is a diagonal matrix. There- fore, the inversion calculation is trivial. This technique can be applied to TRESCAL as well.</p><p>By applying SVD to both A k l and A kr , we have</p><formula xml:id="formula_12">A k l = U k l Σ k l V T k l and A kr = U kr Σ kr V T kr , respectively. The computa- tion of A T kr A kr ⊗ A T k l A k l + λI −1 of Eq. (5)</formula><p>thus becomes:</p><formula xml:id="formula_13">(V k l ⊗ V kr ) λI + Σ 2 k l ⊗ Σ 2 kr −1 (V k l ⊗ V kr ) T .</formula><p>The procedure of updating R is depicted in Al- gorithm 1.</p><p>Complexity Analysis For RESCAL, V and Σ can be computed by finding eigenvectors of A T A. Therefore, computing SVD of A costs O(nr 2 + r 3 ) = O(nr 2 ). Computing Step 4 in Algorithm 1 takes O(nr 2 + pr).</p><p>Step 5 and Step 6 require Algorithm 1 Updating R in TRESCAL Require: X , A, and entity sets R k , L k , ∀k Ensure: R k , ∀k.</p><p>1: for k = 1 . . . m do 2:</p><formula xml:id="formula_14">[U k l , Σ 2 k l , V k l ] ← SVD(A T k l A k l ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>[U kr , Σ 2 kr , V kr ] ← SVD(A T kr A kr ). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><formula xml:id="formula_15">M 1 ← V T k l A T k l X k lr A kr V kr . 5: M 2 ← diag(Σ 2 k l ) diag(Σ 2 kr ) T + λ1.<label>(</label></formula><formula xml:id="formula_16">R k ← V k l (M 1 ./M 2 )V T kr . (</formula><p>The operator "./" is element-wise divi- sion.) 7: end for O(r 2 ) and O(r 3 ), respectively. The overall time complexity of updating R k becomes O(nr 2 +pr).</p><p>Using a similar derivation, the time complex- ity of updating R k in TRESCAL is O(¯ nr 2 + pr). Therefore, the total complexity of each iteration is O(¯ nr 2 + pr).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We conduct two sets of experiments. The first evaluates the proposed TRESCAL algorithm on inferring unknown facts using existing relation- entity triples, while the second demonstrates its application to relation extraction when a text cor- pus is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Knowledge Base Completion</head><p>We evaluate our approach on a knowledge base generated by the CMU Never Ending Language Learning (NELL) project <ref type="bibr">(Carlson et al., 2010)</ref>. NELL collects human knowledge from the web and has generated millions of entity-relation triples. We use the data generated from version 165 for training 3 , and collect the new triples gen- erated between NELL versions 166 and 533 as the development set and those generated between ver- sion 534 and 745 as the test set 4 . The data statistics of the training set are summarized in <ref type="table">Table 1</ref> al., 2013a) have 316k and 483k 5 triples, respec- tively, compared to 1.8M in this dataset.</p><p>In the NELL dataset, the entity type informa- tion is encoded in a specific relation, called Gen- eralization. Each entity in the knowledge base is assigned to at least one category presented by the Generalization relationship. Based on this infor- mation, the compatible entity type constraint of each relation can be easily identified. Specifically, we examined the entities and relations that occur in the triples of the training data, and counted all the types appearing in these instances of a given relation legitimate.</p><p>We implement RESCAL and TRESCAL in MATLAB with the Matlab tensor <ref type="bibr">Toolbox (Bader et al., 2012</ref>). With the efficient implementation described in Section 4.2, all experiments can be conducted on a commodity PC with 16 GB mem- ory. We set the maximal number of iterations of both RESCAL and TRESCAL to be 10, which we found empirically to be enough to generate a sta- ble model. Note that Eq. (4) is non-convex, and the optimization process does not guarantee to con- verge to a global minimum. Therefore, initial- izing the model properly might be important for the performance. Following the implementation of RESCAL, we initialize A by performing singular value decomposition over ¯ X = k (X k + X T k ), such that ¯ X = UΣV T and set A = U. Then, we apply the update rule of R k to initialize {R k }. RESCAL and TRESCAL have two types of param- eters: (1) the rank r of the decomposed tensor and (2) the regularization parameter λ. We tune the rank parameter on development set in a range of {100, 200, 300, 400} and the regularization pa- rameter in a range of {0.01, 0.05, 0.1, 0.5, 1}.</p><p>For comparison, we also use the code released by <ref type="bibr" target="#b4">Bordes et al. (2013a)</ref>, which is implemented using Python and the Theano library ( <ref type="bibr" target="#b2">Bergstra et al., 2010)</ref>, to train a TransE model using the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity Retrieval</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Retrieval TransE RESCAL TRESCAL</head><p>TransE RESCAL TRESCAL w/o type checking 51.41% ‡ 51.59% 54.79% 75.88% 73.15% † 76.12% w/ type checking 67.56% 62.91% ‡ 69.26% 70.71% ‡ 73.08% † 75.70% <ref type="table">Table 2</ref>: Model performance in mean average precision (MAP) on entity retrieval and relation retrieval. † and ‡ indicate the comparison to TRESCAL in the same setting is statistically significant using a paired- t test on average precision of each query, with p &lt; 0.01 and p &lt; 0.05, respectively. Enforcing type constraints during test time improves entity retrieval substantially, but does not help in relation retrieval.</p><p>same NELL dataset. We reserved randomly 1% of the training triples for the code to evaluate the model performance in each iteration. As sug- gested in their paper, we experiment with sev- eral hyper-parameters, including learning rate of {0.01, 0.001}, the latent dimension of {50, 100} and the similarity measure of {L1, L2}. In addi- tion, we also adjust the number of batches of {50, 100, 1000}. Of all the configurations, we keep the models picked by the method, as well as the fi- nal model after 500 training iterations. The final model is chosen by the performance on our devel- opment set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Training Time Reduction</head><p>We first present experimental results demonstrat- ing that TRESCAL indeed reduces the time re- quired to factorize a knowledge database, com- pared to RESCAL. The experiment is conducted on NELL with r = 300 and λ = 0.1. When λ = 0, the original RESCAL algorithm described in <ref type="bibr" target="#b22">(Nickel et al., 2011;</ref><ref type="bibr" target="#b23">Nickel et al., 2012</ref>) cannot handle a large r, because updating matrices {R k } requires O(r 4 ) memory. Later in this section, we will show that in some situation a large rank r is necessary for achieving good testing performance.</p><p>Comparing TRESCAL with RESCAL, each it- eration of TRESCAL takes 1,608 seconds, while that of RESCAL takes 7,415 seconds. In other words, by inducing the entity type information and constraints, TRESCAL enjoys around 4.6 times speed-up, compared to an improved regularized version of RESCAL. When updating A and {R k } TRESCAL only requires operating on sub-matrices of A, {R k } and {X k }, which reduces the compu- tation substantially. In average, TRESCAL filters 96% of entity triples that have incompatible types.</p><p>In contrast, it takes TransE at least 2 days and 19 hours to finish training the model (the default 500 iterations) <ref type="bibr">6</ref> , while TRESCAL finishes the training <ref type="bibr">6</ref> It took almost 4 days to train the best TransE model that in roughly 4 to 5 hours 7 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Test Performance Improvement</head><p>We consider two different types of tasks to evalu- ate the prediction accuracy of different models - entity retrieval and relation retrieval.</p><p>Entity Retrieval In the first task, we collect a set of entity-relation pairs {(e i , r k )} and aim at predicting e j such that the tuple (e i , r k , e j ) is a recorded triple in the NELL knowledge base. For each pair (e i , r k ), we collect triples {(e i , r k , e * j )} from the NELL test corpus as positive samples and randomly pick 100 entries e j to form negative samples {e i , r k , e j }. Given A and R k from the factorization generated by RESCAL or TRESCAL, the score assigned to a triple {e i , r k , e j } is com- puted by a T i R k a j where a i and a j are the i-th and j-th rows of A. In TransE, the score is de- termined by the negative dissimilarity measures of the learned embeddings: −d(e i , r k , e j ) = −−e i + r k − e j 2 2 . We evaluate the performance using mean aver- age precision (MAP), which is a robust and sta- ble metric <ref type="bibr" target="#b18">(Manning et al., 2008)</ref>. As can be observed in <ref type="table">Table 2</ref> (left), TRESCAL achieves 54.79%, which outperforms 51.59% of RESCAL and 51.41% of TransE. Adding constraints during test time by assigning the lowest score to the en- tity triples with incompatible types improves re- sults of all models -TRESCAL still performs the best (69.26%), compared to TransE (67.56%) and RESCAL (62.91%).</p><p>Relation Retrieval In the second task, given a relation type r k , we are looking for the entity pairs (e i , e j ) that have this specific relationship. To gen- erate test data, for each relation type, we collect is included in <ref type="table">Table 2</ref>. <ref type="bibr">7</ref> We also tested the released code from (Socher et al., 2013) for training a neural tensor network model. However, we are not able to finish the experiments as each iteration of this method takes almost 5 hours. gold entity pairs from the NELL knowledge base as positive samples and randomly pick a set of en- tity pairs as negative samples such that the number of positive samples are the same as negative ones.</p><p>Results presented in <ref type="table">Table 2</ref> (right) show that TRESCAL achieves 76.12%, while RESCAL and TransE are 73.15% and 75.88%, respectively. Therefore, incorporating the type information in training seems to help in this task as well. Enforc- ing the type constraints during test time does not help as in entity retrieval. By removing incom- patible entity pairs, the performance of TRESCAL, RESCAL and TransE drop slightly to 75.70%, 73.08% and 70.71% respectively. One possible explanation is that the task of relation retrieval is easier than entity retrieval. The incorrect type in- formation of some entities ends up filtering out a small number of entity pairs that were retrieved correctly by the model.</p><p>Notice that TRESCAL achieves different levels of performance on various relations. For example, it performs well on predicting AthletePlaysSport (81%) and CoachesInLeague (88%), but achieves suboptimal performance on predicting Works- For (49%) and BuildingLocatedInCity (35%). We hypothesize that it is easier to gener- alize entity-relation triples when the relation has several related relations. For examples, AthletePlaysForTeam and TeamPlaysSport may help discover entity-relation triples of Ath- letePlaysSport.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Sensitivity to Parameters</head><p>We also study if TRESCAL is sensitive to the rank parameter r and the regularization parameter λ, where the detailed results can be found in Ap- pendix B. In short, we found that increasing the rank r generally leads to better models. Also, while the model is not very sensitive to the value of the regularization parameter λ, tuning λ is still necessary for achieving the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Relation Extraction</head><p>Next, we apply TRESCAL to the task of extract- ing relations between entities, jointly from a text corpus and a structured knowledge base. We use a corpus from ( <ref type="bibr" target="#b26">Riedel et al., 2013</ref>) that is cre- ated by aligning the entities in NYTimes and Free- base. The corpus consists of a training set and a test set. In the training set, a list of entity pairs are provided, along with surface patterns extracted from NYTimes and known relations obtained from Freebase. In the test set, only the surface patterns are given. By jointly factoring a matrix consist- ing of the surface patterns and relations, <ref type="bibr" target="#b26">Riedel et al. (2013)</ref> show that their model is able to capture the mapping between the surface patterns and the structured relations and hence is able to extract the entity relations from free text. In the following, we show that TRESCAL can be applied to this task.</p><p>We focus on the 19 relations listed in <ref type="table">Table 1</ref> of ( <ref type="bibr" target="#b26">Riedel et al., 2013</ref>) and only consider the surface patterns that co-occur with these 19 re- lations. We prune the surface patterns that oc- cur less than 5 times and remove the entities that are not involved in any relation and surface pat- tern. Based on the training and test sets, we build a 80,698×80,698×1,652 tensor, where each slice captures a particular structured relation or a surface pattern between two entities. There are 72 fine types extracted from Freebase assigned to 53,836 entities that are recorded in Freebase. In addition, special types, PER, LOC, ORG and MISC, are assigned to the remaining 26,862 enti- ties based on the predicted NER tags provided by the corpus. A type is considered incompatible to a relation or a surface pattern if in the training data, none of the argument entities of the relation be- longs to the type. We use r = 400 and λ = 0.1 in TRESCAL to factorize the tensor.</p><p>We compare the proposed TRESCAL model to RI13 ( <ref type="bibr" target="#b26">Riedel et al., 2013</ref>), YA11 (Yao et al., 2011), MI09 ( <ref type="bibr" target="#b21">Mintz et al., 2009</ref>) and SU12 ( <ref type="bibr" target="#b31">Surdeanu et al., 2012)</ref>  <ref type="bibr">8</ref> . We follow the protocol used in ( <ref type="bibr" target="#b26">Riedel et al., 2013)</ref> to evaluate the results. Given a re- lation as query, the top 1,000 entity pairs output by each system are collected and the top 100 ones are judged manually. Besides comparing individ- ual models, we also report the results of combined models. To combine the scores from two models, we simply normalize the scores of entity-relation tuples to zero mean and unit variance and take the average. The results are summarized in <ref type="table" target="#tab_4">Table 3</ref>.</p><p>As can been seen in the table, using TRESCAL alone is not very effective and its performance is only compatible to MI09 and YA11, and is sig- nificantly inferior to RI13. This is understandable because the problem setting favors RI13 as only entity pairs that have occurred in the text or the database will be considered in RI13, both during model training and testing. In contrast, TRESCAL  predicts all the possible combinations between en- tities and relations, which makes the model less fit to the task. However, when combining TRESCAL with a pure text-based method, such as SU12, we can clearly see TRESCAL is complementary to SU12 (0.39 to 0.44 in weighted MAP score), which makes the results competitive to RI13. Interestingly, although both TRESCAL and RI13 leverage information from the knowledge base, we find that by combining them, the performance is improved quite substantially (0.47 to 0.57). We suspect that the reason is that in our construc- tion, each entity has its own vector representa- tion, which is lacked in RI13. As a result, the new triples that TRESCAL finds are very different from those found by RI13. Nevertheless, com- bining more methods do not always yield an im- provement. For example, combining TR, RI13 and SU12 together (not included in <ref type="table" target="#tab_4">Table 3</ref>) achieves almost the same performance as TR+RI13.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper we developed TRESCAL, a tensor decomposition method that leverages relational domain knowledge. We use relational domain knowledge to capture which triples are potentially valid and found that, by excluding the triples that are incompatible when performing tensor decom- position, we can significantly reduce the train- ing time and improve the prediction performance as compared with RESCAL and TransE. More- over, we demonstrated its effectiveness in the ap- plication of relation extraction. Evaluated on the dataset provided in ( <ref type="bibr" target="#b26">Riedel et al., 2013)</ref>, the per- formance of TRESCAL alone is comparable to sev- eral existing systems that leverage the idea of dis- tant supervision. When combined with the state- of-the-art systems, we found that the results can be further improved. For instance, the weighted mean average precision of the previous best ap- proach in ( <ref type="bibr" target="#b26">Riedel et al., 2013)</ref> has been increased by 10 points (47% to 57%).</p><p>There are a number of interesting potential ex- tensions of our work. First, while the experiments in this paper are on traditional knowledge bases and textual data, the idea of leveraging relational domain knowledge is likely to be of value to other linguistic databases as well. For instance, part-of- speech tags can be viewed as the "types" of words. Incorporating such information in other tensor de- composition methods (e.g., <ref type="bibr" target="#b9">(Chang et al., 2013)</ref>) may help lexical semantic representations. Sec- ond, relational domain knowledge goes beyond entity types and their compatibility with specific relations. For instance, the entity-relation triple (e 1 , child-of, e 2 ) can be valid only if e 1 .type = person ∧ e 2 .type = person ∧ e 1 .age &lt; e 2 .age. It would be interesting to explore the possibility of developing efficient methods to leverage other types of relational domain knowledge. Finally, we would like to create more sophisticated models of knowledge base embedding, targeting complex in-ference tasks to better support semantic parsing and question answering.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B Hyper-parameter Sensitivity</head><p>We study if TRESCAL is sensitive to the rank parameter r and the regularization parameter λ. We use the task of relation retrieval and present the model performance on the development set. <ref type="figure" target="#fig_1">Fig. 3</ref> shows the performance of TRESCAL and RESCAL with different rank (r) values while fix- ing λ = 0.01. Results show that both TRESCAL and RESCAL achieve better performance when r is reasonably large. TRESCAL obtains a bet- ter model with smaller r than RESCAL, because TRESCAL only needs to fit the triples of the com- patible entity types. Therefore, it allows to use smaller number of latent variables to fit the train- ing data.</p><p>Fixing r = 400, <ref type="figure" target="#fig_2">Fig. 4</ref> shows the performance of TRESCAL at different values of the regulariza- tion parameter λ, including no regularization at all (λ = 0). While the results suggest that the method is not very sensitive to λ, tuning λ is still necessary for achieving the best performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 illustrates this construction.</head><label>2</label><figDesc>Fig. 2 illustrates this construction. TRESCAL solves the following optimization problem:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Prediction performance of TRESCAL and RESCAL with different rank (r).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Prediction performance of TRESCAL with different regularization parameter (λ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>. The numbers of triples in the development and test sets are 19,665 and 117,889, respectively. Notice that this dataset is substantially larger than the datasets used in recent work. For example, the Freebase data used in (Socher et al., 2013) and (Bordes et NELL # entities 753k # relation types 229 # entity types 300 # entity-relation triples 1.8M Table 1: Data statistics of the training set from NELL in our experiments.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Weighted Mean Average Precisions. The # column shows the number of true facts in the pool. 
Bold faced are winners per relation, italics indicate ties based on a sign test. 

</table></figure>

			<note place="foot" n="1"> This representation can easily be extended for a probabilistic knowledge base by allowing nonnegative real values.</note>

			<note place="foot" n="2"> Notice that we use a slightly different definition of p from the one in (Nickel et al., 2012). The time complexity of multiplying an n × n sparse matrix X k with p non-zero entries by an n × r dense matrix is O(pr) assuming n r.</note>

			<note place="foot" n="3"> http://www.cs.cmu.edu/ ˜ nlao/ 4 http://bit.ly/trescal</note>

			<note place="foot" n="5"> In (Bordes et al., 2013a), there is a much larger dataset, FB1M, that has 17.5M triples used for evaluation. However, this dataset has not been released.</note>

			<note place="foot" n="8"> The corpus and the system outputs are from http:// www.riedelcastro.org/uschema</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Sebastian Riedel for providing the data for experiments. We are also grateful to the anony-mous reviewers for their valuable comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Detailed Derivation</head><p>We first introduce some lemmas that will be useful for our derivation. Lemmas 2, 3 and 4 are the basic properties of the Kronecker product. Their proofs can be found at ( <ref type="bibr" target="#b16">Laub, 2005)</ref>.</p><p>Lemma 4. If A and B are orthogonal matrices, then A ⊗ B will also be an orthogonal matrix.</p><p>Let Z = A ⊗ A and apply singular value decomposition to A = UΣV T . The term Z T Z + λI −1 can be rewritten as: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Temporal analysis of semantic graphs using ASALSAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">G</forename><surname>Richard A Harshman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Matlab tensor toolbox version 2.5. Available online</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference (SciPy)</title>
		<meeting>the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<publisher>Oral Presentation</publisher>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Inside YOGO2s: a transparent information extraction architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Biega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erdal</forename><surname>Kuzey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian M</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="325" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to extract relations from the web using minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="576" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estevam</forename><forename type="middle">R</forename><surname>Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Toward an architecture for neverending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-relational latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1602" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A multilinear singular value decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><forename type="middle">De</forename><surname>Lieven De Lathauwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joos</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1253" to="1278" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Triplerank: Ranking semantic web data by tensor decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antje</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergej</forename><surname>Sizov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Staab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic WebISWC 2009</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="213" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gigatensor: scaling tensor analysis up by 100 times-algorithms and discoveries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Papalexakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="316" to="324" />
		</imprint>
	</monogr>
	<note>Abhay Harpale, and Christos Faloutsos</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards a standardized notation and terminology in multiway analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Henk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kiers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemometrics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="105" to="122" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Three-way arrays: rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics. Linear algebra and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joseph</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="95" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Matrix analysis for scientists and engineers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">J</forename><surname>Laub</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>SIAM</publisher>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="139" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Multi-relational learning using weighted tensor decomposition with modular loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodoros</forename><surname>Rekatsinas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1303.1733" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>University of Maryland College Park</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schutze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Suntec, Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009-08" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Factorizing YAGO: scalable machine learning for linked data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Turbo-smt: Accelerating coupled sparse matrix-tensor factorizations by 200x</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Evangelos E Papalexakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML/PKDD 2010</title>
		<meeting>ECML/PKDD 2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling missing data in distant supervision for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="367" to="378" />
			<date type="published" when="2013-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Relational learning via collective matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ajit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="650" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reasoning With Neural Tensor Networks For Knowledge Base Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Analogyspace: Reducing the dimensionality of common sense knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Lieberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="548" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>EMNLP-CoNLL</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Modelling relational data using Bayesian clustered tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1821" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Some mathematical notes on three-mode factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ledyard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="311" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Connecting language and knowledge bases with embedding models for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1366" to="1371" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Structured relation discovery using generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="1456" to="1466" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
