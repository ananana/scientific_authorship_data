<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NPRF: A Neural Pseudo Relevance Feedback Framework for Ad-hoc Information Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canjia</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingfei</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Computer Network Information Center</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hui</surname></persName>
							<email>kai.hui@sap.com , ayates@mpi-inf.mpg.de, sunle@iscas.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">SAP SE</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Yates</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NPRF: A Neural Pseudo Relevance Feedback Framework for Ad-hoc Information Retrieval</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4482" to="4491"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4482</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Pseudo relevance feedback (PRF) is commonly used to boost the performance of traditional information retrieval (IR) models by using top-ranked documents to identify and weight new query terms, thereby reducing the effect of query-document vocabulary mis-matches. While neural retrieval models have recently demonstrated strong results for ad-hoc retrieval, combining them with PRF is not straightforward due to incompatibilities between existing PRF approaches and neural ar-chitectures. To bridge this gap, we propose an end-to-end neural PRF framework that can be used with existing neural IR models by embedding different neural models as building blocks. Extensive experiments on two standard test collections confirm the effectiveness of the proposed NPRF framework in improving the performance of two state-of-the-art neural IR models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent progress in neural information retrieval models (NIRMs) has highlighted promising per- formance on the ad-hoc search task. State-of-the- art NIRMs, such as DRMM ( , <ref type="bibr">HiNT (Fan et al., 2018)</ref>, (Conv)-KNRM ( <ref type="bibr" target="#b29">Xiong et al., 2017;</ref><ref type="bibr" target="#b2">Dai et al., 2018)</ref>, and (Co)- PACRR ( <ref type="bibr" target="#b11">Hui et al., , 2018</ref>, have successfully implemented insights from traditional IR models using neural building blocks. Meanwhile, existing IR research has already demonstrated the effec- tiveness of incorporating relevance signals from top-ranked documents through pseudo relevance feedback (PRF) models <ref type="bibr" target="#b1">(Buckley and Robertson, 2008;</ref><ref type="bibr" target="#b3">Diaz et al., 2016</ref>). PRF models expand the query with terms selected from top-ranked docu- ments, thereby boosting ranking performance by reducing the problem of vocabulary mismatch be- tween the original query and documents <ref type="bibr" target="#b26">(Rocchio, 1971)</ref>. Existing neural IR models do not have a mechanism for treating expansion terms differently from the original query terms, however, making it non-trivial to combine them with exist- ing PRF approaches. In addition, neural IR models differ in their architectures, making the develop- ment of a widely-applicable PRF approach a chal- lenging task.</p><p>To bridge this gap, we propose a generic neu- ral pseudo relevance feedback framework, coined NPRF, that enables the use of PRF with existing neural IR models. Given a query and a target doc- ument, the top-ranked documents from the initial ranking are consumed by NPRF, which expands the query by interpreting it from different perspec- tives. Given a target document to evaluate, NPRF produces a final relevance score by considering the target document's relevance to these top-ranked documents and to the original query.</p><p>The proposed NPRF framework can directly in- corporate different established neural IR models, which serve as the concrete scorers in evaluat- ing the relevance of a document relative to the top-ranked documents and to the query, without changing their architectures. We instantiate the NPRF framework using two state-of-the-art neu- ral IR models, and we evaluate their performance on two widely-used TREC benchmark datasets for ad-hoc retrieval. Our results confirm that the NPRF framework can substantially improve the performance of both models. Moreover, both neural models perform similarly inside the NPRF framework despite the fact that without NPRF one model performed substantially worse than the other model. The contributions of this work are threefold: 1) the novel NPRF framework; 2) two instantiations of the NPRF framework using two state-of-the-art neural IR models; and 3) the exper- iments that confirm the effectiveness of the NPRF framework.</p><p>The rest of this paper is organized as follows. Section 2 presents the proposed NPRF framework in details. Following that, Section 3 describes the setup of the evaluation, and reports the results. Fi- nally, Section 4 recaps existing literature, before drawing conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this section, we introduce the proposed neu- ral framework for pseudo relevance feedback (NPRF). Recall that existing unsupervised PRF models <ref type="bibr" target="#b26">(Rocchio, 1971;</ref><ref type="bibr" target="#b14">Lavrenko and Croft, 2001;</ref><ref type="bibr" target="#b30">Ye et al., 2009</ref>) issue a query to obtain an initial ranking, identify promising terms from the top-m documents returned, and expand the origi- nal query with these terms. Rather than selecting the expanded terms within the top-m documents, NPRF uses these documents directly as expansion queries by considering the interactions between them and a target document. Thus, each docu- ment's ultimate relevance score depends on both its interactions with the original query and its in- teractions with these feedback documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>Given a query q, NPRF estimates the relevance of a target document d relative to q as described in the following steps. The architecture is summarized in <ref type="figure" target="#fig_0">Figure 1</ref>. Akin to the established neural IR models like DRMM ( , the description is based on a query-document pair, and a ranking can be produced by sorting the documents according to their scores.</p><p>-Create initial ranking. Given a document cor- pus, a ranking method rel q (q, d) is applied to individual documents to obtain the top-m doc- uments, denoted as D q for q.</p><p>-Extract document interactions. To evaluate the relevance of d, each d q in D q is used to ex- pand q, where d is compared against each d q , using a ranking method rel d (d q , d).</p><p>-Combine document interactions. The rele- vance scores rel d (d q , d) for individual d q ∈ D q are further weighted by rel q (q, d q ), which serves as an estimator for the confidence of the contribution of d q relative to q. The weighted combination of these relevance scores is used to produce a relevance score for d, denoted as</p><formula xml:id="formula_0">rel D (q, D q , d).</formula><p>While the same ranking model can be used for both rel q (., .) and rel d (., .), we denote them sep- arately in the architecture. In our experiments, the widely-used unsupervised ranking method BM25 ( <ref type="bibr" target="#b25">Robertson et al., 1995</ref>) serves as rel q (., .); meanwhile two state-of-the-art neural IR rele- vance matching models, namely, DRMM (  and K-NRM ( <ref type="bibr" target="#b29">Xiong et al., 2017)</ref>, serve as the ranking method rel d <ref type="bibr">(., .)</ref>. However, it is worth noting that in principle rel q and rel d can be replaced with any ranking method, and the above choices mainly aim to demonstrate the ef- fectiveness of the NPRF framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Architecture</head><p>The NPRF framework begins with an initial rank- ing for the input query q determined by rel q (., .), which forms D q , the set of the top-m documents D q . The ultimate query-document relevance score</p><formula xml:id="formula_1">rel D (q, D q , d) is computed as follows.</formula><p>Extracting document interactions. Given the target document d and each feedback document</p><formula xml:id="formula_2">d q ∈ D q , rel d (., .</formula><p>) is used to evaluate the rele- vance between d and d q , resulting in m real-valued relevance scores, where each score corresponds to the estimated relevance of d according to one feed- back document d q .</p><p>As mentioned, two NIRMs are separately used to compute rel d (d q , d) in our experiments. Both models take as input the cosine similarities be- tween each pair of terms in d q and d, which are computed using pre-trained word embeddings as explained in Section 3.1. Given that both models consider only unigram matches and do not con- sider term dependencies, we first summarize d q by retaining only the top-k terms according to their tf -idf scores, which speeds up training by reduc- ing the document size and removing noisy terms. In our pilot experiments, the use of top-k tf -idf document summarization did not influence perfor- mance. For different d q ∈ D q , the same model is used as rel d (., .) for different pairs of (d q , d) by sharing model weights.</p><p>Combining document interactions. When de- termining the relevance of a target document d, there exist two sources of relevance signals to consider: the target document's relevance rela- tive to the feedback documents D q and its rel- evance relative to the query q itself. In this step, we combine rel d (d q , d) for each d q ∈ D q into an overall feedback document relevance score </p><formula xml:id="formula_3">rel D (q, D q , d).</formula><p>When combining the relevance scores, the agreement between q and each d q is also important, since d q may differ from q in terms of information needs. The relevance of d q from the initial ranking rel q (q, d q ) is employed to quan- tify this agreement and weight each rel</p><formula xml:id="formula_4">d (d q , d) ac- cordingly.</formula><p>When computing such agreements, it is neces- sary to remove the influence of the absolute ranges of the scores from the initial ranker. For exam- ple, ranking scores from a language model <ref type="bibr" target="#b23">(Ponte and Croft, 1998</ref>) and from BM25 ( <ref type="bibr" target="#b25">Robertson et al., 1995)</ref> can differ substantially in their absolute ranges. To mitigate this, we use a smoothed min-max normalization to rescale rel q (q, d q ) into the range [0. <ref type="bibr">5,</ref><ref type="bibr">1]</ref>. The min-max normalization is applied by considering min(rel q (q, d q )|d q ∈ D q ) and max (rel q (q, d q )|d q ∈ D q ). Hereafter, rel q (q, d q ) is used to denote this relevance score after min-max normalization for brevity. The (normalized) relevance score is smoothed and then weighted by the relevance evaluation of d q , producing a weighted document relevance score rel d (d q , d) for each d q ∈ D q that reflects the rel- evance of d q relative to q. This computation is de- scribed in the following equation.</p><formula xml:id="formula_5">rel d (d q , d) = rel d (d q , d)(0.5 + 0.5 × rel q (q, d q ))</formula><p>(1) As the last step, we propose two variants for combining the rel d</p><formula xml:id="formula_6">(d q , d) for different d q into a single score rel D (q, D q , d):</formula><p>(i) performing a direct summation and (ii) using a feed forward network with a hyperbolic tangent (tanh) non-linear acti- vation. Namely, the first variant simply sums up the scores, whereas the second takes the ranking positions of individual feedback documents into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Optimization and Training</head><p>Each training sample consists of a query q, a set of m feedback documents D q , a relevant target document d + and a non-relevant target document d − according to the ground truth. The Adam opti- mizer ( <ref type="bibr" target="#b13">Kingma and Ba, 2014</ref>) is used with a learn- ing rate 0.001 and a batch size of 20. Training nor- mally converges within 30 epochs, with weights uniformly initialized. A hinge loss is employed for training as shown below.</p><formula xml:id="formula_7">loss(q, D q , d + , d − ) = max(0, 1 − rel (q, D q , d + ) + rel (q, D q , d − ))</formula><p>3 Evaluation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation Setup</head><p>Dataset.</p><p>We evaluate our proposed NPRF framework on two standard test collections, namely, TREC1-3 (Harman, 1993) and Ro- bust04 <ref type="bibr" target="#b28">(Voorhees, 2004</ref>). TREC1-3 consists of 741,856 documents with 150 queries used in the TREC 1-3 ad-hoc search tasks <ref type="bibr" target="#b6">(Harman, 1993</ref><ref type="bibr" target="#b7">(Harman, , 1994</ref><ref type="bibr" target="#b8">(Harman, , 1995</ref>). Robust04 contains 528,155 docu- ments and 249 queries used in the TREC 2004 Robust track <ref type="bibr" target="#b28">(Voorhees, 2004</ref>). We use those two collections to balance between the number of queries and the TREC pooling depth, i.e., 100 on both collections, allowing for sufficient training data. Manual relevance judgments are available on both collections, where both the relevant and non- relevant documents are labeled for each query.</p><p>Two versions of queries are included in our ex- periments: a short keyword query (title query), and a longer description query that restates the cor- responding keyword query's information need in terms of natural language (description query). We evaluate each type of query separately using the metrics Mean Average Precision at 1,000 (MAP), Precision at 20 (P@20) ( <ref type="bibr" target="#b17">Manning et al., 2008)</ref>, and NDCG@20 <ref type="bibr" target="#b12">(Järvelin and Kekäläinen, 2002</ref>). Preprocessing. Stopword removal and Porter's stemmer are applied ( <ref type="bibr" target="#b17">Manning et al., 2008</ref>). The word embeddings are pre-trained based on a pool of the top 2,000 documents returned by BM25 for individual queries as suggested by <ref type="bibr" target="#b3">(Diaz et al., 2016)</ref>. The implementation of Word2Vec 1 from ( <ref type="bibr" target="#b18">Mikolov et al., 2013</ref>) is employed. In par- ticular, we employ CBOW with the dimension set to 300, window size to 10, minimum count to 5, and a subsampling threshold of 10 −3 . The CBOW model is trained for 10 iterations on the target cor- pus. Unsupervised ranking models serve as baselines for comparisons. We use the open source Terrier platform's ( <ref type="bibr" target="#b16">Macdonald et al., 2012</ref>) implementa- tion of these ranking models: </p><formula xml:id="formula_8">-BM25 (Robertson</formula><formula xml:id="formula_9">d (d, d q ) = rel d (d, d q )</formula><p>in place of Equation 1, thereafter combining the scores with a fully connected layer as in NPRF ff . We combine each of the three NPRF variants with the DRMM and K-NRM models, and report re- sults for all six variants. Our implementation of the NPRF framework is available to enable fu- ture comparisons 2 .</p><p>Akin to ( <ref type="bibr" target="#b29">Xiong et al., 2017;</ref>, the NIRM baselines and the pro- posed NPRF are employed to re-rank the search results from BM25. In particular, the top-10 doc- uments from the unsupervised baseline are used as the pseudo relevance feedback documents D q as input for NPRF, where each d q ∈ D q is rep- resented by its top-20 terms with the highest tf - idf weights. As illustrated later in Section 3.3, NPRF's performance is stable over a wide range of settings for both parameters. Cross-validation. Akin to ( <ref type="bibr" target="#b11">Hui et al., 2018)</ref>, ow- ing to the limited number of labeled data, five-fold cross-validation is used to report the results by ran- domly splitting all queries into five equal parti- tions. In each fold, three partitions are used for training, one for validation, and one for testing. The model with the best MAP on the validation set is selected. We report the average performance on all test partitions. A two-tailed paired t-test is used to report the statistical significance at 95% confidence interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Comparison to BM25. We first compare the pro- posed NPRF models with the unsupervised BM25. The results are summarized in <ref type="table" target="#tab_1">Tables 1 and 2</ref>, where the best result in each column is highlighted in bold. From <ref type="table" target="#tab_1">Tables 1 and 2</ref>, it can be seen that the proposed NPRF variants obtain significant im- provement relative to BM25 on both test collec- tions with both kinds of test queries. Moreover, the results imply that the use of different query types does not affect the effectiveness of NPRF, which consistently outperforms BM25. Comparison to neural IR models. NPRF is fur- ther compared with different neural IR models, as summarized in <ref type="table" target="#tab_4">Tables 3 &amp; 4</ref>. It can be seen that NPRF regularly improves on top of the NIRM baselines. For both types of queries, NPRF- DRMM outperforms DRMM and NPRF-KNRM outperforms K-NRM when re-ranking BM25. Re- markably, the proposed NPRF is able to improve the weaker NIRM baseline. For instance, on Robust04, when using the description queries, 2 https://github.com/ucasir/NPRF DRMM and K-NRM obtain highly different re- sults, with MAPs of 0.2630 and 0.1687 after re- ranking the initial results from BM25, respec- tively. When NPRF is used in conjunction with the NIRM models, however, the gap between the two models is closed; that is, MAP=0.2801 for NRFF ds -DRMM and MAP=0.2800 for NRFF ds - KNRM (see <ref type="table" target="#tab_4">Table 4</ref>). This finding highlights that our proposed NPRF is robust with respect to the use of the two embedded NIRM models. A pos- sible explanation for the poor performance of K- NRM on two TREC collections is the lack of train- ing data, as suggested in <ref type="figure" target="#fig_0">(Dai et al., 2018)</ref>. While K-NRM could be improved by introducing weak supervision ( <ref type="bibr" target="#b2">Dai et al., 2018)</ref>, we achieve the same goal by incorporating pseudo relevance feedback information without extra training data.</p><p>While the six NPRF variants exhibit similar results across both kinds of queries, NPRF ds - DRMM in general achieves the best performance on Robust04, and NPRF ds -KNRM appears to be the best variant on TREC1-3. In the meantime, NPRF ds outperforms NPRF ff variants. One dif- ference between the two methods is that NPRF ff considers the position of each d q in the D q ranked documents, whereas NPRF ds simply sums up the scores regardless of the positions. The fact that NPRF ds performs better suggests that the ranking position within the D q documents may not be a useful signal. In the remainder of this paper, we mainly report on the results obtained by NPRF ds .</p><p>Comparison to query expansion baselines. In <ref type="table" target="#tab_6">Table 5</ref>, the proposed NPRF model is compared with three kinds of query expansion baselines, namely, the unsupervised BM25+QE ( <ref type="bibr" target="#b30">Ye et al., 2009</ref>), QL+RM3 <ref type="bibr" target="#b14">(Lavrenko and Croft, 2001</ref>), and DRMM/K-NRM(QE), the neural IR models using expanded queries as input. According to <ref type="table" target="#tab_6">Table 5</ref>, the unsupervised BM25+QE baseline appears to achieve better performance in terms of MAP@1k, owing to its use of query expansion to match rel- evant documents containing the expansion terms from the whole collection. On the other hand, NPRF ds , which reranks the top-1000 documents returned by BM25, outperforms the query expan- sion baselines in terms of early precision, as mea- sured by either NDCG@20 or P@20. These mea- sures on shallow rankings are particularly impor- tant for general IR applications where the qual- ity of the top-ranked results is crucial to the user satisfaction. Moreover, our NPRF outperforms <ref type="table" target="#tab_2">Title  Description  Model  MAP  P@20  NDCG@20  MAP  P@20  NDCG@20  BM25</ref> 0.2408 -0.4803 -0.4947 -0.2094 -0.4613 -0.4838 - NPRF ff -DRMM 0.2669 † 10.85% 0.5010 4.31% 0.5119 3.47% 0.2509 † 19.80% 0.5257 † 13.95% 0.5393 † 11.46% NPRF ff -DRMM 0.2671 † 10.93% 0.5023 † 4.59% 0.5116 3.42% 0.2504 † 19.58% 0.5163 † 11.93% 0.5291 † 9.37% NPRF ds -DRMM 0.2698 † 12.03% 0.5187 † 7.99% 0.5282 † 6.77% 0.2527 † 20.67% 0.5283 † 14.53% 0.5444 † 12.52% NPRF ff -KNRM 0.2633 † 9.34% 0.5033 4.80% 0.5171 4.52% 0.2486 † 18.71% 0.5240 † 13.59% 0.5398 † 11.58% NPRF ff -KNRM 0.2654 † 10.22% 0.5077 † 5.70% 0.5216 † 5.44% 0.2462 † 17.60% 0.5197 † 12.65% 0.5363 † 10.84% NPRF ds -KNRM 0.2707 † 12.41% 0.5303 † 10.42% 0.5406 † 9.29% 0.2505 † 19.61% 0.5270 † 14.24% 0.5460 † 12.87%  NIRM(QE) in most cases, indicating the benefit brought by wrapping up the feedback informa- tion in a document-to-document matching frame- work as in NPRF, as opposed to directly adding unweighted expansion terms to the query. Recall that, it is not straightforward to incorporate these expanded terms within the existing NIRMs' archi- tectures because the NIRMs do not distinguish be- tween them and the original query terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analysis</head><p>Parameter sensitivity. Moreover, we analyze factors that may influence NPRF's performance.</p><p>We report results on NPRF ds using title queries on Robust04 for the sake of brevity, but similar obser- vations also hold for the other NPRF variants, as well as on TREC1-3. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the sen- sitivity of NPRF relative to two parameters: the number of feedback documents m within D q and the number of terms k that are used to summarize each d q ∈ D q . Specifically, <ref type="figure" target="#fig_1">Figure 2</ref> shows the performance of NPRF ds as the number of feed- back documents m varies (top), and as the number of top terms k varies (bottom). The effectiveness of NPRF appears to be stable over a wide range of the parameter configurations, where the proposed model consistently outperforms the BM25 base- line.</p><p>Case study. A major advantage of the proposed NPRF over existing neural IR models is that it al- lows for soft-matching query-related terms that are missing from both the query and the target doc- ument. <ref type="table" target="#tab_7">Table 6</ref> presents an illustrative example of soft matching in NPRF. From <ref type="table" target="#tab_7">Table 6</ref>, it can be seen that there exist query-related terms in the top-10 documents returned by BM25 in the initial ranking. However, since those query-related terms are missing in both the query and the target doc- ument, they are not considered in the document- to-query matching and, consequently, the target document is ranked 122 nd by BM25 despite the facts that it was judged relevant by a human asses- sor. In contrast, the NPRF framework allows for the soft-matching of terms that are missing in both the query and target document. As a result, the matching signals for the query terms and query- related terms in the target document are enhanced. This leads to enhanced effectiveness with the tar- get document now ranked in the 5 th position.</p><p>In summary, the evaluation on two standard TREC test collections shows promising results ob- tained by our proposed NPRF approach, which outperforms state-of-the-art neural IR models in most cases. Overall, NPRF provides effective re- trieval performance that is robust with respect to the two embedded neural models used for encod- ing the document-to-document interactions, the two kinds of queries with varied length, and wide range of parameter configurations.</p><formula xml:id="formula_10">Title Description Model MAP P@20 NDCG@20 MAP P@20 NDCG@20 DRMM 0.2469 -0.4833 -0.4919 -0.2111 -0.4423 -0.4546 - K-NRM 0.2284 -0.4410 -0.4530 -0.1763 -0.3753 -0.3854 - PACRR-firstk 0.2393 -0.4620 -0.4782 -0.1702 -0.3577 -0.3666 - NPRF ff -DRMM 0.2669 †</formula><p>8.12% 0.5010 3.66% 0.5119 4.06% 0.2509 † 18.83% 0.5257 † 18.84% 0.5393 † 18.63% NPRF ff -DRMM 0.2671 † 8.19% 0.5023 3.94% 0.5116 4.01% 0.2504 † 18.61% 0.5163 † 16.73% 0.5291 † 16.40% NPRF ds -DRMM 0.2698 † 9.26% 0.5187 † 7.32% 0.5282 † 7.38% 0.2527 † 19.69% 0.5283 † 19.44% 0.5444 † 19.76% NPRF ff -KNRM 0.2633 † 15.28% 0.5033 † 14.13% 0.5171 † 14.14% 0.2486 † 40.97% 0.5240 † 39.61% 0.5398 † 40.06% NPRF ff -KNRM 0.2654 † 16.20% 0.5077 † 15.12% 0.5216 † 15.15% 0.2462 † 39.65% 0.5197 † 38.45% 0.5363 † 39.13% NPRF ds -KNRM 0.2707 † 18.51% 0.5303 † 20.26% 0.5406 † 19.35% 0.2505 † 42.04% 0.5270 † 40.41% 0.5460 † 41.67% <ref type="table">Table 3</ref>: Comparisons between NPRF and neural IR models on TREC1-3. Relative performances of NPRF-DRMM(KNRM) compared with DRMM (K-NRM) are in percentages, and statistically significant improvements are marked with †.  compared with DRMM (K-NRM) are in percentages, and statistically significant improvements are marked with †.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Recently, several neural IR models (NIRMs) have been proposed to apply deep learning tech- niques in ad-hoc information retrieval. One of the essential ideas from prior work is to model the document-to-query interaction via neural net- works, based on a matrix of document-to-query embedding term similarities, incorporating both the "exact matching" of terms appearing in both the document and query and the "soft matching" of different query and document term pairs that are semantically related. DSSM, one of the earliest NIRMs proposed in ( <ref type="bibr" target="#b9">Huang et al., 2013)</ref>, employs a multi-layer neural network to project queries and document into a common semantic space. The cosine sim- ilarity between a query and a document (docu- ment title) is used to produce a final relevance score for the query-document pair. CDSSM is a convolutional version of DSSM, which uses the convolutional neural network (CNN) and max- pooling strategy to extract semantic matching fea- tures at the sentence level <ref type="bibr" target="#b27">(Shen et al., 2014)</ref>. ( <ref type="bibr" target="#b21">Pang et al., 2016</ref>) also employ a CNN to con- struct the MatchPyramid model, which learns hi- erarchical matching patterns between local inter- actions of document-query pair. (  argue that both DSSM and CDSSM are representation-focused models, and thus are bet- ter suited to capturing semantic matching than relevance matching (i.e., lexical matching), and propose the interaction-focused relevance model named DRMM. DRMM maps the local interac- tions between a query-document pair into a fixed- length histogram, from which the exact matching signals are distinguished from the other matching signals. These signals are fed into a feed for- ward network and a term gating network to pro- duce global relevance scores. Similar to DRMM, K-NRM ( <ref type="bibr" target="#b29">Xiong et al., 2017</ref>) builds its model on top of a matrix of local interaction signals, and utilizes multiple Gaussian kernels to obtain multi- level exact/soft matching features that are input into a ranking layer to produce the final ranking score. K-NRM is later improved by Conv-KNRM, which employs CNN filters to capture n-gram rep- resentations of queries and documents <ref type="bibr" target="#b2">(Dai et al., 2018</ref>). DeepRank ( <ref type="bibr" target="#b22">Pang et al., 2017</ref>) models the relevance generation process by identifying query- centric contexts, processing them with a CNN or LSTM, and aggregating them to produce a final relevance score. Building upon DeepRank, <ref type="bibr" target="#b4">(Fan et al., 2018)</ref> propose to model diverse relevance patterns by a data-driven method to allow rele- <ref type="table" target="#tab_1">TREC1-3  Robust04  Title  Description  Title  Description  Model  MAP  P@20 NDCG@20 MAP  P@20 NDCG@20 MAP  P@20 NDCG@20 MAP  P@20  NDCG@20  BM25+QE</ref> 0.2873 0.5200 0.5330   122 nd by BM25 for query 341 on Robust04, and is promoted to the 5 th by NPRF ds -DRMM. The NPRF mechanism increases the chances of soft-matching query-related terms that appear in the top-ranked documents (terms in blue), but are missing in both the query and the target document. Subsequently, the matching signals with the query terms (in bold) and the query-related terms (in red) in the target document are enhanced.</p><p>vance signals at different granularities to compete with each other for the final relevance assessment.</p><p>Duet ( <ref type="bibr" target="#b19">Mitra et al., 2017</ref>) employs two sepa- rate deep neural networks to build a relevance ranking model, in which a local model estimates the relevance score according to exact matches between query and document terms, and a dis- tributed model estimates relevance by learning dense lower-dimensional representations of query and document text. ( <ref type="bibr" target="#b31">Zamani et al., 2018</ref>) extends the Duet model by considering different fields within a document.</p><p>(  propose the PACRR model based on the idea that an appropriate combina- tion of convolutional kernels and pooling opera- tions can be used to successfully identify both un- igram and n-gram query matches. PACRR is later improved upon by Co-PACRR, a context-aware variant that takes the local and global context of matching signals into account through the use of three new components ( <ref type="bibr" target="#b11">Hui et al., 2018)</ref>.</p><p>( <ref type="bibr" target="#b24">Ran et al., 2017)</ref> propose a document-based neural relevance model that utilizes complemented medical records to address the mismatch prob- lem in clinical decision support. <ref type="bibr" target="#b20">(Nogueira and Cho, 2017)</ref> propose a reinforcement learning ap- proach to reformulating a task-specific query. ( <ref type="bibr" target="#b15">Li et al., 2018)</ref> propose DAZER, a CNN-based neu- ral model upon interactions between seed words and words in a document for zero-shot document filtering with adversarial learning. ( <ref type="bibr" target="#b0">Ai et al., 2018)</ref> propose to refine document ranking by learning a deep listwise context model. In summary, most existing neural IR models are based on query-document interaction signals and do not provide a mechanism for incorporat- ing relevance feedback information. This work proposes an approach for incorporating relevance feedback information by embedding neural IR models within a neural pseudo relevance feed- back framework, where the models consume feed- back information via document-to-document in- teractions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work we proposed a neural pseudo rele- vance feedback framework (NPRF) for incorpo- rating relevance feedback information into exist- ing neural IR models (NIRM). The NPRF frame- work uses feedback documents to better estimate relevance scores by considering individual feed- back documents as different interpretations of the user's information need. On two standard TREC datasets, NPRF significantly improves the perfor- mance of two state-of-the-art NIRMs. Further- more, NPRF was able to improve their perfor- of PRF documents (top) and different umber of terms which are used to summarize the feedback documents (bottom). The •, , correspond to results measured by MAP, P@20 and NDCG@20 respectively, and the empty or solid symbols cor- respond to those for NPRF ds -DRMM and NPRF ds -KNRM. The three dotted lines, from bottom to top, are the BM25 baseline evaluated by MAP, P@20 and NDCG@20, respec- tively.</p><p>mance across two kinds of query tested (namely, short queries and the verbal queries in natural lan- guage). Finally, our analysis demonstrated the ro- bustness of the NPRF framework over different parameter configurations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of the proposed neural pseudo relevance feedback (NPRF) framework.</figDesc><graphic url="image-1.png" coords="3,117.35,62.81,362.84,198.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance of NPRF ds with different numbers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Comparisons between NPRF and BM25 on TREC1-3 dataset. Relative performances compared with BM25 are in percentages. Significant improvements relative to the baselines are marked with †.</figDesc><table>Title 
Description 
Model 
MAP 
P@20 
NDCG@20 
MAP 
P@20 
NDCG@20 
BM25 
0.2533 
-0.3612 
-0.4158 
-0.2479 
-0.3514 
-0.4110 
-
NPRF ff -DRMM 0.2823  † 11.46% 0.3941  † 
9.11% 0.4350  † 4.62% 0.2766  † 11.58% 0.3908  † 11.21% 0.4421  † 
7.56% 
NPRF ff -DRMM 0.2837  † 12.00% 0.3928  † 
8.74% 0.4377  † 5.27% 0.2774  † 11.90% 0.3984  † 13.38% 0.4493  † 
9.32% 
NPRF ds -DRMM 0.2904  † 14.66% 0.4064  † 12.52% 0.4502  † 8.28% 0.2801  † 12.95% 0.4026  † 14.57% 0.4559  † 10.92% 
NPRF ff -KNRM 
0.2809  † 10.90% 0.3851  † 
6.62% 0.4287 3.11% 0.2720  † 
9.71% 0.3867  † 10.06% 0.4356  † 
5.99% 
NPRF ff -KNRM 0.2815  † 11.13% 0.3882  † 
7.48% 0.4264 2.55% 0.2737  † 10.39% 0.3892  † 10.74% 0.4382  † 
6.61% 
NPRF ds -KNRM 0.2846  † 12.36% 0.3926  † 
8.69% 0.4327 4.06% 0.2800  † 12.95% 0.3972  † 13.03% 0.4477  † 
8.94% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Comparisons between NPRF and BM25 on the Robust04 dataset. Relative performances compared with BM25 are in percentages. Significant improvements relative to the baselines are marked with †.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 : Comparisons between NPRF and neural IR models on Robust04. Relative performances of NPRF-DRMM(KNRM)</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc>transec semtex airline ditma security baggage heathrow test device lockerbie klm bomb virgin airport loaded blobby transport detect inspector terrorist identify atlantic depress- ing passenger fail aircraft dummy check inert patchy stein norwich doll regard rupert lapse busiest loophole employee campaign blew procedure traveler passport reconcile glas- gow investigate boeing bags bag harry successive smuggle conscious reconciliation tragedy board wire hidden...</figDesc><table>Comparisons between NPRF and query expansion baselines on TREC1-3 and Robust04. Significant improvements 

over the best baseline is marked with  †. 

TREC Query 341: airport security 
Terms in doc at rank i 
Terms in target document FBIS3-23332 
1. terrorist detect passenger check police scan; 2. heathrow 
terrorist armed aviation police; 3. detect airline passenger 
police scan flight weapon; 4. aviation; 5. detect baggage 
passenger; 6. passenger bomb baggage terrorist explosive 
aviation scan flight weapon; 7. baggage airline detect pas-
senger scan flight weapon; 8. baggage airline passenger 
flight; 9. passenger police aviation; 10. airline baggage 
aviation flight 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc>An illustrative example of soft matching in NPRF. The target document FBIS3-23332, judged relevant, is ranked</figDesc><table></table></figure>

			<note place="foot" n="1"> https://code.google.com/p/word2vec/ (Lavrenko and Croft, 2001), is used as another unsupervised baseline.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported in part by the Na-tional Natural Science Foundation of China (61433015/61472391), and the Beijing Nat-ural Science Foundation under Grant No. (4162067/4142050).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning a deep listwise context model for ranking refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keping</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Relevance feedback track overview: Trec</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<editor>TREC. NIST</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for soft-matching n-grams in ad-hoc search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="126" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Query expansion with locally-trained word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL. The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modeling diverse relevance patterns in ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="375" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A deep relevance matching model for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Overview of the first text retrieval conference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="36" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Overview of the third text retrieval conference (TREC-3)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC, volume Special Publication 500-225</title>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="1" to="20" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Overview of the second text retrieval conference (TREC-2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Manage</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="271" to="289" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">P</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">PACRR: A position-aware neural IR model for relevance matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Co-pacrr: A context-aware neural IR model for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="279" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cumulated gain-based evaluation of ir techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaana</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Relevancebased language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W. Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A deep relevance model for zero-shot document filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiqing</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2300" to="2310" />
		</imprint>
		<respStmt>
			<orgName>The Association for Computer Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From puppy to maturity: Experiences in developing terrier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Mccreadie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSIR at SIGIR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="60" to="63" />
		</imprint>
	</monogr>
	<note>Rodrygo Santos, and Iadh Ounis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Introduction to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to match using local and distributed representations of text for web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1291" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Taskoriented query reformulation with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="574" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A study of matchpyramid models on ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno>abs/1606.04648</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deeprank: A new deep architecture for relevance ranking in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
	<note>Jingfang Xu, and Xueqi Cheng</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A language modeling approach to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A document-based neural relevance model for effective clinical decision support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhua</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BIBM</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="798" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Okapi at TREC-4. In TREC, volume Special Publication 500-236</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micheline</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Gatford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Payne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>NIST</publisher>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The SMART retrieval system: experiments in automatic document processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rocchio</surname></persName>
		</author>
		<editor>Gerard Salton</editor>
		<imprint>
			<date type="published" when="1971" />
			<publisher>Prentice Hall</publisher>
			<biblScope unit="page" from="313" to="323" />
			<pubPlace>Englewood, Cliffs, New Jersey</pubPlace>
		</imprint>
	</monogr>
	<note>Relevance feedback in information retrieval</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning semantic representations using convolutional neural networks for web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW (Companion Volume)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="373" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Overview of the TREC 2004 robust track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC, volume Special Publication 500-261. National Institute of Standards and Technology (NIST)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end neural ad-hoc ranking with kernel pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">York university at TREC 2009: Relevance feedback track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC, volume Special Publication 500-278</title>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural ranking models with multiple document fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="700" to="708" />
		</imprint>
	</monogr>
	<note>Xia Song, Nick Craswell, and Saurabh Tiwary</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
