<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">World Knowledge for Reading Comprehension: Rare Entity Prediction with Hierarchical LSTMs Using External Descriptions</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Long</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie</forename><surname>Chi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kit</forename><surname>Cheung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">World Knowledge for Reading Comprehension: Rare Entity Prediction with Hierarchical LSTMs Using External Descriptions</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="825" to="834"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Humans interpret texts with respect to some background information, or world knowledge, and we would like to develop automatic reading comprehension systems that can do the same. In this paper, we introduce a task and several models to drive progress towards this goal. In particular, we propose the task of rare entity prediction: given a web document with several entities removed, models are tasked with predicting the correct missing entities conditioned on the document context and the lexical resources. This task is challenging due to the diversity of language styles and the extremely large number of rare entities. We propose two recurrent neural network architectures which make use of external knowledge in the form of entity descriptions. Our experiments show that our hierarchical LSTM model performs significantly better at the rare entity prediction task than those that do not make use of external resources.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Reading comprehension is the ability to process some text and understand its contents, in order to form some beliefs about the world. The starting point of this paper is the fact that world knowledge plays a crucial role in human reading comprehen- sion and language understanding. Work in the psy- chology of reading literature has demonstrated this point, for example by showing that readers are bet- ter able to recall the contents of a story when it de- scribes a counter-intuitive but plausible sequence of events, rather than a bizarre or a highly pre- dictable one <ref type="bibr" target="#b2">(Barrett and Nyhof, 2001</ref>). This point is also central to work in the Schankian tradition of scripts ( <ref type="bibr" target="#b22">Schank and Abelson, 1977)</ref>.</p><p>Despite the importance of world knowledge, previous data sets and tasks for reading compre- hension have targeted other aspects of the read- ing comprehension problem, at times explicitly at- tempting to factor out its influence. In the Daily Mail/CNN dataset ( <ref type="bibr" target="#b11">Hermann et al., 2015)</ref>, named entities such Clarkson and Top Gear are replaced by anonymized entity tokens like ent212. The Children's Book Test focuses on the role of con- text and memory <ref type="bibr" target="#b12">(Hill et al., 2016a)</ref>, and the fic- tional genre makes it difficult to connect the enti- ties in the stories to real-world knowledge about those entities.</p><p>As a result, language models have proved to be a highly competitive solution to these tasks. <ref type="bibr" target="#b6">Chen et al. (2016)</ref> showed that their attention- based LSTM model achieves state-of-the-art re- sults on the Daily Mail/CNN data set. In fact, their analysis shows that more than half of the ques- tions can be answered by exact word matching and sentence-level paraphrase detection, and that many of the remaining errors are difficult to solve exactly because the entity anonymization proce- dure removes necessary world knowledge.</p><p>In this paper, we propose a novel task called rare entity prediction, which places the use of ex- ternal knowledge at its core, with the following key features. First, our task is similar in flavour to the Children's Book and other language model- ing tasks, in that the goal of the models is to pre- dict missing elements in text. However, our task involves predicting missing named entities, rather than missing words. Second, the number of unique named entities in the data set is very large, roughly on par with the number of documents. As such, there are very few instances per named entity for systems to train on. Instead, they must rely on ex- ternal knowledge sources such as Freebase ( <ref type="bibr" target="#b4">Bollacker et al., 2008</ref>) in order to make inferences <ref type="table">Table 1</ref>: An abbreviated example from the Wik- ilinks Rare Entity Prediction dataset. Shown is an excerpt from the text (context), with a missing en- tity that must be predicted from a list of candidate entities. Each candidate entity is also provided with its description from Freebase.</p><p>about the likely entities that fit the context.</p><p>For our task, we use a significantly enhanced version of the Wikilinks dataset ( <ref type="bibr" target="#b23">Singh et al., 2012)</ref>, with entity descriptions extracted from Freebase serving as the lexical resources, which we call the Wikilinks Rare Entity Prediction dataset. An example from the Wikilinks Entity Prediction dataset is shown in <ref type="table">Table 1</ref>.</p><p>We also introduce several recurrent neural network-based models for this task which take in entity descriptions of candidate entities. Our first model, DOUBENC, combines information derived from two encoders: one for the text passage be- ing read, and one for the entity description. Our second model, HIERENC, is an extension which considers information from a document-level con- text, in addition to the local sentential context. We show that language modeling baselines that do not consider entity descriptions are unable to achieve good performance on the task. RNN-based mod- els that are trained to leverage external knowl- edge perform much better; in particular, HIERENC achieves a 17% increase in accuracy over the lan- guage model baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Related to our work is the task of entity predic- tion, also called link prediction or knowledge base completion, in the context of multi-relational data. Multi-relational datasets like WordNet <ref type="bibr" target="#b18">(Miller, 1995)</ref> and <ref type="bibr">Freebase (Bollacker et al., 2008)</ref> con- sist of entity-relation triples of the form (head, re- lation, tail). In entity prediction, either the head or tail entity is removed, and the model has to predict the missing entity. Recent efforts have integrated different sources of knowledge, for example com- bining distributional and relational semantics for building word embeddings ( <ref type="bibr" target="#b9">Fried and Duh, 2015;</ref><ref type="bibr" target="#b16">Long et al., 2016)</ref>. While this task requires under- standing and predicting associations between enti- ties, it does not require contextual reasoning with text passages, which is crucial in rare entity pre- diction.</p><p>Rare entity prediction is also clearly distinct from tasks such as entity tagging and recogni- tion ( <ref type="bibr" target="#b21">Ritter et al., 2011</ref>), as models are provided with the actual name of the entity in question, and only have to match the entity with related con- cepts and tags. It is more closely related to the machine reading literature from e.g. <ref type="bibr" target="#b8">Etzioni et al. (2006)</ref>; however, the authors define machine read- ing as primarily unsupervised, whereas our task is supervised.</p><p>A similar supervised reading comprehension task was proposed by <ref type="bibr" target="#b11">Hermann et al. (2015)</ref> us- ing news articles from CNN and the Daily Mail. Given an article, models are tasked with filling in blanks of one-sentence summaries of the article. The original dataset was found to have a low ceil- ing for machine improvement <ref type="bibr" target="#b6">(Chen et al., 2016)</ref>; thus, alternative datasets have been proposed that consist of more difficult questions ( <ref type="bibr" target="#b24">Trischler et al., 2016;</ref><ref type="bibr" target="#b20">Rajpurkar et al., 2016)</ref>. A dataset with a similar task was also proposed by <ref type="bibr" target="#b12">Hill et al. (2016a)</ref>, where models must answer questions about short children's stories. While these tasks require the understanding of unstructured natural language, they do not require integration with ex- ternal knowledge sources. <ref type="bibr" target="#b13">Hill et al. (2016b)</ref> proposed a method of com- bining distributional semantics with an external knowledge source in the form of dictionary defi- nitions. The purpose of their model is to obtain more accurate word and phrase embeddings by combining lexical and phrasal semantics, and they achieve fairly good performance on reverse dictio- naries and crossword puzzle solving tasks.</p><p>Perhaps the most related approach to our work is the one developed by <ref type="bibr" target="#b0">Ahn et al. (2016)</ref>. The authors propose a WikiFacts dataset where Wikipedia descriptions are aligned with Freebase facts. While they also aim to integrate exter- nal knowledge with unstructured natural language, their task differs from ours in that it is primarily a language modeling problem.</p><p>More recently, <ref type="bibr" target="#b1">Bahdanau et al. (2017)</ref> investi- gated a similar approach to generate embeddings for out-of-vocabulary words from their definitions and applied it to a number of different tasks. How- ever, their method mainly focuses on modeling generic concepts and is evaluated on tasks that do not require the understanding of world knowledge specifically. Our work, on the other hand, shows the effectiveness of incorporating external descrip- tions for modeling real-world named entities and is evaluated on a task that explicitly requires the understanding of such external knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Rare Entity Prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Wikilinks Dataset</head><p>The Wikilinks dataset ( <ref type="bibr" target="#b23">Singh et al., 2012</ref>) is a large dataset originally designed for cross-document coreference resolution, the task of grouping en- tity mentions from a set of documents into clusters that represent a single entity. The dataset consists of a list of non-Wikipedia web pages (discovered using the Google search index) that contain hy- perlinks to Wikipedia, such as random blog posts or news articles. Every token with a hyperlink to Wikipedia is then marked and considered an en- tity mention in the dataset. Each entity mention is also linked back to a knowledge base through their corresponding Freebase IDs In order to ensure the hyperlinks refer to the cor- rect Wikipedia pages, additional filtering is con- ducted to ensure that either (1) at least one token in the hyperlink (or anchor) matches a token in the title of the Wikipedia page, or (2) the anchor text matches exactly an anchor from the Wikipedia page text, which can be considered an alias of the page. As many near-duplicate copies of Wikipedia pages can be found online, any web pages where more than 70% of the sentences match those from their linked Wikipedia pages are discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Wikilinks Rare Entity Prediction Dataset</head><p>We use a significantly pre-processed and aug- mented version of the Wikilinks dataset for the pur- pose of entity prediction, which we call the Wik- ilinks Rare Entity Prediction dataset. In particular, we parse the HTML texts of the web pages and ex-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of documents 269,469 Average # blanks per doc 3.69 Average # candidates per doc 3.35 Number of unique entities</head><p>245,116 # entities with n &lt;= <ref type="bibr">5</ref> 207,435 (84.6%) # entities with n &lt;= <ref type="bibr">10</ref> 227,481 (92.8%) # entities with n &lt;= <ref type="bibr">20</ref> 238,025 (97.1%) <ref type="table">Table 2</ref>: Statistics for the augmented version of the Wikilinks dataset, where n represents the en- tity frequency in the corpus. Web documents with more than 10 blanks to fill are filtered out for com- putational reasons.</p><p>tract their page contents to form our corpus. Entity mentions with hyperlinks to Wikipedia are marked and replaced by a special token (**blank**), serv- ing as the placeholder for missing entities that we would like the models to predict. The correct missing entity˜eentity˜ entity˜e is preserved as a target. Addition- ally, we extract the lexical definitions of all enti- ties that are marked in the corpus from Freebase using their Freebase IDs, which are available for all entities in the Wikilinks dataset. These lexical definitions will serve as the external knowledge to our models. <ref type="table">Table 2</ref> shows some basic statistics of a sub- set of the corpus used in our experiments. As we can see, unlike the Children's Book dataset, which has 50k candidate entities for almost 700k context and query pairs ( <ref type="bibr" target="#b12">Hill et al., 2016a</ref>), the number of unique entities found in our dataset has the same order of magnitude as the number of documents available.</p><p>Moreover, the majority of entities appears a rel- atively small number of times, with 92.8% ob- served less than or equal to 10 times across the entire corpus. This suggests that models that only rely on the surrounding context information may not be able to correctly predict the missing enti- ties. This further motivates us to incorporate ad- ditional information into the decision process to improve the performance. In the experiments sec- tion, we show that the external entity descriptions are indeed necessary to achieve better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Task Definition 1</head><p>Here we formalize the task definition of the en- tity prediction problem. Given a document D in <ref type="figure">Figure 1</ref>: An example from the Wikilinks Rare Entity Prediction dataset. Shown is a paragraph from the dataset, along with the context (in blue italics) and the missing entity (in red underline). We also visually show the notation that we use for the remainder of this paper. The correct answer here is Peter Ackroyd. the corpus, we split it into an ordered list of con-</p><formula xml:id="formula_0">texts C = {C 1 , ..., C n } where each context C i (1 ≤ i ≤ n) is a word sequence (w 1 , ..., w m )</formula><p>where the special token **blank** is found. Let E be the set of candidate entities. For each miss- ing entity, we want the model to select the correct entity˜eentity˜ entity˜e ∈ E to fill the blank slot. In our problem setting, the model also has access to the lexical re-</p><formula xml:id="formula_1">source L = {L e | e ∈ E} where L e = (l e 1 , ..., l e k )</formula><p>is the lexical definition of entity e extracted from the knowledge base. Thus, the task of the model is to predict the correct missing entities for each empty slot in D.</p><p>There are several possible ways to specify the candidate set E. For instance, we could define E so that it includes all entities found in the cor- pus. However, given the extremely large amount of unique entities found in the dataset, this would render the task difficult to solve from both a prac- tical and computational perspective. We present a simpler version of the task where E is the set of en- tities that are present in the document D. Note that we can make the task arbitrarily more difficult by randomly sampling other entities from the entity vocabulary and adding them to candidate set.</p><p>We show an example from the Wikilinks Entity Prediction dataset in <ref type="figure">Figure 1</ref>, along with a visual guide to the notation from this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model Architectures</head><p>In this section, we present two models that use the lexical definitions of entities to solve the pro- posed rare entity prediction problem. The basic building blocks of our models are recurrent neu- ral networks (RNN) with long short-term mem- ory (LSTM) units <ref type="bibr" target="#b14">(Hochreiter and Schmidhuber, 1997</ref>). An RNN is a neural network with feedback connections that allows information from the past to be encoded in the hidden layer representation, thus is ideal for modeling sequential data <ref type="bibr" target="#b7">(Dietterich, 2002</ref>) and most language related problems.</p><p>LSTMs are an extension of RNNs which in- clude a memory cell c t alongside their hidden state representation h t . Reads and writes to the mem- ory cell are controlled by a set of three gates that allow the model to either keep or discard infor- mation from the past and update their state with the current input. This allows LSTMs to model potentially longer dependencies and at the same time mitigate the vanishing and exploding gradient problems, which are quite common among regular RNNs ( <ref type="bibr" target="#b3">Bengio et al., 1994)</ref>. In our experiments, we use LSTMs augmented with peephole connec- tions ( <ref type="bibr" target="#b10">Gers et al., 2002</ref>).</p><p>We denote the output (i.e. the last hidden state) of an RNN f operating on a sequence S as f (S), and subscript the t-th hidden state as f t (S).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Double Encoder (DOUBENC)</head><p>This model uses two jointly trained recurrent mod- els, a lexical encoder g(.) and a context encoder f (.), and a logistic predictor P (see <ref type="figure">Figure 2)</ref>.</p><p>The lexical encoder converts the definition of an entity into a vector embedding, while the context encoder repeats the same process for a given con- text to obtain its context embedding. These two embeddings are then used by P to predict if the e → L e :</p><formula xml:id="formula_2">C i : d e h e i w 1 ... ... w m l e 1 ... l e k P (e = ˜ e|C i , L e ) = σ((h e i ) T W d e + b)</formula><p>Figure 2: Our double encoder architecture. Each entity e has an associated lexical definition L e = (l e 1 , l e 2 , ..., l e k ), which is fed through the lexical encoder g (orange squares) to provide an encoding d e . This definition embedding is then fed as the blank input token of context C i to the context encoder f (blue circles), which provides a context embedding h e i .</p><p>given entity-context pair is correct. Additionally, the blank in the context sentence is replaced by the encoded definition embedding to provide more in- formation to f . For an entity e in the candidate set E of docu- ment D, we retrieve its corresponding lexical def- inition L e , itself a sequence of words, to compute its encoding g(L e ) ≡ d e .</p><p>For a given context C i , we replace the em- bedding of the blank token with d e . Thus</p><formula xml:id="formula_3">C i = (w 1 , ..., w blank , ..., w m ) becomes C e i = (w 1 , ..., d e , ..., w m ) 2 .</formula><p>We then compute the con- text embedding of the new C e i , f (C e i ) ≡ h e i . After getting h e i and d e , we wish to compute the probability of candidate e being the correct entity˜e entity˜ entity˜e missing in context C i . This probability is the output of the predictor:</p><formula xml:id="formula_4">P (e = ˜ e|C i , L e ) = σ((h e i ) T W d e + b)</formula><p>where σ is the sigmoid function, W and b are model parameters. The cross term (h e i ) T W d e is a dot product be- tween h e i and d e that weighs the dimensions dif- ferently based on the learned parameters W . Sim- ilar prediction methods have been used success- fully for question answering ( <ref type="bibr" target="#b5">Bordes et al., 2014;</ref><ref type="bibr" target="#b25">Yu et al., 2014</ref>) and dialogue response retrieval ( <ref type="bibr" target="#b17">Lowe et al., 2015)</ref>.</p><p>We also experimented with only feeding h e i to the predictor, without the cross term, and found this slows down training the lexical encoder.</p><p>While h e i is a function of d e , using d e in the cross term (h e i ) T W d e provides a much shorter gradient path from the loss to the lexical encoder through d e , thus allowing both modules to learn at the same pace.</p><p>Given a context, the model outputs a probabil- ity for each entity e ∈ E. Entities in the candidate set are then ranked against each other according to their predicted probabilities. The entity with the highest probability is considered as the most plau- sible answer for the missing entity in the current context. We consider the model to make an error if that entity is not˜enot˜ not˜e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hierarchical Double Encoder (HIERENC)</head><p>The double encoder architecture mentioned above considers each context independently. However, since each document consists of a sequence of contexts, the knowledge carried by other contexts in C could also provide useful information for the decision process of C i . To that end, we propose a hierarchical model structure by adding a LSTM network r, which we call the temporal network (see <ref type="figure">Figure 3</ref>), on top of the double encoder ar- chitecture. Since a document is a sequence of C i s, each time step of this network consists of one such context, and thus is indexed with i.</p><p>Since we already have a context encoder f , we reuse the output of f (C e i ) as the input of r at time step i. More specifically, we combine the embed- dings generated by f into a single one via aver- aging:</p><formula xml:id="formula_5">¯ h i = 1 |E| e ∈E h e i</formula><p>, which then serves as the input to the temporal network for context C i . Note that alternatively, one could aggregate infor- mation about the past predictions through other means like policies or soft attention. However, this would introduce extra complexities to the learning process. As such, we use averaging to that end.</p><p>Finally, at each time step i, the temporal net- work outputs an embedding r i (C 1 , ..., C n ) ≡ r i . We use this temporal embedding to predict the probability of the context-entity pair with a</p><formula xml:id="formula_6">¯ h i = 1 |E| e ∈E h e i ¯ h i−1 ¯ h i−2</formula><p>...</p><formula xml:id="formula_7">P (e = ˜ e|C 1,...,i , L e ) = σ((h e i ) T W d e + r T i V + b) r i ... d e ... d e h e i C i</formula><p>Figure 3: Our hierarchical encoder architecture. Each entity e is encoded as d e , at each time step, h e i is computed for each e. ¯ h i is the average encoding, which is fed as input to the temporal network r (green diamonds). The temporal network produces r i , which is used to compute P (e = ˜ e|C 1,...,i , L e ).</p><p>slightly altered logistic predictor:</p><formula xml:id="formula_8">P (e = ˜ e|C 1,...,i , L e ) = σ((h e i ) T W d e + r T i V + b)</formula><p>where W , V and b are model parameters. The entities in candidate set are again ranked against each other based on their probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>We randomly partition the data into training, vali- dation and test sets. The training set consists of ap- proximately 80% of the total documents, the vali- dation and test sets comprise about 10% each.</p><p>In our experiments, the context windows are de- fined as the sentences where the special **blank** tokens are found; the lexical definitions for each entity are the first sentences of their Freebase de- scriptions. We experimented with different config- urations of defining contexts and entity definitions, such as expanding the context windows by includ- ing sentences that come before and after the one where blank is found, as well as taking more than one sentence out of the entity description. How- ever, results on validation set show that increasing the context window size and the definition size had very little impact on accuracies, but drastically in- creased the training time of all models. We thus chose to use only the immediate sentence of the context and the first sentence of the entity descrip- tion.</p><p>To train our models, we use the correct missing entity for each blank as the positive example and all other entities in the candidate set as the negative examples, which we found to be more beneficial empirically than using only a subset of rest of the candidate set. During the testing phase, we present each entity in the candidate set to our models and record the probabilities output by the models. The entity with the highest probability is chosen as the model prediction. For all gradient-based methods, including both baseline models and our proposed models, the learning objective is to minimize the binary cross-entropy of the training data.</p><p>We measure the performance on our entity pre- diction task using the accuracy; that is, the number of correct entity predictions made by the model di- vided by the total number of predictions. This is equivalent to the metric of Recall@1 that is often used in information retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>In order to demonstrate the effects of using lex- ical resources as external knowledge for solv- ing the task, we present three sets of baselines: one set of simple baselines (RANDOM and FRE- QENT), one LSTM-based model that only relies on the contexts and does not utilize the definitions (CONTENC), and another set of models that do make use of the entity definitions but in a simplis- tic fashion (TF-IDF + COS and AVGEMB + COS).</p><p>RANDOM For each context in a given document, this baseline simply selects an entity from the can- didate set uniformly at random as its prediction.</p><p>FREQENT Under this baseline, we rank all en- tities in the candidate set by the number of times that they appear in the document. For each blank in the document, we always choose the entity with the highest frequency in that document as the pre- diction. Note that this baseline has access to extra information compared to the other models, in par-ticular the total number of times each entity ap- pears in the document.</p><p>CONTENC Instead of using their definitions, entities are treated as regular tokens in vocabu- lary. Thus for a particular entity e, the context sequence C i = (w 1 , ..., w blank , ..., w m ) <ref type="figure">becomes  (w 1 , ..., w e , ..., w m )</ref>. We feed the sequence C i into the context encoder and as usual take the last hid- den state as the context embedding h e i . Thus given C i and e ∈ E, the probability of e being the cor- rect missing entity is:</p><formula xml:id="formula_9">P (e = ˜ e|C i ) = σ((h e i ) T W + b)</formula><p>where again σ is the sigmoid function, W and b are model parameters. This model is essentially a language model baseline, that does not make use of the external a priori knowledge. AVGEMB + COS This baseline computes the context embedding by taking the average of some pre-trained word embeddings. The entities' em- beddings are computed in the same way. In our ex- periments, we choose to use the published GloVe ( <ref type="bibr" target="#b19">Pennington et al., 2014</ref>) pre-trained word embed- dings. Same as above, the prediction is made by considering the cosine similarity between the con- text embedding and the entity embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TF-IDF + COS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Hyperparameters</head><p>For the CONTENC baseline, we choose 300 as the size of hidden state for the encoder. For the DOUBENC and the HIERENC models, the size of hidden state for both the context encoder and the lexical encoder is set to 300. An RNN with 200 LSTM units is used as the temporal net- work in the hierarchical case. All three mod- els are trained with stochastic gradient descent with Adam ( <ref type="bibr" target="#b15">Kingma and Ba, 2015)</ref> as our opti- mizer, with learning rates of 10 −3 used for CON- TENC and 10 −4 used for DOUBENC as well as   HIERENC. Models with the best performance on validation set are saved and used to test on test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head><p>Empirical results are shown in <ref type="table" target="#tab_1">Table 3</ref>. We test our proposed model architectures (detailed in Section 4), along with baselines described in Section 5.2. It is clear from <ref type="table" target="#tab_1">Table 3</ref> that models that only use contextual knowledge give relatively poor per- formance compared to the ones that utilize lexi- cal resources. The large discrepancy between the context encoder and the double encoder shows that lexical resources play a crucial role in solving the task. The best result is achieved by the hierarchi- cal double encoder, which confirms that knowing about previous contexts is indeed beneficial to the prediction at the current time step.</p><p>We performed statistical significance tests on the predictions from CONTENC, compared to the predictions made by DOUBENC and HIERENC re- spectively. Both tests yielded p &lt; 10 −5 . We also computed the p-value between DOUBENC and HI-  ERENC, with p ≈ 0.003. This suggests that the performance improvement achieved by the hierar- chical model is statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Figure 4 provides a performance breakdown of test accuracies over various entity frequencies for CONTENC, DOUBENC, and HIERENC. As we can see, the biggest performance gap between the baseline and our two proposed models oc- curs when n ≤ 5; as entity frequencies increase, the accuracy of CONTENC also increases. This confirms the value and necessity of lexical re- sources, especially when entities appear extremely infrequently. We also see that HIERENC outper- forms DOUBENC consistently over all frequency ranges. This suggests that by propagating infor- mation from the past through temporal network, the hierarchical model is able to reason beyond the local context, thus achieve higher accuracies. <ref type="table" target="#tab_3">Table 4</ref> shows an example found in the test set, along with the predictions from CONTENC and HIERENC. Even though the context encoder base- line was able to identify that the missing entity should be a city, it incorrectly predicted Istanbul. This is likely because Istanbul appears 86 times in the dataset, whereas Larnaca appears only twice in the test set, and not at all in the training set. It seems that, although the context encoder was able to derive a strong association between Istanbul and Middle Eastern geolocations, such knowledge was not learned for Larnaca because of the lack of ex- amples. Conversely, the hierarchical double en- coder was able to take both the context and the external knowledge into account and successfully predicted the correct missing city.</p><p>One interesting observation is the margin of dif- ference in accuracy between the context encoder and the embedding average baseline. The con- text encoder, which is a relatively sophisticated context-only model, only slightly outperforms the simple embedding average baseline that has no learning component. This suggests that the entity definitions are valuable in solving such tasks even when it is used in a rather simplistic way.</p><p>As we discussed in Section 5.1, we found in ini- tial experiments that using a large context window size (including sentences before and after the sen- tence where blank token is found) does not have any significant positive impact on the results. This may imply that words that are most informative about the missing entity in the blank are generally found in vicinity of the blank. It is also likely that more sophisticated models will be able to use the surrounding context information more effectively, leading to greater performance increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper, we examined the use of external knowledge in the form of lexical resources to solve reading comprehension problems. Specifi- cally, we propose the problem of rare entity pre- diction. In our Wikilinks Rare Entity Prediction dataset, the majority of the entities have very low frequencies across the entire corpus; thus, mod- els that solely rely on co-occurrence statistics tend to under-perform. We show that models leverag- ing the Freebase descriptions achieve large per- formance gains, particularly when this informa- tion is incorporated intelligently as in our double encoder-based models.</p><p>For future work, we plan to examine the effects of other knowledge sources. In this paper, we use entity definitions as the source of external knowl- edge. However, Freebase also contains other types of valuable information, such as relational infor- mation between entities. Thus, one potential di- rection for future work would be to incorporate both relational information and lexical definitions.</p><p>We have demonstrated the crucial role that ex- ternal knowledge plays in solving tasks with many rare entities. We believe that incorporating ex- ternal knowledge into other systems, such as dia- logue agents, should also see similar positive re- sults. We plan to explore the idea of external knowledge integration further in future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This method takes the term frequency-inverse document frequency (TF-IDF) vectors of the context and the entity definition as their corresponding embeddings. The aggrega- tions of contexts and definitions are treated as their own corpora, and two separate TF-IDF transform- ers are fitted. Candidate entities are ranked by the cosine similarity between their definition vectors and the context vector. The entity with the highest cosine similarity score is chosen as the prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Accuracies of CONTENC, DOUBENC, and HIERENC on test set, for different frequency ranges; n is entity frequency in the entire corpus.</figDesc><graphic url="image-2.png" coords="7,307.28,261.19,218.26,130.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Empirical results on Wikilinks Entity Pre-
diction dataset for proposed baselines and models. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>An example from the test set, with the 
predictions made by CONTENC and HIERENC; 
HIERENC was able to successfully predict the cor-
rect missing entity, Larnaca. 

</table></figure>

			<note place="foot" n="1"> On notation: we use A to denote sequences, A to denote sets, a to denote words / entities, a to denote vectors, A to denote matrices.</note>

			<note place="foot" n="2"> We mix the word / vector notation here since each word w is replaced by its corresponding word embedding vector.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by Samsung Advanced Institute of Technology (SAIT). We would like to thank the anonymous reviewers for their com-ments and suggestions. We would also like to thank Dzmitry Bahdanau, Tom Bosc, and Pascal Vincent for their discussions on related work, as well as Timothy O'Donnell for his feedback dur-ing the writing of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoul</forename><surname>Sungjin Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanel</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Pärnamaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00318</idno>
		<title level="m">A neural knowledge language model</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Bosc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00286</idno>
		<title level="m">Learning to compute word embeddings on the fly</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spreading non-natural concepts: the role of intuitive conceptual structures in memory and transmission of cultural materials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">L</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><forename type="middle">A</forename><surname>Nyhof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognition and Culture</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="100" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2008 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Open question answering with weakly supervised embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="165" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A thorough examination of the cnn/daily mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Machine learning for sequential data: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Structural, syntactic, and statistical pattern recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="15" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
		<title level="m">Machine reading. In AAAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1517" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incorporating both distributional and relational semantics in word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR&apos;15: International Conference on Learning Representations (workshop)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning precise timing with lstm recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicol</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="115" to="143" />
			<date type="published" when="2002-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1684" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The goldilocks principle: reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to understand phrases by embedding the dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2015</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Leveraging lexical resources for learning entity embeddings in multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie Chi Kit</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Ubuntu dialogue corpus: a large dataset for research in unstructured multi-turn dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGDIAL</title>
		<meeting>SIGDIAL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">GloVe: global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Named entity recognition in tweets: an experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1524" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Scripts, goals, plans, and understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Schank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abelson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<publisher>Erlbaum</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Wikilinks: a large-scale cross-document coreference corpus labeled via links to Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno>UM-CS-2012-015</idno>
		<imprint>
			<date type="published" when="2012" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">NewsQA: a machine comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09830</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning for answer sentence selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<meeting><address><addrLine>Montreal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
