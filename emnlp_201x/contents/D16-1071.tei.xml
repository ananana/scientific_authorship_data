<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LAMB: A Good Shepherd of Morphologically Rich Languages</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ebert</surname></persName>
							<email>ebert@cis.lmu.de</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">M</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sch¨</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LAMB: A Good Shepherd of Morphologically Rich Languages</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="742" to="752"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper introduces STEM and LAMB, em-beddings trained for stems and lemmata instead of for surface forms. For morphologically rich languages, they perform significantly better than standard embeddings on word similarity and polarity evaluations. On a new WordNet-based evaluation, STEM and LAMB are up to 50% better than standard em-beddings. We show that both embeddings have high quality even for small dimension-ality and training corpora.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Despite their power and prevalence, embeddings, i.e., (low-dimensional) word representations in vec- tor space, have serious practical problems. First, large text corpora are necessary to train high-quality embeddings. Such corpora are not available for un- derresourced languages. Second, morphologically rich languages (MRLs) are a challenge for stan- dard embedding models because many inflectional forms are rare or absent even in a large corpus. For example, a Spanish verb has more than 50 forms, many of which are rarely used. This leads to miss- ing or low quality embeddings for such inflectional forms, even for otherwise frequent verbs, i.e., spar- sity is a problem. Therefore, we propose to com- pute normalized embeddings instead of embeddings for surface/inflectional forms (referred to as forms throughout the rest of the paper): STem EMbed- dings (STEM) for word stems and LemmA eMBed- dings (LAMB) for lemmata.</p><p>Stemming is a heuristic approach to reducing form-related sparsity issues. Based on simple rules, forms are converted into their stem. 1 However, often the forms of one word are converted into several dif- ferent stems. For example, present indicative forms of the German verb "brechen" (to break) are mapped to four different stems ("brech", "brich", "bricht", "brecht"). A more principled solution is lemmatiza- tion. Lemmatization unites many individual forms, many of which are rare, in one equivalence class, represented by a single lemma. Stems and equiva- lence classes are more frequent than each individual form. As we will show, this successfully addresses the sparsity issue.</p><p>Both methods can learn high-quality semantic representations for rare forms and thus are most ben- eficial for MRLs as we show below. Moreover, less training data is required to train lemma embeddings of the same quality as form embeddings. Alterna- tively, we can train lemma embeddings that have the same quality but fewer dimensions than form em- beddings, resulting in more efficient applications.</p><p>If an application such as parsing requires in- flectional information, then stem and lemma em- beddings may not be a good choice since they do not contain such information. However, NLP applications such as similarity benchmarks (e.g., <ref type="bibr">MEN (Bruni et al., 2014)</ref>) and (as we show below) polarity classification are semantic and are largely independent of inflectional morphology.</p><p>Our contributions are the following. (i) We intro- duce the normalized embeddings STEM and LAMB and show their usefulness on different tasks for five languages. This paper is the first study that compre- hensively compares stem/lemma-based with form- based embeddings for MRLs. (ii) We show the ad- vantage of normalization on word similarity bench- marks. Normalized embeddings yield better perfor- mance for MRL languages on most datasets (6 out of 7 datasets for German and 2 out of 2 datasets for Spanish). (iii) We propose a new intrinsic related- ness evaluation based on WordNet graphs and pub- lish datasets for five languages. On this new evalua- tion, LAMB outperforms form-based baselines by a big margin. (iv) STEM and LAMB outperform base- lines on polarity classification for Czech and En- glish. (v) We show that LAMB embeddings are effi- cient in that they are high-quality for small training corpora and small dimensionalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There have been a large number of studies on En- glish, a morphologically simple language, that show that the effect of normalization, in particular stem- ming, is different for different applications. For in- stance, <ref type="bibr" target="#b18">Karlgren and Sahlgren (2001)</ref> analyze the impact of morphological analysis on creating word representations for synonymy detection. They com- pare several stemming methods. <ref type="bibr" target="#b5">Bullinaria and Levy (2012)</ref> use stemming and lemmatization be- fore training word representations. The improve- ment of morphological normalization in both stud- ies is moderate in the best case. <ref type="bibr" target="#b23">Melamud et al. (2014)</ref> compute lemma embeddings to predict re- lated words given a query word. They do not com- pare form and lemma representations.</p><p>A finding about English morphology does not provide insight into what happens with the morphol- ogy of an MRL. In this paper we use English to provide a data point for morphologically poor lan- guages. Although we show that normalization for embeddings increases performance significantly on some applications -a novel finding to the best of our knowledge -morphologically simple languages (for which normalization is expected to be less im- portant) are not the main focus of the paper. Instead, MRLs are the main focus. For these, we show large improvements on several tasks.</p><p>Recently, <ref type="bibr">Köper et al. (2015)</ref> compared form and lemma embeddings on English and German focus- ing on morpho-syntactic and semantic relation tasks. Generally, they found that lemmatization has lim- ited impact. We extensively study MRLs and find a strong improvement on MRLs when using normal- ization, on intrinsic as well as extrinsic evaluations.</p><p>Synonymy detection is a well studied problem in the NLP community <ref type="bibr" target="#b40">(Turney, 2001;</ref><ref type="bibr" target="#b39">Turney et al., 2003;</ref><ref type="bibr" target="#b0">Baroni and Bisi, 2004;</ref><ref type="bibr" target="#b33">Ruiz-Casado et al., 2005;</ref><ref type="bibr">Grigonyt˙ e et al., 2010)</ref>. <ref type="bibr" target="#b32">Rei and Briscoe (2014)</ref> classify hyponomy relationships through em- bedding similarity. Our premise is that seman- tic similarity comprises all of these relations and more. Our ranking-based word relation evaluation addresses this issue. Similar to <ref type="bibr" target="#b23">Melamud et al. (2014)</ref>, our motivation is that, in contrast to standard word similarity benchmarks, large resources can be automatically generated for any language with a WordNet. This is also exploited by <ref type="bibr" target="#b38">Tsvetkov et al. (2015)</ref>. Their intrinsic evaluation method requires an annotated corpus, e.g., annotated with WordNet supersenses. Our approach requires only the Word- Net.</p><p>An alternative strategy of dealing with data spar- sity is presented by <ref type="bibr" target="#b36">Soricut and Och (2015)</ref>. They compute morphological features in an unsupervised fashion in order to construct a form embedding by the combination of the word's morphemes. We ad- dress scenarios (such as polarity classification) in which morphological information is less important, thus morpheme embeddings are not needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Stem/Lemma Creation</head><p>The main hypothesis of this work is that normaliza- tion addresses sparsity issues, especially for MRLs, because although a particular word form might not have been seen in the text, its stem or lemma is more likely to be known. For all stemming experiments we use SNOWBALL, 2 a widely used stemmer. It nor- malizes a form based on deterministic rules, such as replace the suffix 'tional' by 'tion' for English.</p><p>For lemmatization we use the pipeline version of the freely available, high-quality lemmatizer LEM-MING <ref type="bibr">(Müller et al., 2015)</ref>. Since it is a language- independent token-based lemmatizer it is especially suited for our multi-lingual experiments. Moreover, it reaches state-of-the-art performance for the five languages that we study. We train the pipeline us- ing the Penn Treebank ( <ref type="bibr" target="#b22">Marcus et al., 1993)</ref> for En- glish, SPMRL 2013 shared task data ( <ref type="bibr" target="#b35">Seddah et al., 2013)</ref> for <ref type="bibr">German and</ref><ref type="bibr">Hungarian, and</ref><ref type="bibr">CoNLL 2009 (Hajič et al., 2009</ref>) datasets for Spanish and Czech. We additionally use a unigram list extracted from Wikipedia datasets and the ASPELL dictionary of each language. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments 4.1 Word Similarity</head><p>Our first experiment evaluates how well STEM/LAMB embeddings predict human word similarity judgments. Given a pair of words (m, n) with a human-generated similarity value and a set of embeddings E we compute their similarity as cosine similarity. For form embeddings E F , we directly use the embeddings of the word pairs' forms (E F m and E F n ) and compute their similarity. For STEM we use E S stem(w) , where stem(w) is the stem of w. For LAMB we use E L lemma(w) , where lemma(w) is the lemma of w; we randomly select one of w's lemmata if there are several. We conduct experiments on English (en), German (de) and Spanish (es). <ref type="table" target="#tab_2">Table 1</ref> gives dataset statistics.</p><p>For good performance, high-quality embeddings trained on large corpora are required. Hence, the training corpora for German and Spanish are web corpora taken from COW14 <ref type="bibr" target="#b34">(Schäfer, 2015)</ref>. Preprocessing includes removal of XML, conver- sion of HTML characters, lowercasing, stemming using SNOWBALL and lemmatization using LEM- MING. We use the entire Spanish corpus (3.7 bil- lion tokens), but cut the German corpus to approxi- mately 8 billion tokens to be comparable to <ref type="bibr">Köper et al. (2015)</ref>. We train CBOW models ( <ref type="bibr" target="#b25">Mikolov et al., 2013</ref>) for forms, stems and lemmata us- ing WORD2VEC 4 with the following settings: 400 dimensions, symmetric context of size 2 (no dy- namic window), 1 training iteration, negative sam- pling with 15 samples, a learning rate of 0.025, min-imum count of words of 50, and a sampling param- eter of 10 −5 . CBOW is chosen, because it trains much faster than skip-gram, which is beneficial on these large corpora.</p><p>Since the morphology of English is rather sim- ple we do not expect STEM and LAMB to reach or even surpass highly optimized systems on any word similarity dataset (e.g., <ref type="bibr" target="#b4">Bruni et al. (2014)</ref>). There- fore, for practical reasons we use a smaller train- ing corpus, namely the preprocessed and tokenized Wikipedia dataset of . <ref type="bibr">5</ref> Embeddings are trained with the same settings (us- ing 5 iterations instead of only 1, due to the smaller size of the corpus: 1.8 billion tokens). <ref type="table" target="#tab_2">Table 1</ref> shows results. We also report the Spear- man correlation on the vocabulary intersection, i.e., only those word pairs that are covered by the vocab- ularies of all models.</p><p>Results. Although English has a simple morphol- ogy, LAMB improves over form performance on MEN and SL. A tie is achieved on RW. These are the three largest English datasets, giving a more re- liable result. Both models perform comparably on WS. Here, STEM is ahead by 1 point. Forms are better on the small datasets MC and RG, where a single word pair can have a large influence on the result. Additionally, these are datasets with high fre- quency forms, where form embeddings can be well trained. Because of the simple morphology of En- glish, STEM/LAMB do not outperform forms or only by a small margin and thus they cannot compete with highly optimized state-of-the-art systems. <ref type="bibr">6</ref> On German, both STEM and LAMB perform bet- ter on all datasets except WS. We set the new state- of-the-art of 0.79 on Gur350 (compared to 0.77 (Szarvas et al., 2011)) and 0.39 on ZG (compared to 0.25 (Botha and Blunsom, 2014)); 0.83 on Gur65 (compared to 0.79 <ref type="bibr">(Köper et al., 2015)</ref>) is the best performance of a system that does not need addi- tional knowledge bases (cf. <ref type="bibr" target="#b30">Navigli and Ponzetto (2012)</ref>, <ref type="bibr" target="#b37">Szarvas et al. (2011)</ref>).</p><p>LAMB's results on Spanish are equally good. 0.82 on MC and 0.58 on WS are again the best per-formances of a system not requiring an additional knowledge base (cf. <ref type="bibr" target="#b30">Navigli and Ponzetto (2012)</ref>). The best performance before was 0.64 for MC and 0.50 for WS <ref type="bibr">(both Hassan and Mihalcea (2009)</ref>). STEM cannot improve over form embeddings, show- ing the difficulty of Spanish morphology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Word Relations</head><p>Word similarity benchmarks are not available for many languages and are expensive to create. To rem- edy this situation, we create word similarity bench- marks that leverage WordNets, which are available for a great number of languages.</p><p>Generally, a representation is deemed good if words related by a lexical relation in WordNet -syn- onymy, hyponymy etc. -have high cosine similar- ity with this representation. Since the gold standard necessary for measuring this property of a represen- tation can be automatically derived from a WordNet, we can create very large similarity benchmarks with up to 50k lemmata for the five languages we investi- gate: Czech, English, German, Hungarian and Span- ish.</p><p>We view each WordNet as a graph whose edges are the lexical relations encoded by the WordNet, e.g., synonymy, antonymy and hyponymy. We then define L as the set of lemmata in a WordNet and the distance d(l, l ) between two lemmata l and l as the length of the shortest path connecting them in the graph. The k-neighborhood N k (l) of l is the set of lemmata l that have distance k or less, excluding l:</p><formula xml:id="formula_0">N k (l) := {l |d(l, l ) ≤ k, l = l }.</formula><p>The rank of l for an embedding set E is defined as:</p><formula xml:id="formula_1">rank k E (l) := argmin i l i ∈ N k (l)<label>(1)</label></formula><p>where l i is the lemma at position i in the list of all lemmata in the WordNet, ordered according to co- sine similarity to l in descending order. We restrict i ∈ <ref type="bibr">[1,</ref><ref type="bibr">10]</ref> and set k = 2 for all experiments in this paper. We omit the indexes k and E when they are clear from context.</p><p>To measure the quality of a set of embeddings we compute the mean reciprocal rank (MRR) on the rank results of all lemmata:</p><formula xml:id="formula_2">MRR E = 1 |L| l∈L 1 rank E (l)<label>(2)</label></formula><p>We create large similarity datasets for five lan- guages: Czech (cz), English (en), German (de), Hungarian (hu) and Spanish (es) by extracting all lemmata from the WordNet version of the respec- tive language. For English and Spanish we use the preprocessed WordNets from the Open Multilingual WordNet <ref type="bibr" target="#b2">(Bond and Paik, 2012)</ref>. We use the <ref type="bibr">Czech and Hungarian WordNets (PALA and SMRZ, 2004;</ref><ref type="bibr" target="#b24">Miháltz et al., 2008)</ref> and GermaNet <ref type="bibr" target="#b14">(Hamp and Feldweg, 1997</ref>) for German. We keep all lemmata that have a known form in the form embeddings and that exist in the lemma embeddings. Moreover, we filter out all synsets that contain only one lemma and discard all multiword phrases. The split into devel- opment and test sets is done in a way that the distri- bution of synset sizes (i.e., the number of lemmata per synset) is nearly equal in both sets.</p><p>The number of lemmata in our evaluation sets can be found in <ref type="table" target="#tab_3">Table 2</ref>. For more insight, we report results on all parts-of-speech (POS), as well as sep- arately for nouns (n), verbs (v) and adjectives (a). <ref type="bibr">7</ref> The data is provided as supplementary material. <ref type="bibr">8</ref> We propose the following models for the embed- ding evaluation. For form embeddings we com- pare three different strategies, a realistic one, an op- timistic one and a lemma approximation strategy. In the realistic strategy (form real), given a query lemma we randomly sample a form, for which we then compute the k-neighborhood. If the neigh- bors contain multiple forms of the same equivalence class, we exclude the repetitions and use the next neighbors instead. For instance, if house is already a neighbor, then houses will be skipped. The opti- mistic strategy (form opt) works similarly, but uses the embedding of the most frequent surface form of a lemma. This is the most likely form to perform best in the form model. This strategy presupposes the availability of information about lemma and sur- face form counts. As a baseline lemma approxi- mation strategy, we sum up all surface form em- beddings that belong to one equivalence class (form sum). For STEM we repeat the same experiments as described for forms, leading to stem real, stem opt and stem sum.     Results. The MRR results in the left half of Ta- ble 3 ("unfiltered") show that for all languages and for all POS, form real has the worst performance among the form models. This comes at no surprise since this model does barely know anything about word forms and lemmata. The form opt model im- proves these results based on the additional infor- mation it has access to (the mapping from lemma to its most frequent form). form sum performs simi- lar to form opt. For Czech, Hungarian and Spanish it is slightly better (or equally good), whereas for English and German there is no clear trend. There is a large difference between these two models on German nouns, with form sum performing consider- ably worse. We attribute this to the fact that many German noun forms are rare compounds and there- fore lead to badly trained form embeddings, which summed up do not lead to high quality embeddings either.</p><note type="other">WS Köper et al. (2015) 280 0.72 0.72 0.71 279, 280, 280 0.72 0.71 0.71 279 ZG Zesch and Gurevych (2006) 222 0.36 0.38 0.39 200, 207, 208 0.36 0.40 0.41 200 en MC Miller and Charles (1991) 30 0.82 0.77 0.80 30, 30, 30 0.82 0.</note><p>Among the stemming models, stem real also is the worst performing model. We can further see that for all languages and almost all POS, stem sum performs worse than stem opt. That indicates that stemming leads to many low-frequency stems or many words sharing the same stem. This is especially apparent in Spanish verbs. There, the stemming models are clearly inferior to form models.</p><p>Overall, LAMB performs best for all languages and POS types. Most improvements of LAMB are significant. The improvement over the best form- model reaches up to 6 points (e.g., Czech nouns). In contrast to form sum, LAMB improves over form opt on German nouns. This indicates that the sparsity issue is successfully addressed by LAMB.</p><p>In general, morphological normalization in terms of stemming or lemmatization improves the result on all languages, leading to an especially substantial improvement on MRLs. For the morphologically very rich languages Czech and Hungarian, the rela- tive improvement of STEM or LAMB to form-based models is especially high, e.g., Hungarian all: 50%. Moreover, we find that MRLs yield lower absolute performance. This confirms the findings of <ref type="bibr">Köper et al. (2015)</ref>. Surprisingly, LAMB yields better perfor- mance on English despite its simple morphology.</p><p>The low absolute results -especially for Hungar- ian -show that we address a challenging task and that our new evaluation methodology is a good eval- uation for new types of word representations.</p><p>For further insight, we restrict the nearest neigh- bor search space to those lemmata that have the same POS as the query lemma. The general findings in the right half of <ref type="table" target="#tab_5">Table 3</ref> ("filtered") are similar to the unrestricted experiment: Normalization leads to su- perior results. The form real and stem real models yield the lowest performance. Form opt improves the performance and form sum is better on average than form opt. Stem sum can rarely improve on stem opt. The best stemming model most often is better than the best form model. LAMB can benefit more from the POS type restriction than the form models. The distance to the best form model generally in- creases, especially on German adjectives and Span- ish verbs. In all cases except on English adjectives, LAMB yields the best performance. Again, in al- most all cases LAMB's improvement over the form- models is significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Polarity Classification</head><p>Our first two evaluations were intrinsic. We now show the benefit of normalization on an extrinsic task. The task is classification of Czech movie reviews (CSFD, <ref type="bibr" target="#b11">Habernal et al. (2013)</ref>) into posi- tive, negative or neutral <ref type="table" target="#tab_6">(Table 4)</ref>. We reimplement lingCNN <ref type="bibr" target="#b7">(Ebert et al., 2015</ref>), a Convolutional Neu- ral Network that uses linguistic information to im- prove polarity classification. This model reaches close to state-of-the-art performance on data of the SemEval 2015 Task 10B (message level polarity). LingCNN takes several features as input: (i) embed- ding features, (ii) linguistic features at word level and (iii) linguistic features at review level.</p><p>We reuse the 50-dimensional Wikipedia embed- dings from Section 4.2 and compare three experi- mental conditions: using forms, STEM and LAMB.</p><p>Linguistic word level features are: (i) SubLex 1.0 sentiment lexicon <ref type="bibr" target="#b41">(Veselovská and Bojar, 2013</ref>) (two binary indicators that word is marked posi- tive/negative); (ii) SentiStrength 10 (three binary in- dicators that word is an emoticon marked as posi- tive/negative/neutral); (iii) prefix "ne" (binary nega- tion indicator in Czech). <ref type="bibr">11</ref> All word level features are concatenated to form a single word representation of the review's input words. The concatenation of these representations is the input to a convolution layer, which has sev- eral filters spanning the whole representation height and several representations (i.e., several words) in width. The output of the convolution layer is input to a k-max pooling layer ( <ref type="bibr" target="#b17">Kalchbrenner et al., 2014</ref>).</p><p>The max values are concatenated with the follow- ing linguistic review level features: (i) the count of elongated words, such as "cooool"; (ii) three count features for the number of positive/negative/neutral emoticons using the SentiStrength list; (iii) a count feature for punctuation sequences, such as "!!!"; (iv) and a feature that counts the number of negated words. (v) A final feature type comprises one count feature each for the number of sentiment words in a review, the sum of sentiment values of these words as provided by the sentiment lexicon, the maximum sentiment value and the sentiment value of the last word ( <ref type="bibr" target="#b27">Mohammad et al., 2013)</ref>. The concatenation of max values and review level features is then for- warded into a fully-connected three-class (positive, negative, neutral) softmax layer. We train lingCNN with AdaGrad ( <ref type="bibr" target="#b6">Duchi et al., 2011</ref>) and early stop- ping, batch size = 100, 200 filters per width of 3-6; k-max pooling with k = 5; learning rate 0.01; and 2 regularization (λ = 5 · 10 −5 ). We also perform this experiment for <ref type="table">English on unfiltered   filtered   form  STEM  form  STEM   lang POS real  opt  sum  real  opt  sum  LAMB  real  opt  sum  real  opt</ref>    <ref type="formula" target="#formula_1">(2015)</ref>'s lexicon fea- tures. They exploit the fact that there are many more sentiment lexicons available in English. Other word level features are the same as above. Sentiment count features at review level are computed sepa- rately for the entire tweet, for all hashtag words and for each POS type <ref type="bibr" target="#b7">(Ebert et al., 2015)</ref>. Considering the much smaller dataset size and shorter sentences of the SemEval data we chose the following hyperparameters: 100k most frequent word types, 100 filters per filter width of 2-5; and k-max pooling with k = 1.</p><p>Results. <ref type="table" target="#tab_7">Table 5</ref> lists the 10-fold cross-validation results (accuracy and macro F 1 ) on the CSFD dataset. LAMB/STEM results are consistently better than form results.</p><p>In our analysis, we found the following example for the benefit of normalization: "popis a název za- jmav´yjmav´y a film je taková filmařská prasárna" (engl. "description and title are interesting, but it is bad film-making"). This example is correctly classified as negative by the LAMB model because it has an embedding for "prasárna" (bad, smut) whereas the form model does not.</p><p>The out-of-vocabulary counts for form and LAMB on the first fold of the CSFD experiment are 26.3k and 25.5k, respectively. The similarity of these two numbers suggests that the quality of word embed- dings (form vs. LAMB) are responsible for the per- formance gain.</p><p>On the SemEval data, LAMB improves the results over form and stem (cf. <ref type="table" target="#tab_7">Table 5</ref>). <ref type="bibr">12</ref> Hence, LAMB can still pick up additional information despite the simple morphology of English. This is probably due to better embeddings for rare words. The SemEval 2015 winner <ref type="bibr" target="#b12">(Hagen et al., 2015</ref>) is a highly domain- dependent and specialized system that we do not outperform.</p><p>In the introduction, we discussed that normaliza- tion removes inflectional information that is nec- essary for NLP tasks like parsing. For polarity classification, comparatives and superlatives can be important. Further analysis is necessary to deter <ref type="table" target="#tab_2">-dataset   total  pos  neg  neu   CSFD  91379 30896 29716 30767  SemEval train  9845  3636  1535  4674  SemEval dev  3813  1572  601  1640  SemEval test  2390  1038</ref> 365 987  mine whether their normalization hurts in our exper- iments. However, note that we evaluate on polarity only, not on valence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>Normalized embeddings deal better with sparsity than form embeddings. In this section, we demon- strate two additional benefits of LAMB based on its robustness against sparsity.</p><p>Embedding Size. We now show that LAMB can train embeddings with fewer dimensions on the same amount of data and still reach the same per- formance as larger form embeddings. We repeat the word relation experiments of Section 4.2 (all POS) and train all models with embedding sizes 10, 20, 30 and 40 for Spanish. We choose Spanish because it has richer morphology than English and more train- ing data than Czech and Hungarian. <ref type="figure">Figure 1</ref> depicts the MRR results of all models with respect to embedding size. The relative rank- ing of form models is real &lt; opt &lt; sum. That comes from the additional information the more complex models have access to. All stemming mod- els reach lower performance than their form coun- terparts (similar to results in <ref type="table" target="#tab_5">Table 3</ref>). That suggests that stemming is not a proper alternative to correctly dealing with Spanish morphology. LAMB reaches higher performance than form real with already 20 dimensions. The 30 dimensional LAMB model is better than all other models. Thus, we can create lower-dimensional lemma embeddings that are as good as higher-dimensional form embeddings; this has the benefits of reducing the number of parame- ters in models using these embeddings and of reduc- ing training times and memory consumption.</p><p>Corpus Size. Our second hypothesis is that less training data is necessary to train good embeddings. We create 10 training corpora consisting of the first k percent, k ∈ {10, 20, . . . , 100}, of the random- ized Spanish Wikipedia corpus. With these 10 sub- corpora we repeat the word relation experiments of Section 4.2 (all POS). As query lemmata, we use the lemmata from before that exist in all subcorpora. <ref type="figure" target="#fig_1">Figure 2</ref> shows that the relative ranking among the models is the same as before. This time how- ever, form sum yields slightly better performance than form opt, especially when little training data is available. The stemming models again are inferior to their form counterparts. Only stem opt is able to reach performance similar to form opt. LAMB always reaches higher performance than form real, even when only 10% of the training corpus is used. With 30% of the training corpus, LAMB surpasses the performance of the other models. <ref type="bibr">13</ref> Again, by requiring less than 30% of the training data, embed- ding training becomes much more efficient. Further- more, in low-resource languages that lack the avail- ability of a large homogeneous corpus, LAMB can still be trained successfully.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented STEM and LAMB, embeddings based on stems and lemmata. In three experiments we have shown the superiority compared to com- monly used form embeddings. Especially (but not only) on MRLs, where data sparsity is a problem, both normalized embeddings perform better than form embeddings by a large margin. In a new chal- lenging WordNet-based experiment we have shown four methods of adding morphological information  <ref type="bibr">(opt, sum, STEM, LAMB)</ref>. Here, LAMB is the best of the proposed ways of using morphological infor- mation, consistently reaching higher performance, often by a large margin. STEM methods are not consistently better, indicating that the more princi- pled way of normalization as done by LAMB is to be preferred. The datasets are available as supplemen- tary material at www.cis.uni-muenchen.de/ ebert/.</p><p>Our analysis shows that LAMB needs fewer em- bedding dimensions and less embedding training data to reach the same performance as form embed- dings, making LAMB appealing for underresourced languages.</p><p>As morphological analyzers are becoming more widely available, our method -which is easy to implement, only requiring running the analyzer - should become applicable to more and more lan- guages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>For</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 1: Embedding size analysis</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Word similarity results. The left part shows dataset information. The right part shows Spearman correlation (ρ) for the 

models with their full vocabulary and for the intersection of vocabularies. Coverage is shown for all models in order of appearance. 

Bold is best per vocabulary and row. 

lang set 
all 
a 
n 
v 

cz 
dev 
9694 
852 
6436 2315 
test 
9763 
869 
6381 2433 

de 
dev 51682 6347 40674 5018 
test 51827 6491 40623 5085 

en 
dev 44448 9713 30825 5661 
test 44545 9665 30736 5793 

es 
dev 12384 1711 
8634 1989 
test 12476 1727 
8773 1971 

hu 
dev 19387 1953 15268 2057 
test 19486 1928 15436 2011 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Number of lemmata in WordNet datasets 

natural choice as corpus, because it is available for 
many languages. Therefore, we use the prepro-
cessed and tokenized Wikipedia datasets of Müller 
and Schütze (2015). We train 50-dimensional 
skip-gram embeddings (Mikolov et al., 2013) with 
WORD2VEC on the original, the stemmed and the 
lemmatized corpus, respectively. Embeddings are 
trained for all tokens, because we need high cover-
age; the context size is set to 5, all remaining param-
eters are left at their default value. 9 

9 We train smaller embeddings than before, because we have 
more models to train and training corpora are smaller. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Word relation results. MRR per language and POS type for all models. unfiltered is the unfiltered nearest neighbor search 

space; filtered is the nearest neighbor search space that contains only one POS.  ‡ (resp.  †): significantly worse than LAMB (sign 

test, p &lt; .01, resp. p &lt; .05). Best unfiltered/filtered result per row is in bold. 

the SemEval 2015 Task 10B dataset (cf. Table 4). 
We reimplement Ebert et al. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Polarity classification datasets 

lang features 
acc 
F 1 

cz 
Brychcin et al. (2013) -
81.53 
form 
80.86 80.75 
STEM 
81.51 81.39 
LAMB 
81.21 81.09 

en 
Hagen et al. (2015) 
-
64.84 
form 
66.78 62.21 
STEM 
66.95 62.06 
LAMB 
67.49 63.01 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Polarity classification results. Bold is best per lan-

guage and column. 

</table></figure>

			<note place="foot" n="1"> In this paper, we use the term &quot;stem&quot; not in its linguistic meaning, but to refer to the character string that is produced when a stemming algorithm like SNOWBALL is applied to a word form. The stem is usually a prefix of the word form, but some orthographic normalization (e.g., &quot;possibly&quot; → &quot;possible&quot; or &quot;possibli&quot;) is often also performed.</note>

			<note place="foot" n="2"> snowball.tartarus.org</note>

			<note place="foot" n="3"> ftp://ftp.gnu.org/gnu/aspell/dict 4 code.google.com/p/word2vec/</note>

			<note place="foot" n="5"> cistern.cis.lmu.de/marmot/naacl2015 6 Baroni et al. (2014)&apos;s numbers are higher on some of the datasets for the best of 48 different parameter configurations. In contrast, we do not tune parameters.</note>

			<note place="foot" n="7"> The all-POS setting includes all POS, not just n, v, a. 8 All supplementary material is available at www.cis. uni-muenchen.de/ebert/</note>

			<note place="foot" n="10"> sentistrength.wlv.ac.uk/ 11 We disregard words with the prefix &quot;nej&quot;, because they indicate superlatives. Exceptions are common negated words with this prefix, such as &quot;nejsi&quot; (engl. &quot;you are not&quot;).</note>

			<note place="foot" n="12"> To be comparable with published results we report the macro F1 of positive and negative classes.</note>

			<note place="foot" n="13"> Recall that form opt is similar to an approach that is used in most systems that have embeddings, which just use the available surface forms.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using cooccurrence statistics and the web to discover synonyms in a technical language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabrina</forename><surname>Bisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Survey of Wordnets and their Licenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyonghee</forename><surname>Paik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Global WordNet Conference</title>
		<meeting>the 6th Global WordNet Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Compositional Morphology for Word Representations and Language Modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">A</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimodal Distributional Semantics. Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page">49</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and SVD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Bullinaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">P</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="890" to="907" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Linguistically Informed Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WASSA</title>
		<meeting>WASSA</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Placing search in context: the concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Rumen Moraliyski, and Pavel Brazdil. 2010. Paraphrase alignment for synonym evidence discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Gintar˙ E Grigonyt˙ E</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaël</forename><surname>Cordeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dias</surname></persName>
		</author>
		<editor>COLING</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using the Structure of a Conceptual Network in Computing Semantic Relatedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iryna Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP</title>
		<meeting>IJCNLP</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sentiment Analysis in Czech Social Media Using Supervised Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Habernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Ptáček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Steinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WASSA</title>
		<meeting>WASSA</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Webis: An Ensemble for Twitter Sentiment Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Büchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Antònia</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaň</forename><surname>Stěpánek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">GermaNeta Lexical-Semantic Net for German</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Birgit</forename><surname>Hamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Feldweg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications</title>
		<meeting>ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cross-lingual Semantic Relatedness Using Encyclopedic Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samer</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">SimLex-999: Evaluating Semantic Models with (Genuine) Similarity Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno>abs/1408.3456</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Convolutional Neural Network for Modelling Sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">From Words to Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jussi</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Real World Intelligence</title>
		<imprint>
			<publisher>CSLI Publications</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multilingual Reliability and &quot;Semantic&quot; Structure of Continuous Word Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Köper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Scheible</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWCS</title>
		<meeting>IWCS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Judgment Language Matters: Multilingual Vector Space Models for Judgment Language Aware Lexical Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Leviant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<idno>abs/1508.00106</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Better Word Representations with Recursive Neural Networks for Morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Building a large annotated corpus of English: The Penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
	<note>Computational linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Probabilistic Modeling of Joint-context in Distributional Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Methods and Results of the Hungarian WordNet Project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Márton</forename><surname>Miháltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Hatvani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judit</forename><surname>Kuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">György</forename><surname>Szarvas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">János</forename><surname>Csirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Prószéky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamás</forename><surname>Váradi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Global WordNet Conference</title>
		<meeting>the 4th Global WordNet Conference</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR: Workshop</title>
		<meeting>ICLR: Workshop</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Contextual correlates of semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><forename type="middle">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Cognitive Processes</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">NRC-Canada: Building the State-of-theArt in Sentiment Analysis of Tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust morphological tagging with word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Joint lemmatization and morphological tagging with Lemming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">BabelRelate! A Joint Multilingual Approach to Computing Semantic Relatedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth AAAI Conference on Artificial Intelligence<address><addrLine>Toronto, Ontario, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07-22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Pala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Smrz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Building Czech Wordnet. Romanian Journal of Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Looking for Hyponyms in Vector Space Language Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Using context-window overlapping in synonym discovery and ontology extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Ruiz-Casado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Castells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RANLP</title>
		<meeting>RANLP</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Processing and querying large web corpora with the COW14 architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Schäfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CMLC</title>
		<meeting>CMLC</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Overview of the SPMRL 2013 shared task: Cross-Framework evaluation of parsing morphologically rich languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Candito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iakes</forename><surname>Goenaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koldo</forename><surname>Gojenola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceddings of SPMRL</title>
		<meeting>eddings of SPMRL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised Morphology Induction Using Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Combining Heterogeneous Knowledge Resources for Improved Distributional Semantic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">György</forename><surname>Szarvas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CICLing</title>
		<meeting>CICLing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Evaluation of Word Vector Representations by Subspace Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Combining independent modules to solve multiple-choice synonym and analogy problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Bigham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shnayder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML</title>
		<meeting>ECML</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kateřina</forename><surname>Veselovská</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Czech SubLex 1.0</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automatically Creating Datasets for Measures of Semantic Relatedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Linguistic Distances</title>
		<meeting>the Workshop on Linguistic Distances</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
