<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Siamese Network-Based Supervised Topic Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghui</forename><surname>Rao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics and Information Technology</orgName>
								<orgName type="institution" key="instit1">The Education University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">Tai Po</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><forename type="middle">Lee</forename><surname>Wang</surname></persName>
							<email>pwang@ouhk.edu.hk</email>
							<affiliation key="aff2">
								<orgName type="department">School of Science and Technology</orgName>
								<orgName type="institution" key="instit1">The Open University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">Ho Man Tin</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Siamese Network-Based Supervised Topic Modeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4652" to="4662"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4652</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Label-specific topics can be widely used for supporting personality psychology, aspect-level sentiment analysis, and cross-domain sentiment classification. To generate label-specific topics, several supervised topic models which adopt likelihood-driven objective functions have been proposed. However, it is hard for them to get a precise estimation on both topic discovery and supervised learning. In this study, we propose a supervised topic model based on the Siamese network, which can trade off label-specific word distributions with document-specific label distributions in a uniform framework. Experiments on real-world datasets validate that our model performs competitive in topic discovery quantitatively and qualitatively. Furthermore, the proposed model can effectively predict categorical or real-valued labels for new documents by generating word embeddings from a label-specific topical space.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As one of the most widely used text mining tech- niques, topic modeling can extract meaningful de- scriptions (i.e., topics) from a corpus <ref type="bibr" target="#b2">(Blei, 2012)</ref>. Most previous topic models, such as probabilis- tic Latent Semantic Analysis (pLSA) <ref type="bibr" target="#b15">(Hofmann, 1999)</ref> and Latent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b4">Blei et al., 2001</ref>) are unsupervised. In unsupervised topic models, each document is defined as a mix- ture distribution over topics and each topic is represented as a mixture distribution over words. Unsupervised topic models only exploit words in documents and do not incorporate the guid- ance of labels into learning processes. There- fore, these models fail to discover label-specific topics, which are important to support personality psychology <ref type="bibr" target="#b33">(Weiner and Graham, 1990)</ref>, aspect- level sentiment analysis <ref type="bibr" target="#b22">(Liu, 2012)</ref>, and cross- domain sentiment classification <ref type="bibr" target="#b13">(He et al., 2011</ref>). For example, label-specific topics generated from sentimental texts can help to find attributions and causes for different sentiments by associating sen- timents with real-world topics/events.</p><p>In light of this consideration, several super- vised topic models are proposed to generate label- specific topics. One of the most representative models is the supervised Latent Dirichlet Alloca- tion (sLDA) <ref type="bibr" target="#b3">(Blei and McAuliffe, 2007)</ref>, which re- stricts a document being associated with one real- valued response variable. To deal with categori- cal labels, multi-class sLDA (sLDAc) ( <ref type="bibr" target="#b31">Wang et al., 2009)</ref> and Labeled Latent Dirichlet Allocation (L- LDA) ( <ref type="bibr" target="#b25">Ramage et al., 2009</ref>) are proposed, but they are only applicable to classification. Recently, a supervised Neural Topic Model (sNTM) <ref type="bibr" target="#b7">(Cao et al., 2015</ref>) is developed to tackle supervised tasks of both classification and regression. As a hybrid method, sNTM is in essence a neural network by following the document-topic distribution in topic models. Unfortunately, the label information has a little effect on topic generation since sNTM mod- els documents and labels separately.</p><p>The above limitation motivates us to develop a supervised topic model which can jointly model documents and labels. Particularly, we propose a Siamese Labeled Topic Model (SLTM) to exploit the information of documents and labels based on the Siamese network ( <ref type="bibr" target="#b5">Bromley et al., 1993;</ref><ref type="bibr" target="#b16">Hu et al., 2014;</ref><ref type="bibr" target="#b32">Wang and Zhang, 2017)</ref>, where weight matrices in SLTM represent conditional distributions. Therefore, by constraining weight matrices during the learning procedure, SLTM can follow probabilistic characteristics of topic models strictly. Compared to previous supervised topic models, the main advantages of our SLTM are summarized as follows. First, SLTM can gener-ate more coherent label-specific topics than oth- ers. This is because the supervision of labels is incorporated into topic modeling for SLTM. On the other hand, the mapping of topics to labels is unconstrained for most existing supervised topic models, which renders many coherent topics be- ing generated outside labels. Second, strengths of neural networks are incorporated into SLTM to bootstrap its inference power on label prediction. Third, each word can be mapped to a topical em- bedding space and represented by a word embed- ding after generating label-specific topics.</p><p>To validate the effectiveness of the proposed model, we evaluate it on two real-world datasets in text mining. Experimental results indicate that our method is able to discover more coherent and label-specific topics than baseline models. More- over, word embeddings learned by the proposed model can be used to predict labels for new docu- ments effectively.</p><p>The remainder of this paper is organized as fol- lows. We summarize related studies on supervised topic modeling in Section 2. For convenience of describing our model, we present the neural net- work view of topic models in Section 3. Then, we detail the proposed SLTM in Section 4. Experi- mental design and analysis of results are shown in Section 5. Finally, we present conclusions and fu- ture work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Topic models, which focus on discovering unob- served class variables named "topics" statistically, have been widely used in text mining. One of the early topic models is pLSA <ref type="bibr" target="#b15">(Hofmann, 1999)</ref>. In pLSA, a document's word vector was decom- posed into a mixture of topics, and a topic was rep- resented as a probability distribution over words. LDA ( <ref type="bibr" target="#b4">Blei et al., 2001</ref>) extended pLSA by adding Dirichlet priors for a document's multinomial dis- tribution over topics and a topic's multinomial dis- tribution over words, which makes it suitable to generate topics for unseen documents.</p><p>The aforementioned models are unsupervised, which may be computationally costly to do some task-specific transformation when there is extra labeling information <ref type="bibr" target="#b7">(Cao et al., 2015)</ref>. To ad- dress this issue, several supervised topic models have been proposed to introduce the label guid- ance in learning processes. One of the most widely used supervised topic models is sLDA <ref type="bibr" target="#b3">(Blei and McAuliffe, 2007)</ref>. In sLDA, each document was paired with a response variable which obeys the Gaussian distribution. By extending the sLDA, BP-sLDA ( <ref type="bibr" target="#b9">Chen et al., 2015</ref>) applied back propa- gation over a deep architecture in conjunction with stochastic gradient/mirror descent for model pa- rameter estimation, leading to scalable and end-to- end discriminative learning characteristics. Based on sLDA, multi-class sLDA (sLDAc) ( <ref type="bibr" target="#b31">Wang et al., 2009</ref>) was proposed to model documents with categorical labels by adding a softmax classifier rather than a linear regression in sLDA to a stan- dard LDA. Another method of tackling corpora with discrete labels is L-LDA ( <ref type="bibr" target="#b25">Ramage et al., 2009)</ref>, which associated each label with only one topic. To improve the performance of L-LDA in the classification task, Dependency-LDA (Dep- LDA) <ref type="bibr" target="#b26">(Rubin et al., 2012)</ref> incorporated an extra topic model to capture the dependencies between labels and took the label dependencies into con- sideration when estimating topic distributions. Re- cently, a nonparametric supervised topic model ( <ref type="bibr" target="#b21">Li et al., 2018</ref>) was proposed to predict the response of interest (e.g., product ratings and sales). The limitation of above models is that they are only applicable to either discrete or continuous data.</p><p>In this paper, we propose a Siamese network- based supervised topic model named SLTM. The most relevant work to SLTM is the supervised Neural Topic Model (sNTM) for both classifica- tion and regression tasks <ref type="bibr" target="#b7">(Cao et al., 2015)</ref>, which constructed two hidden layers to generate the n- gram topic and document-topic representations. However, different from our SLTM using bag- of-words methods, sNTM adopted fixed embed- dings trained on external resources ( <ref type="bibr" target="#b24">Mikolov et al., 2013</ref>). Thus, sNTM can not learn data-specific topics. Furthermore, sNTM is hard to follow prob- abilistic characteristics of the topic-word distribu- tion in topic models, because a topic generated by sNTM is composed of an infinite number of n- grams. Finally, sNTM modeled documents and la- bels separately rather than uniformly in our SLTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>For convenience of describing the proposed model, we use hollow uppercase letters (e.g., D) to represent collections, bold uppercase letters (e.g., W 1 ) to represent matrices, bold lowercase let- ters (e.g., y i ) to represent vectors, regular upper- case letters (e.g., M ) to represent scalar constants, </p><formula xml:id="formula_0">D Document collection d i ∈ D The i-th document V Vocabulary v j ∈ V The j-th word Z Topic collection z k ∈ Z The k-th topic Y Label collection y i ∈ R L Labels for document d i p(v j |d i ) The probability of v j given d i</formula><p>and regular lowercase letters (e.g., v j ) to represent scalar variables. Based on the above convention, frequently used notations are shown in <ref type="table" target="#tab_0">Table 1</ref>. Given a document d i with labels y i , our goal is to discover topics with a neural network frame- work. Therefore, we first describe the neural net- work view of topic models briefly.</p><p>Topic modeling is a popular latent variable in- ference method for co-occurrence data which as- sociates unobserved classes with observations d i and v j , where v j is a word in d i . The conditional probability p(v j |d i ) is defined as:</p><formula xml:id="formula_1">p(v j |d i ) = K k=1 p(v j |z k )p(z k |d i ). (1) Let φ(v j ) = [p(v j |z 1 ), . . . , p(v j |z K )] and θ(d i ) = [p(z 1 |d i ), . . . , p(z K |d i )], then p(v j |d i )</formula><p>in Equation 1 can be represented as the following vector form:</p><formula xml:id="formula_2">p(v j |d i ) = φ(v j ) · θ(d i ).<label>(2)</label></formula><p>We represent horizontal stack by commas and vertical stack by semicolons, thus</p><formula xml:id="formula_3">W 1 = [θ(d 1 ) T , . . . , θ(d M ) T ] ∈ R K×M and W 2 = [φ(v 1 ); . . . ; φ(v N ] ∈ R N ×K , which are con- strained by: W 1 [k, m] ≥ 0, W 2 [n, k] ≥ 0, K k=1 W 1 [k, m] = 1, and N j=1 W 2 [j, k] = 1, where k ∈ [1, K], m ∈ [1, M ], and n ∈ [1, N ].</formula><p>Then, the vector form in Equation 2 can be ex-  tended to:</p><formula xml:id="formula_4">p(V|D) =    p(v 1 |d 1 ) · · · p(v 1 |d M ) . . . . . . . . . p(v N |d 1 ) · · · p(v N |d M )    =    (φ(v 1 ) · θ(d 1 )) · · · (φ(v 1 ) · θ(d M )) . . . . . . . . . (φ(v N ) · θ(d 1 )) · · · (φ(v N ) · θ(d M ))    = W 2 W 1 . (3)</formula><p>With Equation 3, topic models can be viewed as neural networks, where D and V are input sets, p(V|D) is the output set, and W 1 and W 2 are pa- rameter matrices of the neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Siamese Labeled Topic Model</head><p>Similar to generative models such as pLSA, we propose a Siamese Labeled Topic Model (SLTM) based on the aforementioned neural network per- spective of topic models. <ref type="figure">Figure 1</ref> illustrates the framework of generating each word in SLTM, and the process is as follows. For a document d i in D, the topic distribution p(Z|d i ) is estimated by:</p><formula xml:id="formula_5">p(Z|d i ) = W 1 d i ,<label>(4)</label></formula><p>where d i is the indicator vector ( <ref type="bibr" target="#b35">Yang et al., 2013)</ref> of d i , which means that the i-th entry of d i is 1 and other entries are 0. Labels of d i are y i , which are generated from the topic distribution of d i as:</p><formula xml:id="formula_6">y i = W 3 p(Z|d i ). The above equation is constrained by W 3 [l, k] ≥ 0, where l ∈ [1, L], k ∈ [1, K], and L l=1 W 3 [l, k] = 1 if L &gt; 1.</formula><p>A topic z k in Z has its word distribution p(V|z k ), which is computed by:</p><formula xml:id="formula_7">p(V|z k ) = W 2 z k ,<label>(5)</label></formula><p>where z k is the indicator vector of z k . Therefore, words can be generated from d i as:</p><formula xml:id="formula_8">p(V|d i ) = W 2 W 1 d i .<label>(6)</label></formula><p>The architecture of SLTM from the perspec- tive of neural networks is shown in <ref type="figure">Figure 2</ref>. With respect to the model optimization, we adopt the contrastive objective function used in previ- ous works <ref type="bibr" target="#b28">(Socher et al., 2014;</ref><ref type="bibr" target="#b10">Cui et al., 2014;</ref><ref type="bibr" target="#b7">Cao et al., 2015;</ref><ref type="bibr" target="#b12">He et al., 2017)</ref>. For document d i and every word v j in d i , we randomly sam- ple a document from the document set D which does not contain v j , as a negative sample doc- ument. The negative sample document is repre- sented as d</p><formula xml:id="formula_9">(v j −) i</formula><p>and has labels y</p><formula xml:id="formula_10">(v j −) i</formula><p>. As shown in <ref type="figure">Figure 1</ref>, the lower sub-network, which takes d</p><formula xml:id="formula_11">(v j −) i</formula><p>as input, has the same architecture as the the upper sub-network, which takes d i as input. Because the document-topic distribution and the topic-word distribution of a corpus are fixed, W 1 , W 2 and W 3 are shared among two sub-networks of our model. These two sub-networks are twin networks and thus the proposed model is essen- tially the Siamese network. Our objective is to make word v j be learned by topics in document d i , while not be learned by topics in the nega- tive sampled document d</p><formula xml:id="formula_12">(v j −) i</formula><p>. Therefore, we only take word v j in V into consideration during the learning procedure, which can be implemented as dot-multiplying p(V|d i ) with the indicator vector of v j (i.e., v j ) as:</p><formula xml:id="formula_13">ˆ p(v j |d i ) = p(V|d i ) · v j .</formula><p>Par- ticularly, the objective is to make the predicted conditional probabilityˆpprobabilityˆ probabilityˆp(v j |d i ) approach the ob- served conditional probability p(v j |d i ) (i.e., term frequency of word v j in document d i ), while make the conditional probabilityˆpprobabilityˆ probabilityˆp(v j |d</p><formula xml:id="formula_14">(v j −) i</formula><p>) approach zero. Thus, the loss function of predicted condi- tional probabilities and the observed conditional Algorithm 1 Training Algorithm for SLTM Input: S = {D, Y};</p><p>1: repeat 2:</p><formula xml:id="formula_15">for all (d i , y i ) ∈ S do 3:</formula><p>for each word v j in document d i do end for 11: until convergence probability can be defined as:</p><formula xml:id="formula_16">loss(d i , d (v j −) i ) = |p(v j |d i ) − ˆ p(v j |d i ) + ˆ p(v j |d (v j −) i )|.<label>(7)</label></formula><p>We use another loss function loss(y i , y</p><formula xml:id="formula_17">(v j −) i</formula><p>) to capture labels of d i and d</p><formula xml:id="formula_18">(v j −) i</formula><p>, where loss(y i , y</p><formula xml:id="formula_19">(v j −) i ) = loss(y i )+loss(y (v j −) i</formula><p>). In the above, equations of loss(y i ) and loss(y</p><formula xml:id="formula_20">(v j −) i</formula><p>) de- pend on the property of labels. For categorical and real-valued labels, the cross-entropy ( <ref type="bibr" target="#b29">Tang et al., 2014</ref>) and the mean absolute error <ref type="bibr" target="#b34">(Willmott and Matsuura, 2005</ref>) are adopted, respectively.</p><p>The maximization of the weighted sum of con- ditional likelihoods is equivalent to minimize the losses of the weighted sum of loss functions, and these two loss functions are weighted by a hyper- parameter α as in ( <ref type="bibr" target="#b29">Tang et al., 2014</ref>). Thus, the loss function of SLTM is:</p><formula xml:id="formula_21">loss(SLT M ) = α × loss(d i , d (v j −) i ) + (1 − α) × loss(y i , y (v j −) i ). (8)</formula><p>The effect of α on predicting labels and discov- ering topics will be investigated in Section 5.6. Based on loss(SLT M ), three kinds of weights, i.e., W 1 , W 2 , and W 3 can be updated together by a vanilla back propagation (BP) algorithm with the early stopping criteria <ref type="bibr" target="#b1">(Bengio, 2012)</ref>. The train- ing algorithm is shown in Algorithm 1.</p><p>After training, we obtain both document-topic and topic-word distributions. Then, each word can be mapped to a topic-level embedding space and represented as a word embedding. For instance, the word embedding of v j is generated from the topic-word distribution W 2 as:</p><formula xml:id="formula_22">e(v j ) = W 2 [j, :].<label>(9)</label></formula><p>The generated word embeddings can be used for specific applications, such as label prediction. Par- ticularly, we firstly represent a new document d n by its document embeddings e(d n ), where e(d n ) is the sum of word embeddings of all words in d n . Then, the predicted labelsˆylabelsˆ labelsˆy n of document d n can be estimated by:</p><formula xml:id="formula_23">ˆ y n = f (W 4 e(d n )),<label>(10)</label></formula><p>where W 4 denotes weights of each topic con- tributing to labels, and f (.) is the activation func- tion which depends on the type of labels. For cat- egorical and normalized real-valued labels, we re- spectively adopt softmax and sigmoid as activa- tion functions. Note that we do not predict labels for new documents based on W 3 directly, because topic distributions of these documents can only be learned without the supervision of labels, i.e., new documents' topic distributions may be incon- sistent to W 3 . Finally, we update W 4 and word embeddings by RMSprop (Tieleman and Hinton, 2012) for label prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we firstly describe datasets and the setting of experiments. Secondly, we investigate the quality of generated topics by the topic coher- ence score and qualitative analysis. Thirdly, the quality of generated word embeddings is evaluated by label prediction and word similarity. Finally, the effect of the hyper-parameter α is evaluated on coherence of topics and label prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Setting</head><p>To evaluate the effectiveness of our method com- prehensively, we conduct experiments on two real- world datasets with categorical and real-valued la- bels, respectively. The first corpus named ISEAR contains a collection of 7,666 sentences and each item is manually tagged with a categorical label over 7 emotions ( <ref type="bibr" target="#b27">Scherer and Wallbott, 1994)</ref>. The second dataset YouTube 1 is often used for sen- timent strength detection, which contains 3,407 comments on videos and each item is labeled 1 http://sentistrength.wlv.ac.uk/ with a real value between 0.1 (i.e., very nega- tive sentiment) and 0.9 (i.e., very positive senti- ment). These two datasets are selected for their similar word numbers in average. After remov- ing stop words, the mean numbers of words in each document are 8.53 and 8.56 for ISEAR and YouTube. Besides, it is appropriate to evaluate the model performance on predicting emotions and sentiment strengths, because topics play an impor- tant role in understanding sentences or user com- ments ( <ref type="bibr" target="#b22">Liu, 2012)</ref>. Since the proposed SLTM is suitable to both topic discovery and classifi- cation/regression tasks, we employ five kinds of baselines for comparison. The first kind are the support vector machine (SVM), an efficient deep learning model for clas- sification (i.e., fastText) ( , and the following supervised topic models which are confined to categorical labels:</p><p>• sLDAc ( <ref type="bibr" target="#b31">Wang et al., 2009</ref>): it models doc- uments with categorical labels by adding a softmax classifier to a standard LDA.</p><p>• L-LDA ( <ref type="bibr" target="#b25">Ramage et al., 2009)</ref>: it is a super- vised model which associates labels with top- ics by one-to-one correspondence. Accord- ingly, the number of topics in L-LDA must equal the size of the label set.</p><p>• Dep-LDA (Rubin et al., 2012): it extends L- LDA by introducing a multinomial distribu- tion over labels and capturing the dependen- cies between labels. Then, the label depen- dencies are used to sample topic distributions in supervised learning.</p><p>The second kind are the support vector re- gression (SVR), a state-of-the-art deep learn- ing model for sentiment strength detection (i.e., HCNN) ( <ref type="bibr" target="#b8">Chen et al., 2017)</ref>, and the following supervised topic models which are developed for predicting real-valued labels only:</p><p>• sLDA ( <ref type="bibr" target="#b3">Blei and McAuliffe, 2007)</ref>: it is a classical supervised topic model, in which, each document is paired with a response vari- able, and the variable is defined as a Gaussian distribution with a mean value that is com- puted by a linear regression of topics.</p><p>• BP-sLDA ( <ref type="bibr" target="#b9">Chen et al., 2015)</ref>: it applies back propagation over a deep architecture together with stochastic gradient/mirror descent for   The third kind is a supervised n-gram model named sNTM, which is applicable to predict both categorical and real-valued labels for new docu- ments ( <ref type="bibr" target="#b7">Cao et al., 2015)</ref>. In sNTM, each n-gram is represented by a 300-dimensional embedding vec- tor using the available tool word2vec 2 . By follow- ing ( <ref type="bibr" target="#b7">Cao et al., 2015)</ref>, a large-scale Google News dataset with around 100 billion words is adopted for training. For topic discovery, two unsuper- vised topic models, pLSA <ref type="bibr" target="#b15">(Hofmann, 1999</ref>) and LDA ( <ref type="bibr" target="#b4">Blei et al., 2001)</ref>, are used as the fourth kind of baselines. Finally, we adopt two hybrid meth- ods by combining LDA and supervised learning algorithms as baselines. In particular, a softmax classifier and a liner regression (LR) are used to predict categorical and real-valued labels for doc- uments, respectively. Unless otherwise specified, we set α to 0.5 and adopt the stochastic gradient descent with batch size of 100 for training SLTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Coherence Score of Topics</head><p>To investigate the quality of topics discovered by SLTM quantitatively, we use the topic coherence score based on the normalised pointwise mutual information ( <ref type="bibr" target="#b19">Lau et al., 2014</ref>) as the evaluation metric. Intuitively, a topic coherence score that is larger indicates that the quality of topics is bet- ter. All unsupervised topic models (i.e., pLSA and LDA) and supervised methods which associate one label with multiple topics (i.e., sLDAc, sLDA, BP-sLDA, and sNTM) are adopted for compari- son. Although L-LDA and Dep-LDA can iden- tify label-specific topics on ISEAR, these models' one-to-one mapping of labels and topics makes them unsuitable in this evaluation. Particularly, L- LDA and Dep-LDA constraint each topic to words in certain documents with the same label, which renders their coherence scores being estimated by a subset of the corpus only. On the other hand, the quality of topics is evaluated on the whole corpus for SLTM and other baseline models.</p><p>The average coherence scores of topics gener- ated by different models on ISEAR and YouTube are respectively shown in <ref type="table" target="#tab_3">Table 2 and Table 3</ref>, where the number of topics is 20, the number of top words T is set to 5, 10, and 15, and the best scores are highlighted in boldface. The results in- dicate that SLTM can discover more coherent top- ics than both unsupervised topic models and su- pervised methods, except for T = 5 on YouTube. It is also interesting to observe that supervised baseline models (i.e., sLDAc, sLDA, BP-sLDA, and sNTM) perform worse than pLSA and LDA for most cases, which validates that it is challeng- ing to trade off label-specific word distributions with document-specific label distributions <ref type="bibr" target="#b25">(Ramage et al., 2009</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Qualitative Analysis on Topics</head><p>In this part, we conduct qualitative inspection of 20 topics generated by SLTM. The ISEAR dataset which contains multiple labels is used for illus- tration, since it is inappropriate to present the re- sults on YouTube with a single real-valued label. For each model that is applicable to ISEAR, we show top 5 words of the generated label-specific topics in <ref type="table" target="#tab_2">Table 4</ref>. It is worth to note that L-LDA and Dep-LDA achieve the same top words, since their difference only exists in the process of label prediction. The results indicate that although all models can learn meaningful topics, SLTM per- forms better than baseline models in label-specific topic discovery. For example, two words "happy" and "joy" which are strongly related to the label of "joy" are identified by SLTM with large probabil- ities. Similar results can be observed in other la- bels, thus topics discovered by our model are more convenient to be understood than others. Such a kind of performance enhancement is valuable to many real-world applications, e.g., personality ed- ucation and psychotherapy, by producing human interpretable topics/events that evoke users' par- ticular emotions.</p><p>For completeness, we also examine all topics generated by the baseline of sNTM. As mentioned earlier, sNTM is based on n-grams, instead of single words for SLTM and other baseline mod- els. In the practical implementation, only uni- grams and bigrams are considered since the em- bedding representation becomes less precise as n increases ( <ref type="bibr" target="#b7">Cao et al., 2015</ref>). The results indicate that sNTM can generate some topic bigrams such as "smelled disgusting" and "graduation exams", which are more appropriate to expressing a topic. However, only three topics are manually examined to be correlated with the seven emotions. This val- idates that sNTM is hard to introduce the guidance of labels in topic generation, because it models documents and labels separately.</p><p>To further evaluate the interpretability of topics extracted from SLTM, we firstly get topic embed- dings by: emb(z k ) = W 1 <ref type="bibr">[k, :]</ref>. Then, we map emb(z k ) to a two-dimensional space via Princi- pal Component Analysis (PCA). <ref type="figure" target="#fig_2">Figure 3</ref> presents distributions of topics generated by SLTM over the ISEAR dataset. The scatter plot indicates that top- ics corresponding to the same label are closer than those of different labels. Furthermore, the distance between topics on correlated labels such as "fear" and "anger", is closer than that of topics on "joy" and other labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation on Label Prediction</head><p>We here evaluate the quality of word embeddings generated by SLTM on predicting categorical and real-valued labels based on ISEAR and YouTube, respectively. Since there are varied parameters for different models, we randomly select 60% of in- stances as the training set, 20% as the validation set, and the remaining 20% as the testing set. The values of parameters (e.g., the number of topics) for each model are all determined by the valida- tion set. In label prediction, the main difference between SLTM and other supervised topic mod- els is as follows. On one hand, a label-specific word embedding is introduced for predicting la- bels in SLTM according to Equation 10. On the other hand, other supervised topic models for both categorical and real-valued label prediction tasks infer labels for unlabeled documents by topic dis- tributions directly, in which, topic distributions of unlabeled documents are learned without the su- pervision of labels.</p><p>For the task of categorical label prediction, the  <ref type="bibr" target="#b0">and Poesio, 2008)</ref> are used as the evaluation met- rics. <ref type="table" target="#tab_6">Table 5</ref> shows the classification performance of different models on ISEAR, where the best re- sults are highlighted in boldface. For the predic- tion of real-valued labels on YouTube, we com- pare different models' regression performance by the mean absolute error (MAE) and the predictive R 2 (pR 2 ) ( <ref type="bibr" target="#b3">Blei and McAuliffe, 2007</ref>), as shown in <ref type="table" target="#tab_7">Table 6</ref>. From the above results we can observe that SLTM achieves substantial performance im- provement over baselines in predicting both cate- gorical and real-valued labels, which indicates that word embeddings generated from labeled docu- ments are more suitable for label prediction tasks than topic distributions generated from unlabeled documents without the guidance of labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Similarity of Word Embeddings</head><p>Word embeddings can reflect relations between words, and most methods of generating word em- beddings are based on the local context informa- tion. This is because words with similar contexts may have similar semantics. However, a large- scale corpus is required to learn high quality word embeddings from the local context. Different from the previous word embedding generation meth- ods, SLTM generates word embeddings based on the global label-specific topic information (i.e., the topical embedding space). Therefore, we further compare the quality of word embeddings learned by SLTM and three widely used meth- ods: Word2Vec (W2V) ( <ref type="bibr" target="#b24">Mikolov et al., 2013)</ref>, subword information Word2Vec (siW2V) ( , and SSPMI ( <ref type="bibr" target="#b20">Levy and Goldberg, 2014</ref>). Among these baseline word embedding models, W2V and siW2V use the neural network framework, and SSPMI implicitly factorizes the pointwise mutual information (PMI) matrix of the local word co-occurrence patterns.  As our evaluation metric, the word similar- ity is estimated as follows. Firstly, we calcu- late cosine similarity scores for word pairs which occur in both the training set and the testing set. Secondly, word pairs are ranked accord- ing to their cosine similarities in the embedding space and human-assigned similarity scores, re- spectively. Finally, rankings of word similarity scores are evaluated by measuring the Spearman's rank correlation with rankings of human-assigned similarity scores. A higher correlation value in- dicates that it is more consistent to human judge- ments in word similarity. The following standard corpora which contain word pairs associated with human-assigned similarity scores are used for this evaluation: MEN ( <ref type="bibr" target="#b6">Bruni et al., 2014</ref>), SimLex- 999 (SimLex) ( <ref type="bibr" target="#b14">Hill et al., 2015)</ref>, and Rare ( <ref type="bibr" target="#b23">Luong et al., 2013</ref>).</p><p>We train W2V, siW2V, and SSPMI over each corpus by setting the number of context window size to 5. Furthermore, the dimension of word embeddings generated from all models is set to 50 according to <ref type="bibr" target="#b18">(Lai et al., 2016)</ref>. The values of word similarity on ISEAR and YouTube are re- spectively shown in <ref type="table" target="#tab_8">Table 7 and Table 8</ref>, where the best results are highlighted in boldface. We can observe that SLTM outperforms baselines for all cases. The results indicate that word embeddings learned from the global label-specific topic infor- mation are better than those from the local context information without any external corpora. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Effect of the Hyper-parameter</head><p>After validating the effectiveness of SLTM on discovering topics and learning word embed- dings, we now investigate the effect of the hyper- parameter in SLTM on these two aspects. Accord- ing to Equation 8, the hyper-parameter α is used to weight two kinds of loss functions. Since W 2 can be updated subject to α &gt; 0, we evaluate the performance of SLTM by varying α from 0.1 to 1 over the ISEAR dataset, as follows.</p><p>First, we evaluate the influence of hyper- parameter α on topic discovery by the coherence score of topics. To clearly illustrate the perfor- mance trend with different values of α, we set the number of top words T to 5, 10, and 15, and present topic coherence scores in <ref type="figure" target="#fig_3">Figure 4</ref>. The results indicate that SLTM performs stably un- der these α values on topic discovery, except for α = 1 which ignores the label information totally. This validates the importance of label information in generating coherent topics.</p><p>Second, we use the learned word embeddings to predict document labels under different val- ues of α. As shown in <ref type="figure" target="#fig_5">Figure 5</ref>, we can ob- serve that when α = 0.5, i.e., loss(d i , d ) are weighted equally, SLTM achieves the best performance in label prediction. The results indicate that the co-occurrence of doc- uments and words as well as the label information are both important to generate good word embed- dings. Furthermore, the label prediction perfor- mance of SLTM using any of these α values is better than that of most baselines (ref. <ref type="table" target="#tab_6">Table 5</ref>). This validates the robustness of SLTM with differ- ent hyper-parameter values in supervised learning. We also conduct experiments on YouTube using varied α values, which indicates that the hyper- parameter has a similar effect on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed a supervised topic model named SLTM to discover label-specific topics by jointly modeling documents and la- bels. For the SLTM, weight matrices which repre- sent document-topic and topic-word distributions can strictly follow probabilistic characteristics of topic models. Experiments were conducted on datasets with both categorical and real-valued la- bels, which validated that SLTM can not only dis- cover more coherent topics, but also boost the per- formance of supervised learning tasks by learning high quality word embeddings. For future work, we plan to speed-up the training process of SLTM by GPUs and distributed algorithms. With the de- velopment of deep learning techniques, we also plan to de-emphasize irrelevant words with an at- tention mechanism.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: SLTM's word generation framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>parameter estimation of sLDA. The number of hidden layers is set to 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Scatter plot of topics identified by SLTM on ISEAR, where each point indicates a topic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Topic coherence scores on ISEAR using different α values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Label prediction performance on ISEAR using different α values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Frequently used notations.</head><label>1</label><figDesc></figDesc><table>Notation 
Description 

M 
Number of documents 
K 
Number of topics 
N 
Size of the vocabulary 
L 
Size of the label set 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>4 :</head><label>4</label><figDesc></figDesc><table>Sample a document d 

(v j −) 
i 

which 
does not contain v j ; 

5: 

Calculate loss(SLT M ); 

6: 

if loss(SLT M ) is reducing then 

7: 

Update W 1 , W 2 and W 3 ; 

8: 

end if 

9: 

end for 

10: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 : Topic coherence scores on ISEAR using dif- ferent numbers of top words T .</head><label>2</label><figDesc></figDesc><table>5 
10 
15 

pLSA 0.0051 0.0024 -0.0013 
LDA 
0.0954 0.0492 0.0014 
sLDAc 0.0014 0.0031 -0.0035 
sNTM -0.9267 -0.9508 -0.9667 
SLTM 0.1142 0.0680 0.0025 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Topic coherence scores on YouTube using dif- ferent numbers of top words T .</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 : Each label's top 5 words on ISEAR.</head><label>4</label><figDesc></figDesc><table>Labels 
Models 
Top 5 words of label-specific topics 

fear 

sLDAc 
home night car afraid fear 
L-LDA/Dep-LDA 
night afraid car home fear felt 
SLTM 
night afraid fear car home dark 

joy 

sLDAc 
year passed heard exam university 
L-LDA/Dep-LDA 
friend got time passed felt 
SLTM 
happy joy passed got university 

guilt 

sLDAc 
did didn't asked guilty said 
L-LDA/Dep-LDA 
felt guilty friend did mother 
SLTM 
guilty felt mother did friend 

disgust 

sLDAc 
saw man disgusted disgust woman 
L-LDA/Dep-LDA 
disgusted saw felt people friend 
SLTM 
disgusted saw people man disgust 

shame 

sLDAc 
know ashamed teacher happened lot 
L-LDA/Dep-LDA 
ashamed felt friend time did 
SLTM 
ashamed felt shame class teacher 

anger 

sLDAc 
angry called new anger expected 
L-LDA/Dep-LDA 
friend angry did time told 
SLTM 
angry friend anger brother told 

sadness 

sLDAc 
father close died away years 
L-LDA/Dep-LDA 
died friend sad felt time 
SLTM 
died sad death away friend 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 5 : Classification performance on ISEAR.</head><label>5</label><figDesc></figDesc><table>Accuracy Cohen's kappa 

SVM 
0.5063 
0.4240 
fastText 
0.5104 
0.4298 
LDA+softmax 
0.1506 
0.0089 
sLDAc 
0.1875 
0.0540 
L-LDA 
0.4650 
0.3758 
Dep-LDA 
0.4888 
0.4036 
sNTM 
0.2478 
0.1212 
SLTM 
0.5213 
0.4415 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 6 : Regression performance on YouTube.</head><label>6</label><figDesc></figDesc><table>MAE 
pR 2 

SVR 
0.1424 -0.0591 
HCNN 
0.1112 0.3462 
LDA+LR 0.1408 -0.0069 
sLDA 
0.1583 -0.2836 
BP-sLDA 0.1394 -0.0208 
sNTM 
0.1342 0.0807 
SLTM 
0.1005 0.4112 

accuracy and the Cohen's kappa score (Artstein 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="true"><head>Table 7 : Word similarity results on ISEAR.</head><label>7</label><figDesc></figDesc><table>MEN SimLex Rare 

W2V 0.002 -0.008 -0.119 
siW2V 0.002 
0.017 
0.062 
SSPMI 0.023 
0.028 
-0.004 
SLTM 0.169 
0.037 
0.089 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Word similarity results on YouTube. 

MEN SimLex Rare 

W2V -0.018 
0.004 
-0.036 
siW2V -0.002 
0.019 
-0.051 
SSPMI -0.031 
0.038 
-0.026 
SLTM 0.048 
0.040 
0.068 

</table></figure>

			<note place="foot" n="2"> https://code.google.com/p/word2vec/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to the anonymous reviewers for their valuable comments on this manuscript. The </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inter-coder agreement for computational linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="555" to="596" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Practical recommendations for gradient-based training of deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade-Second Edition</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="437" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Probabilistic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="77" to="84" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supervised topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems</title>
		<meeting>the Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems</title>
		<meeting>the Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="601" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Signature verification using a siamese time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roopak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems</title>
		<meeting>the Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A novel neural topic model and its supervised extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 29th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2210" to="2216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sentiment strength prediction using auxiliary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghui</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Lam</forename><surname>Fu Lee Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web Companion</title>
		<meeting>the 26th International Conference on World Wide Web Companion</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end learning of LDA by mirrordescent back propagation over a deep architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinying</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems</title>
		<meeting>the Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1765" to="1773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning topic representation for SMT with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muyun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="133" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An unsupervised neural attention model for aspect extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Wee Sun Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahlmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="388" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatically extracting polarity-bearing topics for cross-domain sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harith</forename><surname>Alani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="123" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="665" to="695" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 15th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems</title>
		<meeting>the Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">How to generate a good word embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="5" to="14" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="530" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems</title>
		<meeting>the Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Supervised topic modeling using hierarchical dirichlet process-based inverse regression: Experiments on e-commerce applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junming</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsinchun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1192" to="1205" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sentiment Analysis and Opinion Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Conference on Computational Natural Language Learning</title>
		<meeting>the 17th Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems</title>
		<meeting>the Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David Leo Wright</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Statistical topic models for multi-label document classification. Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">N</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">America</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="157" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evidence for universality and cultural variation of differential emotion response patterning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><forename type="middle">G</forename><surname>Wallbott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="310" to="328" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning sentimentspecific word embedding for twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Simultaneous image classification and annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2009 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1903" to="1910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A neural model for joint event detection and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4158" to="4164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Attribution in personality psychology. Handbook of personality: Theory and research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Weiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Graham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="465" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Advantages of the mean absolute error (mae) over the root mean square error (rmse) in assessing average model performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Willmott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matsuura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Climate Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="82" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2013 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
