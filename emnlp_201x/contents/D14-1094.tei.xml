<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Balanced Korean Word Spacing with Structural SVM</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changki</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Choi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunki</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Kangwon National University</orgName>
								<address>
									<postCode>200-701</postCode>
									<settlement>Chuncheon-si, Gangwondo</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Korea Electronics and Telecommunications Research Institute</orgName>
								<address>
									<postCode>305-350</postCode>
									<settlement>Daejeon</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Balanced Korean Word Spacing with Structural SVM</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="875" to="879"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Most studies on statistical Korean word spacing do not utilize the information provided by the input sentence and assume that it was completely concatenated. This makes the word spacer ignore the correct spaced parts of the input sentence and erroneously alter them. To overcome such limit, this paper proposes a structural SVM-based Korean word spacing method that can utilize the space information of the input sentence. The experiment on sentences with 10% spacing errors showed that our method achieved 96.81% F-score, while the basic structural SVM method only achieved 92.53% F-score. The more the input sentence was correctly spaced, the more accurately our method performed.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic word spacing is a task to decide boundaries between words, which is frequently used for correcting spacing errors of text mes- sages, Tweets, or Internet comments before using them in information retrieval applications <ref type="bibr" target="#b4">(Lee and Kim, 2012)</ref>. It is also often used in post- processing optical character recognition (OCR) or voice recognition ( . Except for some Asian languages such as Chinese, Japa- nese and Thai, most languages have explicit word spacing that improves the readability of the text and helps readers better understand the meaning of it. Korean especially has a tricky word spacing system and users often make mis- takes, which makes automatic word spacing an interesting and essential task.</p><p>In order to easily acquire the training data, most studies on statistical Korean word spacing assume that well-spaced raw text (e.g. newspaper articles) is perfectly spaced and use it for training ( <ref type="bibr" target="#b4">Lee and Kim, 2012;</ref><ref type="bibr" target="#b5">Lee and Kim, 2013;</ref><ref type="bibr" target="#b8">Shim, 2011)</ref>. This approach, however, cannot observe incorrect spacing since the as- sumption makes the training data devoid of nega- tive example. Consequently, word spacers cannot use the spacing information given by the user, and erroneously alter the correctly spaced parts of the sentence. To utilize the user-given spacing information, a corpus of input sentences and their correctly spaced version is necessary. Construct- ing such corpus, however, requires much time and resource.</p><p>In this paper, to resolve such issue, we propose a structural SVM-based Korean word spacing model that can utilize the word spacing infor- mation given by the user. We name the proposed model "Balanced Word Spacing Model (BWSM)". Our approach trains a basic structural SVM-based Korean word spacing model as in <ref type="bibr" target="#b5">(Lee and Kim, 2013)</ref>, and tries to obtain the sen- tence which achieves the maximum score for the basic model while minimally altering the input sentence.</p><p>In the following section, we discuss related studies. In Section 3, the proposed method and its relation to Karush-Kuhn-Tucker (KKT) con- dition are explained. The experiment and discus- sion is presented in Section 4. Finally, in Section 5, the conclusion and future work for this study is given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There are two common approaches to Korean word spacing: rule-based approach and statistical approach. In rule-based approach, it is not easy to construct rules and maintain them. Further- more, it requires morphological analysis to apply rule-based approach, which slows down the pro- cess. Recent studies, therefore, mostly focus on the statistical approach.</p><p>Most statistical approaches use well-spaced raw corpus as training data (e.g. newspaper arti- cles) assuming that they are perfectly spaced. This is to avoid the expensive job of constructing new training data.  treated the word spacing task as a sequence labeling prob- lem on the input sentence which is a sequence of syllables. They proposed a method based on Hidden Markov Model (HMM). Shim (2011) also considered the word spacing task as a se- quence labeling problem and proposed a method using Conditional Random Field (CRF) ( <ref type="bibr" target="#b2">Lafferty et al., 2001</ref>), which is a well-known powerful model for sequence labeling tasks. <ref type="bibr" target="#b5">Lee and Kim (2013)</ref> tried to solve the sequence labeling prob- lem using structural SVM ( <ref type="bibr" target="#b10">Tsochantaridis et al., 2004;</ref><ref type="bibr" target="#b0">Joachims et al., 2009;</ref><ref type="bibr" target="#b3">Lee and Jang 2010;</ref><ref type="bibr">Shalev-Shwartz et al., 2011</ref>).</p><p>The studies above ( <ref type="bibr" target="#b5">Lee and Kim, 2013;</ref><ref type="bibr" target="#b8">Shim, 2011)</ref>, however, do not take ad- vantage of the spacing information provided by the user, and often erroneously alter the correctly spaced part of the sentence.  tries to resolve this issue by combining an HMM model with an additional confidence model con- structed from another corpus. Given an input sentence, they first apply the basic HMM model to obtain a candidate sentence. For every differ- ent word spacing between the input sentence and the candidate sentence, they calculate and com- pare the confidence using the confidence model, and whichever gets the higher confidence is used. The spacing accuracy was improved from 97.52% to 97.64% <ref type="bibr">1</ref> .</p><p>This study is similar to ( ) in that it utilizes the spacing information given by the user. But unlike ( , BWSM uses structural SVM as the basic model and do not require an additional confidence model. Fur- thermore, while  compares the spacing confidence for each syllable to obtain the final outcome, BWSM considers the whole sen- tence when altering its spacing, enabling it to achieve higher improvement on performance (from 92.53% F-score to 96.81% F-score).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Balanced Word Spacing Model</head><p>Like previous studies, the proposed model treats the Korean word spacing task as a sequence la- beling problem. The label consists of B and I, which are assigned to each syllable of the sen- tence. Assuming that x = &lt;x 1 , x 2 , …, x T &gt; is a se- quence of total T syllables of the input sentence and y = &lt;y 1 , y 2 , …, y T &gt; is a sequence of labels for each syllable, an example could be given as follows 2 :</p><p>Input: ah/beo/ji/ga bang/eh deul/eo/ga/sin/da (Father entered the room) x = &lt;ah, beo, ji, ga, bang, eh, deul, eo, ga, sin, da&gt; y = &lt;B, I, I, I, B, I, B, I, I, I, I&gt; In order to utilize the spacing information pro- vided by the user, we propose a new model, the <ref type="bibr">1</ref> Accuracy was calculated based on syllables. <ref type="bibr">2</ref> Slashes are used for distinguishing between syllables.</p><p>Balanced Word Spacing Model that adheres to the following principles:</p><p>1. The model must obtain the most likely se- quence of labels(y * ), while minimally altering the user-given sequence of labels (í µí°² !"#$% ). 2. We assume that it costs α per syllable to change the spacing of the original sentence, in order to keep the original spacing information as much as possible.</p><p>Mathematically formulating the above princi- ples would give us the following equation:</p><formula xml:id="formula_0">í µí°² * = argmax í µí± í µí±í µí±í µí±í µí± í µí°±, í µí°² − í µí»¼ • í µí°¿ í µí°² !"#$% , í µí°²<label>(1)</label></formula><p>In Equation 1, í µí± í µí±í µí±í µí±í µí± í µí°±, í µí°² calculates how com- patible the sequence of label y is with the input sentence x. It is calculated by a basic word spac- ing model as in ( <ref type="bibr" target="#b5">Lee and Kim, 2013)</ref>. í µí°¿ í µí°² !"#$% , í µí°² counts the number of different la- bels between the user-given sequence of labels í µí°² !"#$% and an arbitrary sequence of labels í µí°². í µí°² * of Equation 1 can be obtained by setting the gra- dient of í µí± í µí±í µí±í µí±í µí± í µí°±, í µí°² − í µí»¼ • í µí°¿ í µí°² !"#$% , í µí°² to 0, which is equivalent to the following equation:</p><formula xml:id="formula_1">∇í µí± í µí±í µí±í µí±í µí± í µí°±, í µí°² * = í µí»¼ • ∇í µí°¿ í µí°² !"#$% , í µí°² *<label>(2)</label></formula><p>In order to view the proposed model in a dif- ferent perspective, we consider BWSM in terms of Karush-Kuhn-Tucker (KKT) condition. KKT condition is a technique for solving optimization problems with inequality constraints. It is a gen- eralized version of Lagrange multipliers, which is a technique for solving optimization problems with equality constraints. Converting the afore- mentioned principles to a constrained optimiza- tion problem gives:</p><p>Maximize: í µí± í µí±í µí±í µí±í µí± í µí°±, í µí°² subject to í µí°¿ í µí°² !"#$% , í µí°² ≤ í µí±</p><p>Equation 3 tries to obtain y that maximizes í µí± í µí±í µí±í µí±í µí± í µí°±, í µí°² , namely the score of the basic model, while maintaining í µí°¿ í µí°² !"#$% , í µí°² below b, which is equivalent to altering the word spacing of the input sentence less than or equal to b times. To solve this constrained optimization problem, we apply KKT condition and define a new Lagran- gian function as follows:</p><p>Λ í µí°±, í µí°², í µí»¼ = í µí± í µí±í µí±í µí±í µí± í µí°±, í µí°² − í µí»¼ í µí°¿ í µí°² !!"#$ , í µí°² − í µí±</p><p>Setting the gradient of the Equation 4 to zero, namely ∇Λ í µí°±, í µí°², í µí»¼ = 0 , we get the following necessary conditions:</p><p>Stationarity: ∇í µí± í µí±í µí±í µí±í µí± í µí°±, í µí°² * = í µí»¼ * ∇í µí°¿ í µí°² !"#$% , í µí°² * Primal feasibility: í µí°¿ í µí°² !"#$% , í µí°² * ≤ í µí± Dual feasibility: í µí»¼ * ≥ 0 Complementary slackness: í µí»¼ * í µí°¿ í µí°² !"#$% , í µí°² * − í µí± = 0 (5)</p><p>Comparing Equation 1 with Equation 4 reveals that they are the same except the constant b. And í µí°² * which satisfies the conditions of Equation 5, and hence the solution to Equation 4, is also the same as í µí°² * which satisfies Equation 2, and hence the solution to Equation 1.</p><p>For the basic word spacing model, we use margin rescaled version of structural SVM as <ref type="bibr" target="#b5">Lee and Kim (2013)</ref>. The objective function of structural SVM is as follows:</p><formula xml:id="formula_4">min í µí²,! ! ! ∥ í µí°° ∥ ! + ! ! í µí¼ ! ! ! , í µí± . í µí±¡. ∀í µí±, í µí¼ ! ≥ 0 ∀í µí±, ∀í µí°² ∈ Y\í µí°² ! : í µí°° ! í µí»¿Ψ í µí°± ! , í µí°² ≥ í µí°¿ í µí°² ! , í µí°² − í µí¼ ! where í µí»¿Ψ í µí°± ! , í µí°² = Ψ í µí°± ! , í µí°² ! − Ψ í µí°± ! , í µí°²<label>(6)</label></formula><p>In Equation 6, í µí°± ! , í µí°² ! represents the i-th se- quence of syllables and its correct spacing labels. í µí°¿ í µí°² ! , í µí°² is a loss function that counts the number of different labels between the correct labels í µí°² ! and the predicted sequence of labels í µí°². Ψ í µí°±, í µí°² is a typical feature vector function. The features used for the basic word spacing model are the same features used in ( <ref type="bibr" target="#b5">Lee and Kim, 2013)</ref>. Since structural SVM was used for the basic word spacing model, the score function of Equa- tion 1 becomes í µí± í µí±í µí±í µí±í µí± í µí°±, í µí°² = í µí°° ! Ψ í µí°±, í µí°² .</p><p>We propose two approaches for implementing Equation 1.</p><p>1. N-best re-ranking: N-best sequences of spac- ing labels are obtained using the basic struc- tural SVM model. For each of the sequence, í µí»¼ * í µí°¿ í µí°² !"#$% , í µí°² * is calculated and subtracted from í µí± í µí±í µí±í µí±í µí± í µí°±, í µí°² . The result of the subtrac- tion is used to re-rank the sequences, and the one with the highest rank is chosen. 2. Modified Viterbi search: Viterbi search algo- rithm, which is used in the basic word spacing model to solve í µí°² * = argmax í µí°° ! Ψ í µí°±, í µí°² , is modified to solve í µí°² * = argmax í µí°° ! Ψ í µí°±, í µí°² − í µí»¼ • í µí°¿ í µí°² !"#$% , í µí°² . Both Ψ í µí°±, í µí°² and í µí°¿ í µí°² !"#$% , í µí°² can be calculated syllable by syl- lable, which makes it easy to modify Viterbi search algorithm.</p><p>The first approach seems straightforward and easy, but it would take a long time to obtain N- best sequences of labels. Furthermore, the correct label sequence might not be in those N-best se- quences, hence degrading the overall perfor- mance. The second approach is fast since it does not calculate N-best sequences, and unlike the first approach, will always consider the correct label sequence as a candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In order to compare the performance of BWSM with HMM-based Korean word spacing and structural SVM-based Korean word spacing, we use Sejong raw corpus ( <ref type="bibr" target="#b1">Kang and Kim, 2004</ref>) as train data and ETRI POS tagging corpus as test data <ref type="bibr">3</ref> . Pegasos-struct algorithm from ( <ref type="bibr" target="#b5">Lee and Kim, 2013</ref>) was used to train the basic structural SVM-based model. The optimal value for the tradeoff variable C of structural SVM was found after conducting several experiments <ref type="bibr">4</ref> .</p><p>The rate of word spacing error varies depend- ing on the corpus. Newspaper articles rarely have word spacing errors but text messages or Tweets frequently contain word spacing errors. To re- flect such variety, we randomly insert spacing errors into the test set to produce various test sets with spacing error rate 0%, 10%, 20%, 35%, 50%, 60%, and 70% 5 .</p><p>Figure 2: Word-based F-score of N-best re- ranking approach. <ref type="figure">Figure 2</ref> shows the relation between í µí»¼(x-axis) and word-based F-score 6 (y-axis) of N-best re- <ref type="bibr">3</ref> The number of words for the training set and test set are 26 million and 290,000 respectively. <ref type="bibr">4</ref> We experimented with 10, 100, 1000, 10000, 100000 and 1000000, the optimal value being 100000. <ref type="bibr">5</ref> We altered the input to the system and retained the origi- nal gold standard's space unit. <ref type="bibr">6</ref> Word-based F-score = 2*Prec word *Recall word / (Prec word + Recall word ), Prec word = (# of correctly spaced words) / (the total number of words produced by the system), ranking approach using test sets with different spacing error rate. When í µí»¼ = 0 , BWSM be- comes a normal structural SVM-based model. As í µí»¼ increases, F-score also increases for a while but decreases afterward. And F-score increases more when using test sets with low error rate. It is worth noticing that when using the test set with 0% error rate, as í µí»¼ increases, F-score con- verges to 98%. The reason it does not reach 100% is that the correct label sequence is sometimes not included in the N-best sequences.  <ref type="figure" target="#fig_1">Figure 3</ref> shows the relation between í µí»¼(x-axis) and word-based F-score(y-axis) of modified Viterbi search approach using test sets with dif- ferent spacing error rate. The graphs are similar to <ref type="figure">Figure 2</ref>, but F-score reaches higher values compared to N-best re-ranking approach. Notice that, when using the test set with 0% error rate, F-score becomes 100% as í µí»¼ surpasses 3. This is because, unlike N-best re-ranking approach, modified Viterbi search approach considers all possible sequences as candidates.</p><p>From <ref type="figure">Figure 2</ref> and 3, it can be seen that BWSM, which takes into consideration the spac- ing information provided by the user, can im- prove performance significantly. It is also appar- ent that modified Viterbi search approach outper- forms N-best re-ranking approach. The optimal value for í µí»¼ varies as test sets with different error rate are used. It is natural that, for test sets with low error rate, the optimal value of í µí»¼ increases, thus forcing the model to more utilize the user- given spacing information. It is difficult to auto- matically obtain the optimal í µí»¼ for an arbitrary input sentence. Therefore we set í µí»¼ to 1, which, according to <ref type="figure" target="#fig_1">Figure 3</ref>, is more or less the optimal value for most of the test sets.</p><p>Recall word = (# of correctly spaced words) / (the total num- ber of words in the test data)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Syllable based precision</head><p>Word based precision HMM (  S-SVM ( <ref type="bibr" target="#b5">Lee and Kim, 2013</ref>  <ref type="table">Table 1</ref>: Precision of BWSM and previous studies</p><p>With í µí»¼ set to 1, and using modified Viterbi search algorithm, the performance of BWSM is shown in <ref type="table">Table 1</ref> with other previous studies ( <ref type="bibr" target="#b5">Lee and Kim, 2013;</ref>. <ref type="table">Table 1</ref> shows that BWSM gives superior performance than other studies that do not utilize user-given spacing information. We also collected Tweets from Twitter and tested modified Viterbi algorithm on them. <ref type="figure" target="#fig_2">Fig- ure 4</ref> shows the relation between í µí»¼ (x-axis) and word-based F-score (y-axis). The raw Tweets showed word-based F-score of approximate 91%, and the basic structural SVM model (í µí»¼ = 0) showed somewhat inferior 88%. Modified Viterbi algorithm showed the similar behavior as <ref type="figure" target="#fig_1">Figure 3</ref>, showing 93.2~93.4% word-based F- score when í µí»¼ was set to 0.5~1. <ref type="figure" target="#fig_2">Figure 4</ref> shows that BWSM is effective not only on text with randomly inserted spacing errors, but also on actual data, Tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed BWSM, a new struc- tural SVM-based Korean word spacing model that utilizes user-given spacing information. BWSM can obtain the most likely sequence of spacing labels while minimally altering the word spacing of the input sentence. Experiments on test sets with various error rate showed that BWSM significantly improved word-based F-score, from 95.47% to 98.39% in case of the test set with 10% error rate.</p><p>For future work, there are two interesting di- rections. First is to improve BWSM so that it can automatically obtain the optimal value of í µí»¼ for an arbitrary sentence. This will require a training set consisting of text with actual human spacing errors and its corrected version. Second is to ap- ply BWSM to other interesting problems such as named entity recognition (NER). Newspaper ar- ticles often use certain symbols such as quotation marks or brackets around the titles of movies, songs and books. Such symbols can be viewed as user-given input, which BWSM will try to re- spect as much as possible while trying to find the most likely named entities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of word spacing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Word-based F-score of modified Viterbi search.</figDesc><graphic url="image-2.png" coords="4,71.04,223.37,218.65,127.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Word-based F-score of modified Viterbi search on Tweets.</figDesc><graphic url="image-3.png" coords="4,306.29,297.87,219.10,123.95" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the IT R&amp;D pro-gram of MSIP/KEIT (10044577, Development of Knowledge Evolutionary WiseQA Platform Technology for Human Knowledge Augmented Services). We would like to thank the anony-mous reviewers for their comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cutting-plane training of structural SVMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Nam John</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sejong Korean Corpora in the making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hunggyu</forename><surname>Beom-Mo Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC</title>
		<meeting>the LREC</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1747" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML</title>
		<meeting>the ICML</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Modified Fixed-threshold SMO for 1-Slack Structural SVM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changki</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung-Gil</forename><surname>Jang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ETRI Journal</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="120" to="128" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic Korean word spacing using structural SVM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changki</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunki</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the KCC</title>
		<meeting>the KCC</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="270" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Automatic Korean word spacing using Pegasos algorithm. Information Processing and Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changki</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunki</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="370" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Automatic word spacing using probabilistic models based on character n-grams. Intelligent Systems IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Do-Gil</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="28" to="35" />
		</imprint>
	</monogr>
	<note>Hae-Chang Rim and Dongsuk Yook</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A new approach for Korean word spacing incorporating confidence value of user&apos;s input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Wook</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ALPIT</title>
		<meeting>the ALPIT</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="92" to="97" />
		</imprint>
	</monogr>
	<note>Hae-Chang Rim and So-Young Park</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic word spacing based on Conditional Random Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Sup</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Korean Journal of Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="233" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pegasos: Primal estimated sub-gradient solver for SVM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Support vector machine learning for interdependent and structured output spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML</title>
		<meeting>the ICML</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
