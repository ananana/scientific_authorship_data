<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mapping Text to Knowledge Graph Entities using Multi-Sense LSTMs</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 1959</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Theoretical and Applied Linguistics</orgName>
								<orgName type="institution">University of Cambridge Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taher</forename><surname>Pilehvar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Theoretical and Applied Linguistics</orgName>
								<orgName type="institution">University of Cambridge Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Theoretical and Applied Linguistics</orgName>
								<orgName type="institution">University of Cambridge Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mapping Text to Knowledge Graph Entities using Multi-Sense LSTMs</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1959" to="1970"/>
							<date type="published">October 31-November 4, 2018. 2018. 1959</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper addresses the problem of mapping natural language text to knowledge base entities. The mapping process is approached as a composition of a phrase or a sentence into a point in a multi-dimensional entity space obtained from a knowledge graph. The composi-tional model is an LSTM equipped with a dynamic disambiguation mechanism on the input word embeddings (a Multi-Sense LSTM), addressing polysemy issues. Further, the knowledge base space is prepared by collecting random walks from a graph enhanced with textual features, which act as a set of semantic bridges between text and knowledge base entities. The ideas of this work are demonstrated on large-scale text-to-entity mapping and entity classification tasks, with state of the art results.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of associating a well-defined action, con- cept or piece of knowledge to a natural language utterance or text is a common problem in natu- ral language processing and generic artificial in- telligence <ref type="bibr" target="#b37">(Tellex et al., 2011)</ref>, and can emerge in many different forms. In NLP, the ability to code text into an entity of a knowledge graph finds applications in tasks such as question an- swering and information retrieval, or any task that involves some form of mapping a definition to a term ( <ref type="bibr" target="#b8">Hill et al., 2016;</ref><ref type="bibr" target="#b34">Rimell et al., 2016)</ref>. Fur- ther, it can be invaluable in providing solutions to domain-specific challenges, for example medical concept normalisation <ref type="bibr" target="#b20">(Limsopatham and Collier, 2016</ref>) and identification of adverse drug reactions (O' <ref type="bibr">Connor et al., 2014)</ref>.</p><p>This paper details a model for efficiently map- ping unrestricted text at the level of phrases and sentences to the entities of a knowledge base (KB)-a task also referred to as text grounding or normalisation. The model aims at characterising short focused texts, such as definitions or tweets. Given a medical KB, for example, a tweet of the form "Can't sleep, too tired to think straight" would be mapped to the entity Insomnia, while in the context of a lexical ontology the definition "Device that detects planets" would be associated to the entity Telescope.</p><p>Note that such a task cannot be approached as standard classification, since the "classes" (en- tities) are usually in one-to-one correspondence with the available inputs. To address this we pro- pose the use of a continuous vector space for em- bedding the entities of the KB graph, where text is projected by a neural network. We rely on the notion of distributional semantics, where a word is represented as a multi-dimensional vector ob- tained either by collecting co-occurrence statistics with a selected set of contexts or by directly opti- mising an objective function in a neural network- based architecture <ref type="bibr" target="#b4">(Collobert and Weston, 2008)</ref>. Interestingly, similar techniques can be used for the multi-dimensional representation of nodes in a KB graph; for example, by collecting random walks following the edges of a graph it is possi- ble for one to construct an artificial "corpus", to which a distributional model applies in the usual way ( <ref type="bibr" target="#b30">Perozzi et al., 2014)</ref>.</p><p>By exploiting this representational compatibil- ity, we treat the process of text-to-entity map- ping as a transformation from a textual vector space where words live, to a KB vector space cre- ated from a graph and populated by vectors rep- resenting entities. A sentence is coded as a se- quence of word vectors, composed by a modi- fied Long Short-Term Memory network <ref type="bibr">(LSTMHochreiter and Schmidhuber, 1997</ref>) into a multi- dimensional point in the entity space. One of our aims is to specifically deal with lexical ambiguity and polysemy which can be an important factor for the task at hand. To this end, each word is as- sociated with a number of sense embeddings, and the LSTM is extended with an attentional disam- biguation mechanism that dynamically selects and updates the right sense vector for each word given its context during training. We dub this formula- tion Multi-Sense LSTM (MS-LSTM).</p><p>An important issue is the provision of a set of reliable anchors; that is, points in one-to-one cor- respondence between the two representations that would enforce some degree of structural similar- ity between pieces of text and KB entities and thus make the mapping more efficient. We deal with this problem by extending the original KB graph with nodes corresponding to textual fea- tures, i.e. to words strongly associated to a spe- cific entity and collected from various resources. A novel sampling strategy is detailed for incor- porating these nodes to random walks, which are then fed to the skipgram model for producing an entity space. The results indicate that the textual nodes, being words and KB entities at the same time, do an extremely effective job in transform- ing the geometry of the entity space to the benefit of mapping the textual modality.</p><p>The proposed model is evaluated in three tasks: text-to-entity mapping on a dataset extracted from SNOMED CT 1 , a medical knowledge base of 327,000 concepts; a reverse dictionary task based on WordNet <ref type="bibr" target="#b26">(Miller, 1998)</ref>, where the goal is to associate a multi-word definition to the correct lemma ( <ref type="bibr" target="#b8">Hill et al., 2016)</ref>; and document classifica- tion on the Cora dataset ( <ref type="bibr" target="#b23">McCallum et al., 2000</ref>). The results demonstrate the effectiveness of our methods by improving the current state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Aligning meaning between text and entities in a knowledge graph is a task traditionally based on heuristic methods exploiting text features such as string matching, word weighting, syntactic re- lations, or dictionary lookups ( <ref type="bibr" target="#b22">McCallum et al., 2005;</ref><ref type="bibr" target="#b21">Lu et al., 2011;</ref><ref type="bibr" target="#b29">O'Connor et al., 2014</ref>). Ma- chine learning techniques have been also exploited in various forms, for example <ref type="bibr" target="#b17">Leaman et al. (2013)</ref> use a pairwise learning-to-rank technique to learn the similarity between different terms, while <ref type="bibr" target="#b19">Limsopatham and Collier (2015)</ref> apply statistical ma- chine translation to "translate" social media text to domain-specific terminology. There is little work based on neural networks; the most relevant to us is a study by <ref type="bibr" target="#b8">Hill et al. (2016)</ref>, who tested a num- ber of compositional neural architectures trained to approximate word embeddings on a reverse dic- tionary task. Compared to their work, this paper proposes the use of a distinct target space for rep- resenting ontological knowledge, where every en- tity in the graph lives.</p><p>The goal of a graph embedding method is to em- bed components of a knowledge graph into a low- dimensional space. One research direction focuses on the relations, i.e. the edges of the graph <ref type="bibr" target="#b1">(Bordes et al., 2013;</ref><ref type="bibr" target="#b36">Socher et al., 2013;</ref><ref type="bibr" target="#b44">Xiao et al., 2016</ref>) and aims at tasks such as link prediction and KB completion. Such work is outside the scope of the current paper, the subject of which is the efficient low-dimensional representation of entities (nodes). In this line of research, a preva- lent method involves the collection of a set of ran- dom walks, starting from each node in the graph ( <ref type="bibr" target="#b30">Perozzi et al., 2014;</ref><ref type="bibr" target="#b6">Grover and Leskovec, 2016)</ref>. There is a direct analogy between such a set of random walks and a text corpus: each node corre- sponds to a word and the sequence of nodes vis- ited during a random walk is analogous to a sen- tence. Thus, any distributional model that takes as input this artificial "corpus" can generate multi- dimensional representations of the nodes in the graph. Random walks have also been used for KB inference <ref type="bibr" target="#b16">(Lao et al., 2011</ref>) with success.</p><p>While random walk-based methods are not the only way to construct graph spaces-alternatives include factorisation ( <ref type="bibr" target="#b0">Ahmed et al., 2013</ref>) and deep autoencoders ( -they have been found very effective in capturing multiple as- pects of the graph structure ( <ref type="bibr" target="#b41">Wang et al., 2017;</ref><ref type="bibr" target="#b5">Goyal and Ferrara, 2017)</ref>. The current paper pro- poses a random walk generation strategy that im- proves and complements existing approaches.</p><p>The idea of using textual features to improve the entity vectors is not well explored, and most of the existing work focuses again on the representation of relations ( <ref type="bibr" target="#b45">Xie et al., 2016;</ref><ref type="bibr" target="#b42">Wang et al., 2014;</ref><ref type="bibr" target="#b43">Wang and Li, 2016)</ref> as opposed to entities. Closer to us is the work of <ref type="bibr" target="#b46">Yamada et al. (2017)</ref> and <ref type="bibr" target="#b47">Yang et al. (2015)</ref>, with the latter to incorporate text fea- tures in the concept embeddings by exploiting ma- trix factorisation properties.</p><p>Representing the meaning of words using a number of sense vectors is an old and well-established idea in NLP-see for example <ref type="bibr" target="#b35">(Schütze, 1998;</ref><ref type="bibr" target="#b32">Reisinger and Mooney, 2010;</ref><ref type="bibr" target="#b28">Neelakantan et al., 2014</ref>). However, most of the relevant research is evaluated on intrinsic tasks such as word similarity, while in the few works based in real end tasks, disambiguation is usu- ally treated as a prior stand-alone step <ref type="bibr" target="#b18">Li and Jurafsky, 2015;</ref><ref type="bibr" target="#b31">Pilehvar et al., 2017</ref>). The crucial difference of this work is that the ambiguity resolution mecha- nism is part of the compositional model itself, and the sense embeddings are trained simultaneously with the rest of the parameters. A close work is by <ref type="bibr" target="#b3">Cheng and Kartsaklis (2015)</ref>, who used a siamese network with an integrated disambiguation mech- anism for paraphrase detection. For more infor- mation on multi-sense embeddings see <ref type="bibr">(CamachoCollados and Pilehvar, 2018</ref>). <ref type="figure" target="#fig_0">Fig. 1</ref> provides a high-level illustration of our methodology, consisting of two stages: (1) the KB graph is extended with weighted textual fea- tures, and an artificial "corpus" of random walks is created and used as input to the skipgram model ( <ref type="bibr" target="#b25">Mikolov et al., 2013</ref>) for generating an enhanced KB space-this part is covered in §3.1; (2) the transformation from text to entities is performed by a supervised multi-sense compositional model, which generates a point in the KB space for every input text. This is achieved with an LSTM recur- rent network, equipped with an attentional mech- anism that provides a finer level of granularity to the different ways a word is used in the data-we detail this part in §3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Textual features for entity vectors</head><p>For our KB space, we follow the generic recipe proposed by <ref type="bibr" target="#b30">Perozzi et al. (2014)</ref> and we assemble an artificial corpus of random walks from the KB graph, which is then used as input to the skipgram model ( <ref type="bibr" target="#b25">Mikolov et al., 2013</ref>). For a random walk of nodes n 1 n 2 . . . n T and a context window size c, skipgram maximises the following quantity:</p><formula xml:id="formula_0">1 T T t=1 −c≤j≤c,j =0 log p(n t+j |n t )<label>(1)</label></formula><p>i.e. for a target node n t , the objective is to pre- dict all other nodes in the same context. As a consequence, two vectors of the resulting space will be close if their corresponding nodes occur in topological proximity within the graph. However, while such a topology allows perhaps for mean- ingful comparisons between points in this space, it is not directly compatible with the task of mapping text to entities. The reason is that the communities formed in a KB graph (and thus the topology of the resulting vector space) mostly reflect domain- specific hierarchies and ontological relationships that are not necessarily evident by the textual rep- resentations referring to the entities. An impor- tant question therefore with regard to the proposed methodology is how to provide meaningful links between the two representations that would allow for the efficient translation of one form (text) to another (entities). In this work, we address this problem by associ- ating every node in the graph with a set of textual features, each one of which is weighted according to their importance with respect to the node. Our methodology is as follows: For each entity, we collect all available textual descriptions found in the knowledge base itself and the English portion of BabelNet ( <ref type="bibr" target="#b27">Navigli and Ponzetto, 2012)</ref>, which is a very large dictionary integrating numerous re- sources, such as WordNet, Wikipedia, FrameNet and many others. The textual descriptions are treated as short documents, and each word in them is assigned a specific TF-IDF value, forming the set of textual features for the specific entity.</p><p>The KB graph is extended in the following way: Let T c be the set of textual features for an entity c; then, for each t in T c , we add an edge (c, t) with weight tf-idf c (t), where tf-idf c (t) is the TF- IDF value of t with respect to c. In contrast to <ref type="bibr" target="#b30">Perozzi et al. (2014)</ref> who utilise a uniform node sampling strategy, we define the random walk gen- eration process as follows: Given a randomly se- lected node n, let C n = {c 1 , c 2 , · · · c N } be the set of all entity nodes in its immediate vicinity, and</p><formula xml:id="formula_1">T n = {t 1 , t 2 , · · · t M } the set of all textual features λ = 0 λ = 0.5 λ = 1</formula><p>Figure 2: Effect of λ parameter. Blue nodes indicate entities, red nodes textual features, and red paths re- fer to random walks. As λ increases, the probability of "hops" between originally unlinked nodes increases accordingly.</p><p>of n; the next node x in the path is drawn from a categorical distribution defined as below:</p><formula xml:id="formula_2">P X (x) =      (1 − λ) 1 N if x ∈ Cn λ tf-idfn(ti) M j=1 tf-idfn(tj) if x ∈ Tn (2)</formula><p>for X a discrete random variable with range C n ∪ T n . In the above, λ defines the proportion of the probability mass allocated to textual features, when both C n and T n are non-empty; if one of the sets is empty, all of the probability mass is allo- cated to the other set, and λ becomes irrelevant. Further, in contrast to what is the case for the tex- tual nodes, the probabilities of the entity nodes in Equation 2 are defined uniformly, since we lack any mechanism for fine-tuning them in a way that objectively reflects the importance of the nodes. It is instructive to examine how the above sam- pling strategy works. As expected, setting λ = 0 will result in a sampling process that ignores the textual features and produces a path comprised solely of entity nodes; this is equivalent to the original model by <ref type="bibr" target="#b30">Perozzi et al. (2014)</ref>, known as DeepWalk. On the other hand, the effect of setting λ = 1 is less intuitive: Recall that, by construc- tion, each textual node is connected only to entity nodes; that is, when the current node is textual, the next node will be always sampled from C n . Therefore, setting λ = 1 creates paths following an alternating pattern, where each entity node is followed by a textual node, which in turn is fol- lowed by an entity node. Values of λ between 0 and 1 scale this behaviour accordingly <ref type="figure">(Fig. 2)</ref>.</p><p>Advantages. The introduction of textual fea- tures in the graph achieves two goals. Firstly, the textual nodes serve as links between entities which, although perhaps related to each other in some way, lie in different parts of the KB graph (e.g. being parts of different hierarchies). As Further, due to the presence of inability in their contexts, the vectors of alexia and insomnia (concepts originally quite far apart in the graph) will now have a common part reflecting that they are both conditions related to forms of incompetence.</p><p>a result, points that would normally be unjustifi- ably apart of each other in the vector space are now brought closer, providing additional coher- ence. This behaviour is controlled by the λ pa- rameter, as <ref type="figure">Fig. 2</ref> shows. <ref type="figure" target="#fig_1">Fig. 3</ref> presents an illus- trative example, taken from a real random walk on SNOMED CT.</p><p>The second advantage of introducing textual features in the graph is a consequence of the dual nature of these features in the context of learn- ing: they essentially represent words, but since they are also nodes of the graph, they get vector representations exactly as every other normal en- tity in the knowledge base. The textual features, therefore, paired with their assigned vectors, form a set of anchors that links pieces of text with the KB space, and can be used to support the train- ing process of the mapping system. In §4.1 we will see that this approach leads to substantial im- provements in the accuracy of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A multi-sense LSTM</head><p>We now proceed to present our neural architecture for text-to-entity mapping. The goal of the model is, given a certain piece of text, to produce a point in the KB space corresponding to an appropriate entity or concept. The model is trained on pairs of texts and entity vectors created from a graph ex- tended with textual features, as discussed in §3.1.</p><p>Our architecture needs to explicitly take into ac- count the fact that the task at hand is very sen- sitive to lexical ambiguity. Specifically, while it is true that the level of homonymy (words hav- ing more than one disjoint meanings) is substan- tially decreased when we move from the generic domain to more specialised domains, on the other hand the increase in polysemy (words with many slightly different meanings) is exponential. As an example, while the lemma for the word "fever" in a dictionary usually contains two or three defini- tions, the term occurs in many dozens of different forms and contexts in SNOMED. Note that most of the different uses of the term correspond to distinct KB nodes, a fact that makes the job of a text-to- entity mapping system especially hard. <ref type="bibr">2</ref> This mo- tivates the employment of a dedicated mechanism that would handle the extra complexity imposed by the polysemous words. The compositional setting of this paper, equip- ped with such a mechanism, is shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. It consists of a generic word embedding layer, a word sense disambiguation layer, and two con- secutive LSTM networks responsible for encoding the embeddings into a vector in the KB space. The objective is to minimise the mean squared error between the predicted vectors and the target vec- tors (prepared as in §3.1):</p><formula xml:id="formula_3">MSE = 1 N N i=1 y i − f (x i ) 2<label>(3)</label></formula><p>where N is the number of training examples, x the input text, y the target entity vector, and f the neural network.</p><p>To address the polysemy issues discussed above, every word is associated with a sin- gle generic embedding and k sense embeddings, where k is a fixed number. These sense embed- dings can be seen as centroids of clusters denoting different uses of the word in the training set, and are dynamically updated during training. Specif- ically, for each word w i in a training example, a context vector c i is computed as the average of the generic vectors of all other words in the sentence. The probability of each sense vector s ij given this context is then calculated via an attentional mech- anism equipped with a softmax layer, as follows:</p><formula xml:id="formula_4">p(s ij |c i ) = exp (w j s ij ) k l=1 exp (w l s il )<label>(4)</label></formula><p>where s ij = tanh(W s ij +U c i ), and W , U and W the parameters of the attentional network. Each sense vector is subsequently updated by addition of the context vector weighted by its similarity with the specific sense:</p><formula xml:id="formula_5">s t+1 ij = s t ij + (s t ij c t i )c t i<label>(5)</label></formula><p>The output of the attention is a weighted sum of the sense vectors given their probabilities (i.e. we apply soft attention), which is used as input to the compositional network-a 2-layer LSTM. The overall model is optimised on the MSE of the LSTM's output vector and the target entity vec- tor. At inference time, a predicted vectorˆyvectorˆ vectorˆy can be classified to the entity with the closest vectorial representation according to some metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The ideas presented in the previous sections are evaluated on three tasks, two of which are related to text-to-entity mapping, and one to classifica- tion of KB entities. The purpose of the classifica- tion task ( §4.3) is to provide a direct comparison of the textually enhanced vectors against vectors produced by the original graph, but independently of the compositional part. On the other hand, the text mapping experiments ( § §4.1, 4.2) evalu- ate the overall architecture of <ref type="figure" target="#fig_0">Fig. 1</ref> (including the compositional model and the dynamic disam- biguation mechanism) on appropriate end tasks. Comparisons are provided with the most relevant previous work. Specifically, in all tasks, no inclu- sion of textual features corresponds to the standard DeepWalk model of <ref type="bibr" target="#b30">Perozzi et al. (2014)</ref>; in §4.2 our compositional architecture is compared to the work of <ref type="bibr" target="#b8">Hill et al. (2016)</ref> in their reverse dictio- nary task; and §4.3 compares our method for tex- tually enhancing the entity space with that of <ref type="bibr" target="#b47">Yang et al. (2015)</ref>, and other state-of-the-art deep mod- els. The last subsection, §4.4, examines a few se- lected cases from a qualitative perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Text-to-entity mapping</head><p>We begin with a large scale text-to-entity mapping experiment. We construct a dataset of 21,000 med- ical concepts extracted from SNOMED CT, each of which is associated with a multi-word textual description, taken from the knowledge base or Ba- belNet. The criterion for including a concept in the dataset was the availability of at least one tex- tual description with 4 or more words. The objec- tive of the task is to associate each one of these descriptions to the correct concept. Given a pre- dicted vectorˆvvectorˆ vectorˆv, we assemble a list of all candidate concept vectors ranked by their cosine similarity withˆvwithˆ withˆv. We compute strict accuracy (based on how many times the vector of the correct concept is at the top of the list) and accuracy on the first 20 el- ements of the list. Further, we also present results based on the mean reciprocal rank (MRR).</p><p>In all experiments, we create KB vectors of 150 dimensions by applying the skipgram objec- tive on a set of random walks of length 20, and with window size of 5. The graph is extended with 102,500 textual nodes weighted by their TF- IDF values with regard to the corresponding enti- ties and selected as described in §3.1 (textual fea- tures that occur in the testing set are not taken into account). Each node in the graph serves as the starting point of 10 random walks. For the compo- sitional model, we use embeddings of 150 dimen- sions, and 200-dimensional hidden states. The at- tentional mechanism is implemented as a 2-layer MLP, with 50 units allocated to the hidden layer for each sense. The overall model contains two dropout layers for regularisation purposes, and is optimised with Adam ( <ref type="bibr" target="#b14">Kingma and Ba, 2015)</ref> (α = 0.001, β 1 = 0.9, β 2 = 0.999). <ref type="bibr">3</ref> Following usual practice, we split our dataset in three parts: a training set (14,754 instances), a testing set (4,187 instances), and a development set (2,000 instances). We use the dev set to opti- mise the two main hyper-parameters of our model, namely the probability mass given to textual fea- tures (λ) and the number of senses for each word (k). The experiments on the dev set showed that increasing the probability mass for the inclusion of textual features in the random walks leads to con- sistently better performance for all tested models, so for the main experiment we set λ to its highest possible value, 1.00. <ref type="bibr">4</ref> Further, a number of senses  <ref type="table">Table 1</ref>: Results for the SNOMED dataset. For the MS- LSTM we set k = 3, while TF vectors refers to our textually enhanced vectors (λ = 1). The difference between MS-LSTM and LSTM is s.s. with p &lt; 0.01 according to a two-tailed z-test.</p><p>equal to 3 achieved the highest performance.</p><p>We compare our MS-LSTM with a number of baselines: In Baselines 1 and 2 a vector for each textual description is computed as the average of pre-computed word vectors, and compared to con- cept vectors prepared in a similar way, i.e. by av- eraging pre-computed vectors for all words in the qualified name of the entities. We used two differ- ent word spaces, a standard Word2Vec space cre- ated from Google News <ref type="bibr">5</ref> and a custom Word2Vec model trained on a corpus of 4B tokens from med- ical articles indexed in PubMed <ref type="bibr">6</ref> . In Least squares and CCA, an averaged vector for each textual de- scription is again computed as before, and a linear mapping is learned between the textual space and the KB space, using least squares and canonical correlation analysis <ref type="bibr" target="#b7">(Hardoon et al., 2004</ref>).</p><p>In Standard LSTM, we use a configuration simi- lar to that of <ref type="figure" target="#fig_2">Fig. 4</ref>, but without the multi-sense as- pect; here, the word embeddings are just parame- ters of the model randomly initialised before train- ing. Further, we also test a standard LSTM where the length of the single embeddings is k times big- ger (k is the number of senses in the MS-LSTM), so that the overall dimensionality of embeddings in LSTM and MS-LSTM is the same.</p><p>The results are presented in <ref type="table">Table 1</ref>. Each model is tested against two target KB spaces, one consisting of simple DeepWalk vectors <ref type="bibr">7</ref> and one of textually enhanced vectors (TF vectors, λ = 1) according to the procedure of §3.1. There are three observations: (1) Using the enhanced vectors as a target space improves the performance of all tested models by a large margin; (2) the MS-LSTM con- figuration of <ref type="figure" target="#fig_2">Fig. 4</ref> achieves the highest overall performance, showing that explicitly handling pol- ysemy during the composition is beneficial for the task at hand; and (3) despite the equal dimension- ality between the two models, the standard LSTM with the long embeddings presents performance inferior to that of the MS-LSTM.</p><p>The last row of the table presents results after extending the training dataset with the textual an- chors, that is, all the textual features paired with their learned KB vectors, as described in the Ad- vantages section in §3.1. Specifically, recall that each textual feature (a word or a two-word com- pound), being also a node in the graph, is associ- ated with a vector according to the process of §3.1. It is possible for one then to use these (textual fea- ture, vector) pairs as additional examples during the training of the MS-LSTM. The last row of Ta- ble 1 shows the results after extending the training set with the 102,500 textual features. This setting achieves the highest performance, increasing fur- ther the strict accuracy by 6%, to 0.90.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Reverse dictionary</head><p>We proceed to the reverse dictionary task of <ref type="bibr" target="#b8">Hill et al. (2016)</ref>, the goal of which is to return a can- didate term given a definition. Many forms of this task have been proposed in the past, see for exam- ple ( <ref type="bibr" target="#b12">Kartsaklis et al., 2012;</ref><ref type="bibr" target="#b38">Turney, 2014;</ref><ref type="bibr" target="#b34">Rimell et al., 2016</ref>). In ( <ref type="bibr" target="#b8">Hill et al., 2016)</ref>, the authors test a number of supervised models under two evalu- ation modes: (1) "seen", in which the testing in- stances are also included in the training set; and (2) "unseen", where the evaluation is done on a held-out set. In both cases the datasets consisted of 500 term-definition pairs from WordNet.</p><p>We treat WordNet as a graph, the edges of which are defined by the various relationships between the synsets. This graph is further ex- tended with 96,734 textual nodes extracted from the synset descriptions. We compute synset vec- tors of 150 dimensions, on random walks of length 20 and with window size of 5. For the seen evalu- ation, we train the compositional model on the to- tality of WordNet 3.0 synsets (117,659) and their descriptions. For the unseen evaluation, we re- move from the graph any textual features occur- ring in the testing part, and create a new set of synset vectors; further, any testing instance is re- moved from the training set of the compositional model. The evaluation is done by comparing the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Acc-10 Acc-100 Seen (500 WordNet definitions) OneLook ( <ref type="bibr" target="#b8">Hill et al., 2016)</ref> 0.89 0.91 RNN cosine ( <ref type="bibr" target="#b8">Hill et al., 2016)</ref> 0.48 0.73 Std LSTM (150 dim.) + TF vec.</p><p>0.86 0.96 Std LSTM (k × 150 dim.) + TF vec. 0.93 0.98 MS-LSTM +TF vectors 0.95 0.99 MS-LSTM +TF vectors + anchors 0.96 0.99 Unseen (500 WordNet definitions) RNN w2v cosine <ref type="figure" target="#fig_0">(Hill et al., 2016)</ref> 0.44 0.69 BOW w2v cosine <ref type="figure" target="#fig_0">(Hill et al., 2016)</ref> 0 <ref type="table">Table 2</ref>: Results for the reverse dictionary task, com- pared with the highest numbers reported by <ref type="bibr" target="#b8">Hill et al. (2016)</ref>. TF vectors refers to textually enhanced vectors with λ = 1. For the MS-LSTM, k is set to 3.</p><note type="other">.46 0.71 Std LSTM (150 dim.) + TF vec. 0.72 0.88 Std LSTM (k × 150 dim.) + TF vec. 0.77 0.90 MS-LSTM + TF vectors 0.79 0.90 MS-LSTM + TF vectors + anchors 0.80 0.91</note><p>predicted vector with the vectors of all WordNet synsets (a search space of 117,659 points) and cre- ating a ranked list as before, by cosine similarity. Following ( <ref type="bibr" target="#b8">Hill et al., 2016)</ref>, we compute accu- racy on top-10 and top-100. λ and k are tuned on a dev set of 2,000 synsets, showing a behaviour very similar to that of the SNOMED task. <ref type="table">Table 2</ref> shows the results, based on a MS-LSTM setup similar to that of §4.1. Note that the MS- LSTM achieves 0.95-0.96 top-10 accuracy for the seen evaluation, significantly higher not only than the best model of <ref type="bibr" target="#b8">Hill et al. (2016)</ref>, but also higher than OneLook, a commercial system with access to more than 1000 dictionaries. It also presents considerably higher performance in the unseen evaluation. We are not aware of any other mod- els with higher performance on the specific task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Document classification</head><p>Our last experiment is a document classification task, performed on Cora ( <ref type="bibr" target="#b23">McCallum et al., 2000</ref>), a dataset containing 2708 machine learning papers linked by citation relationships into a graph. Each document is a short text extracted from the title or the abstract of the paper. The task is to predict the category of a document (a total of 7 classes), given its vector-so here we only evaluate the textually enhanced vectors as inputs to a classifier, indepen- dently of the compositional part.</p><p>In <ref type="table" target="#tab_1">Table 3</ref> we report results for two evaluation settings. In Evaluation 1, we provide a compari- son with the method of <ref type="bibr" target="#b47">Yang et al. (2015)</ref> who in- clude textual features in graph embeddings based on matrix factorisation, and two topic models used as baselines in their paper. Using the same clas-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Accuracy Evaluation 1 (training ratio=0.50) PLSA <ref type="bibr" target="#b10">(Hofmann, 1999)</ref> 0.68 NetPLSA ( <ref type="bibr" target="#b24">Mei et al., 2008)</ref> 0.85 TADW ( <ref type="bibr" target="#b47">Yang et al., 2015)</ref> 0.87 Linear SVM + DeepWalk vectors 0.85 Linear SVM + TF vectors 0.88 Evaluation 2 (training ratio=0.05) Planetoid ( <ref type="bibr" target="#b48">Yang et al., 2016)</ref> 0.76 GCN ( <ref type="bibr" target="#b15">Kipf and Welling, 2017)</ref> 0.81 GAT <ref type="bibr" target="#b39">(Veličkovi´Veličkovi´c et al., 2018)</ref> 0.83 Linear SVM + DeepWalk vectors 0.72 Linear SVM + TF vectors 0.82 sification algorithm (a linear SVM) and training ratio (0.50) with them, we present state-of-the-art results for vectors of 150 dimensions, prepared by a graph extended with 1422 textual features. We set λ = 0.5 by tuning on a dev set of 677 randomly selected entries from the training data. <ref type="bibr">8</ref> In Evaluation 2, using the same linear SVM classifier and λ as before, we reduce the train- ing ratio to 0.05 in order to make our task com- parable to the experiments reported by Veličkovi´Veličkovi´c et al. <ref type="formula" target="#formula_0">(2018)</ref> for a number of deep learning models: specifically, the graph attention network (GAT) of Veličkovi´ <ref type="bibr" target="#b39">Veličkovi´c et al. (2018)</ref>, the graph convolu- tional network (GCN) of <ref type="bibr" target="#b15">Kipf and Welling (2017)</ref>, and the Planetoid model of <ref type="bibr" target="#b48">Yang et al. (2016)</ref>. Again, our simple setting presents results within the state of the art range, comparable to (or bet- ter than) those of much more sophisticated models that have been specifically designed for the task of node classification. We consider this as a strong indication for the effectiveness of the textually en- hanced vectors as representations of KB entities. <ref type="figure" target="#fig_3">Fig. 5</ref> provides a visualisation of the Cora clas- ses based on node vectors created with λ = 0 and λ = 0.5, correspondingly, demonstrating the im- pact of textual features in terms of cluster coher- ence and separation. <ref type="table" target="#tab_2">Table 4</ref> compares the performance of the multi- sense approach with that of the single-sense model for a number of selected cases of text mapping. The predictions in the top part (for definitions taken from the unseen evaluation of the reverse dictionary task) show that, in contrast to the single-sense model, the multi-sense approach was able to capture subtle variations of meaning be- tween different synsets due to polysemy, as moti- vated in §3.2. The lower part of the table contains short phrases with ambiguous words, specifically selected to demonstrate the effect of the multi- sense approach. In all these cases, the multi-sense model was able to effectively disambiguate the ambiguous parts of the phrase by using the avail- able context, and predict a very relevant synset; in contrast, the predictions of the single-sense model were based on choosing a wrong sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative evaluation</head><p>Finally, <ref type="table" target="#tab_3">Table 5</ref> presents the derived senses for word table, expressed as lists of nearest neigh- bouring words in the space. The model was able to effectively distinguish between a table as a kitchen furniture (sense 2), and a table as a structured way of presenting data (senses 1 and 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>The experimental work shows that using a graph embedding space as a target for mapping text to entities is an effective approach. This was mostly evident in the reverse dictionary task of §4.2, where the model was found to perform sub- stantially better than previous approaches by <ref type="bibr" target="#b8">Hill et al. (2016)</ref>, who used a compositional architec- ture similar to ours but optimised on the word em- beddings of the target terms. Note that this is sub- optimal in the sense that, unless specific measures are taken, a word embedding reflects ambiguous meaning; therefore, trying to associate a definition like "keyboard musical instrument with pipes" to the vector for word "organ" introduces a certain amount of noise in the model, since the definition will be partly associated with features related to Definition from the unseen dataset of the reverse dictionary task k = 3 (correct pred.) k = 1 (wrong pred.) the branch of engineering that deals with things smaller than 100 nm nanotechnology microelectronics floor consisting of open space at the top of a house just below roof loft balcony a board game for two players; pieces move according to dice throws backgammon checkers an address of a religious nature sermon rogation Example short phrase with ambiguous words k = 3 prediction k = 1 prediction a rechargeable cell nickel-cadmium battery karyolysis (biological process) a state capital Curitiba (Brazilian state capital) assert (verb) the lap of a person upper side of thighs lapper (garment) a band named Queen band leader neckband (garment)  the "body part" sense of the word. In our model, homonymy issues are resolved by design: each point in the target space corresponds to a well- defined unambiguous concept or synset. Further, the attentional mechanism of <ref type="figure" target="#fig_2">Fig. 4</ref> handles subtle variations of each distinct sense due to polysemy. The effectiveness of the textual feature mech- anism was demonstrated in every task we at- tempted, but to different extents. As our tuning on the dev sets showed, for tasks closer to text- to-entity mapping ( § §4.1-4.2) the more the textual features in the random walks, the better the results were. However, the best performance on the clas- sification task came by λ values between 0.50 and 0.75, i.e. by walks visiting more entity nodes than textual nodes. The reason is that entity classifi- cation is a task very sensitive to the topology of the KB graph, since entities belonging to a spe- cific class are very likely to be located at the same sub-hierarchy, hence in topological proximity. On the other hand, one of the motivations for intro- ducing textual features was exactly to broaden the context of a node by connecting distant parts of the graph (see <ref type="figure" target="#fig_1">Figures 2-3)</ref>. So, while small amounts of textual features can be still useful for classifica- tion purposes, excessive use introduces unwanted noise in the model.</p><p>The dynamic disambiguation mechanism inte- grated in the compositional architecture improved further the performance of the model. This finding is consistent with previous work on simpler tensor- based models, which showed that applying some form of word sense disambiguation when compos- ing word vectors can provide consistent improve- ments on end tasks such as sentence similarity and paraphrase detection ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and future work</head><p>We presented and evaluated a text-to-entity map- ping system based on a continuous KB space en- hanced with textual features and capable of han- dling polysemy. The reasonable next step will be to extend our methods for modelling the relations (edges) of a KB graph, which will allow appli- cations in tasks such as link prediction and KB completion. Furthermore, having a mechanism that translates arbitrary text to points in a contin- uous space creates many opportunities for inter- esting research. For example, while the size of a knowledge base is finite, the space itself consists of infinite number of points, each one of which corresponds to a valid-yet not explicitly stated in the KB-entity of the same domain. The ex- citing question of how can we exploit this extra information-for instance in order to enrich the knowledge base with new data-constitutes one of our future directions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The text-to-entity mapping system in a nutshell. The red nodes indicate textual features, while "MSE" stands for mean squared error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Linking of distant concepts with textual features (red boxes) for λ = 1. The textual feature understanding correctly links a related medical finding (lying on a different branch) to the condition known as alexia. Further, due to the presence of inability in their contexts, the vectors of alexia and insomnia (concepts originally quite far apart in the graph) will now have a common part reflecting that they are both conditions related to forms of incompetence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Detailed architecture of the Multi-Sense LSTM (MS-LSTM), shown here for the first word of a phrase. Red vectors refer to senses, while the green vector is a target vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualisation of the Cora classes based on a 2D t-SNE projection of the node vectors before the inclusion of textual features (left) and after (right).</figDesc><graphic url="image-267.png" coords="8,273.24,43.96,288.04,161.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results for the Cora dataset. TF vectors refers 
to textually enhanced KB vectors (λ = 0.5). Differ-
ence between our best models and GAT/GCN/TADW 
are not s.s. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Qualitative comparison of a few selected cases for multi-and single-sense LSTMs. 

Sense 1. formulation, uncommonly, rauwolfia, cardiol-
ogy, hypodermic, malleability, points, optic, dendrite, ru-
biaceae, nonparametric, meninges, deviation, anesthetics 
Sense 2. tableware, meal, expectation, heartily, kitchen, 
hum, eating, forestay, suitors, croupier, companionship, 
restaurant, dishes, candles, cup, tea 
Sense 3. reassigned, projective, ultracentrifuge, polemo-
niaceous, thyronine, assumptions, lymphocyte, atomic, 
difficulties, intracellular, virgil, elementary, cartesian 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Derived senses for word table, visualised as lists of nearest neighbouring words in the vector space.</figDesc><table></table></figure>

			<note place="foot" n="1"> https://www.snomed.org/snomed-ct</note>

			<note place="foot" n="2"> See also §4.4 for some concrete examples.</note>

			<note place="foot" n="3"> Python code will be released at https://github.com/ cambridgeltl/SIPHS. 4 Recall that this means half of the nodes in a random walk will be textual (see Fig. 2).</note>

			<note place="foot" n="5"> https://code.google.com/archive/p/word2vec 6 http://www.ncbi.nlm.nih.gov/pubmed/ 7 In our setting, this is equivalent to having λ = 0.</note>

			<note place="foot" n="8"> We also attempted a second classification experiment on a dataset of 200k concepts extracted from SNOMED, observing a similar behaviour of λ (details are not reported due to space). This difference in the behaviour of λ between text-toentity mapping and classification tasks is discussed in §5.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Victor Prokhorov and Ehsan Shareghi for useful discussions on the paper, as well as the anonymous reviewers for their suggestions. The idea of a dynamic disambigua-tion mechanism embedded in a neural architec-ture originally appeared in ( <ref type="bibr" target="#b3">Cheng and Kartsaklis, 2015)</ref> as part of a generic multi-prototype embed-ding model. Kartsaklis and Collier acknowledge support by EPSRC grant EP/M005089/1; Pile-hvar and Collier are supported by MRC grant No. MR/M025160/1. We are also grateful to NVIDIA Corporation for the donation of a Titan XP GPU.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distributed large-scale natural graph factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shravan</forename><surname>Narayanamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanja</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd International Conference on World Wide Web, WWW &apos;13</title>
		<meeting>the 22Nd International Conference on World Wide Web, WWW &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">From word to sense embeddings: A survey on vector representations of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Camacho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Collados</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taher</forename><surname>Pilehvar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2018" />
			<publisher>JAIR</publisher>
		</imprint>
	</monogr>
	<note>To appear in</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Syntaxaware multi-sense word embeddings for deep compositional models of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1531" to="1542" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Ferrara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02801</idno>
		<title level="m">Graph embedding techniques, applications, and performance: A survey</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16</title>
		<meeting>the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis: An overview with application to learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandor</forename><surname>David R Hardoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shawetaylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2639" to="2664" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to understand phrases by embedding the dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="17" to="30" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long Short-term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Prior disambiguation of word tensors for constructing sentence vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1590" to="1601" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A unified sentence space for categorical distributional-compositional semantics: Theory and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 24th International Conference on Computational Linguistics (COLING 2012): Posters</title>
		<meeting>24th International Conference on Computational Linguistics (COLING 2012): Posters<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
	<note>The COLING 2012 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Separating disambiguation from composition in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="114" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Random walk inference and learning in a large scale knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dnorm: Disease name normalization with pairwise learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rezarta</forename><surname>Islamaj Dogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2909" to="2917" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Do multi-sense embeddings improve natural language understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1722" to="1732" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adapting phrase-based machine translation to normalise medical terms in social media messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nut</forename><surname>Limsopatham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1675" to="1680" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Normalising medical concepts in social media texts by learning semantic representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nut</forename><surname>Limsopatham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association of Computational Linguistics Annual Meeting</title>
		<meeting>the Association of Computational Linguistics Annual Meeting<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1014" to="1023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The gene normalization task in BioCreative III</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Hsuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Ju</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunnan</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Tzong-Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Jie</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Cheol</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illes</forename><surname>Gerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Solt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifan</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dina</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Vishnyakova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Romacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanmitra</forename><surname>Rinaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padmini</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfang</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Matos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">M</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W. John</forename><surname>Livingston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilbur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A conditional random field for discriminatively-trained finite-state string edit distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kedar</forename><surname>Bellare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-First Conference on Uncertainty in Artificial Intelligence<address><addrLine>Arlington, Virginia, United States</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automating the construction of internet portals with machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristie</forename><surname>Seymore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="163" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Topic modeling with network regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th international conference on World Wide Web</title>
		<meeting>the 17th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop at the International Conference on Learning Representations</title>
		<meeting>the Workshop at the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Babelnet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page" from="217" to="250" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient nonparametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha,Quatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pharmacovigilance on Twitter? mining tweets for adverse drug reactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Karen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranoti</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azadeh</forename><surname>Pimpalkhute</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Nikfarjam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><forename type="middle">L</forename><surname>Ginn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graciela</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA annual symposium proceedings</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="page">924</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards a seamless integration of word senses into downstream nlp applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Taher Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1857" to="1869" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-prototype vector-space models of word meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">RELPRON: a relative clause evaluation dataset for compositional distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Polajnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic Word Sense Discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="97" to="123" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reasoning With Neural Tensor Networks For Knowledge Base Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Understanding natural language commands for robotic navigation and mobile manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashis</forename><forename type="middle">Gopal</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><forename type="middle">J</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Semantic composition and decomposition: From recognition to generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter D Turney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.7908</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličkovi´veličkovi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietrolì</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16</title>
		<meeting>the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Knowledge graph and text jointly embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1591" to="1601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Text-enhanced representation learning for knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI&apos;16</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI&apos;16</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1293" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Transg : A generative model for knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2316" to="2325" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI&apos;16</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence, AAAI&apos;16</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2659" to="2665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning distributed representations of texts and entities from knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="397" to="411" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Network representation learning with rich text information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2111" to="2117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>William W Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of Machine Learning (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
