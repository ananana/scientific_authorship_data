<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Online Representation Learning in Recurrent Neural Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
							<email>marek.rei@cl.cam.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">The ALTA Institute Computer Laboratory University of Cambridge United Kingdom</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Online Representation Learning in Recurrent Neural Language Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We investigate an extension of continuous online learning in recurrent neural network language models. The model keeps a separate vector representation of the current unit of text being processed and adaptively adjusts it after each prediction. The initial experiments give promising results, indicating that the method is able to increase language modelling accuracy, while also decreasing the parameters needed to store the model along with the computation required at each step.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, neural network models have shown impressive performance on many natural language processing tasks, such as speech recogni- tion ( <ref type="bibr" target="#b3">Chorowski et al., 2014;</ref><ref type="bibr" target="#b5">Graves et al., 2013)</ref>, machine translation ( <ref type="bibr" target="#b6">Kalchbrenner and Blunsom, 2013;</ref>), text classification ( <ref type="bibr">Le and Mikolov, 2014;</ref><ref type="bibr" target="#b7">Kalchbrenner et al., 2014</ref>) and image description generation ( <ref type="bibr" target="#b8">Kiros et al., 2014)</ref>. One of the main advantages of these methods is the ability to learn smooth vector representations for words, thereby reducing the sparsity problem inherent in any natural language dataset.</p><p>Language modelling is another task where neu- ral networks have delivered excellent results <ref type="bibr" target="#b0">(Bengio et al., 2003;</ref><ref type="bibr">Mikolov et al., 2011</ref>). <ref type="bibr" target="#b1">Chelba et al. (2014)</ref> have recently benchmarked several well-known language models by training on very large datasets. They found that a recurrent neu- ral network language model (RNNLM) combined with a 9-gram MaxEnt model was able to give the best results and lowest perplexity.</p><p>In this work we investigate a possible extension of RNNLM, by allowing it to continue learning and adapting during testing. The model keeps a vector representation of the current sentence that is being processed, and continuously modifies it based on an error signal. We refer to this as a ver- sion of online learning, as gradient descent is used to optimise the vector even during testing.</p><p>The technique is inspired by work on represen- tation learning <ref type="bibr" target="#b4">(Collobert and Weston, 2008;</ref><ref type="bibr">Mnih and Hinton, 2008;</ref><ref type="bibr">Mikolov et al., 2013)</ref>, espe- cially <ref type="bibr">Le and Mikolov (2014)</ref> who use a related model to learn representations for text classifica- tion. We extend the idea to recurrent models and apply it to the task of language modelling. Our results indicate that by exchanging some existing model parameters for a component using online learning, the system is able to achieve lower per- plexity while also reducing the necessary compu- tation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RNNLM</head><p>We base our implementation of the RNNLM on <ref type="bibr">Mikolov et al. (2011)</ref>, shown in <ref type="figure">Figure 1</ref>. The in- put layer to the network consists of a 1-hot vec- tor representing the previous word in the sequence, and the hidden vector from the previous time step. These are multiplied by corresponding weight ma- trices and the resulting vectors are passed through an activation function to calculate the hidden vec- <ref type="figure">Figure 1</ref>: Recurrent neural network language model (RNNLM) tor at the current time step. <ref type="bibr">1</ref> Class-based output architecture is used to avoid calculating the softmax over all words in the vo- cabulary. The probability distributions over words and classes are calculated by multiplying the hid- den vector with the corresponding weight matrix and applying the softmax function:</p><formula xml:id="formula_0">hidden t = σ(E · input t + W h · hidden t−1 ) classes = sof tmax(W c · hidden t ) output = sof tmax(W (c) o · hidden t )</formula><p>where σ is the logistic function and</p><formula xml:id="formula_1">W (c) o</formula><p>is the weight matrix between the hidden layer and the output words in class c.</p><p>Finally, we multiply the probability of the next word belonging to class c with the output proba- bility of the next word given the class to get the overall probability of the next word given the pre- vious words:</p><formula xml:id="formula_2">P (w t+1 |w t 1 ) ≈ classes c · output w t+1</formula><p>Negative log-probability is used as the loss function, which optimises the network to assign a high probability to the correct words. The net- work is trained using gradient descent and back- propagation through time. In the basic model, this means unrolling the recurrent network for a fixed number of time steps, essentially turning it into a deep feedforward network which outputs proba- bility distributions on different layers. Instead of using a fixed number of steps, our implementation unrolls each sentence from the last word to the first word, making it more suitable for processing indi- vidual sentences as opposed to longer texts.</p><p>In addition, we introduce a special vector to use as the hidden vector at the start of each sentence. The values in this vector are treated as parame- ters and optimised during training. This allows the network to learn a suitable starting point when no other information is available, giving slight perfor- mance improvements in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RNNLM with online learning</head><p>We extend the RNNLM by introducing an addi- tional document/context vector, shown as doc in <ref type="figure">Figure 2</ref>. This vector will represent the current <ref type="figure">Figure 2</ref>: RNNLM with an additional document vector for active learning document being processed, whether that is a sen- tence, paragraph or a larger text. When calculat- ing output probabilities over classes and words, we also condition them on this new document vector:</p><formula xml:id="formula_3">classes = sof tmax(W c · hidden t + W dc · doc) output = sof tmax(W (c) o · hidden t + W (c) do · doc)</formula><p>where W dc is the weight matrix between the docu- ment vector and class layer, and W (c) do is the weight matrix between the document vector and output words in class c.</p><p>We construct the document vector by treating the values as parameters and optimising them dur- ing both training and testing using backpropaga- tion. At each time step, the system first performs a forward pass through the network and outputs probability distributions over classes and words. We then use the next word in the sequence to cal- culate the error derivatives in the output and back- propagate them back into the document vector. The update is not able to to affect the output at the current time step, but it will modify the doc- ument vector which will be used in the next time step. The same word that is used for modifying the document vector for the next time step is also available in the input layer of the next time step, therefore the system receives no additional knowl- edge as input.</p><p>We are interested in modelling individual sen- tences, therefore at the beginning of each sentence the document vector is reset to a specific start- ing state, which is optimised during training and shared between all sentences. During testing, the values in the document vector are continuously modified depending on the error derivatives be- ing backpropagated from the output layer, while all other parameters in the model stay constant.</p><p>When dealing with larger texts and domain- specific corpora, similar ideas of iterative learning can be applied to any language model. After pro- cessing a certain amount of data during testing, a new model could be trained using the previously seen testing examples as additional training data. Since this process adds more training data which is likely to be similar to upcoming testing exam- ples, the system is likely to achieve a better per- formance.</p><p>However, when dealing with independent sen- tences, online learning becomes more difficult to apply. Each sentence contains very little addi- tional data, and even if the language model is adjusted after every individual word, it only ob- tains evidence of previous words in the sentence, whereas these words are relatively unlikely to oc- cur again in the same sentence. Therefore, instead of adjusting individual word representations, our approach learns a distributed document vector to represent the specific unit of text that is currently being processed. This vector is then used as addi- tional evidence when calculating output probabil- ities.</p><p>Le and Mikolov (2014) use a similar method for learning vector representations of documents and paragraphs. They construct a feedforward language model and include a paragraph vector as an additional vector in the input layer. The model parameters are trained on the training set, and when given unseen test data, the system opti- mises the paragraph vector according to the error signal. They use these vectors as input to a logis- tic regression classifier and achieve state-of-the-art performance on sentiment classification of movie reviews. However, they did not consider the effect of this model modification directly on the task of language modelling.</p><p>While the system of <ref type="bibr">Le and Mikolov (2014)</ref> uses a basic feedforward language model, we ex- tend the idea to recurrent neural network language models, as they are currently used in state-of- the-art language modelling systems ( <ref type="bibr" target="#b1">Chelba et al., 2014</ref>). Attaching the document vector to the input layer is not preferable for RNNLM, as the error is only backpropagated into the input layer after several time steps. When this time step is reached and the network is unrolled to perform backprop- agation through time, several words have already passed without receiving any additional informa- tion. Since our implementation performs the un- rolling only at the end of each sentence, the up- dates would not have any effect. Therefore, we attach the document vector directly to the output layer, in parallel with the recurrent hidden compo- nent. Parameters in the document vector can then be updated at each time step, while the unrolling and backpropagation through time still happens at the end of the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We constructed a dataset from English Wikipedia to evaluate language modelling performance over individual sentences. The text was tokenised, sen- tence split and lowercased. The sentences were shuffled, in order to minimise any transfer effects between consecutive sentences, and then split into training, development and test sets. The final sen- tences were sampled randomly, in order to obtain reasonable training times for the experiments. The dataset sizes are shown in  Model performance is measured using perplex- ity, therefore lower values indicate a model which is able to better predict the data. Special tokens are used to mark the beginning and end of a sen- tence. The sentence end token is also included in the evaluation, whereas the sentence start to- ken is only used as context in the input layer. Any words that occur less than 30 times in the training data were replaced by a special token for unknown words, leaving a vocabulary of 16,514 unique words. General learning rate was set to 0.1 and decreased during training, whereas the learn- ing rate of the document vector was fixed at 0.1 for both training and testing.</p><p>As the baseline, we use the regular RNNLM with 100-dimensional hidden layers and word vec- tors (M = 100). In the experiments we increase the capacity of the model and measure how that affects the perplexity on the datasets. First, we increase the value of M, allowing more informa- tion to be stored into word representations, while also increasing the number of hidden-hidden and hidden-output connections. As can be seen in Ta- ble 1, this improves the overall performance of the Next, instead of increasing M , we add a D- dimensional document vector to the model and use this for online learning. When the same num- ber of elements is added to M or D, our results show consistently better performance when using the document vector. Increasing M by 35 gives perplexity 95.71, whereas using a 35-dimensional document vector gives perplexity 90.29. We also performed the same experiment using only half of the training data, and the difference was even larger -105.50 and 98.23 correspondingly.</p><p>One reason why online learning during model deployment is not commonly used is because it is computationally expensive. Continuously re- training the model and adjusting parameters can be very time-consuming compared to a simple feed- forward process through the network. However, extra computation is also needed when using a hidden vector of size M , as opposed to using a smaller value. When increasing the value of M to M + X, the RNNLM will contain</p><formula xml:id="formula_4">X · C + 2 · X · V + 2 · X · M + X 2</formula><p>additional parameters and needs to perform</p><formula xml:id="formula_5">2 · X · M + X 2 + X · C + X · E[O]</formula><p>additional operations at each time step. 2 C is the number of classes, V is vocabulary size, and E <ref type="bibr">[O]</ref> is the expected number of words that need to be processed in the output layer during one step.</p><p>The corresponding number of additional param- eters in a RNNLM model using a D-dimensional document vector for online learning is  <ref type="table">Table 1</ref> contains the ad- ditional values for the experiments, showing that replacing some hidden vector parameters with the actively learned document vector leads to fewer total parameters and fewer operations, along with lower perplexity. <ref type="figure" target="#fig_1">Figure 3</ref> presents the relationship between per- plexity and the number of additional parameters, when increasing either M or D. The results are averaged over 10 runs with different random ini- tialisations. As can be seen, using a small docu- ment vector lowers the perplexity with fewer pa- rameters, compared to simply increasing the main components of the network. The graph of per- plexity with respect to additional operations in the model also has a very similar shape. Both Hufnagel and Marston also joined the long-standing technical death metal band Gorguts.</p><formula xml:id="formula_6">D + D · V + D · C</formula><p>1. The band eventually went on to become the post-hardcore band Adair. 2. The band members originally came from different death metal bands, bonding over a common interest in d-beat. 3. The proceeds went towards a home studio, which enabled him to concentrate on his solo output and songs that were to become his debut mini-album "Feeding The Wolves".</p><p>The Chiefs reclaimed the title on September 29, 2014 in a Monday Night Football game against the New England Patriots, hitting 142.2 decibels.</p><p>1. He played in twenty-four regular season games for the Colts, all off the bench. 2. In May 2009 the Warriors announced they had re-signed him until the end of the 2011 season.</p><p>3. The team played inconsistently throughout the campaign from the outset, losing the opening two matches before winning four consecutive games during September 1927.</p><p>He was educated at Llandovery College and Jesus College, Oxford, where he obtained an M.A. degree.</p><p>1. He studied at the Orthodox High School, then at the Faculty of Mathematics.</p><p>2. Kaigama studied for the priesthood at St. Augustine's Seminary in Jos with further study in theology in Rome. 3. Under his stewardship, Zahira College became one of the leading schools in the country. <ref type="table">Table 3</ref>: Examples of using the document vectors to find similar sentences in the development data.</p><p>In order to further explore the relationship be- tween D and M , we trained a number of smaller models with different values, under the constraint D + M = 100. To reduce computation time, only half of the training data was used in these experi- ments. The lowest perplexity was achieved in the region of D = 23 and M = 77, and making the document vectors much smaller or larger led to a decrease in performance. This indicates that including the document vector does help increase model accuracy, but as it contains no information about the training data, this vector should be small compared to the main model.</p><p>Intuitively, this approach works by having the document vector capture the unique aspects of each sentence. While the general RNNLM is a smooth static representation of the entire training data, the document vector is optimised to repre- sent how each sentence differs from the main lan- guage model. Therefore we performed a quali- tative evaluation and found that the learned sen- tence vectors were also very good predictors of semantic similarity. The RNN language model was trained on the training set, and then used to process the development set. The last state of the document vector of each sentence was used to calculate cosine similarity. <ref type="table">Table 3</ref> contains ran- domly sampled sentences from the development set, together with corresponding development sen- tences that have the highest similarity (excluding the original sentence). Even though there is al- most no word overlap, the retrieved sentences are semantically very similar.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2</head><label></label><figDesc>We only count the matrix multiplication operations, as they take the majority of the time in a neural network lan- guage model. and additional operations 2 · D · E[O] + 2 · D · C which includes the error backpropagation at each time step. For our experiments V = 16, 514, C = 100 and E[O] ≈ 50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Perplexity as a function of additional parameters when increasing either M or D. The x-axis shows the number of additional parameters in the model, with respect to the baseline of M = 100, D = 0. The y-axis shows the perplexity on the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Train 
Dev 
Test 

Words 
9,990,782 237,037 4,208,847 
Sentences 
419,278 
10,000 
176,564 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Dataset sizes</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Train PPL Dev PPL Test PPL +Parameters +Operations</head><label>Train</label><figDesc></figDesc><table>Baseline M=100 
92.65 
103.56 
102.51 
-
-

M=120 
88.60 
98.78 
97.79 
666,960 
7,400 
M=100, D=20 
87.28 
95.36 
94.39 
332,300 
6,000 

M=135 
85.17 
96.33 
95.71 
1,167,705 
13,475 
M=100, D=35 
80.11 
91.05 
90.29 
581,525 
10,500 

Table 1: Perplexity and additional parameters/operations for different language model configurations 

model -setting M to 120 and 135 leads to pro-
gressively lower perplexity. 
</table></figure>

			<note place="foot" n="1"> Explicit multiplication for the word vectors can be avoided by using data structures that retrieve the correct vector in constant time.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have described a possible extension of RNNLM which uses continuous online learning. The model includes a separate vector to represent the unit of text, such as a sentence, being cur-rently processed. The vector starts in a default state and is continuously updated using backprop-agation, leading to a more informative representa-tion. The modified language model achieves lower perplexity with a more optimal use of parameters.</p><p>The idea of continuous training and adaptation is natural and also established in biological learn-ing processes, yet it is not widely used due to com-putational complexity. Our experiments indicate that by including this active learning component in the neural network model, the system is able to achieve higher accuracy, while also decreasing the parameters needed to store the model and decreas-ing the computation required.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
		<title level="m">A Neural Probabilistic Language Model</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Thorsten Brants, Phillipp Koehn, and Tony Robinson</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
	<note>Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">End-to-end Continuous Speech Recognition using Attention-based Recurrent NN : First Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Navdeep Jaitly, and Abdel-rahman Mohamed</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ASRU 2013</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recurrent Continuous Translation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 52nd Annual Meeting of the Association for Computational Linguistics. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multimodal Neural Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
