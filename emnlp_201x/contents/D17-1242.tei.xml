<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Demographic-Aware Word Associations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aparna</forename><surname>Garimella</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan Ann Arbor</orgName>
								<address>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan Ann Arbor</orgName>
								<address>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan Ann Arbor</orgName>
								<address>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Demographic-Aware Word Associations</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2285" to="2295"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Variations of word associations across different groups of people can provide insights into people&apos;s psychologies and their world views. To capture these variations, we introduce the task of demographic-aware word associations. We build a new gold standard dataset consisting of word association responses for approximately 300 stimulus words, collected from more than 800 respondents of different gender (male/female) and from different locations (India/United States), and show that there are significant variations in the word associations made by these groups. We also introduce a new demographic-aware word association model based on a neu-ral net skip-gram architecture, and show how computational methods for measuring word associations that specifically account for writer demographics can outper-form generic methods that are agnostic to such information.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Understanding the associations that are formed in the mind is paramount to understanding the way humans acquire language throughout a lifetime of learning ( <ref type="bibr" target="#b8">Elman et al., 1997;</ref><ref type="bibr" target="#b28">Rogers and McClelland, 2004</ref>). Furthermore, word associations are believed to mirror the mental model of the concep- tual connections in a human mind, and constitute a direct path to assessing one's semantic knowledge ( <ref type="bibr" target="#b27">Nelson et al., 2004;</ref><ref type="bibr" target="#b26">Mollin, 2009)</ref>.</p><p>Word associations start forming early in life, as language is acquired and one learns based on the environment where concepts lie in relation to each other. For example, we may learn to associate "mother" with "warmth," or "fire" with "burn." Yet, this mental model is not static but highly dy- namic, and is shaped by new experiences over a lifetime. For instance, <ref type="bibr" target="#b31">(Tresselt and Mayzner, 1964)</ref> showed that word associations change with time, and that for respondents in younger age groups their variability is lower, while for those in older age groups the variability is higher, as their life experiences modify the commonality between respondents from the same group.</p><p>Computational linguistics has traditionally taken the "one-size-fits-all" approach, with most models being agnostic to the language of the speakers behind the language. With the introduc- tion and adoption of Web 2.0, there has been an exponential increase in the availability of digital user-centric data in the form of blogs, microblogs and other forms of online participation. Such data often times can be augmented with demographic or other user-focused attributes, whether these are user-provided (e.g., from a user's online profile) or labeled using an automatic system. This enables computational linguists to go beyond generic corpus-based metrics of word associations, and attempt to extract associations that pertain to given demographic groups that would not have been possible without administering time consuming and resource intensive word association surveys.</p><p>While current NLP methods generally deal with more advanced tasks (relation extraction, text sim- ilarity, etc.), at their very core many of these tasks assume some way of drawing connections (or associations) between words. Therefore, as a step toward demographic-aware NLP, we choose to work on the core task of "word association." The algorithms we introduce can be immediately applied to demographic-aware word similarity, and with some minor changes to demographic- aware text similarity. Future stages could also include demographic-aware labeled associations, and more advanced applications such as informa- tion retrieval (which relies heavily on word asso- ciations/similarity), demographic-aware keyword extraction, dialogue personalization, and so forth.</p><p>Note that a few other researchers have explored demographic-aware NLP models with promising results, primarily focusing on the use of demo- graphics for various forms of text classification <ref type="bibr" target="#b12">(Hovy, 2015)</ref> or sentiment and subjectivity clas- sification ( <ref type="bibr" target="#b32">Volkova et al., 2013</ref>).</p><p>The paper makes several main contribu- tions.</p><p>First, we create a novel dataset of demographic-aware word associations, consist- ing of approximately 300 stimulus words along with 800 responses per word collected from a demographically-diverse group of respondents, for a total of 228,800 responses. Removing spam responses resulted in 176,097 responses. Analyses that we perform on this dataset demonstrate that indeed word associations vary across user dimen- sions. <ref type="bibr">1</ref> Second, we show that the associations we obtained follow the same pattern as those elicited during traditional classroom surveys. Third, we propose an evaluation metric suited for the free association norms task. Fourth, we introduce a demographic-aware model based on a skip-gram architecture and through several comparative ex- periments, we show that we are able to surpass the performance attainable on demographic agnostic models.</p><p>We specifically focus on two demographic di- mensions: location and gender. For location, we consider India and United States (US), choice made primarily because these two countries have a large English-speaking population, represented both on social media and on crowdsourcing plat- forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Word associations have captured the attention of psychologists since at least the early <ref type="bibr">1900</ref><ref type="bibr" target="#b18">. In (1910</ref>, Kent and Rosanoff proposed the use of a set of 100 emotionally neutral words for word associations surveys. A psycholinguistics study that looked at the impact that the nationality of re- spondents may have on formed word associations was carried out by <ref type="bibr" target="#b29">Rosenzweig (1961)</ref>, employ- ing the stimulus word list proposed by <ref type="bibr" target="#b18">Kent and Rosanoff (1910)</ref> manually translated into several West European languages. Based on the primary responses coming from native speakers of English, French, German and Italian, which were mapped 1 This work is not centered around comparing different word forms, as one would encounter for example in British English and American English, but rather around different word associations that people with a particular demographic characteristic are inclined to make, e.g., "health" in India is more strongly associated with "wealth", while in the United States it is more strongly associated with "sick." back into English, the author concludes that the associations formed by speakers of the four lan- guages are very similar, with "almost half the com- parisons in any pair of languages yielding agree- ments," where the most frequent responses are en- countered across pairs of languages. Given that the primary responses were compared across lan- guages and people with a relatively common ori- gin (West European), our work seeks to investi- gate whether similar results are encountered when looking at different locations (namely US versus India). Furthermore, our study is conducted in English from the beginning, to eliminate a third party's subjectivity in mapping primary responses from one language to another.</p><p>There have also been attempts in computational linguistics to derive associations not based on sur- vey results (which are static and resource inten- sive), but based on statistics derived from large corpora ( <ref type="bibr" target="#b3">Church et al., 1989;</ref><ref type="bibr" target="#b33">Wettler and Rapp, 1989;</ref><ref type="bibr" target="#b4">Church and Hanks, 1990)</ref>. Research in se- mantic similarity can also be used to model as- sociations based on several directions: (1) co- occurrence metrics that rely on large corpora such as PMI <ref type="bibr" target="#b4">(Church and Hanks, 1990)</ref>, second order PMI ( <ref type="bibr" target="#b14">Islam and Inkpen, 2008)</ref>, or Dice <ref type="bibr" target="#b7">(Dice, 1945)</ref>; (2) distributional similarity-based mea- sures, that characterize a word by its surround- ing context such as LSA <ref type="bibr" target="#b19">(Landauer and Dumais, 1997)</ref>, ESA ( <ref type="bibr" target="#b9">Gabrilovich and Markovitch, 2007)</ref>, or SSA (Hassan and Mihalcea, 2011); and (3) knowledge-based metrics that rely on resources such as lexica or thesauri ( <ref type="bibr" target="#b20">Leacock and Chodorow, 1998;</ref><ref type="bibr" target="#b21">Lesk, 1986;</ref><ref type="bibr" target="#b16">Jarmasz and Szpakowics, 2003;</ref><ref type="bibr" target="#b13">Hughes and Ramag, 2007)</ref>. However, most of these metrics have so far been applied to model the relatedness between two words, namely given a word pair, to score how similar the two words are; as such, they have not been used to predict free association norms, namely given a word, to attempt to determine the most likely word that a human would associate with that stimulus.</p><p>Large word association databases exist, such as the one collected by <ref type="bibr" target="#b5">Deyne et al. (2013)</ref>, who used a set of 12,000 stimulus words and surveyed 70,000 participants. Yet to our knowledge, no con- certed attempt has been made to gather word asso- ciations jointly with the demographic characteris- tics of the people behind them.</p><p>While not directly seeking to extract word as- sociations but rather trying to represent language meaning through a locality lens, ( <ref type="bibr" target="#b0">Bamman et al., 2014</ref>) have proposed using distributed representa- tions to model words employed by social media users from different US states. They were able to show that the regional meaning of words can suc- cessfully be carried by word embeddings, for ex- ample the word "wicked" was most similar to the word "evil" in Kansas, while in Massachusetts, it was most similar to "super" (based on the cosine similarity of the words' vectorial representation). In contrast, our rationale in this article is to explore if word associations can be automatically derived from large corpora annotated with user-centered attributes such as location or gender.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Word Associations Dataset</head><p>Word association data collection typically consists of providing participants with a list of words, also known in the psycholinguistics literature as stim- ulus words, and asking them to provide the first word that comes to mind in response to each stim- ulus. For instance, given a stimulus word such as cat, one would expect answers such as dog or mouse. Earlier work on word associations admin- istered the tests in classroom settings, with 100 words per survey, and the results were compiled into tables of norms of word associations <ref type="bibr" target="#b18">(Kent and Rosanoff, 1910;</ref><ref type="bibr" target="#b27">Nelson et al., 2004</ref>).</p><p>Since our goal is to explore the effect of demo- graphics on word associations, we created a task on Amazon Mechanical Turk (AMT) able to reach a wide and demographically diverse audience. The survey was structured into two sections: the word association part, followed by a demographic sur- vey. Given the online nature of the survey, and since we aimed for a high quality dataset, each participant was presented with a set of 50 stimu- lus words at a time (instead of 100). The demo- graphic section consisted of seven questions cov- ering gender, age, location, occupation, ethnicity, education, and income.</p><p>Stimuli. The stimulus list consists of a set of ap- proximately 300 words. Among these, 99 words are sourced from the word list proposed by <ref type="bibr" target="#b18">Kent and Rosanoff (1910)</ref> (standard list). <ref type="bibr">2</ref> The remain- ing words are identified using the method for find- ing word-usage differences between two groups introduced in ( <ref type="bibr" target="#b10">Garimella et al., 2016)</ref>, which re- lies on large collections of texts authored by the two groups to identify words that can be accu- rately classified by an automatic classifier as be- longing to one group versus another. Using their method, we obtain 100 words as the top most dif-ferent words between US and India (culture list), and another set of 100 words as the top most differ- ent words between male and female (gender list). The reunion of these three lists results in 286 stim- ulus words for which we collect word associations. Examples are shown in <ref type="table">Table 1</ref>.</p><p>Responses. The task was published separately for respondents from US and India, as AMT has an option of only presenting the survey to people from a preselected geographical location. Six dif- ferent surveys, each including approximately 50 stimulus words, were administered for each re- gion. The survey was conducted in English for both countries, noting that one of the official lan- guages of India is English (alongside Hindi). Each survey also included four spam-checking ques- tions with previously known answers (e.g., What is the color of the sky?, with five options blue, red, pink, green, yellow), which were used to filter out respondents who were filling out the survey with- out reading the questions.</p><p>For each set, we gathered 400 responses per re- gion, resulting in 800 responses for both US and India. After removing the respondents who did not pass the spam-checking questions, we were left with an average of 752 responses per word, which we then balanced by gender, to retain an equal number of Indian women, Indian men, US women, and US men. This resulted in 492 and 480 responses for the two sets of 50 standard stimulus words, 436 and 468 for the culture words, and 440 and 432 for the gender words. Similar to <ref type="bibr" target="#b29">(Rosenzweig, 1961</ref>), all the responses were normalized (i.e. plural was mapped to singular, gerund to in- finitive, etc.); in our case we used the Stanford CoreNLP Lemmatizer ( <ref type="bibr" target="#b22">Manning et al., 2014</ref>), ulti- mately aggregating the responses into a gold stan- dard. <ref type="table">Table 1</ref> shows the top associations for a few sample stimuli, as collected from India and US, and males and females. Finer-grained qualita- tive analyses also reveal interesting distinctions. For instance bath is overwhelmingly associated by men with water, while US women associate it with bubble, and Indian women with soap. Interest- ingly, US men seem to provide responses based on collocations, e.g., they answer Kane for citizen (citizen Kane), weight for heavy (heavyweight), or lion for mountain (mountain lion); on the contrary, women more often provide responses that consist of synonym or antonym words, e.g., person for cit- izen, health for sick, or light for heavy.</p><p>For further insight, <ref type="table">Table 2 shows the average Gender   Location  Word  Male  Female  India  US  beautiful girl, woman, pretty  pretty, girl, ugly  girl, nature, flower  pretty, girl, ugly  cheese  pizza, bread, milk  butter, mouse, pizza  pizza, butter, bread  cracker, swiss, cheddar  hard  soft, rock, work  soft, work, rock  work, stone, rock  soft, rock, time  health</ref> good, wealth, care good, wealth, sick wealth, good, fitness good, sick, care range distance, gun, shooting gun, rover, mountain price, rover, wide gun, distance, rover admit hospital, guilt, card hospital, confess, one hospital, card, accept guilt, one, confess mix tape, match, juice cake, tape, stir juice, tape, match stir, tape, cake organize clean, arrange, party clean, arrange, meeting arrange, meeting, party clean, sort, neat stack pile, book, box book, pile, hay book, queue, pile pile, book, pancake <ref type="table">Table 1</ref>: Top three most frequent responses for sample stimulus words.</p><p>number of different responses obtained for a given stimulus word, with the lowest variability word, and the highest variability word. <ref type="bibr">3</ref> The second col- umn lists the correlations between the frequency of the primary response and the number of dif- ferent responses, as also reported by <ref type="bibr" target="#b17">(Jenkins and Palermo, 1965)</ref>. This correlation is negative, as the more people agree on the primary response, the fewer overall unique answers for a stimulus word are provided. Additionally, <ref type="figure" target="#fig_0">Figure 1</ref> shows the Zipfian distribution of average norm frequency; the most frequent response is given on average by 24% of the respondents, while the third most fre- quent response is given by 7% of them.  <ref type="table">Table 2</ref>: Average number of responses obtained for a given stimulus word, correlation between fre- quency of primary response and number of dif- ferent responses, words exhibiting the lowest vari- ability, and words with the highest variability.</p><p>Analyses of Demographic Variations. To model norm strength within a given demographic group or across groups, we tabulate how often respondents from a group match the most frequent answer (Primary) or one of the most frequent ten answers for that group (Top10). That is, given the response for one stimulus word as provided by one held-out survey respondent at a time, we determine whether that response matches the most frequent association of the remaining members of the same group <ref type="table" target="#tab_2">(Table 3</ref>, Primary columns), or one of the top 10 associations pertaining to that same group <ref type="table" target="#tab_2">(Table 3</ref>, Top10 columns). Similarly, we measure the match with the most frequent or the top 10 responses from the other group, as shown in <ref type="table" target="#tab_3">Table 4</ref>. As expected, the intra-group similarities are significantly higher than the inter-group similarities, which supports our hypothesis that different groups make dif- ferent word associations, which tend to be more coherent within a group than across groups. While males and females have similar ranges for their agreement figures, we notice that on average US respondents have stronger intra-group agreements. Note also that inter-group similarities are asymmetrical, as multiple words may have the same association frequency for one group, yet for the complementary group that may not be the case.</p><p>As an additional analysis of demographic vari- ations in the responses received, for each respon- dent, we predict his / her demographic group using a majority vote conducted across all the user's re- sponses using a simple rule-based system that as- signs each response to the group having the high- est frequency for that particular association. For instance, given the response sun obtained from a respondent for the stimulus yellow, we assign the respondent to either India or US depending on the highest normalized frequency of the re- sponse sun for the same stimulus in each of those groups. A similar rule-based assignment is also used for gender. Thus, we compute the response words and their normalized frequencies based on the responses from 80% of the users chosen ran- domly, and accordingly predict the demographic group for the remaining 20% of the users based on a decision across the entire set of a user's re- sponses. <ref type="table" target="#tab_4">Table 5</ref> shows the results of these predic- tions, which indicate high location variability (i.e., we can predict with high accuracy the location of a respondent), and medium gender variability.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Computational Models of Word Associations</head><p>We first introduce a new model for measuring word associations that leverages a shallow neu- ral net architecture to embed demographically- enriched words. We then compare the perfor- mance of the predicted associations to those re- sulting from other approaches, including tradi- tional corpus-based measures such as mutual in- formation or vector-space models, as well as a recent distributed learning model with word em- beddings. For each of these methods, we predict, evaluate, and compare generic associations (de- void of any demographic information), as well as demographic-aware associations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Composite Skip-gram Models</head><p>We introduce a new word association model, which relies on the skip-gram neural net architec- ture ( <ref type="bibr" target="#b24">Mikolov et al., 2013)</ref>, and leverages its effi- ciency and ability to deal with less frequent words.</p><p>The skip-gram model tries to predict the context given a word, that is, for each word w i in the input sequence w 1 , . . . , w T , the model tries to predict w i−2 , w i−1 , w i+1 and w i+2 , assuming, for exam- ple, a sliding window of five words. Mathemati- cally, the model maximizes the objective function</p><formula xml:id="formula_0">J = 1 T T i=1 c j=−c,j =0 log P (w i+j |w i )<label>(1)</label></formula><p>where T is the number of tokens in the data set, c is the number of context words on each side of the target word w i and P (w i+j |w i ) is the probability to observe word w i+j in the context of word w i . To make this model demographic-aware, we propose two variations, which we refer to as com- posite skip-gram models (C − SGM ). In the first one (EM B1), the target word w i is tagged with a demographic label L (e.g., gender). For exam- ple, for the target word "formula L=female " we try to predict a high probability for "baby" and "milk" occurring in the neighboring context. The under- lying reasoning is that tagged words that appear in similar contexts will be nudged toward each other, while those that do not, will further distance them- selves. This allows discrepancies to emerge be- tween how the words are embedded given a de- mographic dimension.</p><p>In the second variation (EM B2), we also in- clude the demographic label in the context. That is, for each skip-gram (c i,left , w i , c i,right ) we gener- ate three skip-grams</p><formula xml:id="formula_1">(c label i,left , w i , c i,right ) (c i,left , w label i , c i,right ) (c i,left , w i , c label i,right )<label>(2)</label></formula><p>The two models seek to capture different sce- narios. In the first model, where we only add the demographic label to the target word, the em- bedding of the labeled word is optimized with re- spect to the generic embedding of the context. In the second model, the optimization is rather symmetric, allowing tagged and generic embed- dings to influence each other. Thus, the optimiza- tion function seeks to predict both tagged and un- tagged words in the vicinity given a target word, instead of only focusing on predicting untagged words like EMB1. The embeddings resulting from such a model should allow for more accurate rep- resentations across the tagged and untagged vo- cabulary, where for example the word "mother" uttered by a female would be close to the word "mother" (regardless of author gender). In both scenarios, the embeddings space accommodates both tagged and untagged words at the same time, being very computationally robust, and allowing comparisons across the tagged version of words, as well as between generic words and their tagged surrogates. For both variations, we compute the cosine similarity between the stimulus word and each of the vocabulary words (whether generic or demographic-enhanced), and retain the clos- est unique candidates (after dropping their demo- graphic tag).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Other Word Association Models</head><p>Mutual Information (MI). We implement the information theoretic measure proposed by <ref type="bibr">Church and Hill (1990)</ref>. It is defined as follows:</p><p>I(x, y) = log 2 P (x, y) P (x)P (y)</p><p>This measure compares the probability of observ- ing words x and y together (the joint probability) with the probabilities of observing x and y inde- pendently. The joint probability, P (x, y), is gen- erally estimated by counting the number of times x is followed by y in a window of w words, and normalizing this count with the size of the corpus. We follow Church and Hill and set the window size w to five, as it is large enough to capture verb- argument constraints, and not so large to restrict to strict adjacency. For a given stimulus word, (1) we use the entire corpus and compute the generic MI word association with the rest of the vocabulary, and get the top associations according to their MI scores; and (2) we use the section of the corpus obtained for a given demographic, and determine the top demographic-aware MI word associations.</p><p>Vector-Space Model (VSM). We also imple- ment the traditional vector-space model, where each word is represented by a tf.idf weighted vec- tor inside the term-document matrix (represent- ing term occurrences inside the documents in the corpus), with a length equal to the number of documents D in the corpus ( <ref type="bibr" target="#b30">Salton and McGill, 1986)</ref>. For a given stimulus word, cosine simi- larities are computed with all the remaining word vectors in the vocabulary, and those words hav- ing the highest similarity are considered as the top responses. Similar to MI, we use all the docu- ments in the corpus to produce generic word as- sociations, while only those documents pertaining to a specific demographic value are utilized to de- rive demographic-aware associations.</p><p>Skip-gram Language Model. We also use the distributional representation technique of word embeddings (SGLM ) proposed by <ref type="bibr" target="#b0">Bamman et al. (2014)</ref>. Specifically, information about the speaker (geography, in their case) is used while learning the vector-space representations of word meanings from textual data that is supplemented with metadata about the authors. In addition to the global embedding matrix W main that contains low-dimensional representations for every word in the vocabulary ( <ref type="bibr" target="#b24">Mikolov et al., 2013)</ref>, this ap- proach has an additional |C| matrices {W c } of the same size as W main , where |C| denotes the num- ber of values the demographic variable has in the data (e.g., if gender is the demographic variable, C = {f emale, male} and |C| = 2). Each of these |C| matrices captures the effect that each de- mographic variable value has on each word in the vocabulary. To index the embedding of a stimulus word w ∈ R |V |×k , the hidden layer h is computed as the sum of the matrix multiplications with each of the independent embeddings:</p><formula xml:id="formula_3">h = w T W main + Σ c∈C w T W c<label>(4)</label></formula><p>It then predicts the value of the context word y us- ing another parameter matrix X ∈ R |V |×k based on a softmax function o = sof tmax(Xh), where o ∈ R |V |×k . Backpropagation using (input x, out- put y) word tuples learns the values of the various embedding matrices W and parameter matrix X, which maximize the likelihood of context words y conditioned on the stimulus word x.</p><p>We use this approach in its original implemen- tation provided by <ref type="bibr" target="#b0">(Bamman et al., 2014</ref>) to com- pute the word embedding vectors for all the words in the vocabulary. Given a stimulus word, the clos- est vocabulary words with the highest cosine sim- ilarity are retained as the top association predic- tions for the given stimulus word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>All our models require textual data with demo- graphic information. We introduce below the data we used and the metrics we adopted for evaluation.</p><p>Data. Given the requirement of having gender and location information associated with the data, we resort to blogs, and collect from Google Blog- ger 4 a large set of blog posts authored between 1999 and 2016. <ref type="table" target="#tab_6">Table 6</ref> shows the breakdown of the raw blog counts per demographic category. From these, we retain only those posts with non- empty content, and preprocess the data by remov- ing HTML tags, converting all the tokens to their lemmatized forms, <ref type="bibr">5</ref> and discarding those lemmas with a frequency less than 10, in order to avoid misspellings and other noise characteristic to so- cial media content.  From the above pool of blog posts, we cre- ate two datasets with complementary demographic classes (1) location: India-US and (2) gender: male-female.</p><p>We process each of these datasets so that they are profile-balanced with no peaks for any specific years, by applying several heuristics: (1) Com- pute the minimum number of users n over all the classes (e.g., Indian and US authors in the case of the location dataset). (2) From each class, se- lect the top n users based on the number of years they were blogging and the number of posts they wrote. <ref type="bibr">6</ref> This ensures that the maximum amount of data will be available for the selected users. (3) For each of these n users, pick at most 50 posts in a round-robin fashion from the years in which they blogged. (4) Let M be the total number of posts collected in this manner from all the classes. In order to avoid having most of the posts com- ing from a small number of years, set a cutoff X as a fraction of M . For each year, a maximum of X posts will be chosen from the set of M posts (X = 0.1M ). (5) To ensure that all the users get to contribute posts, and that the contribution of prolific writers is kept in check, maintain user participation scores: p(user) = posts collected from user total number of posts collected (5)</p><p>These scores are updated after every year is pro- cessed, as explained further. (6) Sort the years in increasing order of number of posts and iterate through them; identify the lowest number of posts contributed by the least prolific writer, then col- lect the minimum number of posts from all users who published in that year in a round-robin man- ner. Then, select additional posts from users in increasing order of participation scores, until the number of posts for the year reaches the cutoff X. <ref type="formula">(7)</ref> After each year, update the user partici- pation scores. <ref type="table" target="#tab_6">Table 6</ref> shows the number of users and posts retained after balancing. This particular composition is used in our location data set (con- sisting of India and US posts) and gender data set (consisting of females and males posts).</p><p>Metrics. Given that the word association task is relatively similar to the lexical substitution task, in terms of open vocabulary and lack of a "right" answer, we decided to borrow the best and out- of-ten (oo10) evaluation metrics traditionally used for the latter <ref type="bibr" target="#b23">(McCarthy and Navigli, 2009</ref>), yet corrected for weight ( <ref type="bibr" target="#b15">Jabbari et al., 2010)</ref>. Briefly, these measures take the best (or top ten) responses from a system, and compare them against the gold standard, while accounting for the frequencies of the responses in the gold standard. In addition, since <ref type="figure" target="#fig_0">Figure 1</ref> shows that the top three ranking norms are provided as answers by approximately 42% of the respondents, with the remaining norms following a long Zipfian distribution in terms of frequency of appearance, we also compute out- of-three (oo3), which represents a more focused approximation of our ability to predict human as- sociations (note that out-of-ten covers 62% of the responses). Several recent papers on word as- sociations evaluated their models indirectly via Pearson or Spearman correlation performance on a word similarity task <ref type="bibr" target="#b2">(Chaudhari et al., 2011;</ref><ref type="bibr" target="#b6">Deyne et al., 2016)</ref>; we choose instead to evaluate word associations directly, by using metrics that more closely align with the evaluations performed in the field of psychology where the best output of a system is compared against the most frequent human response <ref type="bibr" target="#b1">(Bel-Enguix, 2014;</ref><ref type="bibr" target="#b25">Mohammad, 2011)</ref>. For a given stimulus word w with human re- sponses H w , suppose a system returns a set of answers S w . We estimate how well this system can find a best substitute for w using Equation 6, where the function f req w (s) returns the count of a system response s in H w , and maxf req w returns the maximum count of any response in H w .</p><formula xml:id="formula_4">best(w) = Σ s∈Sw f req w (s) maxf req w × |S w | (6) oon(w) = Σ s∈S n w f req w (s) |H w | (7)</formula><p>Equation 7 measures the coverage of a system by allowing it to offer a set S n w of n responses for w, where each response s is weighted by its fre- quency f req w (s) in H w .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluations and Discussions</head><p>We conduct evaluations using all the word associ- ation models described in Section 4. The results using the best, out-of-three, and out-of-ten evalua- tion metrics are listed in <ref type="table" target="#tab_7">Table 7</ref>. For all the em- beddings experiments, we use 300 latent dimen- sions. The Gen variation uses the demographic- blind dataset, whereas the DA variation uses the demographic-aware dataset. <ref type="bibr">7</ref> The MI and VSM models do not perform well in the word association prediction task, whether considering the generic or the demographic-aware data. We should emphasize, however, that the generic version of these models is able to consider co-occurrences across the entire generic datasets, while the demographic-aware co-occurrences can only be computed from the section of the dataset that matches a particular demographic; as such, these latter models are placed at a disadvantage.</p><p>Perhaps not surprisingly, the neural network skip-gram-based architectures, whether SGLM or our C-SGM, always achieve better results when compared to MI or VSM. The demographic-aware variation proposed by <ref type="bibr" target="#b0">(Bamman et al., 2014</ref>) uses an extended skip-gram architecture that encodes a generic embedding, and several demographic- based filters per class, which in our case trans- lates into three matrices of 300 dimensions each, the first for the generic words, and the subsequent ones for skews to be applied to the generic words in order to render the embedding through the lens of a given demographic. SGLM − Gen in our case are the predictions based on the generic ma- trix, while SGLM − DA are the predictions mod- ified along the lines of a particular demographic.</p><p>Our composite skip-gram models encode a sin- gle matrix that contains a mix of demographic- aware and generic words expressed as 300 latent dimensions. For both gender and location, our gender-aware models (EM B1 and EM B2) sur- pass the SGLM gender-aware model. Surpris- ingly, while SGLM was never meant to be generic, the predictions based on its generic embedding matrix prove to be a difficult baseline to surpass, similar to C-SGM generic. Nonetheless, the com- posite skip-gram models (EM B1 and EM B2) do achieve best and second best rankings in the vast majority of cases (when compared to the best among all the other methods), with EM B1 be- ing the more robust variation performing well both for gender and for location. Focusing on the per- formance of EM B1, the highest gains are ob- served for India-based predictions, for best (from 0.05 to 0.08) and out-of-three (from 0.07 to 0.12); for male-based predictions increasing from 0.11 to 0.13 for best, and from 0.17 to 0.20 for out-of- three; and for female-based predictions, increas- ing from 0.13 to 0.14 for best, and from 0.17 to 0.20 for out-of-three. US-based associations are the hardest to predict, probably because of the di- verse makeup of society; additional evaluations are needed to pinpoint the exact cause.</p><p>To determine how susceptible the embedding model is to skewed, but larger training data, we also run a separate experiment on the entire raw set of blogs we collected (described on the left of <ref type="table" target="#tab_6">Table 6</ref>), where we re-generate the EM B1 and EM B2 models. While the entire dataset is signif- icantly larger than the balanced set, it is also sig- nificantly skewed: the data in the India:US dataset was skewed in a proportion of 1:0.48 tokens, while for Female:Male the proportion was 1:0.41 to- kens. As was the case for the balanced dataset, the EM B1 model is still the most robust (see the bottom section in <ref type="table" target="#tab_7">Table 7)</ref>, and it achieves signifi- cant gains when compared to its balanced counter- part, in particular for best (for the US demographic from 0.03 to 0.13, and for India from 0.08 to 0.11), and for out-of-three (for US from 0.07 to 0.15, and for India from 0.12 to 0.17), which suggests that as an avenue for future research, we can explore the use of significantly larger even if unbalanced datasets to train our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we introduced the task of demographic-aware word associations. To under- stand the various ways in which people associate words, we collected a new large demographics- <ref type="table" target="#tab_2">best   oo3  oo10  best  oo3  oo10  Method  Type  IN  US  IN  US  IN  US  M  F  M  F  M  F  MI  Gen  0</ref>.00 0.00 0.01 0.01 0.02 0.01 0.01 0.01 0.01 0.01 0.01 0.01 DA 0.00 0.00 0.01 0.00 0.02 0.02 0.00 0.01 0.01 0.01 0.01 0.02 VSM Gen 0.00 0.00 0.01 0.01 0.03 0.03 0.00 0.00 0.02 0.02 0.05 0.05 DA 0.00 0.00 0.02 0.01 0.04 0.02 0.00 0.01 0.02 0.01 0.04 0.06 SGLM Gen 0.02 0.02 0.03 0.03 0.06 0.05 0.13 0.13 0.18 0.18 0.20 0.21 DA 0.05 0.01 0.07 0.02 0.11 0.03 0.10 0.13 0.16 0.18 0.  enhanced dataset of approximately 300 stimulus words and their associated norms compiled from 800 respondents for a total of 176,097 non-spam responses, and show that for people of different demographics, associations do differ with gender and location. We proposed a new demographic-aware word association method based on composite skip-gram models that are able to jointly embed generic and gender tagged words. We showed that this method improves over its generic counterpart, and also outperforms previously proposed models of word association, thus demonstrating that it is useful to account for the demographics of the people behind the language when performing the task of auto- matic word association. We regard this as a first step toward demographic-aware NLP, and in fu- ture work we plan to address other more advanced NLP tasks while accounting for demographics.</p><p>The word association dataset introduced in this paper is publicly available from http://lit. eecs.umich.edu/downloads.html.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Primary response frequency (in percent) versus rank for the Standard word list.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Intra-group similarities (the higher the 
similarity, the more cohesive the group is). 

Demo-
Standard 
All 
graphic 
Primary Top10 Primary Top10 
India-US 
0.18 
0.55 
0.14 
0.50 
US-India 
0.20 
0.60 
0.16 
0.56 
Male-Female 
0.22 
0.63 
0.17 
0.59 
Female-Male 
0.24 
0.66 
0.19 
0.61 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Inter-group similarities (the higher the 
similarity, the less distinct the groups are). 

Demographic Standard All 
Gender 
0.60 
0.56 
Location 
0.94 
0.94 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 : Predictions based on similarity to group.</head><label>5</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc>Raw and balanced blog dataset statistics.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Best, out-of-three (oo3), and out-of-ten (oo10) scores across the various methods. IN: India, 
US: United States, M: Male, F: Female. The numbers in bold mark the highest scores, those in italics, 
the second highest. 

</table></figure>

			<note place="foot" n="2"> Note that this list originally included 100 words. The word &quot;foot&quot; was however misspelled in our survey, and instead we gathered answers for &quot;food.&quot;</note>

			<note place="foot" n="3"> In several of our data analyses, in order to allow for a direct comparison with the word list from (Kent and Rosanoff, 1910), in addition to showing statistics for the entire dataset (All), we also show statistics separately compiled for the list from (Kent and Rosanoff, 1910) (Standard).</note>

			<note place="foot" n="4"> www.blogger.com 5 We normalize the word forms using the Stanford CoreNLP lemmatizer (Manning et al., 2014). 6 Prolific users will be chosen first. For a class with exactly n users, all users will be chosen.</note>

			<note place="foot" n="7"> To place the results in this table in perspective, it is important to note that results for this task are traditionally low. Given that the most frequent response is selected on average by 24% of respondents (see Figure 1), we can see that even for humans, the highest score would be around 0.24.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This material is based in part upon work sup-ported by the Michigan Institute for Data Sci-ence, by the National Science Foundation (grant #1344257), and by the John Templeton Founda-tion (grant #48503). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not neces-sarily reflect the views of the Michigan Institute for Data Science, the National Science Founda-tion, or the John Templeton Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distributed representations of geographically situated language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="828" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Retrieving word associations with a simple neighborhood algorithm in a graph-based resource</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Bel-Enguix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon</title>
		<meeting>the 4th Workshop on Cognitive Aspects of the Lexicon<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="60" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lexical co-occurrence, statistical significance, and word association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dipak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Om</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srivatsan</forename><surname>Damani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laxman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1058" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Parsing, word associations and typical predicate-argument relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Hindle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Speech and Natural Language. Association for Computational Linguistics</title>
		<meeting>the workshop on Speech and Natural Language. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="page" from="75" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">Ward</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Better explanations of lexical and semantic cognition using networks derived from continued rather than single-word associations. Behavior Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Simon De Deyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gert</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Storms</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="480" to="498" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting human similarity judgments with distributional models: The value of word associations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Simon De Deyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel J</forename><surname>Perfors</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Navarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1861" to="1870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Measures of the amount of ecologic association between species</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="302" />
			<date type="published" when="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rethinking innateness: a connectionist perspective on development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><forename type="middle">A</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">H</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annette</forename><surname>Karmiloff-Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Domenico</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Plunkett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Computing semantic relatedness using Wikipediabased explicit semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th AAAI International Conference on Artificial Intelligence (AAAI</title>
		<meeting>the 20th AAAI International Conference on Artificial Intelligence (AAAI<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1606" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identifying cross-cultural differences in word usage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aparna</forename><surname>Garimella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Pennebaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="674" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Measuring semantic relatedness using salient encyclopedic concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samer</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Special Issue xx(xx</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Demographic factors improve classification performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="752" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lexical semantic relatedness with random graph walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thad</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ramag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="581" to="589" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic text similarity using corpus-based word similarity and string similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aminul</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
		<idno>10:1-10:25</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluation metrics for the lexical substitution task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanaz</forename><surname>Jabbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hepple</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louise</forename><surname>Guthrie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2010)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="289" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rogets thesaurus and semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Jarmasz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowics</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Recent Advances in Natural Language Processing</title>
		<meeting>Recent Advances in Natural Language Processing<address><addrLine>Borovetz, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Further data on changes in word-association norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">S</forename><surname>Palermo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="309" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A study of association in insanity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">J</forename><surname>Kent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosanoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Psychiatry</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="96" />
			<date type="published" when="1910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A solution to Plato&apos;s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="240" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Combining local context and WordNet similarity for word sense identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WordNet: An Electronic Lexical Database</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="305" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic sense disambiguation using machine readable dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lesk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th annual international conference on Systems documentation (SIGDOC 1986). Toronto, Ontario</title>
		<meeting>the 5th annual international conference on Systems documentation (SIGDOC 1986). Toronto, Ontario</meeting>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="page" from="24" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics System Demonstrations (ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The english lexical substitution task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="139" to="159" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems Conference (NIPS 2013)</title>
		<meeting>the Neural Information Processing Systems Conference (NIPS 2013)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Colourful language: Measuring word-colour associations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics</title>
		<meeting>the 2nd Workshop on Cognitive Modeling and Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="97" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Combining corpus linguistic and psychological data on word co-occurrences: Corpus collocates versus word associations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Mollin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Corpus Linguistics and Linguistic Theory</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="175" to="200" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The University of South Florida free association, rhyme, and word fragment norms. Behavior research methods, instruments, &amp; computers : a journal of the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">L</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mcevoy</forename><surname>Cathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schreiber</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inc</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="402" to="407" />
			<date type="published" when="2004" />
			<publisher>Psychonomic Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Semantic cognition: a parallel distributed processing approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">R</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Comparisons among word-association responses in English, French, German, and Italian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosenzweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="360" />
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Introduction to modern information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>McGraw-Hill, Inc</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The Kent-Rosanoff word association: Word association norms as a function of age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><forename type="middle">E</forename><surname>Tresselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Mayzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1-12</biblScope>
			<biblScope unit="page" from="65" to="66" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploring demographic language variations to improve multilingual sentiment analysis in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svitlana</forename><surname>Volkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1815" to="1827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A connectionist system to simulate lexical decisions in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Wettler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Rapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Connectionism in Perspective. Amsterdam</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989" />
			<biblScope unit="page" from="463" to="469" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
