<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Semantic Representations for Nonterminals in Hierarchical Phrase-Based Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Soochow University</orgName>
								<address>
									<postCode>215006</postCode>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Soochow University</orgName>
								<address>
									<postCode>215006</postCode>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Soochow University</orgName>
								<address>
									<postCode>215006</postCode>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Semantic Representations for Nonterminals in Hierarchical Phrase-Based Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In hierarchical phrase-based translation, coarse-grained nonterminal Xs may generate inappropriate translations due to the lack of sufficient information for phrasal substitution. In this paper we propose a framework to refine nonterminals in hierarchical translation rules with real-valued semantic representations. The semantic representations are learned via a weighted mean value and a minimum distance method using phrase vector representations obtained from large scale monolin-gual corpus. Based on the learned semantic vectors, we build a semantic non-terminal refinement model to measure semantic similarities between phrasal substitutions and nonterminal Xs in translation rules. Experiment results on Chinese-English translation show that the proposed model significantly improves translation quality on NIST test sets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hierarchical phrase-based translation <ref type="bibr" target="#b5">(Chiang, 2007)</ref> explores formal synchronous context free grammar (SCFG) rules for translation. Two types of nonterminal symbols are used in translation rules: nonterminal X in ordinary SCFG rules and nonterminal S in glue rules that are specially intro- duced to concatenate nonterminal Xs in a mono- tonic manner. The same generic symbol X for all ordinary nonterminals makes it difficult to distin- guish and select proper translation rules.</p><p>In order to address this issue, researchers ei- ther use syntactic labels to annotate nontermi- nal Xs ( <ref type="bibr" target="#b32">Zollmann and Venugopal, 2006;</ref><ref type="bibr" target="#b33">Zollmann and Vogel, 2011;</ref><ref type="bibr" target="#b16">Li et al., 2012;</ref><ref type="bibr" target="#b10">Hanneman and Lavie, 2013)</ref>, or employ syntactic information * Corresponding author from parse trees to refine nonterminals with real- valued vectors ( <ref type="bibr" target="#b29">Venugopal et al., 2009;</ref><ref type="bibr" target="#b13">Huang et al., 2013)</ref>. In addition to syntactic knowledge, se- mantic structures are also leveraged to refine non- terminals ( <ref type="bibr" target="#b8">Gao and Vogel, 2011</ref>). All these efforts focus on incorporating linguistic knowledge into hierarchical translation rules.</p><p>Unfortunately, syntactic or semantic parsers for many languages are not accessible due to the lack of labeled training data. In contrast, a large amount of unlabeled data are easily available. Therefore, can we mine syntactic or semantic properties for nonterminals from unlabeled data? Or can we exploit these data to refine nontermi- nals for SMT?</p><p>Learning semantic representations for terminals (words, multi-word phrases or sentences) from un- labeled data has achieved substantial progress in recent years ( <ref type="bibr" target="#b20">Mitchell and Lapata, 2008;</ref><ref type="bibr" target="#b28">Turian et al., 2010;</ref><ref type="bibr" target="#b24">Socher et al., 2010;</ref><ref type="bibr" target="#b19">Mikolov et al., 2013c;</ref><ref type="bibr" target="#b1">Blunsom et al., 2014</ref>). These rep- resentations have been used successfully in var- ious NLP tasks. However, there is no attempt to learn semantic representations for nontermi- nals from unlabeled data. In this paper we pro- pose a framework to learn semantic representa- tions for nonterminal Xs in translation rules. Our framework is established on the basis of real- valued vector representations learned for multi- word phrases, which are substituted with nonter- minal Xs during hierarchical rule extraction. We propose a weighted mean value and a minimum distance method to obtain nonterminal representa- tions from representations of their phrasal substi- tutions. We further build a semantic nonterminal refinement model with semantic representations of nonterminals to compute similarities between phrasal substitutions and nonterminals. In doing so, we want to enhance phrasal substitution and translation rule selection during decoding.</p><p>The big challenge here is that thousands of tar-get phrasal substitutions will be generated for one single nonterminal during decoding. Computing vector representations for all these phrases will be very time-consuming. We therefore introduce two different methods to handle it. In the first method, we project representations of source phrases onto their target counterparts linearly/nonlinearly via a neural network. These projected vectors are used as approximations to real target representa- tions to compute semantic similarities. In the sec- ond method, we decode sentences in two passes. The first pass collects target phrase candidates from n-best translations of sentences generated by the baseline. The second pass calculates vector representations of these collected target phrases and then computes similarities between them and target-side nonterminals. Our contributions are two-fold. First, we learn semantic representations for nonterminals from their phrasal substitutions with two different meth- ods. This is the first time, to the best of our knowl- edge, to induce semantic representations for non- terminals from unlabeled data in the context of SMT. Second, we successfully address the issue of time-consuming target-side phrase-nonterminal similarity computation mentioned above. We in- corporate both source-/target-side semantic non- terminal refinement model and their combination based on learned nonterminal representations into translation system. Experiment results show that our method can achieve an improvement of 1.16 BLEU points over the baseline system on NIST MT evaluation test sets.</p><p>The rest of this paper is organized as follows. Section 2 briefly reviews related work. Section 3 presents our approach of learning semantic vectors for nonterminals, followed by Section 4 describing the details of our semantic nonterminal refinement model. Section 5 introduces the integration of the proposed model into SMT. Experiment results are reported in Section 6. Finally, we conclude our work in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>A variety of approaches have been explored for nonterminal refinement in hierarchical phrase- based translation. These approaches can be cat- egorized into two groups: 1) augmenting the non- terminal symbol X with informative labels, and 2) attaching distributional linguistic knowledge to each nonterminal in hierarchical rules. The former only allows substitution operations with matched labels. The latter normally builds an additional model as a new feature of the log-linear model to incorporate attached knowledge.</p><p>Among approaches which directly refine the single label to more fine-grained labels, syntac- tic and semantic knowledge are explored in vari- ous ways. The syntactically augmented translation model (SAMT) proposed by Zollmann and Venu- gopal (2006) uses syntactic categories extracted from target-side parse trees to augment nontermi- nals in hierarchical rules. Unfortunately, there is a data sparseness problem in this model due to thousands of extracted syntactic categories. One solution to address this issue is to reduce the num- ber of syntactic categories. <ref type="bibr" target="#b33">Zollmann and Vogel (2011)</ref> use word tags, generated by either POS tagger or unsupervised word class induction, in- stead of syntactic categories. Hanneman and Lavie (2013) coarsen the label set by introducing a label collapsing algorithm to SAMT grammars <ref type="bibr" target="#b32">(Zollmann and Venugopal, 2006</ref>). Yet another solution is easing restrictions on label matching. <ref type="bibr" target="#b23">Shen et al. (2009)</ref> penalize substitution with unmatched la- bels while Chiang (2010) uses soft match features to model substitutions with various labels. Simi- lar to <ref type="bibr" target="#b32">Zollmann and Venugopal (2006)</ref>, <ref type="bibr" target="#b11">Hoang and Koehn (2010)</ref> decorate some hierarchical rules with source-side syntax information and use un- decorated, decorated, and partially decorated rules in their translation model. Mylonakis and Sima'an (2011) employ source-side syntax-based labels to define a joint probability synchronous grammar. Combinatory Categorial Grammar (CCG) labels or CCG contextual labels are also used to enrich nonterminals <ref type="bibr" target="#b0">(Almaghout et al., 2011;</ref><ref type="bibr" target="#b30">Weese et al., 2012</ref>). <ref type="bibr" target="#b16">Li et al. (2012)</ref> incorporate head in- formation extracted from source-side dependency structures into translation rules. Besides, seman- tic knowledge is also used to refine nonterminals. <ref type="bibr" target="#b8">Gao and Vogel (2011)</ref> utilize target-side semantic roles to form SRL-aware SCFG rules. Most of ap- proaches introduced here explicitly require syntac- tic or semantic parsers trained on manually labeled data.</p><p>On the other hand, efforts have also been di- rected towards attaching distributional linguistic knowledge to nonterminals. <ref type="bibr" target="#b29">Venugopal et al. (2009)</ref> propose a preference grammar to annotate nonterminals based on preference distributions of syntactic categories. <ref type="bibr" target="#b12">Huang et al. (2010)</ref> learn la-tent syntactic distributions for each nonterminal. They use these distributions to decorate nontermi- nal Xs in SCFG rules with a real-valued feature vectors and utilize these vectors to measure the similarities between source phrases and applied rules. Similar to this work, <ref type="bibr" target="#b13">Huang et al. (2013)</ref> utilize treebank tags based on dependency parsing to learn latent distributions. <ref type="bibr" target="#b3">Cao et al. (2014)</ref> at- tach translation rules with dependency knowledge, which contains both dependency relations inside rules and dependency relations between rules and their contexts.</p><p>The difference of our work from these studies is that our semantic representations are learned from unlabeled bilingual (or monolingual) data and do not depend on any linguistic resources, e.g., parsers. We also believe that our model is able to exploit both syntactic and semantic infor- mation for nonterminals since vector representa- tions learned in our way are able to capture both syntactic and semantic properties ( <ref type="bibr" target="#b28">Turian et al., 2010;</ref><ref type="bibr" target="#b24">Socher et al., 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Semantic Representations for Nonterminals</head><p>In our framework, semantic representations for nonterminal Xs are automatically induced from word-aligned parallel corpus. In this section, we detail the essential component of our approach, i.e., how to learn semantic vectors for nonter- minals and how to project source semantic vec- tors onto target language semantic space. Be- fore discussing nonterminal representations, we briefly introduce vector representations for words and phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Prerequisite: Learning Words and Phrases Representations</head><p>We employ a neural method, specifically the continuous bag-of-words model ( <ref type="bibr" target="#b17">Mikolov et al., 2013a</ref>) to learn high-quality vector representations for words. Once we complete the training of the continuous bag-of-words model, word embed- dings form an embedding matrix M ∈ R d×|V | , where d is a pre-determined embedding dimen- sionality and each word w in the vocabulary V corresponds to a vector v ∈ R d . Given the em- bedding matrix M , mapping words to vectors can be done by simply looking up their respective columns in M . We further feed these learned word embeddings to recursive autoencoders (RAE) <ref type="bibr" target="#b26">(Socher et al., 2011</ref>) for learning phrase representations. In tra- ditional RAE (shown in <ref type="figure" target="#fig_0">Figure 1</ref>), given two in- put children representation vectors c 1 ∈ R d and c 2 ∈ R d , their parent representation p can be cal- culated as follows:</p><formula xml:id="formula_0">p = f (1) (W (1) [ c 1 ; c 2 ] + b (1) )<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">[ c 1 ; c 2 ] ∈ R 2d</formula><p>is the concatenation of vec- tors of two children, W (1) ∈ R d×2d is a weight matrix, b (1) ∈ R d is a bias term, and f <ref type="formula" target="#formula_0">(1)</ref> is an element-wise activation function such as tanh.</p><p>The above output representation p can be used as a child vector to construct the representation for a larger subphrase. This process is repeated until a binary tree covering the whole input phrase is gen- erated.</p><p>In order to evaluate how well the parent vector represents its children, we can reconstruct the chil- dren in a reconstruction layer:</p><formula xml:id="formula_2">[ c 1 ; c 2 ] = f (2) (W (2) p + b (2) )<label>(2)</label></formula><p>where c 1 and c 2 are the reconstructed children,</p><formula xml:id="formula_3">W (2) is a weight matrix for reconstruction, b (2)</formula><p>is a bias term for reconstruction, and f <ref type="bibr">(2)</ref> is an element-wise activation function.</p><p>For each node in the generated binary tree, we compute Euclidean distance between the original input vectors and the reconstructed vectors to mea- sure the reconstruction error:</p><formula xml:id="formula_4">E rec ([ c 1 ; c 2 ]) = 1 2 [ c 1 ; c 2 ] − [ c 1 ; c 2 ] 2 (3)</formula><p>By minimizing the total reconstruction error over all nonterminal nodes, we can learn parameters of RAE.</p><p>Socher et al. <ref type="formula" target="#formula_0">(2011)</ref> propose a greedy unsuper- vised RAE as an extension to the above traditional RAE. The main difference is that in the unsuper- vised RAE there is no tree structure which is given for traditional RAE. It can learn both representa- tions and tree structures of phrases or sentences. In this work, we adopt the unsupervised RAE to learn vector representations for phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inducing Nonterminal Representations from Phrase Representations</head><p>As we extract hierarchical rules from phrases by replacing subphrases with nonterminal symbols, a nonterminal X is generalized from a number of subphrases. We believe that these subphrases de- termine syntactic and semantic properties of the nonterminal X. We therefore enrich each nonter- minal X with a semantic vector induced from vec- tor representations of phrases that are replaced by the nonterminal during rule extraction.</p><p>For an SCFG rule, we can learn semantic vec- tors for nonterminals on both the source and target side. Due to the space limitation, we introduce the procedure of learning nonterminal vectors on the source side. Semantic vectors on the target side can be learned analogically.</p><p>For each source-side nonterminal X of a hi- erarchical rule, we collect all source subphrases replaced by X in a source subphrase set P = {p 1 , p 2 , · · · , p m }. We also count the number of times of these phrases being replaced by non- terminal X on training data during rule extrac- tion. We collect these numbers in a count set C = {c 1 , c 2 , · · · , c m }. Based on the phrase set P , count set C and learned phrase vector representa- tions in P , we can compute a semantic vector v x for nonterminal X in each SCFG rule.</p><p>We propose two general approaches to obtain semantic vectors for nonterminals: a weighted mean value method and a minimum distance method. Given phrase vector representations P r = { p 1 , p 2 , . . . , p m } , we calculate the seman- tic vector for a nonterminal generalized from these phrases as follows.</p><p>Weighted mean value method (MV) computes semantic vector v x as:</p><formula xml:id="formula_5">v x = m i=1 c i · p i m i=1 c i<label>(4)</label></formula><p>Minimum distance method (MD) finds a point in semantic space to minimize the sum of Eu- clidean distances of vectors in P r to this point. Formally,</p><formula xml:id="formula_6">v x = argmin vx m i=1 d j=1 (p ij − v xj ) 2<label>(5)</label></formula><p>We use the stochastic gradient descent algorithm to find the minimal distance and the point v</p><note type="other">x . The component v xj can be updated by v xj ← v xj + λ ∂f ∂vx j where f is m i=1 d j=1 (p ij − v xj ) 2 and λ is the learning rate.</note><p>Similar to the center of gravity, the semantic vector v x learned by this method acts as a semantic centroid for all vectors of phrases that are substi- tuted by X. Nonterminals in different hierarchical translation rules will have different semantic cen- troids. These centroids will help translation model capture semantic diversity to a certain degree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Mapping Source-Side Representations onto Target-Side Semantic Space</head><p>As we discussed in Section 1, directly learning vector representations for target phrases is very costly in practice. Inspired by <ref type="bibr" target="#b18">Mikolov et al. (2013b)</ref>, we adopt vector projection to alleviate this problem. Different from mapping represen- tations from the source side to the target side by learning a linear matrix on word alignments (Mikolov et al., 2013b), we project source multi- word phrase representations onto the target seman- tic space in a nonlinear manner as we believe that nonlinear relations between languages are more reasonable. Specifically, we use a neural network to achieve this goal. Our neural network is a multi- layer feed-forward neural network with one hid- den layer. The functional form can be written in the following equation:</p><formula xml:id="formula_7">p = tanh(W (4) (tanh(W (3) src) + b (3) ) + b (4) )<label>(6)</label></formula><p>where src is the input vector which is learned in the source semantic space, W (3) denotes the weight matrix for connections between input and hidden neurons and W (4) denotes the weight ma- trix for links between hidden neurons and output, b (3) and b (4) are bias terms. To train the neural network, we optimize the following objective:</p><formula xml:id="formula_8">J = argmin W (3) ,W (4) 1 N N i=1 trg i − p i 2 + R(θ) (7)</formula><p>where N is the number of training examples, trg i is the target vector representation for the ith ex- ample learned by RAE and p i is the output of the neural network for the source vector representa- tion src i of ith example. R(θ) is the regularizer on parameters:</p><formula xml:id="formula_9">R(θ) = λ L 2 W 2<label>(8)</label></formula><p>where W denotes parameters for parameter matri- ces W (3) , W (4) and bias terms b (3) , b (4) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Semantic Nonterminal Refinement Model</head><p>In this section, we describe our semantic nonter- minal refinement model on the basis of induced real-valued semantic vectors for nonterminals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Nonterminal Representations in Hierarchical Rules</head><p>We incorporate learned semantic representa- tions of nonterminals into hierarchical rules. In particular, ordinary hierarchical rules take the fol- lowing form:</p><formula xml:id="formula_10">X → aX s b, cX t d<label>(9)</label></formula><p>where a/b, c/d are strings of terminals on the source and target side, s and t are placeholders de- noting the nonterminal X on the source or target side, X s and X t are aligned to each other. Representations for nonterminals can be on ei- ther the source or target side. They are attached to hierarchical rules as follows:</p><formula xml:id="formula_11">X → aX s b, cX t d, v xs , v xt<label>(10)</label></formula><p>where v x. is the source-or target-side semantic representation for nonterminal. In this way, we keep original translation rules intact and decorate nonterminals with their semantic representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Model</head><p>The proposed semantic nonterminal refinement model estimates the semantic similarity between a phrase p and nonterminal X. The phrase p and nonterminal X will have a high similarity score in the representation space if they are semantically similar. The higher semantic similarity scores are, the more compatible nonterminals are with corre- sponding phrases.</p><p>There is another nonterminal S in glue rules, which are formalized as follows:</p><formula xml:id="formula_12">S → S 1 X 2 , S 1 X 2 (11) S → X 1 , X 1<label>(12)</label></formula><p>This nonterminal S is different from X. We there- fore treat it as a special case in the computation of semantic similarity. In this work, we explore two approaches to compute similarity: one based on cosine similarity and the other based on Euclidean distance.</p><p>Given a phrase vector representation p and non- terminal X semantic vector v x , Cosine Similarity (CS) is computed as:</p><formula xml:id="formula_13">cos( p, v x ) = p · v x p v x<label>(13)</label></formula><p>We set α for the Cosine Similarity between the glue rule and its corresponding phrase as follows:</p><formula xml:id="formula_14">SeSim = cos( p, vx) hierarchical rules α glue rules<label>(14)</label></formula><p>As for Euclidean Distance (ED), it is computed according to the following formula:</p><formula xml:id="formula_15">dist( p, v x ) = d i=1 (p i − v xi ) 2<label>(15)</label></formula><p>and similarly we set β for glue rules:</p><formula xml:id="formula_16">SeSim = dist( p, vx) hierarchical rules β glue rules<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Decoding</head><p>We incorporate the proposed model as a new feature into the hierarchical phrase-based transla- tion system. Specifically, two features are added into the baseline system:</p><p>1. Source-side semantic similarity between source phrases and nonterminals 2. Target-side semantic similarity between tar- get phrases and nonterminals</p><p>We compute source-and target-side similari- ties based on representations of nonterminals and phrasal substitutions for each applied rule, and sum up these similarities to calculate the total score of a derivation on the two features. The integration of the source-side semantic nonterminal refinement model into the decoder is trivial. For the target-side model, however, we have to consider the efficiency issue as we men- tioned in Section 1. We introduce two different methods to integrate the target-side model into the decoder: 1) projection and 2) two-pass decoding. In the first integration method, a mapping neu- ral network is trained to map source phrase rep- resentations onto the target semantic space as de- scribed in Section 3.3. The projection can be lin- ear if we remove the hidden layer in the projection neural network. This is similar to the mapping matrix learned by <ref type="bibr" target="#b18">Mikolov et al. (2013b)</ref>. We calculate semantic similarities between projected representations of phrases and those of nontermi- nals. In the two-pass decoding, we collect tar- get phrase candidates from 100-best translations for each source sentence generated by the base- line in the first pass and learn vector represen- tations for these target phrase candidates. Then in the second pass, we decode source sentence with our target semantic nonterminal refinement model using learned target phrase vector represen- tations. If a target phrase appears in the collected set, the target-side semantic nonterminal refine- ment model will calculate the semantic similarity between the target phrase and the corresponding nonterminal on the target semantic space; other- wise the model will give a penalty. This is because this phrase is not a desirable phrase as it is not used in 100-best translations.</p><p>The weights of these two features are tuned by the Minimum Error Rate Training (MERT) <ref type="bibr" target="#b22">(Och, 2003)</ref>, together with weights of other sub-models on a development set. <ref type="figure" target="#fig_1">Figure 2</ref> shows the architec- ture of SMT system with the proposed semantic nonterminal refinement model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiment</head><p>In this section, we conducted a series of exper- iments on Chinese-to-English translation using large-scale bilingual training data, aiming at the following questions:</p><p>1. Which approach is better for learning nonter- minal representations, weighted mean value or minimum distance?</p><p>2. Can the target-side semantic nonterminal re- finement model improve translation quality? And which method is better for integrating the target-side semantic model into transla- tion, projection or two-pass decoding?</p><p>3. Does the combination of source and target se- mantic nonterminal refinement models pro- vide further improvement?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Setup</head><p>Our training corpus contains 2.9M sentence pairs with 80.9M Chinese words and 86.4M English words from LDC data <ref type="bibr">1</ref> . We used NIST MT03 as our development set, NIST MT06 as our develop- ment test set and MT08 as our final test set. We ran Giza++ on the training corpus in both Chinese-to-English and English-to-Chinese direc- tions and applied the "grow-diag-final" refine- ment rule ( <ref type="bibr" target="#b14">Koehn et al., 2003</ref>) to obtain word alignments. We used the SRI Language Model- ing Toolkit 2 ( <ref type="bibr">Stolcke and others, 2002</ref>) to train our language models. MERT <ref type="bibr" target="#b22">(Och, 2003</ref>) was adopted to tune feature weights of the decoder. We used the case-insensitive BLEU 3 as our eval- uation metric. In order to alleviate the instabil- ity of MERT , we followed <ref type="bibr" target="#b7">Clark et al. (2011)</ref> to perform three runs of MERT and reported average BLEU scores over the three runs for all our exper- iments.</p><p>We used word2vec toolkit 4 to train our word embeddings and set the vector dimension d to 30. In our training experiment, we used the continu- ous bag-of-words model with a context window of size 5. The monolingual corpus, which was used to pre-train word embeddings, is extracted from the above parallel corpus in SMT. To train vec- tor representations for multi-word phrases, we ran- domly selected 1M bilingual sentences 5 as train- ing set and used the unsupervised greedy RAE fol- lowing <ref type="bibr" target="#b26">(Socher et al., 2011</ref>). We used a learning rate of 10 −3 for our minimum distance method that learned the centroid of phrase representations as the vector representation of the corresponding nonterminal.</p><p>For projection neural network in Section 3.3, we set 300 units for the hidden layer and dimen- sionality of 30 for both input and output vectors. Learning rate was set to 10 −3 and the regulariza- tion coefficient λ L was set to 10 −3 . To construct the training set for the projection neural network, we selected phrase pairs from our rule table and used their representations on the source and target side as training examples. We randomly selected 5M examples as training set, 10k examples as de- velopment set and 10k examples as test set. The multi-layer projection neural network was trained with the back-propagation and stochastic gradient descent algorithm with a mini-batch size of 5k.</p><p>Our baseline system is an in-house hierarchical phrase-based system <ref type="bibr" target="#b5">(Chiang, 2007)</ref>. The features used in the baseline system includes a 4-gram language model trained on the Xinhua section of the English Gigaword corpus, a 3-gram language model trained on the target part of the bilingual training data, bidirectional translation probabili- ties, bidirectional lexical weights, a word count, a phrase count and a glue rule count.</p><p>In order to compare our proposed models with previous methods on nonterminal refinement, we re-implemented a syntax mismatch model (Syn- Mis) which was used by <ref type="bibr" target="#b13">Huang et al. (2013)</ref> and integrated it into hierarchical phrase-based sys- tem. Syn-Mis model decorates each nontermi- nal with a distribution of head POS tags and uses this distribution to measure the degree of syntactic compatibility of translation rules with correspond- ing source spans. In order to obtain head POS tags for Syn-Mis model, we used the Stanford depen- dency parser <ref type="bibr">6 (Chang et al., 2009</ref>) to parse Chi- nese sentences in our training corpus and NIST de- velopment/test sets. <ref type="bibr">5</ref> We choose bilingual sentences because we want to ob- tain bilingual training examples to train our projection neural network as described in Section 3.  <ref type="table">Table 1</ref>: BLEU scores of our models against the baseline and Syn-Mis model. /*" and /+" : sig- nificantly better than Baseline at significance level p &lt; 0.01 and p &lt; 0.05 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Different Approaches to Learn Vector Representations for Nonterminals</head><p>Our first group of experiments were carried out to investigate which approach is more appropri- ate to learn semantic vectors for nonterminals. We only used the source-side semantic nonterminal refinement model in these experiments. In order to validate the effectiveness of the proposed ap- proaches for learning nonterminal semantic vec- tors, we combined the minimum distance method (MD) with the Euclidean Distance (ED) because both of them are distance-based, and combined the weighted mean value method (MV) with the Cosine Similarity model (CS) as they belong to vector-based approaches. We chose α = 1.0, 0, -1.0 and β = 0, 0.5, 1.0 for glue rules to study the impact of these parameters. We compared our model with the baseline and Syn-Mis model.</p><p>Results are shown in <ref type="table">Table 1</ref>. From <ref type="table">Table 1</ref>, we observe that the proposed two approaches are able to achieve significant improvements over the base- line. (MV + CS) and (MD + ED) achieve up to an absolute improvement of 1.09 and 0.81 (when α = 0 and β = 0.5) BLEU points respectively over the baseline on the development test set MT06. And the approach (MV + CS) with α = 0 outperforms Syn-Mis by 0.4 BLEU points on MT06 without using any syntactic information. The approach (MV + CS) achieves better performance and it is more efficient than (MD + ED) where the com- putation of semantic centroids is time-consuming. Therefore, we adopt the approach (MV + CS) with α = 0 to learn semantic vectors for nonterminals and compute semantic similarities in the follow- ing experiments.  <ref type="table">Table 2</ref>: Comparison of two-pass decoding, linear and nonlinear projection methods for integrating the target-side semantic nonterminal refinement model in terms of BLEU scores. /*" and /+" : sig- nificantly better than Baseline at significance level p &lt; 0.01 and p &lt; 0.05 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MT06</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MT08 Avg</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Effect of the Target Semantic Nonterminal Refinement Models</head><p>In the second set of experiments, we further val- idate the effectiveness of semantic nonterminal vectors learned on the target side. In these exper- iments, learning vector representations and com- puting semantic similarities were performed on the target language semantic space. We also com- pared the two integration methods discussed in Section 5 for the target-side model. With regard to the projection method, we further compared the linear projection (the projection neural network without hidden layer) with the nonlinear projec- tion (with hidden layer). Experiment results are shown in <ref type="table">Table 2</ref>.</p><p>From <ref type="table">Table 2</ref>, we can see that</p><p>• Two-pass decoding achieves the highest BLEU scores, which are higher than those of the baseline by 0.75 and 0.66 BLEU points on MT06 and MT08 respectively. The rea- son may be that noisy translation candidates are filtered out in the first pass. This finding is consistent with many other multiple-pass systems in natural language processing, e.g., two-pass parsing <ref type="bibr" target="#b31">(Zettlemoyer and Collins, 2007</ref>).</p><p>• Nonlinear projection achieves an improve- ment of 0.62 BLEU points over the baseline on MT06. It outperforms linear projection method on both sets. These empirical results support our assumption that nonlinear rela- tions between languages are more reasonable than linear relations.</p><p>• The results prove that the target-side seman- tic nonterminal refinement model is also able  <ref type="table">Table 3</ref>: BLEU scores of the combination of the source-and target-side semantic nonterminal re- fine model. /*" and /+" : significantly better than Baseline at significance level p &lt; 0.01 and p &lt; 0.05 respectively.</p><p>to improve the baseline system, although the gain is less than that of the source-side coun- terpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Combination of the Source and Target Models</head><p>Finally, we integrated both the source-and target- side semantic nonterminal refinement models into the baseline system. In this experiment, we adopted nonlinear projection to obtain target se- mantic vector representations for target phrases. These two models collectively achieve a gain of up to 1.16 BLEU points over the baseline and 0.41 BLEU points over Syn-Mis model on aver- age, which is shown in <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented a framework to refine non- terminal X in hierarchical translation rules with semantic representations. The semantic vectors are derived from vector representations of phrasal substitutions, which are automatically learned us- ing an unsupervised RAE. As the semantic non- terminal refinement model is capable of select- ing more semantically similar translation rules, it achieves statistically significant improvements over the baseline on Chinese-to-English transla- tion. Experiment results have shown that</p><p>• Using (MV + CS) approach to learn semantic representations for nonterminals can achieve better performance than (MD + ED) in terms of BLEU scores.</p><p>• Target-side semantic nonterminal refinement model is able to substantially improve trans- lation quality over the baseline. Two-pass de-coding method is superior to the projection method.</p><p>• The simultaneous incorporation of the source-and target-side models can achieve further improvements over a single-side model.</p><p>For the future work, we are interested in learn- ing bilingual representations <ref type="bibr" target="#b15">(Lauly et al., 2014;</ref><ref type="bibr" target="#b9">Gouws et al., 2014</ref>) for nonterminals. We also would like to extend our work by using more con- textual lexical information to derive semantic vec- tors for nonterminals.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture of a recursive autoencoder, adapted from (Socher et al., 2011). Blue nodes are original vectors and yellow nodes are reconstructed vectors which are used to compute reconstruction errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of SMT system with the proposed semantic nonterminal refinement model.</figDesc></figure>

			<note place="foot" n="1"> The corpora include LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 2 http://www.speech.sri.com/projects/srilm/download.html 3 ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 4 https://code.google.com/p/word2vec/</note>

			<note place="foot" n="1"> (MV + CS α = 0) is used. 2 Nonlinear Projection is used.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The work was sponsored by the National Nat-ural Science Foundation of China (Grants No. 61403269, 61432013 and 61333018) and Natu-ral Science Foundation of Jiangsu Province (Grant No. BK20140355). We would like to thank three anonymous reviewers for their insightful com-ments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ccg contextual labels in hierarchical phrase-based smt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hala</forename><surname>Almaghout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Conference of the European Association for Machine Translation</title>
		<meeting>the 15th Annual Conference of the European Association for Machine Translation</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Soft dependency matching for hierarchical phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative reordering with chinese grammatical relations features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pi-Chuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huihsin</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation</title>
		<meeting>the Third Workshop on Syntax and Structure in Statistical Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="51" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Hierarchical phrase-based translation. computational linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="201" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to translate with source and target syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1443" to="1452" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Better hypothesis testing for statistical machine translation: Controlling for optimizer instability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Utilizing target-side semantic role labels to assist hierarchical phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<meeting>the Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="107" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.2455</idno>
		<title level="m">Bilbowa: Fast bilingual distributed representations without word alignments</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving syntax-augmented machine translation by coarsening the label set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Hanneman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="288" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved translation with source syntax labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR</title>
		<meeting>the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="409" to="417" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martiň</forename><surname>Cmejrek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="138" to="147" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Factored soft source syntactic constraints for hierarchical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="556" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An autoencoder approach to learning bilingual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1853" to="1861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Head-driven hierarchical phrasebased translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Van Genabith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="33" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46st Annual Meeting on Association for Computational</title>
		<meeting>the 46st Annual Meeting on Association for Computational</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="236" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning hierarchical translation structure with linguistic annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markos</forename><surname>Mylonakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="642" to="652" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Effective use of linguistic and contextual information for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="72" to="80" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning continuous phrase representations and syntactic parsing with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS-2010</title>
		<meeting>the NIPS-2010</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<title level="m">Deep Learning and Unsupervised Feature Learning Workshop</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Srilm-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Preference grammars: Softening syntactic constraints to improve statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Venugopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Zollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="236" to="244" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Using categorial grammar to label translation rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Weese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Statistical Machine Translation</title>
		<meeting>the Seventh Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="222" to="231" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Online learning of relaxed ccg grammars for parsing to logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="678" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Syntax augmented machine translation via chart parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Zollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Venugopal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Statistical Machine Translation</title>
		<meeting>the Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="138" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A word-class approach to labeling pscfg rules for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Zollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
