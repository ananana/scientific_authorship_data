<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Language Modeling with Sparse Product of Sememe Experts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence State Key Lab on Intelligent Technology and Systems</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of CST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence State Key Lab on Intelligent Technology and Systems</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of EE</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence State Key Lab on Intelligent Technology and Systems</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of CST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence State Key Lab on Intelligent Technology and Systems</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of CST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Search Product Center</orgName>
								<orgName type="department" key="dep2">WeChat Search Application Department</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence State Key Lab on Intelligent Technology and Systems</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of CST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fen</forename><surname>Lin</surname></persName>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Search Product Center</orgName>
								<orgName type="department" key="dep2">WeChat Search Application Department</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyu</forename><surname>Lin</surname></persName>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Search Product Center</orgName>
								<orgName type="department" key="dep2">WeChat Search Application Department</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Language Modeling with Sparse Product of Sememe Experts</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4642" to="4651"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4642</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Most language modeling methods rely on large-scale data to statistically learn the sequential patterns of words. In this paper , we argue that words are atomic language units but not necessarily atomic semantic units. Inspired by HowNet, we use se-memes, the minimum semantic units in human languages, to represent the implicit semantics behind words for language model-ing, named Sememe-Driven Language Model (SDLM). More specifically, to predict the next word, SDLM first estimates the sememe distribution given textual context. Afterwards, it regards each sememe as a distinct semantic expert , and these experts jointly identify the most probable senses and the corresponding word. In this way, SDLM enables language models to work beyond word-level manipulation to fine-grained sememe-level semantics, and offers us more powerful tools to fine-tune language models and improve the interpretabil-ity as well as the robustness of language models. Experiments on language modeling and the downstream application of headline generation demonstrate the significant effectiveness of SDLM. Source code and data used in the experiments can be accessed at https:// github.com/thunlp/SDLM-pytorch.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language Modeling (LM) aims to measure the probability of a word sequence, reflecting its flu- ency and likelihood as a feasible sentence in a human language. Language Modeling is an es- sential component in a wide range of natural lan- guage processing (NLP) tasks, such as Machine Translation ( <ref type="bibr" target="#b6">Brown et al., 1990;</ref><ref type="bibr" target="#b5">Brants et al., 2007)</ref>, Speech Recognition <ref type="bibr" target="#b24">(Katz, 1987)</ref>, Informa- tion Retrieval <ref type="bibr" target="#b4">(Berger and Lafferty, 1999</ref>; Ponte ⇤ Equal contribution.</p><p>† Correspondence author. and <ref type="bibr" target="#b35">Croft, 1998;</ref><ref type="bibr" target="#b31">Miller et al., 1999;</ref><ref type="bibr" target="#b16">Hiemstra, 1998)</ref> and Document Summarization ( <ref type="bibr" target="#b38">Rush et al., 2015</ref>; <ref type="bibr" target="#b2">Banko et al., 2000)</ref>. A probabilistic language model calculates the conditional probability of the next word given their contextual words, which are typically learned from large-scale text corpora. Taking the sim- plest language model for example, N-Gram es- timates the conditional probabilities according to maximum likelihood over text corpora <ref type="bibr" target="#b23">(Jurafsky, 2000)</ref>. Recent years have also witnessed the ad- vances of Recurrent Neural Networks (RNNs) as the state-of-the-art approach for language model- ing ( <ref type="bibr">Mikolov et al., 2010)</ref>, in which the context is represented as a low-dimensional hidden state to predict the next word.</p><p>Those conventional language models including neural models typically assume words as atomic symbols and model sequential patterns at word level. However, this assumption does not neces- sarily hold to some extent. Let us consider the fol- lowing example sentence for which people want to predict the next word in the blank,</p><p>The U.S. trade deficit last year is initially estimated to be 40 billion .</p><p>People may first realize a unit should be filled in, then realize it should be a currency unit. Based on the country this sentence is talking about, the U.S., one may confirm it should be an American cur-rency unit and predict the word dollars. Here, the unit, currency, and American can be regarded as basic semantic units of the word dollars. This pro- cess, however, has not been explicitly taken into consideration by conventional language models. That is, although in most cases words are atomic language units, words are not necessarily atomic semantic units for language modeling. We ar- gue that explicitly modeling these atomic semantic units could improve both the performance and the interpretability of language models.</p><p>Linguists assume that there is a limited close set of atomic semantic units composing the se- mantic meanings of an open set of concepts (i.e. word senses). These atomic semantic units are named sememes ( <ref type="bibr" target="#b8">Dong and Dong, 2006</ref>). i Since sememes are naturally implicit in human lan- guages, linguists have devoted much effort to ex- plicitly annotate lexical sememes for words and build linguistic common-sense knowledge bases. HowNet ( <ref type="bibr" target="#b8">Dong and Dong, 2006</ref>) is one of the representative sememe knowledge bases, which annotates each Chinese word sense with its se- memes. The philosophy of HowNet regards the parts and attributes of a concept can be well rep- resented by sememes. HowNet has been widely utilized in many NLP tasks such as word similar- ity computation ( <ref type="bibr" target="#b25">Liu, 2002</ref>) and sentiment analy- sis ( <ref type="bibr" target="#b9">Fu et al., 2013)</ref>. However, less effort has been devoted to exploring its effectiveness in language models, especially neural language models.</p><p>It is non-trivial for neural language models to incorporate discrete sememe knowledge, as it is not compatible with continuous representations in neural models. In this paper, we propose a Sememe-Driven Language Model (SDLM) to leverage lexical sememe knowledge. In order to predict the next word, we design a novel sememe- sense-word generation process: (1) We first esti- mate sememes' distribution according to the con- text. (2) Regarding these sememes as experts, we propose a sparse product of experts method to se- lect the most probable senses. (3) Finally, the dis- tribution of words could be easily calculated by marginalizing out the distribution of senses.</p><p>We evaluate the performance of SDLM on the language modeling task using a Chinese news- i Note that although sememes are defined as the mini- mum semantic units, there still exist several sememes for capturing syntactic information. For example, the word å "with" corresponds to one specific sememe ü˝Õ "FunctWord". paper corpus People's Daily ii (Renmin Ribao), and also on the headline generation task using the Large Scale Chinese Short Text Summarization (LCSTS) dataset ( <ref type="bibr" target="#b19">Hu et al., 2015)</ref>. Experimen- tal results show that SDLM outperforms all those data-driven baseline models. We also conduct case studies to show that our model can effectively pre- dict relevant sememes given context, which can improve the interpretability and robustness of lan- guage models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Language models target at learning the joint probability of a sequence of words P (w 1 , w 2 , · · · , w n ), which is usually factor- ized as P (w 1 , w 2 , · · · , w n ) = Q n t=1 P (w t |w &lt;t ). <ref type="bibr" target="#b3">Bengio et al. (2003)</ref> propose the first Neural Lan- guage Model as a feed-forward neural network. <ref type="bibr">Mikolov et al. (2010)</ref> use RNN and a softmax layer to model the conditional probability. To be specific, it can be divided into two parts in series. First, a context vector g t is derived from a deep recurrent neural network. Then, the probability P (w t+1 |w t ) = P (w t+1 ; g t ) is derived from a linear layer followed by a softmax layer based on g t . Let RNN(·, ·; ✓ NN ) denote the deep recurrent neural network, where ✓ NN denotes the parameters. The first part can be formulated as</p><formula xml:id="formula_0">g t = RNN(x w t , {h t1 l } L l=1 ; ✓ NN ).<label>(1)</label></formula><p>Here we use subscripts to denote layers and su- perscripts to denote timesteps. Thus h t l represents the hidden state of the L-th layer at timestep t. x w t 2 R H 0 is the input embedding of word w t where H 0 is the input embedding size. We also have g t 2 R H 1 , where H 1 is the dimension of the context vector.</p><p>Supposing that there are N words in the lan- guage we want to model, the second part can be written as</p><formula xml:id="formula_1">P (w t+1 ; g t ) = exp(g t T w w t+1 ) P w 0 exp(g t T w w 0 ) ,<label>(2)</label></formula><p>where w w is the output embedding of word w and</p><formula xml:id="formula_2">w 1 , w 2 , · · · w N 2 R H 2 .</formula><p>Here H 2 is the output embedding size. For a conventional neural lan- guage model, H 2 always equals to H 1 .</p><p>ii http://paper.people.com.cn/rmrb/ () "apple(fruit)" () "apple(computer)" () "pear(fruit)" "apple" … … "pear" P(word) P(sense) "fruit" "bring" "computer" "SpeBrand" "able" "PatternVal" Given the corpus {w t } n t=1 , the loss function is defined by the negative log-likelihood:</p><formula xml:id="formula_3">L(✓) = 1 n n X t=1 log P (w t |w &lt;t ; ✓),<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">✓ = {{x i } N i=1 , {w i } N i=1</formula><p>, ✓ NN } is the set of parameters that are needed to be trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we present our SDLM which uti- lizes sememe information to predict the probabil- ity of the next word. SDLM is composed of three modules in series: Sememe Predictor, Sense Pre- dictor and Word Predictor. The Sememe Predictor first takes the context vector as input and assigns a weight to each sememe. Then each sememe is re- garded as an expert and makes predictions about the probability distribution over a set of senses in the Sense Predictor. Finally, the probability of each word is obtained in the Word Predictor.</p><p>Here we use an example shown in <ref type="figure" target="#fig_1">Figure 2</ref> to illustrate our architecture. Given context ⌘(ú ÌX "In the orchard, I pick", the actual next word could be ˘ú "apples". From the context, espe- cially the word úÌ "orchard" and X "pick", we can infer that the next word probably represents a kind of fruit. So the Sememe Predictor assigns a higher weight to the sememe 4ú "fruit" (0.9) and lower weights to irrelevant sememes like 5 ⌘ "computer" (0.1). Therefore in the Sense Pre- dictor, the sense ˘ú (4ú) "apple (fruit)" is as- signed a much higher probability than the sense ˘ ú (5⌘) "apple (computer)". Finally, the prob- ability of the word ˘ú "apple" is calculated as the sum of the probabilities of its senses ˘ú (4 "apple" "PatternVal" "computer" "able" "bring" "SpeBrand" "fruit" ú) "apple(fruit)" and ˘ú (5⌘) "apple (com- puter)".</p><formula xml:id="formula_5">m</formula><p>In the following subsections, we first introduce the word-sense-sememe hierarchy in HowNet, and then give details about our SDLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word-Sense-Sememe Hierarchy</head><p>We also use the example of "apple" to illustrate the word-sense-sememe hierarchy. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the word ˘ú "apple" has two senses, one is the Apple brand, the other is a kind of fruit. Each sense is annotated with several sememes or- ganized in a hierarchical structure. More specifi- cally, in HowNet, sememes "PatternVal", "bring", "SpeBrand", "computer" and "able" are annotated with the word "apple" and organized in a tree structure. In this paper, we ignore the structural relationship between sememes. For each word, we group all its sememes as an unordered set.</p><p>We present the notations that we use in the fol- lowing subsections as follows. We define the over- all sememe, sense, and word set as E, S and W. And we suppose the corpus contains K = |E| se- memes, M = |S| senses and N = |W| words. For word w 2 W, we denote its corresponding sense set as S (w) . For sense s 2 S (w) , we de- note its corresponding sememes as an unordered set</p><formula xml:id="formula_6">E (s) = {e n 1 , e n 2 , · · · , e n k } ⇢ E = {e k } K k=1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sememe Predictor</head><p>The Sememe Predictor takes the context vec- tor g 2 R H 1 as input and assigns a weight to each sememe. We assume that given the context w 1 , w 2 , · · · , w t1 , the events that word w t con- tains sememe e k (k 2 {1, 2, · · · , K}) are indepen- dent, since the sememe is the minimum semantic unit and there is no semantic overlap between any two different sememes. For simplicity, we ignore the superscript t. We design the Sememe Predic- tor as a linear decoder with the sigmoid activation function. Therefore, q k , the probability that the next word contains sememe e k , is formulated as</p><formula xml:id="formula_7">q k = P (e k |g) = (g T v k + b k ),<label>(4)</label></formula><p>where v k 2 R H 1 , b k 2 R are trainable parameters, and (·) denotes the sigmoid activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sense Predictor and Word Predictor</head><p>The architecture of the Sense Predictor is moti- vated by Product of Experts (PoE) <ref type="bibr" target="#b17">(Hinton, 1999)</ref>. We regard each sememe as an expert that only makes predictions on the senses connected with it.</p><p>Let D (e k ) denote the set of senses that contain se- meme e k , the k-th expert. Different from conven- tional neural language models, which directly use the inner product of the context vector g 2 R H 1 and the output embedding w w 2 R H 2 for word w to generate the score for each word, we use (k) (g, w) to calculate the score given by expert e k . And we choose a bilinear function parame- terized with a matrix U k 2 R H 1 ⇥H 2 as a straight implementation of (k) (·, ·):</p><formula xml:id="formula_8">(k) (g, w) = g T U k w.<label>(5)</label></formula><p>Let w s denote the output embedding of sense s. The score of sense s provided by sememe ex- pert e k can be written as (k) (g, w s ). Therefore, P (e k ) (s|g), the probability of sense s given by ex- pert e k , is formulated as</p><formula xml:id="formula_9">P (e k ) (s|g) = exp(q k C k,s (k) (g, ws)) P s 0 2D (e k ) exp(q k C k,s 0 (k) (g, w s 0 )) ,<label>(6)</label></formula><p>where C k,s is a normalization constant because sense s is not connected to all experts (the connections are sparse with approximately N edges, &lt; 5). Here we can choose either</p><formula xml:id="formula_10">C k,s = 1/|E (s) | (left normalization) or C k,s = 1/ p |E (s) ||D (e k ) | (symmetric normalization).</formula><p>In the Sense Predictor, q k can be viewed as a gate which controls the magnitude of the term C k,s (k) (g, w ws ), thus control the flatness of the sense distribution provided by sememe expert e k . Consider the extreme case when q k ! 0, the pre- diction will converge to the discrete uniform dis- tribution. Intuitively, it means that the sememe ex- pert will refuse to provide any useful information when it is not likely to be related to the next word.</p><p>Finally, we summarize the predictions on sense s by taking the product of the probabilities given by relevant experts and then normalize the result; that is to say, P (s|g), the probability of sense s, satisfies</p><formula xml:id="formula_11">P (s|g) / Y e k 2E (s) P (e k ) (s|g).<label>(7)</label></formula><p>Using Equation 5 and 6, we can formulate P (s|g) as</p><formula xml:id="formula_12">P (s|g) = exp( P e k 2E (s) q k C k,s g T U k ws) P s 0 exp( P e k 2E (s 0 ) q k C k,s 0 g T U k w s 0 )</formula><p>. <ref type="formula">(8)</ref> It should be emphasized that all the supervision information provided by HowNet is embodied in the connections between the sememe experts and the senses. If the model wants to assign a high probability to sense s, it must assign a high prob- ability to some of its relevant sememes. If the model wants to assign a low probability to sense s, it can assign a low probability to its relevant sememes. Moreover, the prediction made by se- meme expert e k has its own tendency because of its own (k) (·, ·). Besides, the sparsity of con- nections between experts and senses is also de- termined by HowNet itself. For our dataset, on average, a word is connected with 3.4 sememe ex- perts and each sememe expert will make predic- tions about 22 senses.</p><p>As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, in the Word Predic- tor, we get P (w|g), the probability of word w, by summing up probabilities of corresponding s given by the Sense Predictor, that is</p><formula xml:id="formula_13">P (w|g) = X s2S (w) P (s|g).<label>(9)</label></formula><p>3. </p><formula xml:id="formula_14">U k = R X r=1 ↵ k,r Qr,<label>(10)</label></formula><p>where Q r 2 R H 1 ⇥H 2 , ↵ k,r &gt; 0 are trainable pa- rameters, and P R r=1 ↵ k,r = 1. Weight Tying To incorporate the weight tying strategy <ref type="bibr" target="#b20">(Inan et al., 2017;</ref><ref type="bibr" target="#b36">Press and Wolf, 2017)</ref>, we use the same output embedding for multiple senses of a word. To be specific, the sense output embedding w s for each s 2 S (w) is the same as the word input embedding x w .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our SDLM on a Chinese language modeling dataset, namely People's Daily based on perplexity. iii Furthermore, to show that our SDLM structure can be a generic Chinese word-level de- coder for sequence-to-sequence learning, we con- duct a Chinese headline generation experiment on the LCSTS dataset. Finally, we explore the inter- pretability of our model with cases, showing the effectiveness of utilizing sememe knowledge. as a three-layer neural network, which serves as a very strong baseline for word-level language mod- eling. We build it with the code released by the authors v .</p><p>Variants of Softmax Meanwhile, to compare our SDLM with other language modeling decoders, we set cHSM (Class-based Hierarchical Softmax) <ref type="bibr" target="#b12">(Goodman, 2001</ref>), tHSM (Tree-based Hierarchi- cal Softmax) ( <ref type="bibr" target="#b30">Mikolov et al., 2013</ref>) and MoS (Mixture of Softmaxes) ( <ref type="bibr" target="#b41">Yang et al., 2018)</ref> as the baseline add-on structures to the architectures above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Settings</head><p>We apply our SDLM and other variants of softmax structures to the architectures mentioned above: LSTM (medium / large), Tied LSTM (medium / large) and AWD-LSTM. MoS and SDLM are only applied on the models that incorporate weight ty- ing, while tHSM is only applied on the models without weight tying, since it is not compatible with this strategy. For a fair comparison, we train these mod- els with same experimental settings and conduct a hyper-parameter search for baselines as well as our models (the search setting and the opti- mal hyper-parameters can be found in Appendix C.1). We keep using these hyper-parameters in our SDLM for all architectures. It should be empha- sized that we use the SGD optimizer for all archi- tectures, and we decrease the learning rate by a factor of 2 if no improvement is observed on the validation set. We uniformly initialize the word embeddings, the class embeddings for cHSM and the non-leaf embeddings for tHSM in [0.1, 0.1]. In addition, we set R, the number of basis matri- ces, to 5 in Tied LSTM architecture and to 10 in AWD-LSTM architecture. We choose the left nor- malization strategy because it performs better. <ref type="table" target="#tab_3">Table 1</ref> shows the perplexity on the validation and test set of our models and the baseline models. From <ref type="table" target="#tab_3">Table 1</ref>, 2, and 3, we can observe that: 1. Our models outperform the corresponding base- line models of all structures, which indicates the effectiveness of our SDLM. Moreover, our SDLM not only consistently outperforms state-of-the-art MoS model, but also offers much better inter- pretability (as described in Sect. 4.3), which makes it possible to interpret the prediction pro- cess of the language model. Note that under a fair comparison, we do not see MoS's improvement over AWD-LSTM while our SDLM outperforms it by 1.20 with respect to perplexity on the test set. 2. To further locate the performance improve- ment of our SDLM, we study the perplexity of the single-sense words and multi-sense words sep- arately on Tied LSTM (medium) and Tied LSTM (medium) + SDLM. Improvements with respect to perplexity are presented in <ref type="table" target="#tab_4">Table 2</ref>. The perfor- mance on both single-sense words and multi-sense words gets improved while multi-sense words benefit more from SDLM structure because they have richer sememe information. 3. In <ref type="table" target="#tab_6">Table 3</ref> we study the perplexity of words with different mean number of sememes. We can see that our model outperforms baselines in all cases and is expected to benefit more as the mean number of sememes increases.   We also test the robustness of our model by ran- domly removing 10% sememe-sense connections in HowNet. The test perplexity for Tied LSTM iv We find that multi-layer AWD-LSTM has problems con- verging when adopting cHSM, so we skip that result.   (medium) + SDLM slightly goes up to 97.67, com- pared to 97.32 with a complete HowNet, which shows that our model is robust to tiny incomplete- ness of annotations. However, the performance of out model is still largely dependent upon the accuracy of sememe annotations. As HowNet is continuously updated, we expect our model to perform better with sememe knowledge of higher quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Headline Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>We use the LCSTS dataset to evaluate our SDLM structure as the decoder of the sequence-to- sequence model. As its author suggests, we di- vide the dataset into the training set, the validation set and the test set, whose sizes are 2.4M, 8.7k and 725 respectively. Details can be found in Ap- pendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>For this task, we consider two models for compar- ison. RNN-context As described in ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>, RNN-context is a basic sequence-to- sequence model with a bi-LSTM encoder, an LSTM decoder and attention mechanism adopted. The context vector is concatenated with the word embedding at each timestep when decoding. It's widely used for sequence-to-sequence learning, so we set it as the baseline model.</p><p>RNN-context-SDLM Based on RNN-context, we substitute the decoder with our proposed SDLM and name it RNN-context-SDLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Settings</head><p>We implement our models with PyTorch, on top of the OpenNMT libraries v . For both models, we set the word embedding size to 250, the hidden unit size to 250, the vocabulary size to 40000, and the beam size of the decoder to 5. For RNN-context- SDLM, we set the number of basis matrices to 3. We conduct a hyper-parameter search for both models (see Appendix C.2 for settings and optimal hyper-parameters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>Following previous works, we report the F1-score of ROUGE-1, ROUGE-2, and ROUGE-L on the test set. <ref type="table" target="#tab_1">Table 4</ref> shows that our model outperforms the baseline model on all metrics. We attribute the improvement to the use of SDLM structure.</p><p>Words in headlines do not always appear in the corresponding articles. However, words with the same sememes have a high probability to appear in the articles intuitively. Therefore, a probable rea- son for the improvement is that our model could predict sememes highly relevant to the article, thus generate more accurate headlines. This could be corroborated by our case study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Rouge  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Case Study</head><p>The above experiments demonstrate the effective- ness of our SDLM. Here we present some samples from the test set of the People's Daily Corpus in <ref type="table">Table 5</ref> as well as the LCSTS dataset in <ref type="table" target="#tab_9">Table 6</ref> and conduct further analysis.</p><p>For each example of language modeling, given the context of previous words, we list the Top 5 words and Top 5 sememes predicted by our SDLM. The target words and the sememes anno- tated with them in HowNet are blackened. Note that if the target word is an out-of-vocabulary Example (1) ªt é˝ 8◆⌃Ó e 0° : &lt;N&gt; ⇥ The U.S. trade deficit last year is initially estimated to be &lt;N&gt; . Top 5 word prediction é é é C C C "dollar" "," ⇥ "." ÂC "yen" å "and" Top 5 sememe prediction F F F ⇢ ⇢ ⇢ "commerce" ---ç ç ç "finance" U U U M M M "unit" ⇢⌘ "amount" ◆ "proper name" Example (2) ? ;⌃ Ú ~r Ü Ü y }‰ ⇥ Albanian Prime Minister has signed an order. Top 5 word prediction Ö "inside" &lt;unk&gt; ( "at" T "tower" å "and" Top 5 sememe prediction ? ? ? "politics" ∫ ∫ ∫ "person" ±I "flowers" ≈ ≈ ≈ ˚ ˚ ˚ "undertake" 4fl "waters" <ref type="table">Table 5</ref>: Some examples of word and sememe predic- tions on the test set of the People's Daily Corpus.</p><p>(OOV) word, helpful sememes that are related to the target meaning are blackened.</p><p>Sememes annotated with the corresponding sense of the target word éC "dollar" are UM "unit", F⇢ "commerce", -ç "finance", ' "money" and é˝ "US". In Example (1), the tar- get word "dollar" is predicted correctly and most of its sememes are activated in the predicting pro- cess. It indicates that our SDLM has learned the word-sense-sememe hierarchy and used sememe knowledge to improve language modeling.</p><p>Example <ref type="formula" target="#formula_1">(2)</ref> shows that our SDLM can provide interpretable results on OOV word prediction with sememe information associated with it. The tar- get word here should be the name of the Albanian prime minister, which is out of vocabulary. But with our model, one can still conclude that this word is probably relevant to the sememe "poli- tics", "person", "flowers", "undertake" and "wa- ters", most of which characterize the meaning of this OOV word -the name of a politician. This feature can be helpful when the vocabulary size is limited or there are many terminologies and names in the corpus.</p><p>For the example of headline generation, given the article and previous words, when generating the word "student", except the sememe Ñô "predict", all other Top 5 predicted sememes have high relevance to either the predicted word or the context. To be specific, the sememe f` "study" is annotated with "student" in HowNet. ⇤ ' "exam" indicates "college entrance exam". y ö L P "brand" indicates "BMW". And ÿ I "higher" indicates "higher education", which is the next step after this exam. We can conclude that with sememe knowledge, our SDLM structure can extract critical information from both the given ar- ticle and generated words explicitly and produce better summarization based on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Neural Language Modeling. RNNs have achieved state-of-the-art performance in the language modeling task since <ref type="bibr">Mikolov et al. (2010)</ref> first apply RNNs for language modeling. Much work has been done to improve RNN-based language modeling. For example, a variety of work ( <ref type="bibr" target="#b42">Zaremba et al., 2014;</ref><ref type="bibr" target="#b10">Gal and Ghahramani, 2016;</ref><ref type="bibr" target="#b28">Merity et al., 2017</ref><ref type="bibr" target="#b27">Merity et al., , 2018</ref>) introduces many regularization and optimization methods for RNNs. Based on the observation that the word</p><formula xml:id="formula_15">Article 8 Â ⌧ ∞ ùl f ¬ † ÿ⇤ Ñ 7 ⇤ ⇤: \⌦´ì\⌦´\⌦´ì ‡ · -⇤ ⇤ °6 \⌦ K : OE Ã ⇢ ⌃ s -⇤ ⇤ OE í 9 0 ≤ ⇥ v " ˙ ¬ ⇢ " ` ÂS ⌘ 8 / J ` 1 Â ⌘ " ÓM S∫ ⇤ Ú ´ÿY ⇥<label>On</label></formula><p>the 8th in Fuxin, a male student drove a BMW to take the college entrance exam and was caught cheating. Because the teacher confiscated his mobile phone, he kicked the teacher from the last row to the podium and shouted: "Do you know who my dad is? How dare you catch me!" Currently, this student has been detained. Gold 77 ÿ⇤ \⌦ ˝S -⇤ ⇤ ⇢` ÂS ⌘ 8 / In the college entrance exam, a male student caught cheating hit the teacher: Do you know who my dad is?</p><p>RNN-context-SDLM ÿ⇤ \⌦´ì\⌦´\⌦´ì ⇢` ÂS ⌘ 8 / J In the college entrance exam, a student was caught cheating: Do you know who my dad is?</p><p>Top 5 sememe prediction ⇤ ⇤ ⇤ ' ' ' "exam" f f f ` ` ` "study" y y y ö ö ö L L L P P P "brand" Ñô "predict" ÿ ÿ ÿ I I I "higher" appearing in the previous context is more likely to appear again, some work ( <ref type="bibr">Grave et al., 2017a,b)</ref> proposes to use cache for improvements. In this paper, we mainly focus on the output decoder, the module between the context vector and the predicted probability distribution. Similar to our SDLM, <ref type="bibr" target="#b41">Yang et al. (2018)</ref> propose a high-rank model which adopts a Mixture of Softmaxes structure for the output decoder. However, our model is sememe-driven with each expert corresponding to an interpretable sememe.</p><p>Hierarchical Decoder Since softmax computa- tion on large vocabulary is time-consuming, there- fore being a dominant part of the model's com- plexity, various hierarchical softmax models have been proposed to address this issue. These mod- els can be categorized to class-based models and tree-based models according to their hierarchi- cal structure. Goodman (2001) first proposes the class-based model which divides the whole vocab- ulary into different classes and uses a hierarchi- cal softmax decoder to model the probability as P(word) = P(word|class)P(class), which is sim- ilar to our model. For the tree-based models, all words are organized in a tree structure and the word probability is calculated as the probability of always choosing the correct child along the path from the root node to the word node. While Morin and <ref type="bibr" target="#b33">Bengio (2005)</ref> utilize knowledge from Word- Net to build the tree, <ref type="bibr" target="#b32">Mnih and Hinton (2008)</ref> build it in a bootstrapping <ref type="bibr">way and Mikolov et al. (2013)</ref> construct a Huffman Tree based on word frequencies. Recently, <ref type="bibr" target="#b21">Jiang et al. (2017)</ref> reform the tree-based structure to make it more efficient on GPUs. The major differences between our model and theirs are the purpose and the moti- vation. Our model targets at improving the per- formance and interpretability of language model- ing using external knowledge in HowNet. There- fore, we take its philosophy of the word-sense- sememe hierarchy to design our hierarchical de- coder. Meanwhile, the class-based and tree-based models are mainly designed to speed up the soft- max computation in the training process.</p><p>Sememe. Recently, there are a lot of works con- centrating on utilizing sememe knowledge in tra- ditional natural language processing tasks. For ex- ample, <ref type="bibr" target="#b34">Niu et al. (2017)</ref> use sememe knowledge to improve the quality of word embeddings and cope with the problem of word sense disambigua- tion.  apply matrix factorization to predict sememes for words. <ref type="bibr" target="#b22">Jin et al. (2018)</ref> im- prove their work by incorporating character-level information. Our work extends the previous works and tries to combine word-sense-sememe hierar- chy with the sequential model. To be specific, this is the first work to improve the performance and interpretability of Neural Language Modeling with sememe knowledge.</p><p>Product of Experts. As Hinton <ref type="bibr">(1999,</ref><ref type="bibr">2002)</ref> propose, the final probability can be calculated as the product of probabilities given by experts. <ref type="bibr" target="#b11">Gales and Airey (2006)</ref> apply PoE to the speech recog- nition where each expert is a Gaussian mixture model. Unlike their work, in our SDLM, each expert is mapped to a sememe with better inter- pretability. Moreover, as the final distribution is a categorical distribution, each expert is only re- sponsible for making predictions on a subset of the categories (usually less than 10), so we call it Sparse Product of Experts.</p><p>Headline Generation. Headline generation is a kind of text summarization tasks. In recent years, with the advances of RNNs, a lot of works have been done in this domain. The encoder- decoder models <ref type="bibr" target="#b7">Cho et al., 2014)</ref> have achieved great success in sequence- to-sequence learning. <ref type="bibr" target="#b38">Rush et al. (2015)</ref> pro- pose a local attention-based model for abstractive sentence summarization. <ref type="bibr" target="#b15">Gu et al. (2016)</ref> intro- duce the copying mechanism which is close to the rote memorization of the human being. <ref type="bibr" target="#b0">Ayana et al. (2016)</ref> employ the minimum risk training strategy to optimize model parameters. Different from these works, we focus on the decoder of the sequence-to-sequence model, and adopt SDLM to utilize sememe knowledge for sentence genera- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Further Work</head><p>In this paper, we propose an interpretable Sememe-Driven Language Model with a hier- archical sememe-sense-word decoder. Besides interpretability, our model also achieves state- of-the-art performance in the Chinese Language Modeling task and shows improvement in the Headline Generation task. These results indicate that SDLM can successfully take advantages of se- meme knowledge.</p><p>As for future work, we plan the following re- search directions: (1) In language modeling, given a sequence of words, a sequence of correspond- ing sememes can also be obtained. We will uti- lize the context sememe information for better se- meme and word prediction. (2) Structural infor- mation about sememes in HowNet is ignored in our work. We will extend our model with the hi- erarchical sememe tree for more accurate relations between words and their sememes. (3) It is imag- inable that the performance of SDLM will be sig- nificantly influenced by the annotation quality of sememe knowledge. We will also devote to fur- ther enrich the sememe knowledge for new words and phrases, and investigate its effect on SDLM.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Decoder of (a) Conventional Language Model, (b) Sememe-Driven Language Model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example of the architecture of our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example of the word-sense-sememe hierarchy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>[</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>4 .1 Language Modeling</head><label>4</label><figDesc></figDesc><table>Dataset 

We choose the People's Daily Corpus, which is 
widely used for Chinese NLP tasks, as the re-
source. It contains one month's news text from 
People's Daily (Renmin Ribao). Taking Penn 
Treebank (PTB) (Marcus et al., 1993) as a ref-
erence, we build a dataset for Chinese language 
modeling based on the People's Daily Corpus with 
734k, 10k and 19k words in the training, valida-
tion and test set. After the preprocessing similar 
to (Mikolov et al., 2010) (see Appendix A), we get 
our dataset and the final vocabulary size is 13,476. 

Baseline 

As for baselines, we consider three kinds of neural 
language modeling architectures with LSTM cells: 
simple LSTM, Tied LSTM and AWD-LSTM. 
LSTM and Tied LSTM Zaremba et al. (2014) 
use the dropout strategy to prevent overfitting for 
neural language models and adopt it to two-layer 
LSTMs with different embedding and hidden size: 
650 for medium LSTM, and 1500 for large LSTM. 
Employing the weight tying strategy, we get Tied 
LSTM with better performance. We set LSTM and 
Tied LSTM of medium and large size as our base-
line models and use the code from PyTorch exam-
ples iv as their implementations. 
AWD-LSTM Based on several strategies for reg-
ularizing and optimizing LSTM-based language 
models, Merity et al. (2018) propose AWD-LSTM 

iii Although we only conduct experiments on Chinese cor-
pora, we argue that this model has the potential to be ap-
plied to other languages in the light of works on construc-
tion sememe knowledge bases for other languages, such 
as (Qi et al., 2018). 
iv https://github.com/pytorch/examples/ 
tree/master/word_language_model 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Single model perplexity on validation and test 
sets on the People's Daily dataset. 

#senses = 1 #senses &gt; 1 
Baseline ppl 
93.21 
121.18 
SDLM ppl 
87.22 
111.88 
ppl 
5.99 
9.29 
ppl/Baseline ppl 
6.4% 
7.8% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Perplexity of words with different number of senses on the test set.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 : Perplexity of words with different mean num- ber of sememes on the test set.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 4 :</head><label>4</label><figDesc>ROUGE scores of both models on the LCSTS test set.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 :</head><label>6</label><figDesc>An example of generated headlines on the LC- STS test set.</figDesc><table></table></figure>

			<note place="foot">v https://github.com/salesforce/ awd-lstm-lm</note>

			<note place="foot">v http://opennmt.net</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Shiqi Shen Ayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01904</idno>
		<title level="m">Neural headline generation with minimum risk training</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Headline generation based on statistical translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vibhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Witbrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="318" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rejean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Information retrieval as statistical translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="222" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large language models in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A statistical approach to machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen A Della</forename><surname>Cocke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrick</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paul S Roossin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="85" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Hownet and the computation of meaning (with Cd-rom)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Dong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>World Scientific</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multi-aspect sentiment analysis for chinese online social reviews based on topic modeling and hownet lexicon. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianghua</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="186" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Product of gaussians for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Airey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="40" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Classes for fast maximum entropy training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="561" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unbounded cache model for online language modeling with open vocabulary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving neural language models with a continuous cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A linguistically motivated probabilistic model of information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djoerd</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TPDL</title>
		<meeting>TPDL</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="569" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Products of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks, 1999. ICANN 99. Ninth International Conference on</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lcsts: A large scale chinese short text summarization dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangze</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tying word vectors and word classifiers: A loss framework for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Hakan Inan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploration of tree-based hierarchical softmax for recurrent language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenge</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenge</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Incorporating chinese characters of words for lexical sememe prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyu</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2439" to="2449" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>Speech &amp; language processing. chapter 4. Pearson Education India</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Estimation of probabilities from sparse data for the language model component of a speech recognizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slava</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on acoustics, speech, and signal processing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="400" to="401" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Word similarity computing based on hownet. Computational linguistics and Chinese language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="59" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Regularizing and optimizing LSTM language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Revisiting activation regularization for language rnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.01009</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">JaňJaň Cernock`Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A hidden markov model information retrieval system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard M</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="214" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improved word representation learning with sememes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2049" to="2058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A language modeling approach to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Bruce</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Crosslingual lexical sememe prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanchao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Lexical sememe prediction via word embeddings and matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4200" to="4206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Breaking the softmax bottleneck: A high-rank rnn language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
