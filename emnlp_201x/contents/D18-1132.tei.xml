<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Translating a Math Word Problem to a Expression Tree</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Future Media and School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">UESTC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Future Media and School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">UESTC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Translating a Math Word Problem to a Expression Tree</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1064" to="1069"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1064</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Sequence-to-sequence (SEQ2SEQ) models have been successfully applied to automatic math word problem solving. Despite its simplicity, a drawback still remains: a math word problem can be correctly solved by more than one equations. This non-deterministic transduction harms the performance of maximum likelihood estimation. In this paper, by considering the uniqueness of expression tree, we propose an equation normalization method to normalize the duplicated equations. Moreover, we analyze the performance of three popular SEQ2SEQ models on the math word problem solving. We find that each model has its own specialty in solving problems, consequently an ensemble model is then proposed to combine their advantages. Experiments on dataset Math23K show that the ensemble model with equation normal-ization significantly outperforms the previous state-of-the-art methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Developing computer systems to automatically solve math word problems (MWPs) has been an interest of NLP researchers since 1963 <ref type="bibr" target="#b3">(Feigenbaum et al., 1963;</ref><ref type="bibr" target="#b1">Bobrow, 1964)</ref>. A typical MWP is shown in <ref type="table">Table 1</ref>. Readers are asked to infer how many pens and pencils Jessica have in total, based on the textual problem descrip- tion provided. Statistical machine learning-based methods ( <ref type="bibr" target="#b7">Kushman et al., 2014;</ref><ref type="bibr" target="#b0">Amnueypornsakul and Bhat, 2014;</ref><ref type="bibr" target="#b18">Zhou et al., 2015;</ref><ref type="bibr" target="#b9">Mitra and Baral, 2016;</ref><ref type="bibr" target="#b11">Roy and Roth, 2018)</ref> and semantic parsing-based methods ( <ref type="bibr" target="#b12">Shi et al., 2015;</ref><ref type="bibr">KoncelKedziorski et al., 2015;</ref><ref type="bibr" target="#b10">Roy and Roth, 2015;</ref><ref type="bibr" target="#b5">Huang et al., 2017</ref>) are proposed to tackle this problem, yet they still require considerable manual * The work was done when Lei Wang and Deng Cai were interns at Tencent AI Lab.</p><p>Problem: Dan has 5 pens and 3 pencils, Jes- sica has 4 more pens and 2 less pencils than him. How many pens and pencils does Jessica have in total? Equation: x = 5 + 4 + 3 − 2; Solution: 10 <ref type="table">Table 1</ref>: A math word problem efforts on feature or template designing. For more literatures about solving math word problems au- tomatically, refer to a recent survey paper .</p><p>Recently, the Deep Neural Networks (DNNs) have opened a new direction towards automatic MWP solving. <ref type="bibr" target="#b8">Ling et al. (2017)</ref> take multiple- choice problems as input and automatically gener- ate rationale text and the final choice.  then make the first attempt of applying deep reinforcement learning to arithmetic word problem solving. <ref type="bibr" target="#b8">Wang et al. (2017)</ref> train a deep neural solver (DNS) that needs no hand-crafted features, using the SEQ2SEQ model to automati- cally learn the problem-to-equation mapping.</p><p>Although promising results have been reported, the model in ( <ref type="bibr" target="#b8">Wang et al., 2017</ref>) still suffers from an equation duplication problem: a MWP can be solved by multiple equations. Taking the prob- lem in <ref type="table">Table 1</ref> as an example, it can be solved by various equations such as x = 5 + 4 + 3 − 2, x = 4 + (5 − 2) + 3 and x = 5 − 2 + 3 + 4. This duplication problem results in a non-deterministic output space, which has a negative impact on the performance of most data-driven methods. In this paper, by considering the uniqueness of expres- sion tree, we propose an equation normalization method to solve this problem.</p><p>Given the success of different SEQ2SEQ mod- els on machine translation (such as recurrent encoder-decoder ( <ref type="bibr" target="#b16">Wu et al., 2016</ref>), Convolutional SEQ2SEQ model ( <ref type="bibr" target="#b4">Gehring et al., 2017)</ref> and Trans-former ( <ref type="bibr" target="#b13">Vaswani et al., 2017)</ref>), it is promising to adapt them to MWP solving. In this paper, we compare the performance of three state-of-the-art SEQ2SEQ models on MWP solving. We observe that different models are able to correctly solve different MWPs, therefore, as a matter of course, an ensemble model is proposed to achieve higher performance. Experiments on dataset Math23K show that by adopting the equation normaliza- tion and model ensemble techniques, the accuracy boosts from 60.7% to 68.4%.</p><p>The remaining part of this paper is organized as follows: we first introduce the SEQ2SEQ Frame- work in Section 2. Then the equation normaliza- tion process is presented in Section 3, following which three SEQ2SEQ models and an ensemble model are applied to MWP solving in Section 4. The experimental results are presented in Section 5. Finally we conclude this paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SEQ2SEQ Framework</head><p>The process of using SEQ2SEQ model to solve MWPs can be divided into two stages ( <ref type="bibr" target="#b8">Wang et al., 2017</ref>). In the first stage (number mapping stage), significant numbers (numbers that will be used in real calculation) in problem P are mapped to a list of number tokens {n 1 , . . . , n m } by their natural order in the problem text. Throughout this paper, we use the significant number identification (SNI) module proposed in ( <ref type="bibr" target="#b8">Wang et al., 2017</ref>) to iden- tify whether a number is significant. In the second stage, SEQ2SEQ models can be trained by taking the problem text as the source sequence and equa- tion templates (equations after number mapping) as the target sequence.</p><p>Taking the problem P in <ref type="table">Table 1</ref> as an example, first we can obtain a number mapping M : {n 1 = 5; n 2 = 3; n 3 = 4; n 4 = 2; }, and trans- form the given equation E P : x = 5 + 4 + 3 − 2 to an equation template T P : x = n 1 + n 3 + n 2 − n 4 . During training, the objective of our SEQ2SEQ model is to maximize the conditional probabil- ity P (T p |P ), which will be decomposed to token- wise probabilities. During decoding, we use beam search to approximate the most likely equation template. After that, we replace the number tokens with actual numbers and calculate the solution S with a math solver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Equation Normalization</head><p>In the number mapping stage, the equations E P have been successfully transformed to equation templates T P . However, due to the equation du- plication problem introduced in Section 1, this problem-equation templates formalization is a non-deterministic transduction that will have ad- verse effects on the performance of maximum likelihood estimation. There are two types of equation duplication: 1) order duplication such as "n1 + n3 + n2" and "n1 + n2 + n3", 2) bracket duplication such as "n 1 + n 3 − n 2 " and "n1 + (n 3 − n 2 )".</p><p>To normalize the order-duplicated templates, we define two normalization rules:</p><p>• Rule 1: Two duplicated equation templates with unequal length should be normalized to the shorter one. For example, two equation templates "n 1 + n 2 + n 3 + n 3 − n 3 ", "n 1 + n 2 + n 3 " should be normalized to the latter one.</p><p>• Rule 2: The number tokens in equation tem- plates should be ordered as close as possible to their order in number mapping. For exam- ple, three equation templates "n 1 + n 3 + n 2 ", "n 1 + n 2 + n 3 " and "n 3 + n 1 + n 2 " should be normalized to "n 1 + n 2 + n 3 ".</p><p>To solve the bracket duplication problem, we further normalize the equation templates to an ex- pression tree. Every inner node in the expres- sion tree is an operator with two children, while each leaf node is expected to be a number to- ken. An example of expressing equation template n 1 + n 2 + n 3 − n 4 as the unique expression tree is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Models</head><p>In this section, we present three types of SEQ2SEQ models to solve MWPs: bidirectional Long Short Term Memory network (BiLSTM) ( <ref type="bibr" target="#b16">Wu et al., 2016</ref>), Convolutional SEQ2SEQ model ( <ref type="bibr" target="#b4">Gehring et al., 2017)</ref>, and Transformer ( <ref type="bibr" target="#b13">Vaswani et al., 2017)</ref>. To benefit the output accuracy with all three architectures, we propose to use a simple en- semble method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">BiLSTM</head><p>The BiLSTM model uses two LSTMs (forward and backward) to learn the representation of each token in the sequence based on both the past and the future context of the token. At each time step of decoding, the deocder uses a global attention mechanism to read those representations.</p><p>In more detail, we use two-layer Bi-LSTM cells with 256 hidden units as encoder, and two lay- ers LSTM cells with 512 hidden units as decoder. In addition, we use Adam optimizer with learning rate 1e −3 , β 1 = 0.9, and β 2 = 0.99. The epochs, minibatch size, and dropout rate are set to 100, 64, and 0.5, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ConvS2S</head><p>ConvS2S ( <ref type="bibr" target="#b4">Gehring et al., 2017</ref>) uses a convolu- tional architecture instead of RNNs. Both encoder and decoder share the same convolutional struc- ture that uses n kernels striding from one side to the other, and uses gate linear units as non- linearity activations over the output of convolu- tion.</p><p>Our ConvS2S model adopts a four layers en- coder and a three layers decoder, both using ker- nels of width 3 and hidden size 256. We adopt early stopping and learning rate annealing and set max-epochs equals to 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Transformer</head><p>Vaswani et al. <ref type="formula">(2017)</ref> proposed the Transformer based on an attention mechanism without rely- ing on any convolutional or recurrent architecture. Both encoder and decoder are composed of a stack of identical layers. Each layer contains two parts: a multi-head self-attention module and a position- wise fully-connected feed-forward network.</p><p>Our transformer is four layers deep, with n head = 16, d k = 12, d v = 32, and d model = 512, where n head is the number of heads of its self- attention, d k is the dimension of keys, d v is the di- mension of values, and d model is the output dimen- sion of each sub-layer. In addition, we use Adam optimizer with learning rate 1e −9 , β 1 = 0.9, β 2 = 0.99, and dropout rate of 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ensemble Model</head><p>Through careful observation (detailed in Section 5.2), we find that each model has a speciality in solving problems. Therefore, we propose an en- semble model which selects the result according to models' generation probability:</p><formula xml:id="formula_0">p(y) = T t=1 p(y t |y &lt;t , x)</formula><p>where y = {y 1 , ..., y T } is the target sequence, and x = {x 1 , ..., x S } is the source sequence. Finally, the output of the model with the highest generation probability is selected as the final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head><p>In this section, we conduct experiments on dataset Math23K to examine the performance of differ- ent SEQ2SEQ models. Our main experimental result is to show a significant improvement over the baseline methods. We further conduct a case study to analyze why different SEQ2SEQ models can solve different kinds of MWPs.</p><p>Dataset: Math23K 1 collected by <ref type="bibr" target="#b8">Wang et al. (2017)</ref>    problems are linear algebra questions with only one unknown variable. Baselines: We compare our methods with two baselines: DNS and DNS-Hybrid. Both of them are proposed in ( <ref type="bibr" target="#b8">Wang et al., 2017)</ref>, with state- of-the-art performance on dataset Math23K. The DNS is a vanilla SEQ2SEQ model that adopts GRU ( <ref type="bibr" target="#b2">Chung et al., 2014</ref>) as encoder and LSTM as decoder. The DNS-Hybrid is a hybrid model that combines DNS and a retrieval-based solver to achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results</head><p>In experiments, we use the testing set in Math23K as the test set, and randomly split 1, 000 prob- lems from the training set as validation set. Eval- uation results are summarized in <ref type="table" target="#tab_1">Table 2</ref>. First, to examine the effectiveness of equation nor- malization, model performance with and with- out equation normalization are compared. Then the performance of DNS, DNS-Hybrid, Bi-LSTM, ConvS2S, Transformer, and Ensemble model are examined on the dataset.</p><p>Several observations can be made from the re- sults. First, the equation normalization process significantly improves the performance of each model. The accuracy of different models gain increases from 2.7% to 7.1% after equation nor- malization. Second, Bi-LSTM, ConvS2S, Trans- former can achieve much higher performance than DNS, which means that popular machine transla- tion models are also efficient in automatic MWP solving. Third, by combining the SEQ2SEQ mod- els, our ensemble model gains additional 1.7% in- crease on accuracy.</p><p>In addition, we have further conducted three ex- tra experiments to disentangle the benefits of three different EN techniques. <ref type="table" target="#tab_2">Table 3</ref> gives the details of the ablation study of the three SEQ2SEQ mod- els. Taking Bi-LSTM as an example, accuracies of rule 1 (SE), rule 2 (OE) and eliminating brackets (EB) are 63.1%, 63.7% and 65.3%, respectively. Obviously, the performance of SEQ2ESQ models benefits from the equation normalization technolo- gies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Case Study</head><p>Further, we conduct a case analysis on the capa- bility of different SEQ2SEQ models and provide three examples in <ref type="table">Table 4</ref>. Our analysis is summa- rized as follows: 1) Transformer occasionally gen- erates mathematically incorrect templates, while Bi-LSTM and ConvS2S almost do not, as shown in Example 1. This is probably because the size of training data is still not enough to train the multi-head self-attention structures; 2) In Exam- ple 2, the Transformer is adapted to solve prob- lems that require complex inference. It is mainly because different heads in a self-attention structure can model various types of relationships between number tokens; 3) The multi-layer convolutional block structure in ConvS2S can properly process the context information of number tokens. In Ex- ample 3, it is the only one that captures the rela- tionship between stamp A and stamp B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we first propose an equation normal- ization method that normalizes duplicated equa- tion templates to an expression tree. We test dif- ferent SEQ2SEQ models on MWP solving and pro- pose an ensemble model to achieve higher per- formance. Experimental results demonstrate that the proposed equation normalization method and the ensemble model can significantly improve the state-of-the-art methods.</p><p>Example 1: Two biological groups have produced 690 (n 1 ) butterfly specimens in 15 (n 2 ) days. The first group produced 20 (n 3 ) each day. How many did the second group produced each day? Bi-LSTM: n 1 n 2 /n 3 −; (correct) ConvS2S: n 2 n 3 + n 1 * ; (error) Transformer: n 2 n 1 n 3 n 3 +; (error) Example 2: A plane, in a speed of 500 (n 1 ) km/h, costs 3 (n 2 ) hours traveling from city A to city B. It only costs 2 (n 3 ) hours for return. How much is the average speed of the plane during this round-trip? Bi-LSTM: n 1 n 2 * n 3 * ; (error) ConvS2S: 11 + 1n 1 /1n 2 / + /; (error) Transformer: n 1 n 2 * n 3 * n 2 n 3 + /; (correct) Example 3: Stamp A is 2 (n 1 ) paise denomination, and stamp B is 7 (n 2 ) paise denomination. If we are asked to buy 10 (n 3 ) of each, how much more does it cost to buy stamps A than to buy stamps B. Bi-LSTM: n 1 n 2 n 3 * −; (error) ConvS2S: n 1 n 3 * n 2 n 3 * −; (correct) Transformer: n 2 n 2 * n 2 n 3 * −; (error) <ref type="table">Table 4</ref>: Three examples of solving MWP with SEQ2SEQ model. Note that the results are postorder traversal of expression trees, and the problems are translated to English for brevity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A Unique Expression Tree After equation normalization, the SEQ2SEQ models can solve MWPs by taking problem text as source sequence and the postorder traversal of an unique expression tree as target sequence, as shown in Figure 2.</figDesc><graphic url="image-1.png" coords="2,362.41,575.30,108.00,85.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Framework of SEQ2SEQ models</figDesc><graphic url="image-2.png" coords="3,73.13,62.81,216.00,193.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>contains 23 ,162 labeled MWPs. All these</head><label>23</label><figDesc></figDesc><table>Acc w/o EN (%) Acc w/ EN (%) 
DNS 
58.1 
60.7 
Bi-LSTM 
59.6 
66.7 
ConvS2S 
61.5 
64.2 
Transformer 
59.0 
62.3 
Ensemble 
66.4 
68.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Model comparison. EN is short for equation 
normalization 

Bi-LSTM Transformer ConvS2S 
w/o EN 
59.6 
59.0 
61.5 
+ SE 
63.1 
59.9 
62.2 
+ OE 
63.7 
60.7 
62.9 
+ EB 
65.3 
61.2 
62.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The ablation study of three equation normal-
ization methods. SE is the first Rule mentioned in Sec-
tion 3. OE is the second rule mentioned in Section 3. 
EB means eliminating the brackets. 

</table></figure>

			<note place="foot" n="1"> https://ai.tencent.com/ailab/Deep_ Neural_Solver_for_Math_Word_Problems. html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is supported in part by the National Nature Science Foundation of China under grants No. 61602087, and the Fundamental Research Funds for the Central Universities under grants No. ZYGX2016J080.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Machine-guided solution to mathematical word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bussaba</forename><surname>Amnueypornsakul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suma</forename><surname>Bhat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Pacific Asia Conference on Language, Information and Computation, PACLIC 28, Cape Panwa Hotel</title>
		<meeting>the 28th Pacific Asia Conference on Language, Information and Computation, PACLIC 28, Cape Panwa Hotel<address><addrLine>Phuket, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-12" />
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Natural language input for a computer problem solving system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bobrow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Computers and thought</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Edward A Feigenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feldman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1963" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03122</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning fine-grained expressions to solve math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="805" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Parsing algebraic word problems into equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siena Dumas</forename><surname>Ang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="585" to="597" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning to automatically solve algebra word problems. Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Program induction by rationale generation: Learning to solve and explain algebraic word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="158" to="167" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to use formulas to solve simple arithmetic problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Solving general arithmetic word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<publisher>Portugal</publisher>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="1743" to="1752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mapping to declarative knowledge for word problem solving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="159" to="172" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatically solving number word problems by semantic parsing and reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuehui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1721" />
			<biblScope unit="page" from="1132" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mathdqn: Solving arithmeticword problems via deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianli</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence</title>
		<meeting>the ThirtySecond AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep neural solver for math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="845" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The gap of semantic parsing: A survey on automatic math word problem solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><forename type="middle">Tian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07290</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learn to solve algebra word problems using quadratic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lipu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaixiang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="817" to="822" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
