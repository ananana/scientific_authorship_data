<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Krimping texts for better summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Litvak</surname></persName>
							<email>marinal@sce.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Sami Shamoon College of Engineering</orgName>
								<address>
									<addrLine>Beer Sheva</addrLine>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Vanetik</surname></persName>
							<email>natalyav@sce.ac.il</email>
							<affiliation key="aff1">
								<orgName type="institution">Sami Shamoon College of Engineering</orgName>
								<address>
									<settlement>Beer Sheva</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Last</surname></persName>
							<email>mlast@bgu.ac.il</email>
							<affiliation key="aff2">
								<orgName type="institution">Gurion University of the Negev</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Krimping texts for better summarization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Automated text summarization is aimed at extracting essential information from original text and presenting it in a minimal, often predefined, number of words. In this paper, we introduce a new approach for unsupervised extractive summariza-tion, based on the Minimum Description Length (MDL) principle, using the Krimp dataset compression algorithm (Vreeken et al., 2011). Our approach represents a text as a transactional dataset, with sentences as transactions, and then describes it by itemsets that stand for frequent sequences of words. The summary is then compiled from sentences that compress (and as such, best describe) the document. The problem of summarization is reduced to the maximal coverage, following the assumption that a summary that best describes the original text, should cover most of the word sequences describing the document. We solve it by a greedy algorithm and present the evaluation results.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many unsupervised approaches for extractive text summarization follow the maximal coverage prin- ciple ( <ref type="bibr" target="#b14">Takamura and Okumura, 2009;</ref><ref type="bibr" target="#b4">Gillick and Favre, 2009)</ref>, where the extract that maximally covers the information contained in the source text, is selected. Since the exhaustive solution de- mands an exponential number of tests, approxima- tion techniques, such as a greedy approach or a global optimization of a target function, are uti- lized. It is quite common to measure text infor- mativeness by the frequency of its components- words, phrases, concepts, and so on. A different approach that received much less atten- tion is based on the Minimum Description Length (MDL) principle, defining the best summary as the one that leads to the best compression of the text by providing its shortest and most concise descrip- tion. The MDL principle is widely useful in com- pression techniques of non-textual data, such as summarization of query results for OLAP appli- cations. ( <ref type="bibr" target="#b5">Lakshmanan et al., 2002</ref>; <ref type="bibr" target="#b2">Bu et al., 2005</ref>) However, only a few works on text summarization using MDL can be found in the literature. Authors of <ref type="bibr" target="#b11">(Nomoto and Matsumoto, 2001</ref>) used K-means clustering extended with MDL principle for find- ing diverse topics in the summarized text. Nomoto in (2004) also extended the C4.5 classifier with MDL for learning rhetorical relations. In <ref type="bibr" target="#b10">(Nguyen et al., 2015</ref>) the problem of micro-review summa- rization is formulated within the MDL framework, where the authors view the tips as being encoded by snippets, and seek to find a collection of snip- pets that produce the encoding with the minimum number of bits. This paper introduces a new MDL-based approach for extracting relevant sentences into a summary. The approach represents documents as a sequen- tial transactional dataset and then compresses it by replacing frequent sequences of words by codes. The summary is then compiled from sentences that best compress (or describe) the document content. The intuition behind this approach says that a sum- mary that best describes the original text should cover its most frequent word sequences. As such, the problem of summarization is very naturally re- duced to the maximal coverage problem. We solve it by the greedy method which ranks sentences by their coverage of best compressing frequent word sequences and selects the top-ranked sentences to a summary. There are a few works that applied the common data mining techniques for calculat- ing frequent itemsets from transactional data to the text summarization task ( <ref type="bibr" target="#b1">Baralis et al., 2012;</ref><ref type="bibr" target="#b0">Agarwal et al., 2011;</ref><ref type="bibr" target="#b3">Dalal and Zaveri, 2013</ref>), but none of them followed the MDL principle. The comparative results on three different corpora show that our approach outperforms other unsu- pervised state-of-the-art summarizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>The proposed summarization methodology, de- noted by Gamp 1 , is based on the MDL princi- ple that is defined formally as follows <ref type="bibr" target="#b8">(Mitchell, 1997)</ref>. Given a set of models M, a model M ∈ M is considered the best if it minimizes L(M ) + L(D|M ) where L(M ) is the bit length of the de- scription of M and L(D|M ) is the bit length of the dataset D encoded with M . In our approach, we first represent an input text as a transactional dataset. Then, using the Krimp dataset compression algorithm <ref type="bibr" target="#b15">(Vreeken et al., 2011</ref>), we build the MDL for this dataset using its frequent sequential itemsets (word sequences). The sentences that cover most frequent word se- quences are chosen to a summary. The following subsections describe our methodology in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem statement</head><p>We are given a single text or a collection of texts about the same topic, composed of a set of sen- tences S 1 , . . . , S n over terms t 1 , . . . , t m . The word limit W is defined for the final summary. We represent a text as a sequential transactional dataset. Such a dataset consists of transactions (sentences), denoted by T 1 , . . . , T n , and unique items (terms 2 ) I 1 , . . . , I m . Items are unique across the entire dataset. The number n of transactions is called the size of a dataset. Transaction T i is a sequence of items from I 1 , . . . , I m , denoted by T i = (I i 1 , . . . , I i k ); the same item may appear in different places within the same transaction. Sup- port of an item sequence s in the dataset is the ra- tio of transactions containing it as a subsequence to the dataset size n, i.e., supp(s)</p><formula xml:id="formula_0">= |{T ∈D|s⊆T }| n . Given a support bound Supp ∈ [0, 1], a sequence s is called frequent if supp(s) ≥ Supp.</formula><p>According to the MDL principle, we are inter- ested in the minimal size of a compressed dataset D|CT after frequent sequences in D are encoded with the compressing set-codes from the Coding <ref type="table">Table CT</ref> , where shorter codes are assigned to more frequent sequences. The description length of non-encoded terms is assumed proportional to their length (number of characters). We rank sen- tences by their coverage of the best compressing set, which is the number of CT members in the sentence. The sentences with the highest cover- age score are added to the summary as long as its length does not exceed W .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Krimping text</head><p>The purpose of the Krimp algorithm <ref type="bibr" target="#b15">(Vreeken et al., 2011</ref>) is to use frequent sets (or sequences) to compress a transactional database in order to achieve MDL for that database. Let FreqSeq be the set of all frequent sequences in the database. A collection CT of sequences from FreqSeq (called the Coding <ref type="table">Table)</ref> is called best when it minimizes L(CT ) + L(D|CT ). We are interested in both ex- act and inexact sequences, allowing a sequence to have gaps inside it as long as the ratio of sequence length to sequence length with gaps does not ex- ceed a pre-set parameter Gap ∈ (0, 1]. Sequences with gaps make sense in text data, as phrasing of the same fact or entity in different sentences may differ. In order to encode the database, every mem- ber s ∈ CT is associated with its binary prefix code c (such as Huffman codes for 4 members: 0, 10, 110, 111), so that the most frequent code has the shortest length. We use an upper bound C on the number of encoded sequences in the coding ta- ble CT , in order to limit document compression. Krimp-based extractive summarization (see Algo- rithm 1) is given a document D with n sentences and m unique terms. The algorithm parameters are described in <ref type="table">Table 1:</ref> # note description affects 1 W summary words limit summary length 2 Supp minimal support bound -number of frequent minimal fraction of word sequences sentences containing |FreqSeq|, a frequent sequence compression rate 3 C maximal number of codes as in 2 4 Gap maximal allowed sequence gap ratio as in 2 <ref type="table">Table 1</ref>: Parameters of Algorithm 1.</p><p>The algorithm consists of the following steps.</p><p>1. We find all frequent term sequences in the document using Apriori-TID algorithm (R and R, 1994) for the given Supp and Gap and store them in set FreqSeq, which is kept in Standard Candidate Order 3 . The coding ta- ble CT is initialized to contain all single nor- malized terms and their frequencies. CT is always kept in Standard Cover Order 4 (Steps 1 and 2 in Algorithm 1).</p><p>2. We repeatedly choose frequent sequences from the set FreqSeq so that the size of the encoded dataset is minimal, with every se- lected sequence replaced by its code. Selec- tion is done by computing the decrease in the size of the encoding when each one of the se- quences is considered to be a candidate to be added to CT (Step 3 in Algorithm 1).</p><p>3. The summary is constructed by incremen- tally adding sentences with the highest cov- erage of encoded term sequences (Step 4 in Algorithm 1) that are not covered by previ- ously selected sentences. The sentences are selected in the greedy manner as long as the word limit W is not exceeded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Gamp: Krimp-based extractive summarization with gaps</head><p>Input: (1) A document, containing sentences S1, . . . , Sn after tokenization, stemming and stop-word removal; (2) normalized terms T1, . . . , Tm; (3) summary size W (3) limit C on the number of codes to use; (4) minimal support bound Supp; (5) maximal gap ratio Gap. Output: Extractive summary Summary /* STEP 1: Frequent sequence mining */ FreqSeq ← inexact frequent sequences of terms from {T1, . . . , Tm} appearing in at least Supp fraction of sentences and having a gap ratio of at least Gap; Sort </p><formula xml:id="formula_1">= ∅ do BestCode ← c ∈ FreqSeq such that L(CT ∪ {c}) + L(PrefixEncoding({S1, . . . , Sn}, CT ∪ {c})) is minimal; CT ← CT ∪ {BestCode}; FreqSeq ← FreqSeq \ {BestCode};</formula><p>Remove supersets of BestCode from FreqSeq; CodeCount++; end Summary ← ∅; /* STEP 4: Build the summary */ while #words(Summary) &lt; W do Find the sentence Si that covers the largest set T of terms in CT and add it to Summary; Remove terms of T from CT ; end return Summary Example 2.1 Let dataset D contain following three sentences (taken from the "House of cards" TV show): S1 = A hunter must stalk his prey until the hunter becomes the hunted. S2 = Then the prey becomes the predator. S3 = Then the predator and the hunter fight.</p><p>After stemming, tokenization and stop-word re- moval we obtain unique (stemmed) terms: t1 = hunter, t2 = must, t3 = stalk, t4 = prei, t5 = becom, t6 = hunt, t7 = predat, t8 = fight. supp(t1) = supp(t4) = supp(t5) = supp(t7) = 2 3 . supp(t2) = supp(t3) = supp(t8) = 1 3 .</p><p>Now we can view sentences as the following se- quences of normalized terms. S1 = (t1, t2, t3, t4, t1, t5, t4) S2 = (t4, t5, t7) S3 = (t7, t1, t8)</p><p>Initial coding table CT will contain all fre- quent single terms in Standard Cover Order: t 5 , t 1 , t 7 , t 4 , t 8 , t 2 , t 3 . Let the minimal support bound be 2 3 , i.e., to be frequent a sequence must appear in at least 2 sentences, and let the gap ra- tio be 1 2 . Also, let the limit C be 4, meaning that only the first four entries of the coding table will be used for encoding. There exists a frequent se- quence (t 4 , t 5 ) that appears twice in the text, once in S 1 with a gap and once in S 2 without a gap. We add it to the coding Now S 1 covers 3 out of 4 entries in CT , while S 2 and S 3 cover 2 terms each. If our summary is to contain just one sentence, we select S 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental settings and results</head><p>We performed experiments on three corpora from the Document Understanding Conference (DUC) <ref type="bibr">: 2002, 2004, and 2007 (duc, 2002 2007)</ref>, summa- rized in  <ref type="table" target="#tab_1">'02 533  2-3  100  1-20  SD  DUC'04 50  4  100  20-89  MD (50x10)  DUC'07 23  4  250</ref> 28-131 SD by the ROUGE-1 and ROUGE-2 (Lin, 2004) re- call scores, with the word limit indicated in <ref type="table" target="#tab_1">Ta- ble 2</ref>, without stemming and stopword removal.</p><p>The results of the introduced algorithm (Gamp) may be affected by several input parameters: min- imum support (Supp), codes limit (C), and the maximal gap allowed between frequent words (Gap). In order to find the best algorithm set- tings for a general case, we performed experi- ments that explored the impact of these parame- ters on the summarization results. First, we exper- imented with different values of support count in the range of <ref type="bibr">[2,</ref><ref type="bibr">10]</ref>. The results show that we get the best summaries using the sequences that occur in at least four document sentences. A limit on the number of codes is an additional parameter.We explored the impact of this param- eter on the quality of generated summaries. As we could conclude from our experiments, the best summarization results are obtained if this param- eter is set to the maximal number of words in the summary, W . Consequently, we used 100 codes for summarizing DUC 2002 and DUC 2004 docu- ments and 250 codes for DUC 2007 documents. The maximal gap ratio defines a pattern for gener- ating the frequent sequences and has a direct effect on their structure and number. Our experiments showed that allowing a small gap between words of a sequence helps to improve slightly the rank- ing of sentences, but the improvement is not sig- nificant. Thus we used Gap = 0.8 in comparative experiments for all corpora. The resulting settings for each corpus are shown in  We compared the Gamp algorithm with the two known unsupervised state-of-the-art summarizers denoted by <ref type="bibr" target="#b4">Gillick (Gillick and Favre, 2009)</ref> and McDonald <ref type="bibr" target="#b7">(McDonald, 2007)</ref>. As a baseline, we used a very simple approach that takes first sen- tences to a summary (denoted by TopK). <ref type="table" target="#tab_5">Table 4</ref> contains the results of comparative evaluations. The best scores are shown in bold. Gamp out- performed the other methods on all datasets (us- ing ROUGE-1 score). The difference between the scores of Gamp and Gillick (second best system) on DUC 2007 is highly significant according to the Wilcoxon matched pairs test. Based on the same test, the difference of scores obtained on the DUC 2004 is not statistically significant. On the DUC 2002, Gamp is ranked first, with insignifi- cant difference from the second best (McDonald's) scores. Based on this result, we can conclude that MDL-based summarization using frequent se- quences works better on long documents or multi- document domain. Intuitively, it is a very logical conclusion, because single short documents do not contain a sufficient number of frequent sequences. It is noteworthy that, in addition to the greedy ap- proach, we also evaluated the global optimization with maximizing coverage and minimizing redun- dancy using Linear Programming (LP). However, experimental results did not provide any improve- ment over the greedy approach. Therefore, we re- port only the results of the greedy solution.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROUGE-1 Recall ROUGE-2 Recall</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we introduce a new approach for summarizing text documents based on their Min- imal Description Length. We describe documents using frequent sequences of their words. The sen- tences with the highest coverage of the best com- pressing set are selected to a summary. The ex- perimental results show that this approach outper- forms other unsupervised state-of-the-art methods when summarizing long documents or sets of re- lated documents. We would not recommend using our approach for summarizing single short doc- uments which do not contain enough content for providing a high-quality description. In the future, we intend to apply the MDL method to keyword extraction, headline generation, and other related tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>FreqSeq according to Standard Candidate Order; /* STEP 2: Initialize the coding table */ Add all terms T1, . . . , Tm and their support to CT ; Keep CT always sorted according to Standard Cover Order; Initialize prefix codes according to the order of sequences in CT ; /* STEP 3: Find the best encoding */ EncodedData ← PrefixEncoding({S1, . . . , Sn}, CT ); CodeCount ← 0; while CodeCount &lt; B and FreqSeq</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>table according to</head><label>according</label><figDesc></figDesc><table>the Stan-
dard Cover Order, generate prefix codes for the 
first four entries, and get 

CT = 

seq 
supp code 
code len 
(prei, becom) 
2/3 
0 
1 
becom 
2/3 
10 
2 
hunter 
2/3 
110 
3 
predat 
2/3 
111 
3 
prei 
2/3 
− 
− 
... 
... 
− 
− 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>DUC 2002 and 2007 each con-
tains a set of single documents, and DUC 2004 
contains 50 sets of 10 related documents. We 
generated summaries per single documents in the 
DUC 2002 and 2007 corpora, and per each set 
of related documents (by considering each set 
of documents as one meta-document) in DUC 
2004, following the corresponding length restric-
tions. The summarization quality was measured </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Corpora statistics.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>Corpus 
Supp Max. codes Gap 
DUC 2002 and 2004 4 
100 
0.8 
DUC 2007 
4 
250 
0.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : Best settings.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 : Comparative results.</head><label>4</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> abbreviation of two words: GAp and kriMP 2 normalized words following tokenization, stemming, and stop-word removal</note>

			<note place="foot" n="3"> first, sorted by increasing support, then by decreasing sequence length, then lexicographically 4 first, sorted by decreasing sequence length, then by decreasing support, and finally, in lexicographical order</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by the U.S. De-partment of the Navy, Office of Naval Research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scisumm: A multidocument summarization system for scientific articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Gvr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Shankar Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><forename type="middle">Penstein</forename><surname>Ros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-HLT 2011 System Demonstrations</title>
		<meeting>the ACL-HLT 2011 System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="115" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-document summarization exploiting frequent itemsets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Baralis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Cagliero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saima</forename><surname>Jabeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Fiori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual ACM Symposium on Applied Computing, SAC &apos;12</title>
		<meeting>the 27th Annual ACM Symposium on Applied Computing, SAC &apos;12</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="782" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mdl summarization with holes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofeng</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Laks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Lakshmanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Very Large Data Bases, VLDB &apos;05</title>
		<meeting>the 31st International Conference on Very Large Data Bases, VLDB &apos;05</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="433" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semisupervised learning based opinion summarization and classification for online product reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukesh</forename><forename type="middle">A</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaveri</surname></persName>
		</author>
		<ptr target="http://duc.nist.gov" />
	</analytic>
	<monogr>
		<title level="m">Document Understanding Conference</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Applied Computational Intelligence and Soft Computing</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Scalable Global Model for Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Favre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing</title>
		<meeting>the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The generalized mdl approach for summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Laks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Lakshmanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><forename type="middle">Xing</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><forename type="middle">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Very Large Data Bases, VLDB &apos;02</title>
		<meeting>the 28th International Conference on Very Large Data Bases, VLDB &apos;02</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="766" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ROUGE: A Package for Automatic Evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Text Summarization Branches Out (WAS 2004)</title>
		<meeting>the Workshop on Text Summarization Branches Out (WAS 2004)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="25" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A study of global inference algorithms in multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="557" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcgraw-Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Inc</surname></persName>
		</author>
		<imprint>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>1 edition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Review synthesis for micro-review summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Son</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hady</forename><forename type="middle">W</forename><surname>Lauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panayiotis</forename><surname>Tsaparas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, WSDM &apos;15</title>
		<meeting>the Eighth ACM International Conference on Web Search and Data Mining, WSDM &apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A new approach to unsupervised text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadashi</forename><surname>Nomoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;01</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;01</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="26" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Machine Learning Approaches to Rhetorical Parsing and Open-Domain Text Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadashi</forename><surname>Nomoto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>Nara Institute of Science and Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast algorithms for mining association rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrawal</forename><surname>Srikant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th International Conference on Very Large Databases</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="487" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Text summarization model based on maximum coverage problem and its variant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL &apos;09: Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="781" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Krimp: Mining itemsets that compress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilles</forename><surname>Vreeken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Siebes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="169" to="214" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
