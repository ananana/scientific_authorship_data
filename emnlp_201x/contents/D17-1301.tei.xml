<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Cross-Sentence Context for Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyue</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ADAPT Centre</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="department" key="dep3">Tencent AI Lab</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<country>Ireland, China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
							<email>tuzhaopeng@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ADAPT Centre</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="department" key="dep3">Tencent AI Lab</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<country>Ireland, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ADAPT Centre</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="department" key="dep3">Tencent AI Lab</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<country>Ireland, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ADAPT Centre</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="department" key="dep3">Tencent AI Lab</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<country>Ireland, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploiting Cross-Sentence Context for Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2826" to="2831"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In translation, considering the document as a whole can help to resolve ambiguities and inconsistencies. In this paper, we propose a cross-sentence context-aware approach and investigate the influence of historical contextual information on the performance of neural machine translation (NMT). First, this history is summarized in a hierarchical way. We then integrate the historical representation into NMT in two strategies: 1) a warm-start of en-coder and decoder states, and 2) an auxiliary context source for updating decoder states. Experimental results on a large Chinese-English translation task show that our approach significantly improves upon a strong attention-based NMT system by up to +2.1 BLEU points.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation (NMT) has been rapidly developed in recent years <ref type="bibr" target="#b7">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b15">Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b18">Tu et al., 2016</ref>). The encoder- decoder architecture is widely employed, in which the encoder summarizes the source sentence into a vector representation, and the decoder generates the target sentence word by word from the vector representation. Using the encoder-decoder frame- work as well as gating and attention techniques, it has been shown that the performance of NMT has surpassed the performance of traditional sta- tistical machine translation (SMT) on various lan- guage pairs ( <ref type="bibr" target="#b9">Luong et al., 2015)</ref>.</p><p>The continuous vector representation of a sym- bol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of * Corresponding Author: Zhaopeng Tu a word. Consequently, NMT needs to spend a substantial amount of its capacity in disambiguat- ing source and target words based on the context defined by a source sentence ( <ref type="bibr" target="#b3">Choi et al., 2016)</ref>. Consistency is another critical issue in document- level translation, where a repeated term should keep the same translation throughout the whole document <ref type="bibr" target="#b20">(Xiao et al., 2011;</ref><ref type="bibr" target="#b2">Carpuat and Simard, 2012</ref>). Nevertheless, current NMT models still process a documents by translating each sentence alone, suffering from inconsistency and ambigu- ity arising from a single source sentence. These problems are difficult to alleviate using only lim- ited intra-sentence context.</p><p>The cross-sentence context, or global context, has proven helpful to better capture the meaning or intention in sequential tasks such as query sug- gestion ( <ref type="bibr" target="#b14">Sordoni et al., 2015)</ref> and dialogue model- ing ( <ref type="bibr" target="#b19">Vinyals and Le, 2015;</ref><ref type="bibr" target="#b13">Serban et al., 2016)</ref>. The leverage of global context for NMT, how- ever, has received relatively little attention from the research community. <ref type="bibr">1</ref> In this paper, we pro- pose a cross-sentence context-aware NMT model, which considers the influence of previous source sentences in the same document. <ref type="bibr">2</ref> Specifically, we employ a hierarchy of Recur- rent Neural Networks (RNNs) to summarize the cross-sentence context from source-side previous sentences, which deploys an additional document- level RNN on top of the sentence-level RNN en- coder ( <ref type="bibr" target="#b14">Sordoni et al., 2015)</ref>. After obtaining the global context, we design several strategies to inte- grate it into NMT to translate the current sentence:</p><p>• Initialization, that uses the history represen-tation as the initial state of the encoder, de- coder, or both;</p><p>• Auxiliary Context, that uses the history rep- resentation as static cross-sentence context, which works together with the dynamic intra- sentence context produced by an attention model, to good effect.</p><p>• Gating Auxiliary Context, that adds a gate to Auxiliary Context, which decides the amount of global context used in generating the next target word at each step of decoding.</p><p>Experimental results show that the proposed ini- tialization and auxiliary context (w/ or w/o gat- ing) mechanisms significantly improve translation performance individually, and combining them achieves further improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>Given a source sentence x m to be translated, we consider its K previous sentences in the same document as cross-sentence context C = {x m−K , ..., x m−1 }. In this section, we first model C, which is then integrated into NMT. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Summarizing Global Context</head><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we summarize the represen- tation of C in a hierarchical way:</p><p>Sentence RNN For a sentence x k in C, the sentence RNN reads the corresponding words {x 1,k , ..., x n,k , . . . , x N,k } sequentially and up- dates its hidden state:</p><formula xml:id="formula_0">h n,k = f (h n−1,k , x n,k )<label>(1)</label></formula><p>where f (·) is an activation function, and h n,k is the hidden state at time n. The last state h N,k stores order-sensitive information about all the words in x k , which is used to represent the summary of the whole sentence, i.e. S k ≡ h N,k . After processing each sentence in C, we can obtain all sentence- level representations, which will be fed into docu- ment RNN.</p><p>Document RNN It takes as input the se- quence of the above sentence-level representations {S 1 , ..., S k , ..., S K } and computes the hidden state as:</p><formula xml:id="formula_1">h k = f (h k−1 , S k ) (2)</formula><p>where h k is the recurrent state at time k, which summarizes the previous sentences that have been processed to the position k. Similarly, we use the last hidden state to represent the summary of the global context, i.e. D ≡ h K .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Integrating Global Context into NMT</head><p>We propose three strategies to integrate the history representation D into NMT:</p><p>Initialization We use D to initialize either NMT encoder, NMT decoder or both. For encoder, we use D as the initialization state rather than all-zero states as in the standard NMT ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>). For decoder, we rewrite the calculation of the initial hidden state</p><formula xml:id="formula_2">s 0 = tanh(W s h N ) as s 0 = tanh(W s h N + W D D)</formula><p>where h N is the last hidden state in encoder and {W s , W D } are the cor- responding weight metrices.</p><p>Auxiliary Context In standard NMT, as shown in <ref type="figure" target="#fig_3">Figure 2</ref> (a), the decoder hidden state for time i is computed by</p><formula xml:id="formula_3">s i = f (s i−1 , y i−1 , c i )<label>(3)</label></formula><p>where y i−1 is the most recently generated target word, and c i is the intra-sentence context sum- marized by NMT encoder for time i. As shown in <ref type="figure" target="#fig_3">Figure 2</ref> (b), Auxiliary Context method adds the representation of cross-sentence context D to jointly update the decoding state s i :</p><formula xml:id="formula_4">s i = f (s i−1 , y i−1 , c i , D)<label>(4)</label></formula><p>In this strategy, D serves as an auxiliary informa- tion source to better capture the meaning of the source sentence. Now the gated NMT decoder has four inputs rather than the original three ones. The concatenation [c i , D], which embeds both intra- and cross-sentence contexts, can be fed to the de- coder as a single representation. We only need to modify the size of the corresponding parame- ter matrix for least modification effort.    Gating Auxiliary Context The starting point for this strategy is an observation: the need for information from the global context differs from step to step during generation of the target words. For example, global context is more in de- mand when generating target words for ambiguous source words, while less by others. To this end, we extend auxiliary context strategy by introducing a context gate ( <ref type="bibr" target="#b16">Tu et al., 2017a</ref>) to dynamically con- trol the amount of information flowing from the auxiliary global context at each decoding step, as shown in <ref type="figure" target="#fig_3">Figure 2 (c)</ref>.</p><p>Intuitively, at each decoding step i, the context gate looks at decoding environment (i.e., s i , y i−1 , and c i ), and outputs a number between 0 and 1 for each element in D, where 1 denotes "completely transferring this" while 0 denotes "completely ig- noring this". The global context vector D is then processed with an element-wise multiplication be- fore being fed to the decoder activation layer.</p><p>Formally, the context gate consists of a sigmoid neural network layer and an element-wise mul- tiplication operation. It assigns an element-wise weight to D, computed by</p><formula xml:id="formula_5">z i = σ(U z s i−1 + W z y i−1 + C z c i )<label>(5)</label></formula><p>Here σ(·) is a logistic sigmoid function, and {W z , U z , C z } are the weight matrices, which are trained to learn when to exploit global context to maximize the overall translation performance. Note that z i has the same dimensionality as D, and thus each element in the global context vector has its own weight. Accordingly, the decoder hidden state is updated by</p><formula xml:id="formula_6">s i = f (s i−1 , y i−1 , c i , z i ⊗ D)<label>(6)</label></formula><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>We carried out experiments on Chinese-English translation task. As the document information is necessary when selecting the previous sentences, we collect all LDC corpora that contain document boundary.  <ref type="bibr" target="#b12">(Sennrich and Haddow, 2016;</ref><ref type="bibr" target="#b11">Sennrich et al., 2017)</ref>. We limited the source and target vocab- ularies to the most frequent 35K words in Chi- nese and English, covering approximately 97.1% and 99.4% of the data in the two languages re- spectively. We trained each model on sentences of length up to 80 words in the training data with early stopping. The word embedding dimension was 600, the hidden layer size was 1000, and the batch size was 80. All our models considered the previous three sentences (i.e., K = 3) as cross- sentence context. <ref type="bibr">3</ref> The LDC corpora indexes are: <ref type="bibr">2003E07, 2003E14, 2004T07, 2005E83, 2005T06, 2006E24, 2006E34, 2006E85, 2006E92, 2007E87, 2007E101, 2007T09, 2008E40, 2008E56, 2009E16, 2009E95</ref>. <ref type="bibr">4</ref> Available at https://github.com/EdinburghNLP/nematus.  <ref type="table">Table 1</ref>: Evaluation of translation quality. "Init" denotes Initialization of encoder ("enc"), decoder ("dec"), or both ("enc+dec"), and "Auxi" denotes Auxiliary Context. " †" indicates statistically significant difference (P &lt; 0.01) from the baseline NEMATUS. <ref type="table">Table 1</ref> shows the translation performance in terms of BLEU score. Clearly, the proposed approaches significantly outperforms baseline in all cases. Auxiliary Context Strategies (Rows 6-7) The gating auxiliary context strategy achieves a sig- nificant improvement of around +1.0 BLEU point over its non-gating counterpart. This shows that, by acting as a critic, the introduced context gate learns to distinguish the different needs of the global context for generating target words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Combining (Row 8) Finally, we combine the best variants from the initialization and auxiliary context strategies, and achieve the best perfor- mance, improving upon NEMATUS by +2.1 BLEU points. This indicates the two types of strategies are complementary to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analysis</head><p>We first investigate to what extent the mis- translated errors are fixed by the proposed system.</p><p>We randomly select 15 documents (about 60 sen- tences) from the test sets. As shown in <ref type="table" target="#tab_4">Table 2</ref>, we count how many related errors: i) are made by NMT (Total), and ii) fixed by our method (Fixed); as well as iii) newly generated (New). About Ambiguity, while we found that 38 words/phrases were translated into incorrect equivalents, 76% of them are corrected by our model. Similarly, we solved 75% of the Inconsistency errors including lexical, tense and definiteness (definite or indefi- nite articles) cases. However, we also observe that our system brings relative 21% new errors.  Hist. ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Errors Ambiguity Inconsistency All</head><p>Input ?</p><p>Ref.</p><p>Can it inhibit and deter corrupt offi- cials?</p><p>NMT Can we contain and deter the enemy?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our</head><p>Can it contain and deter the corrupt officials? <ref type="table">Table 3</ref>: Example translations. We italicize some mis-translated errors and highlight the correct ones in bold.</p><p>Case Study <ref type="table">Table 3</ref> shows an example. The word "" (corrupt officials) is mis-translated as "enemy" by the baseline system. With the help of the similar word "" in the previous sen- tence, our approach successfully correct this mis- take. This demonstrates that cross-sentence con- text indeed helps resolve certain ambiguities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>While our approach is built on top of hierarchi- cal recurrent encoder-decoder (HRED) ( <ref type="bibr" target="#b14">Sordoni et al., 2015)</ref>, there are several key differences which reflect how we have generalized from the original model. <ref type="bibr" target="#b14">Sordoni et al. (2015)</ref> use HRED to summarize a single representation from both the current and previous sentences, which limits it- self to (1) it is only applicable to encoder-decoder framework without attention model, (2) the rep- resentation can only be used to initialize decoder. In contrast, we use HRED to summarize the pre- vious sentences alone, which provides additional cross-sentence context for NMT. Our approach is more flexible at (1) it is applicable to any encoder- decoder frameworks (e.g., with attention), (2) the cross-sentence context can be used to initialize ei- ther encoder, decoder or both. While both our approach and <ref type="bibr" target="#b13">Serban et al. (2016)</ref> use Auxiliary Context mechanism for in- corporating cross-sentence context, there are two main differences: 1) we have separate parameters to better control the effects of the cross-and intra- sentence contexts, while they only have one pa- rameter matrix to manage the single representa- tion that encodes both contexts; 2) based on the intuition that not every target word generation re- quires equivalent cross-sentence context, we intro- duce a context gate ( <ref type="bibr" target="#b16">Tu et al., 2017a</ref>) to control the amount of information from it, while they don't.</p><p>At the same time, some researchers propose to use an additional set of an encoder and attention to model more information. For example, <ref type="bibr" target="#b6">Jean et al. (2017)</ref> use it to encode and select part of the previous source sentence for generating each target word. <ref type="bibr" target="#b1">Calixto et al. (2017)</ref> utilize global image features extracted using a pre-trained con- volutional neural network and incorporate them in NMT. As additional attention leads to more com- putational cost, they can only incorporate limited information such as single preceding sentence in <ref type="bibr" target="#b6">Jean et al. (2017)</ref>. However, our architecture is free to this limitation, thus we use multiple pre- ceding sentences (e.g. K = 3) in our experiments.</p><p>Our work is also related to multi-source <ref type="bibr" target="#b21">(Zoph and Knight, 2016)</ref> and multi-target NMT <ref type="bibr" target="#b5">(Dong et al., 2015)</ref>, which incorporate additional source or target languages. They investigate one-to- many or many-to-one languages translation tasks by integrating additional encoders or decoders into encoder-decoder framework, and their exper- iments show promising results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>We proposed two complementary approaches to integrating cross-sentence context: 1) a warm- start of encoder and decoder with global con- text representation, and 2) cross-sentence context serves as an auxiliary information source for up- dating decoder states, in which an introduced con- text gate plays an important role. We quantita- tively and qualitatively demonstrated that the pre- sented model significantly outperforms a strong attention-based NMT baseline system. We release the code for these experiments at https:// www.github.com/tuzhaopeng/LC-NMT.</p><p>Our models benefit from larger contexts, and would be possibly further enhanced by other doc- ument level information, such as discourse rela- tions. We propose to study such models for full length documents with more linguistic features in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Summarizing global context with a hierarchical RNN (x k is the k-th source sentence).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architectures of NMT with auxiliary context integrations. act. is the decoder activation function, and σ is a sigmoid function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Baseline</head><label></label><figDesc>(Rows 1-2) NEMATUS significantly outperforms Moses -a commonly used phrase- based SMT system (Koehn et al., 2007), by 2.3 BLEU points on average, indicating that it is a strong NMT baseline system. It is consistent with the results in (Tu et al., 2017b) (i.e., 26.93 vs. 29.41) on training corpora of similar scale. Initialization Strategy (Rows 3-5) Init enc and Init dec improve translation performance by around +1.0 and +1.3 BLEU points individually, prov- ing the effectiveness of warm-start with cross- sentence context. Combining them achieves a fur- ther improvement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 : Translation error statistics.</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> To the best of our knowledge, our work and Jean et al. (2017) are two independently early attempts to model crosssentence context for NMT. 2 In our preliminary experiments, considering target-side history inversely harms translation performance, since it suffers from serious error propagation problems.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the Science Foun-dation of Ireland (SFI) ADAPT project (Grant No.:13/RC/2106). The authors also wish to thank the anonymous reviewers for many helpful com-ments with special thanks to Henry Elder for his generous help on proofreading of this manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations<address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Incorporating global visual features into attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Campbell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06521</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The trouble with smt consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Simard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Workshop on Statistical Machine Translation. Montreal</title>
		<meeting>the 7th Workshop on Statistical Machine Translation. Montreal<address><addrLine>Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="442" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Context-dependent word representation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoul</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.00578</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Clause restructuring for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivona</forename><surname>Kucerova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-task learning for multiple language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Assocaition for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Beijing</title>
		<meeting>the 53rd Annual Meeting of the Assocaition for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Beijing<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1723" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05135</idno>
		<title level="m">Does neural machine translation benefit from larger context? arXiv preprint</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume the Demo and Poster Sessions<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bleu: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Läubli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jozef</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mokry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04357</idno>
		<title level="m">Nematus: a toolkit for neural machine translation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">chapter Linguistic Input Features Improve Neural Machine Translation</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="83" to="91" />
		</imprint>
	</monogr>
	<note>Proceedings of the First Conference on Machine Translation</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3776" to="3783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A hierarchical recurrent encoderdecoder for generative context-aware query suggestion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Vahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">Grue</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyun</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 24th ACM International Conference on Information and Knowledge Management<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="553" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Neural Information Processing Systems</title>
		<meeting>the 2014 Neural Information Processing Systems<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Context gates for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural machine translation with reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th AAAI Conference on Artificial Intelligence (AAAI17)</title>
		<meeting>the 31th AAAI Conference on Artificial Intelligence (AAAI17)<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3097" to="3103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th annual meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A neural conversational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning, Deep Learning Workshop</title>
		<meeting>the International Conference on Machine Learning, Deep Learning Workshop</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Document-level consistency verification in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Translation Summit</title>
		<meeting><address><addrLine>Xiamen, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="131" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.00710</idno>
		<title level="m">Multi-source neural translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
