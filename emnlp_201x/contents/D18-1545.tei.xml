<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data Augmentation via Dependency Tree Morphing for Low-Resource Languages</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gözde</forename><forename type="middle">G ¨</forename><surname>Ul</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="laboratory">UKP Lab</orgName>
								<orgName type="institution">Technische Universität Darmstadt Darmstadt</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">¸</forename><surname>Ahin</surname></persName>
							<email>sahin@ukp.informatik.tu-darmstadt.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="laboratory">UKP Lab</orgName>
								<orgName type="institution">Technische Universität Darmstadt Darmstadt</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
							<email>steedman@inf.ed.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Edinburgh Edinburgh</orgName>
								<address>
									<country key="GB">Scotland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data Augmentation via Dependency Tree Morphing for Low-Resource Languages</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="5004" to="5009"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>5004</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural NLP systems achieve high scores in the presence of sizable training dataset. Lack of such datasets leads to poor system performances in the case low-resource languages. We present two simple text augmentation techniques using dependency trees, inspired from image processing. We &quot;crop&quot; sentences by removing dependency links, and we &quot;ro-tate&quot; sentences by moving the tree fragments around the root. We apply these techniques to augment the training sets of low-resource languages in Universal Dependencies project. We implement a character-level sequence tagging model and evaluate the augmented datasets on part-of-speech tagging task. We show that crop and rotate provides improvements over the models trained with non-augmented data for majority of the languages, especially for languages with rich case marking systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most recently, various deep learning methods have been proposed for many natural language under- standing tasks including sentiment analysis, ques- tion answering, dependency parsing and semantic role labeling. Although these methods have re- ported state-of-the-art results for languages with rich resources, no significant improvement has been announced for low-resource languages. In other words, feature-engineered statistical mod- els still perform better than these neural models for low-resource languages. <ref type="bibr">1</ref> Generally accepted reason for low scores is the size of the training data, i.e., training labels being too sparse to extract meaningful statistics.</p><p>Label-preserving data augmentation techniques are known to help methods generalize better by increasing the variance of the training data. It has been a common practice among researchers in computer vision field to apply data augmen- tation, e.g., flip, crop, scale and rotate images, for tasks like image classification ( <ref type="bibr" target="#b0">Ciresan et al., 2012;</ref><ref type="bibr" target="#b6">Krizhevsky et al., 2012</ref>). Similarly, speech recognition systems made use of augmentation techniques like changing the tone and speed of the audio ( <ref type="bibr" target="#b5">Ko et al., 2015;</ref><ref type="bibr" target="#b9">Ragni et al., 2014</ref>), noise addition ( <ref type="bibr" target="#b3">Hartmann et al., 2016)</ref> and synthetic au- dio generation ( <ref type="bibr" target="#b10">Takahashi et al., 2016)</ref>. Compara- ble techniques for data augmentation are less ob- vious for NLP tasks, due to structural differences among languages. There are only a small num- ber of studies that tackle data augmentation tech- niques for NLP, such as <ref type="bibr" target="#b12">Zhang et al. (2015)</ref> for text classification and <ref type="bibr" target="#b1">Fadaee et al. (2017)</ref> for machine translation.</p><p>In this work, we focus on languages with small training datasets, that are made available by the Universal Dependency (UD) project. These lan- guages are dominantly from Uralic, Turkic, Slavic and Baltic language families, which are known to have extensive morphological case-marking sys- tems and relatively free word order. With these languages in mind, we propose an easily adapt- able, multilingual text augmentation technique based on dependency trees, inspired from two common augmentation methods from image pro- cessing: cropping and rotating. As images are cropped to focus on a particular item, we crop the sentences to form other smaller, meaningful and focused sentences. As images are rotated around a center, we rotate the portable tree fragments around the root of the dependency tree to form a synthetic sentence. We augment the training sets of these low-resource languages via crop and ro- tate operations. In order to measure the impact of augmentation, we implement a unified character- level sequence tagging model. We systematically train separate parts-of-speech tagging models with the original and augmented training sets, and eval- uate on the original test set. We show that crop and rotate provide improvements over the non- augmented data for majority of the languages, es- pecially for languages with rich case marking sys- tem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>We borrow two fundamental label-preserving aug- mentation ideas from image processing: cropping and rotation. Image cropping can be defined as removal of some of the peripheral areas of an im- age to focus on the subject/object (e.g., focusing on the flower in a large green field). Following this basic idea, we aim to identify the parts of the sen- tence that we want to focus and remove the other chunks, i.e., form simpler/smaller meaningful sen- tences 2 . In order to do so, we take advantage of dependency trees which provide us with links to focuses, such as subjects and objects. The idea is demonstrated in <ref type="figure" target="#fig_1">Fig. 1b</ref> on the Turkish sentence given in <ref type="figure" target="#fig_1">Fig. 1a</ref>. Here, given a predicate (wrote) that governs a subject (her father), an indirect ob- ject (to her) and a direct object (a letter); we form three smaller sentences with a focus on the sub- ject (first row in <ref type="figure" target="#fig_1">Fig. 1b</ref>: her father wrote) and the objects (second and third row) by removing all dependency links other than the focus (with its subtree). Obviously, cropping may cause seman- tic shifts on a sentence-level. However it preserves local syntactic tags and even shallow semantic la- bels.</p><p>Images are rotated around a chosen center with a certain degree to enhance the training data. Sim- ilarly, we choose the root as the center of the sen- tence and rotate the flexible tree fragments around the root for augmentation. Flexible fragments are usually defined by the morphological typology of the language ( <ref type="bibr" target="#b2">Futrell et al., 2015</ref>). For instance, languages close to analytical typology such as En- glish, rarely have inflectional morphemes. They do not mark the objects/subjects, therefore words have to follow a strict order. For such languages, sentence rotation would mostly introduce noise. On the other hand, large number of languages such as Latin, Greek, Persian, Romanian, Assyr- ian, Turkish, Finnish and Basque have no strict word order (though there is a preferred order) due   to their extensive marking system. Hence, flexible parts are defined as marked fragments which are again, subjects and objects. Rotation is illustrated in <ref type="figure" target="#fig_1">Fig. 1c</ref> on the same sentence. In order to investigate the impact of the aug- mentation, we design a simple sequence tagging model that operates on the character level. Many low-resource languages we deal with in the Exper- iments section are morphologically rich. There- fore, we use a character-level model to address the rare word problem and to learn morphological reg- ularities among words.</p><p>For each sentence s, we produce a label se- quence l, where l t refers to POS tag for the t-th token. Given g as gold labels and θ as model pa- rameters we find the values that minimize the neg- ative log likelihood of the sequence:</p><formula xml:id="formula_0">ˆ θ = arg min θ − n t=1 log(p(g t |θ, s))<label>(1)</label></formula><p>To calculate p(l t |θ, s), we first calculate a word embedding, w, for each word. We consider words as a sequence of characters c 0 , c 1 , .., c n and use a bi-LSTM unit to compose the character sequence into w, as in <ref type="bibr" target="#b7">Ling et al. (2015)</ref>:</p><formula xml:id="formula_1">hw f , hw b = bi-LSTM(c 0 , c 1 , .., c n ) (2) w = W f · hw f + W b · hw b + b<label>(3)</label></formula><p>Later, these embeddings are passed onto another bi-LSTM unit:</p><formula xml:id="formula_2">h f , h b = bi-LSTM( w t )<label>(4)</label></formula><p>Hidden states from both directions are concate- nated and mapped by a linear layer to the label space. Then label probabilities are calculated by a softmax function:</p><formula xml:id="formula_3">p( l t |s, p) = softmax(W l · [ h f ; h b ] + b l ) (5)</formula><p>Finally the label with the highest probability is as- signed to the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>We use the data provided by Universal Dependen- cies v2.1 (Nivre et al., 2017) project. Since our focus is on languages with low resources, we only consider the ones that have less than 120K tokens. The languages without standard splits and sizes less than 5K tokens, are ignored. We use the uni- versal POS tags defined by UD v2.1. To keep our approach as language agnostic and simple as possible, we use the following univer- sal dependency labels and their subtypes to extract the focus and the flexible fragment: NSUBJ (nom- inal subject), IOBJ (indirect object), OBJ (indirect object) and OBL (oblique nominal). These depen- dency labels are referred to as Label of Interest (LOI). The root/predicate may be a phrase rather than a single token. We use the following relations to identify such cases: FIXED, FLAT, COP (copula) and COMPOUND. Other labels such as ADVMOD can also be considered flexible, however are ig- nored for the sake of simplicity. We enumerate all flexible chunks and calculate all reordering per- mutations. Keeping the LOI limited is necessary to reduce the number of permutations. We apply reordering only to the first level in the tree. Our method overgeneralizes, to include sequences that are not grammatical in the language in question. We regard the ungrammatical sentences as noise.</p><p>Number of possible cropping operations are limited to the number of items that are linked via an LOI to the root. If we call it n, then the num- ber of possible rotations would be (n + 1)! since n pieces and the root are flexible and can be placed anywhere in the sentence. To limit the number of rotations, we calculate all possible permuta- tions for reordering the sentence and then ran- domly pick n of them. Each operation is applied with a certain probability p to each sentence, (e.g., if p = 1, n number of crops; if p = 0.5 an average of n/2 crops will be done).</p><p>We use the model in Sec. 2 to systematically train part-of-speech taggers on original and aug- mented training data sets. To be able measure the impact of the augmentation, all models are trained with the same hyperparameters. All tokens are lowercased and surrounded with special start-end symbols. Weight parameters are uniformly ini- tialized between −0.1 and +0.1. We used one layer bi-LSTMs both for character composition and POS tagging with hidden size of 200. Char- acter embedding size is chosen as 200. We used dropout, gradient clipping and early stopping to prevent overfitting for all experiments. Stochastic gradient descent with an initial learning rate as 1 is used as the optimizer. Learning rate is reduced by half if scores on development set do not improve.</p><p>Average of multiple runs for 20 languages are given in <ref type="figure" target="#fig_1">Fig. 1</ref>. Here, Org column refers to our baseline with non-augmented, original train- ing set, where Imp% is the improvement over the baseline by the best crop/flip model for that lan- guage. It is evident that, with some minor excep- tions, all languages have benefited from a type of augmentation. We see that the biggest improve- ments are achieved on Irish and Lithuanian, the ones with the lowest baseline scores and the small- est training sets 3 . Our result on both languages show that both operations reduced the generaliza- tion error surprisingly well in the lack of training data.</p><p>Tagging results depend on many factors such as the training data size, the source of the tree- bank (e.g., news may have less objects and sub- jects compared to a story), and the language typol- ogy (e.g., number/type of case markers it uses). In <ref type="figure" target="#fig_2">Fig. 2</ref>, the relation between the data size and the improvement by the augmentation is shown. Pear- son correlation coefficient for two variables is cal- culated as −0.35.</p><p>Indo-European (IE) : Baltic and Slavic lan- guages are known to have around 7 distinct case markers, which relaxes the word order. As ex- pectedly, both augmentation techniques improve the scores for Baltic (Latvian, Lithuanian) and Slavic (Belarusian, Slovak, Serbian, Ukranian) crop rotate #Tokens Lang Type Org p = 0.3 p = 0.7 p = 1 p = 0.3 p = 0.7 p = 1 Imp%  <ref type="table">Table 1</ref>: POS tagging accuracies on UDv2.1 test sets. Best scores are shown with bold. Org: Original. p: operation probability. Imp%: Improvement over original (Org) by the best model trained with the augmented data.</p><formula xml:id="formula_4">&lt; 20K Lithuanian IE,</formula><p>languages, except for Old Church Slavic (OCS). OCS is solely compiled from bible text which is known to contain longer and passive sentences. We observe that rotation performs slightly better than cropping for Slavic languages. In the pres- ence of a rich marking system, rotation can be considered a better augmenter, since it greatly in- creases the variance of the training data by shuf- fling. For Germanic (Gothic, Afrikaans, Danish) languages, we do not observe a repeating gain, due to lack of necessary markers.</p><p>Uralic and Turkic : Both language types have an extensive marking system. Hence, similar to Slavic languages, both techniques improve the score.</p><p>Dravidian : Case system of modern Tamil de- fines 8 distinct markers, which explains the im- proved accuracies of the augmented models. We would expect a similar result for Telugu. However Telugu treebank is entirely composed of sentences from a grammar book which may not be expres- sive and diverse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Similar to sentence cropping, <ref type="bibr" target="#b11">Vickrey and Koller (2008)</ref> define transformation rules to simplify sen- tences (e.g., I was not given a chance to eat -I ate) and shows that enrichening training set with simplified sentences improves the results of se- mantic role labeling. One of the first studies in text augmentation ( <ref type="bibr" target="#b12">Zhang et al., 2015)</ref>, replaces a randomly chosen word with its randomly chosen synonym extracted from a thesaurus. They report improved test scores when a large neural model is trained with the augmented set. Jia and Liang (2016) induce grammar from semantic parsing training data and generate new data points by sam- pling to feed a sequence to sequence RNN model. <ref type="bibr" target="#b1">Fadaee et al. (2017)</ref> chooses low-frequency words instead of a random word, and generate synthetic sentence pairs that contain those rare words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Unlike majority of previous NLP augmentation techniques, the proposed methods are meaning- preserving, i.e., they preserve the fundamental meaning of the sentence for most of the tested lan- guages. Therefore can be used for variety of prob- lems such as semantic role labeling, sentiment analysis, text classification. Instead of those prob- lems, we evaluate the idea on the simplest possible task (POS) for the following reasons:</p><p>• It gets harder to measure the impact of the idea as the system/task gets complicated due to large number of parameters. • POS tagging performance is a good indicator of performances of other structured predic- tion tasks, since POS tags are crucial features for higher-level NLP tasks.</p><p>Our research interest was to observe which aug- mentation technique would improve which lan- guage, rather than finding one good model. There- fore we have not used development sets to choose one good augmentation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>Neural models have become a standard approach for many NLP problems due to their ability to extract high-level features and generalization ca- pability. Although they have achieved state-of- the-art results in NLP benchmarks with languages with large amount of training data, low-resource languages have not yet benefited from neural mod- els. In this work, we presented two simple text augmentation techniques using dependency trees inspired by image cropping and rotating. We eval- uated their impact on parts-of-speech tagging in a number of low-resource languages from various language families. Our results show that:</p><p>• Language families with rich case marking systems (e.g., Baltic, Slavic, Uralic) bene- fit both from cropping and rotation. How- ever, for such languages, rotation increases the variance of the data relatively more, lead- ing to slightly better accuracies.</p><p>• Both techniques provide substantial improve- ments over the baseline (non-augmented data) when only a tiny training dataset is available.</p><p>This work aimed to measure the impact of the basic techniques, rather than creating the best text augmentation method. Following these en- couraging results, method can be improved by (1) considering the preferred chunk order of the language during rotation, (2) taking language- specific flexibilities into account (e.g., Spanish typically allows free subject inversion (unlike ob- ject)). Furthermore, we plan to extend this work by evaluating the augmentation on other NLP benchmarks such as language modeling, depen- dency parsing and semantic role labeling. The code is available at https://github.com/ gozdesahin/crop-rotate-augment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>Gözde Gül S ¸ ahin was a PhD student at Istanbul Technical University and a visiting research stu- dent at University of Edinburgh during this study. She was funded by Tübitak (The Scientific and Technological Research Council of Turkey) 2214- A scholarship during her visit to University of Ed- inburgh. This work was supported by ERC H2020 Advanced Fellowship GA 742137 SEMANTAX and a Google Faculty award to Mark Steedman. We would like to thank Adam Lopez for fruitful discussions, guidance and support during the first author's visit. We thank to the anonymous review- ers for useful comments and to Ilia Kuznetsov for his valuable feedback.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Babası</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Demonstration of augmentation ideas on the Turkish sentence "Babası ona bir mektup yazdı" (Her father wrote her a letter). S: Subject, V: Verb, O:Object, IO: Indirect Object. Arrows are drawn from dependent to head. Both methods are applied to the Labels of Interest (LOI).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Treebank size versus gain by augmentation</figDesc><graphic url="image-1.png" coords="5,122.24,65.60,256.55,161.87" type="bitmap" /></figure>

			<note place="foot" n="1"> For example, in the case of dependency parsing, recent best results from CoNLL-18 parsing shared task can be compared to the results of traditional language-specific models.</note>

			<note place="foot" n="2"> Focus should not be confused with the grammatical category FOC.</note>

			<note place="foot" n="3"> Although the total size of the Irish dataset is larger than many, the splits are unbalanced. The training set contains 121 trees while the test has 454 trees.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ueli</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06-16" />
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Data augmentation for low-resource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marzieh</forename><surname>Fadaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="567" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quantifying word order freedom in dependency corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Futrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Dependency Linguistics</title>
		<meeting>the Third International Conference on Dependency Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="100" />
		</imprint>
		<respStmt>
			<orgName>Uppsala University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Twostage data augmentation for low-resourced speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Tsakalidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard M</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2378" to="2382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Data recombination for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Audio augmentation for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2015, 16th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-06" />
			<biblScope unit="page" from="3586" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12-36" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
	<note>Hinton</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ramon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Universal dependencies 2.1. LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics, Faculty of Mathematics and Physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>Charles University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Data augmentation for low resource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Ragni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><forename type="middle">M</forename><surname>Knill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">J F</forename><surname>Rath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2014, 15th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09-14" />
			<biblScope unit="page" from="810" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks and data augmentation for acoustic event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beat</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Interspeech</title>
		<imprint>
			<biblScope unit="page" from="2982" to="2986" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sentence simplification for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vickrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2008, Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Columbus, Ohio, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1520" />
			<biblScope unit="page" from="344" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
