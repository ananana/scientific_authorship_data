<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context-Dependent Sense Embedding *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Qiu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Context-Dependent Sense Embedding *</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="183" to="191"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Word embedding has been widely studied and proven helpful in solving many natural language processing tasks. However, the ambiguity of natural language is always a problem on learning high quality word embed-dings. A possible solution is sense embedding which trains embedding for each sense of words instead of each word. Some recent work on sense embedding uses context clustering methods to determine the senses of words, which is heuristic in nature. Other work creates a probabilistic model and performs word sense disambiguation and sense embedding iteratively. However, most of the previous work has the problems of learning sense embeddings based on imperfect word embeddings as well as ignoring the dependency between sense choices of neighboring words. In this paper, we propose a novel probabilistic model for sense embedding that is not based on problematic word embedding of polysemous words and takes into account the dependency between sense choices. Based on our model, we derive a dynamic programming inference algorithm and an Expectation-Maximization style unsupervised learning algorithm. The empirical studies show that our model outperforms the state-of-the-art model on a word sense induction task by a 13% relative gain.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributed representation of words (aka word em- bedding) aims to learn continuous-valued vectors to * The second author was supported by the National Natural Science Foundation of China (61503248).</p><p>represent words based on their context in a large cor- pus. They can serve as input features for algorithms of natural language processing (NLP) tasks. High quality word embeddings have been proven helpful in many NLP tasks <ref type="bibr" target="#b5">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b29">Turian et al., 2010;</ref><ref type="bibr" target="#b6">Collobert et al., 2011;</ref><ref type="bibr" target="#b17">Maas et al., 2011;</ref><ref type="bibr" target="#b3">Chen and Manning, 2014)</ref>. Recently, with the development of deep learning, many novel neural network architectures are proposed for train- ing high quality word embeddings ( <ref type="bibr" target="#b18">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b19">Mikolov et al., 2013b</ref>).</p><p>However, since natural language is intrinsically ambiguous, learning one vector for each word may not cover all the senses of the word. In the case of a multi-sense word, the learned vector will be around the average of all the senses of the word in the em- bedding space, and therefore may not be a good rep- resentation of any of the senses. A possible solution is sense embedding which trains a vector for each sense of a word. There are two key steps in training sense embeddings. First, we need to perform word sense disambiguation (WSD) or word sense induc- tion (WSI) to determine the senses of words in the training corpus. Then, we need to train embedding vectors for word senses according to their contexts.</p><p>Early work on sense embedding <ref type="bibr" target="#b25">(Reisinger and Mooney, 2010;</ref><ref type="bibr" target="#b12">Huang et al., 2012;</ref><ref type="bibr" target="#b23">Neelakantan et al., 2014;</ref><ref type="bibr" target="#b15">Kageback et al., 2015;</ref><ref type="bibr" target="#b16">Li and Jurafsky, 2015)</ref> proposes context clus- tering methods which determine the sense of a word by clustering aggregated embeddings of words in its context. This kind of methods is heuristic in nature and relies on external knowledge from lexicon like WordNet <ref type="bibr" target="#b20">(Miller, 1995)</ref>.</p><p>Recently, sense embedding methods based on complete probabilistic models and well-defined learning objective functions ( <ref type="bibr" target="#b28">Tian et al., 2014;</ref><ref type="bibr" target="#b0">Bartunov et al., 2016;</ref><ref type="bibr" target="#b13">Jauhar et al., 2015</ref>) become more popular. These methods regard the choice of senses of the words in a sentence as hidden vari- ables. Learning is therefore done with expectation- maximization style algorithms, which alternate be- tween inferring word sense choices in the training corpus and learning sense embeddings.</p><p>A common problem with these methods is that they model the sense embedding of each center word dependent on the word embeddings of its context words. As we previously explained, word embed- ding of a polysemous word is not a good repre- sentation and may negatively influence the qual- ity of inference and learning. Furthermore, these methods choose the sense of each word in a sen- tence independently, ignoring the dependency that may exist between the sense choices of neighbor- ing words. We argue that such dependency is im- portant in word sense disambiguation and therefore helpful in learning sense embeddings. For exam- ple, consider the sentence "He cashed a check at the bank". Both "check" and "bank" are ambiguous here. Although the two words hint at banking related senses, the hint is not decisive (as an alternative in- terpretation, they may represent a check mark at a river bank). Fortunately, "cashed" is not ambiguous and it can help disambiguate "check". However, if we consider a small context window in sense em- bedding, then "cashed" cannot directly help disam- biguate "bank". We need to rely on the dependency between the sense choices of "check" and "bank" to disambiguate "bank".</p><p>In this paper, we propose a novel probabilistic model for sense embedding that takes into account the dependency between sense choices of neighbor- ing words. We do not learn any word embeddings in our model and hence avoid the problem with em- bedding polysemous words discussed above. Our model has a similar structure to a high-order hidden Markov model. It contains a sequence of observable words and latent senses and models the dependency between each word-sense pair and between neigh- boring senses in the sequence. The energy of neigh- boring senses can be modeled using existing word embedding approaches such as CBOW and Skip- gram ( <ref type="bibr" target="#b18">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b19">Mikolov et al., 2013b</ref>). Given the model and a sentence, we can perform ex- act inference using dynamic programming and get the optimal sense sequence of the sentence. Our model can be learned from an unannotated corpus by optimizing a max-margin objective using an al- gorithm similar to hard-EM.</p><p>Our main contributions are the following:</p><p>1. We propose a complete probabilistic model for sense embedding. Unlike previous work, we model the dependency between sense choices of neighboring words and do not learn sense embeddings dependent on problematic word embeddings of polysemous words.</p><p>2. Based on our proposed model, we derive an exact inference algorithm and a max-margin learning algorithm which do not rely on ex- ternal knowledge from any knowledge base or lexicon (except that we determine the numbers of senses of polysemous words according to an existing sense inventory).</p><p>3. The performance of our model on contex- tual word similarity task is competitive with previous work and we obtain a 13% relative gain compared with previous state-of-the-art methods on the word sense induction task of SemEval-2013.</p><p>The rest of this paper is organized as follows. We introduce related work in section 2. Section 3 de- scribes our models and algorithms in detail. We present our experiments and results in section 4. In section 5, a conclusion is given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Distributed representation of words (aka word em- bedding) was proposed in 1986 <ref type="bibr" target="#b11">(Hinton, 1986;</ref><ref type="bibr" target="#b27">Rumelhart et al., 1986)</ref>. In 2003, <ref type="bibr" target="#b2">Bengio et al. (2003)</ref> proposed a neural network architecture to train language models which produced word em- beddings in the neural network. <ref type="bibr" target="#b21">Mnih and Hinton (2007)</ref> replaced the global normalization layer of Bengio's model with a tree-structure to accel- erate the training process. <ref type="bibr" target="#b5">Collobert and Weston (2008)</ref> introduced a max-margin objective function to replace the most computationally expensive max- likelihood objective function. Recently proposed Skip-gram model, CBOW model and GloVe model ( <ref type="bibr" target="#b18">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b19">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b24">Pennington et al., 2014</ref>) were more efficient than tradi- tional models by introducing a log-linear layer and making it possible to train word embeddings with a large scale corpus. With the development of neu- ral network and deep learning techniques, there have been a lot of work based on neural network mod- els to obtain word embedding ( <ref type="bibr" target="#b29">Turian et al., 2010;</ref><ref type="bibr" target="#b6">Collobert et al., 2011;</ref><ref type="bibr" target="#b17">Maas et al., 2011;</ref><ref type="bibr" target="#b3">Chen and Manning, 2014</ref>). All of them have proven that word embedding is helpful in NLP tasks.</p><p>However, the models above assumed that one word has only one vector as its representation which is problematic for polysemous words. <ref type="bibr" target="#b25">Reisinger and Mooney (2010)</ref> proposed a method for con- structing multiple sense-specific representation vec- tors for one word by performing word sense dis- ambiguation with context clustering. <ref type="bibr" target="#b12">Huang et al. (2012)</ref> further extended this context cluster- ing method and incorporated global context to learn multi-prototype representation vectors.  extended the context clustering method and performed word sense disambiguation according to sense glosses from WordNet <ref type="bibr" target="#b20">(Miller, 1995)</ref>. <ref type="bibr" target="#b23">Neelakantan et al. (2014)</ref> proposed an extension of the Skip-gram model combined with context clustering to estimate the number of senses for each word as well as learn sense embedding vectors. Instead of performing word sense disambiguation tasks, <ref type="bibr" target="#b15">Kageback et al. (2015)</ref> proposed the instance-context em- bedding method based on context clustering to per- form word sense induction tasks. <ref type="bibr" target="#b16">Li and Jurafsky (2015)</ref> introduced a multi-sense embedding model based on the Chinese Restaurant Process and applied it to several natural language understanding tasks.</p><p>Since the context clustering based models are heuristic in nature and rely on external knowledge, recent work tends to create probabilistic models for learning sense embeddings. <ref type="bibr" target="#b28">Tian et al. (2014)</ref> proposed a multi-prototype Skip-gram model and designed an Expectation-Maximization (EM) algo- rithm to do word sense disambiguation and learn sense embedding vectors iteratively. <ref type="bibr" target="#b13">Jauhar et al. (2015)</ref> extended the EM training framework and retrofitted embedding vectors to the ontology of WordNet. <ref type="bibr" target="#b0">Bartunov et al. (2016)</ref> proposed a non- parametric Bayesian extension of Skip-gram to au- tomatically learn the required numbers of represen- tations for all words and perform word sense induc- tion tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Context-Dependent Sense Embedding Model</head><p>We propose the context-dependent sense embedding model for training high quality sense embeddings which takes into account the dependency between sense choices of neighboring words. Unlike pervi- ous work, we do not learn any word embeddings in our model and hence avoid the problem with embed- ding polysemous words discussed previously. In this section, we will introduce our model and describe our inference and learning algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model</head><p>We begin with the notation in our model. In a sen- tence, let w i be the i th word of the sentence and s i be the sense of the i th word. S(w) denotes the set of all the senses of word w. We assume that the sets of senses of different words do not overlap. Therefore, in this paper a word sense can be seen as a lexeme of the word ( <ref type="bibr" target="#b26">Rothe and Schutze, 2015)</ref>. Our model can be represented as a Markov net- work shown in <ref type="figure" target="#fig_0">Figure 1</ref>. It is similar to a high- order hidden Markov model. The model contains a sequence of observable words (w 1 , w 2 , . . .) and la- tent senses (s 1 , s 2 , . . .). It models the dependency between each word-sense pair and between neigh- boring senses in the sequence. The energy function is formulated as follows:</p><formula xml:id="formula_0">E(w, s) = i E 1 (w i , s i ) + E 2 (s i−k , . . . , s i+k )<label>(1)</label></formula><p>Here w = {w i |1 ≤ i ≤ l} is the set of words in a sentence with length l and s = {s i |1 ≤ i ≤ l} is the set of senses. The function E 1 models the de- pendency between a word-sense pair. As we assume that the sets of senses of different words do not over- lap, we can formulate E 1 as follows: </p><formula xml:id="formula_1">E 1 (w i , s i ) = 0 s i ∈ S(w i ) +∞ s i / ∈ S(w i )<label>(2)</label></formula><p>Here we assume that all the matched word-sense pairs have the same energy, but it would also be interesting to model the degrees of matching with different energy values in E 1 . In Equation 1, the function E 2 models the compatibility of neighboring senses in a context window with fixed size k. Ex- isting embedding approaches like CBOW and Skip- gram ( <ref type="bibr" target="#b18">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b19">Mikolov et al., 2013b)</ref> can be used here to define E 2 . The formulation us- ing CBOW is as follows:</p><formula xml:id="formula_2">E 2 (s i−k , . . . , s i+k ) = − σ i−k≤j≤i+k,j =i V T (s j )V (s i )<label>(3)</label></formula><p>Here V (s) and V (s) are the input and output em- bedding vectors of sense s. The function σ is an activation function and we use the sigmoid function here in our model. The formulation using Skip-gram can be defined in a similar way:</p><formula xml:id="formula_3">E 2 (s i−k , . . . , s i+k ) = − i−k≤j≤i+k,j =i σ V T (s j )V (s i )<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inference</head><p>In this section, we introduce our inference algo- rithm. Given the model and a sentence w, we want to infer the most likely values of the hidden variables (i.e. the optimal sense sequence of the sentence) that minimize the energy function in Equation 1:</p><formula xml:id="formula_4">s * = arg min s E(w, s)<label>(5)</label></formula><p>We use dynamic programming to do inference which is similar to the Viterbi algorithm of the hidden </p><formula xml:id="formula_5">+ E 1 (w i , A i ) + E 2 (A i−2k , . . . , A i )<label>(6)</label></formula><p>Once we finish the forward process, we can retrieve the best sense sequence with a backward process. The time complexity of the algorithm is O(n 4k l) where n is the maximal number of senses of a word. Because most words in a typical sentence have either a single sense or far less than n senses, the actual running time of the algorithm is very fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning</head><p>In this section, we introduce our unsupervised learn- ing algorithm. In learning, we want to learn all the input and output sense embedding vectors that opti- mize the following max-margin objective function:</p><formula xml:id="formula_6">Θ * = arg min Θ w∈C min s w i=1 sneg∈Sneg(w i ) max 1 + E 1 (w i , s i ) + E 2 (s i−k , . . . , s i+k )− E 2 (s i−k , . . . , s i−1 , s neg , s i+1 , . . . , s i+k ), 0<label>(7)</label></formula><p>Here Θ is the set of all the parameters includ- ing V and V for all the senses. C is the set of training sentences. Our learning objective is similar to the negative sampling and max-margin objective proposed for word embedding <ref type="bibr" target="#b5">(Collobert and Weston, 2008</ref>). S neg (w i ) denotes the set of negative samples of senses of word w i which is defined with the following strategy. For a polysemous word w i , S neg (w i ) = S(w i )\{s i }. For the other words with a single sense, S neg (w i ) is a set of randomly selected senses of a fixed size.</p><p>The objective in Equation 7 can be optimized by coordinate descent which in our case is equivalent to the hard Expectation-Maximization algorithm. In the hard E step, we run the inference algorithm us- ing the current model parameters to get the optimal sense sequences of the training sentences. In the M step, with the sense sequences s of all the sentences fixed, we learn sense embedding vectors. Assume we use the CBOW model for E 2 (Equation 3), then the M-step objective function is as follows:</p><formula xml:id="formula_7">Θ * = arg min Θ w∈C w i=1 sneg∈Sneg(w i ) max 1 − σ( i−k≤j≤i+k,j =i V (s j ) T V (s i )) + σ( i−k≤j≤i+k,j =i V (s j ) T V (s neg )), 0<label>(8)</label></formula><p>Here E 1 is omitted because the sense sequences produced from the E-step always have zero E 1 value. Similarly, if we use the Skip-gram model for E 2 (Equation 4), then the M-step objective function is:</p><formula xml:id="formula_8">Θ * = arg min Θ w∈C w i=1 i−k≤j≤i+k,j =i sneg∈Sneg(w i ) max 1 − σ(V (s j ) T V (s i )) + σ(V (s j ) T V (s neg )), 0<label>(9)</label></formula><p>We optimize the M-step objective function using stochastic gradient descent.</p><p>We use a mini batch version of the hard EM al- gorithm. For each sentence in the training corpus, we run E-step to infer its sense sequence and then immediately run M-step (for 1 iteration of stochas- tic gradient descent) to update the model parameters based on the senses in the sentence. Therefore, the batch size of our algorithm depends on the length of each sentence.</p><p>The advantage of using mini batch is twofold. First, while our learning objective is highly non- convex ( <ref type="bibr" target="#b28">Tian et al., 2014</ref>), the randomness in mini batch hard EM may help us avoid trapping into local optima. Second, the model parameters are updated more frequently in mini batch hard EM, resulting in faster convergence.</p><p>Note that before running hard-EM, we need to determine, for each word w, the size of S(w). In our experiments, we used the sense inventory pro- vided by Coarse-Grained English All-Words Task of <ref type="bibr">SemEval-2007</ref><ref type="bibr">Task 07 (Navigli et al., 2007</ref>) to de- termine the number of senses for each word. The sense inventory is a coarse version of WordNet sense inventory. We do not use the WordNet sense in- ventory because the senses in WordNet are too fine- grained and are difficult to recognize even for human annotators <ref type="bibr" target="#b8">(Edmonds and Kilgarriff, 2002</ref>). Since we do not link our learned senses with external sense inventories, our approach can be seen as performing WSI instead of WSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>This section presents our experiments and results. First, we describe our experimental setup includ- ing the training corpus and the model configuration. <ref type="table" target="#tab_0">Nearest Neigbors  bank 1  banking, lender, loan  bank 2  river, canal, basin  bank 3  slope, tilted, slant  apple 1 macintosh, imac, blackberry  apple 2</ref> peach, cherry, pie date 1 birthdate, birth, day date 2 appointment, meet, dinner fox 1 cbs, abc, nbc fox 2 wolf, deer, rabbit Then, we perform a qualitative evaluation on our model by presenting the nearest neighbors of senses of some polysemous words. Finally, we introduce two different tasks and show the experimental re- sults on these tasks respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Training Corpus</head><p>Our training corpus is the commonly used Wikipedia corpus. We dumped the October 2015 snapshot of the Wikipedia corpus which contains 3.6 million articles. In our experiments, we removed the infrequent words with less than 20 occurrences and the training corpus contains 1.3 billion tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Configuration</head><p>In our experiments, we set the context window size to 5 (5 words before and after the center word). The embedding vector size is set to 300. The size of negative sample sets of single-sense words is set to 5. We trained our model using AdaGrad stochas- tic gradient decent ( <ref type="bibr" target="#b7">Duchi et al., 2010</ref>) with initial learning rate set to 0.025. Our configuration is simi- lar to that of previous work.</p><p>Similar to Word2vec, we initialized our model by randomizing the sense embedding vectors. The number of senses of all the words is determined with the sense inventory provided by Coarse-Grained En- glish All-Words Task of <ref type="bibr">SemEval-2007</ref><ref type="bibr">Task 07 (Navigli et al., 2007</ref> as we explained in section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Case Study</head><p>In this section, we give a qualitative evaluation of our model by presenting the nearest neighbors of the senses of some polysemous words. <ref type="table" target="#tab_0">Table 1</ref> shows the results of our qualitative evaluation. We list sev- eral polysemous words in the table, and for each word, some typical senses of the word are picked. The nearest neighbors of each sense are listed aside. We used the cosine distance to calculate the distance between sense embedding vectors and find the near- est neighbors.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, we can observe that our model pro- duces good senses for polysemous words. For exam- ple, the word "bank" can be seen to have three dif- ferent sense embedding vectors. The first one means the financial institution. The second one means the sloping land beside water. The third one means the action of tipping laterally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Word Similarity in Context</head><p>This section gives a quantitative evaluation of our model on word similarity tasks. Word similar- ity tasks evaluate a model's performance with the Spearman's rank correlation between the similarity scores of pairs of words given by the model and the manual labels. However, traditional word similarity tasks like Wordsim-353 ( <ref type="bibr" target="#b10">Finkelstein et al., 2001</ref>) are not suitable for evaluating sense embedding models because these datasets do not include enough am- biguous words and there is no context information for the models to infer and disambiguate the senses of the words. To overcome this issue, <ref type="bibr" target="#b12">Huang et al. (2012)</ref> released a new dataset named Stanford's Contextual Word Similarities (SCWS) dataset. The dataset consists of 2003 pairs of words along with human labelled similarity scores and the sentences containing these words.</p><p>Given a pair of words and their contexts, we can perform inference using our model to disam- biguate the questioned words. A similarity score can be calculated with the cosine distance between the two embedding vectors of the inferred senses of the questioned words. We also propose another method for calculating similarity scores. In the inference process, we compute the energy of each sense choice of the questioned word and consider the negative en- ergy as the confidence of the sense choice. Then we calculate the cosine similarity between all pairs of senses of the questioned words and compute the av- erage of similarity weighted by the confidence of the senses.   <ref type="table" target="#tab_2">Table 2</ref> shows the results of our context- dependent sense embedding models on the SCWS dataset. In this table, ρ refers to the Spearman's rank correlation and a higher value of ρ indicates better performance. The baseline performances are from From <ref type="table" target="#tab_2">Table 2</ref>, we can observe that our model out- performs the other probabilistic models and is not as good as the best context clustering based model. The context clustering based models are overall bet- ter than the probabilistic models on this task. A possible reason is that most context clustering based methods make use of more external knowledge than probabilistic models. However, note that <ref type="bibr" target="#b9">Faruqui et al. (2016)</ref> presented several problems associated with the evaluation of word vectors on word simi- larity datasets and pointed out that the use of word similarity tasks for evaluation of word vectors is not sustainable. <ref type="bibr" target="#b0">Bartunov et al. (2016)</ref> also suggest that SCWS should be of limited use for evaluating word representation models. Therefore, the results on this task shall be taken with caution. We consider that more realistic natural language processing tasks like word sense induction are better for evaluating sense embedding models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Word Sense Induction</head><p>In this section, we present an evaluation of our model on the word sense induction (WSI) tasks. The WSI task aims to discover the different meanings for words used in sentences. Unlike a word sense disambiguation (WSD) system, a WSI system does not link the sense annotation results to an existing sense inventory. Instead, it produces its own sense inventory and links the sense annotation results to this sense inventory. Our model can be seen as a WSI system, so we can evaluate our model with WSI tasks.</p><p>We used the dataset from task 13 of SemEval- 2013 as our evaluation set ( <ref type="bibr" target="#b14">Jurgens and Klapaftis, 2013)</ref>. The dataset contains 4664 instances inflected from one of the 50 lemmas. Both single-sense instances and instances with a graded mixture of senses are included in the dataset. In this paper, we only consider the single sense instances. <ref type="bibr" target="#b14">Jurgens and Klapaftis (2013)</ref> propose two fuzzy measures named Fuzzy B-Cubed (FBC) and Fuzzy Normalized Mu- tual Information (FNMI) for comparing fuzzy sense assignments from WSI systems. the FBC measure summarizes the performance per instance while the FNMI measure is based on sense clusters rather than instances. <ref type="table">Table 3</ref> shows the results of our context- dependent sense embedding models on this dataset. Here HM is the harmonic mean of FBC and FNMI. The result of AI-KU is from <ref type="bibr" target="#b1">Baskaya et al. (2013)</ref>, MSSG is from <ref type="bibr" target="#b23">Neelakantan et al. (2014)</ref>, ICE- online and ICE-kmeans are from <ref type="bibr" target="#b15">Kageback et al. (2015)</ref>. Our models are denoted in the same way as in the previous section.</p><p>From <ref type="table">Table 3</ref>, we can observe that our models Model FBC(%) FNMI(%) HM AI-KU 35.1 4.5 8.0 MSSG 45.9 3.7 6.8 ICE-online 48.7 5.5 9.9 ICE-kmeans 51.1 5.9 10.6 Ours + CBOW 53.8 6.3 11.3 Ours + Skip-gram 56.9 6.7 12.0 <ref type="table">Table 3</ref>: Results of single-sense instances on task 13 of <ref type="bibr">SemEval-2013</ref> outperform the previous state-of-the-art models and achieve a 13% relative gain. It shows that our mod- els can beat context clustering based models on re- alistic natural language processing tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we propose a novel probabilistic model for learning sense embeddings. Unlike previous work, we do not learn sense embeddings dependent on word embeddings and hence avoid the problem with inaccurate embeddings of polysemous words. Furthermore, we model the dependency between sense choices of neighboring words which can help us disambiguate multiple ambiguous words in a sen- tence. Based on our model, we derive a dynamic programming inference algorithm and an EM-style unsupervised learning algorithm which do not rely on external knowledge from any knowledge base or lexicon except that we determine the number of senses of polysemous words according to an existing sense inventory. We evaluate our model both quali- tatively by case studying and quantitatively with the word similarity task and the word sense induction task. Our model is competitive with previous work on the word similarity task. On the word sense in- duction task, our model outperforms the state-of- the-art model and achieves a 13% relative gain.</p><p>For the future work, we plan to try learning our model with soft EM. Besides, we plan to use shared senses instead of lexemes in our model to improve the generality of our model. Also, we will study unsupervised methods to link the learned senses to existing inventories and to automatically determine the numbers of senses. Finally, we plan to evaluate our model with more NLP tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Context-Dependent Sense Embedding Model with window size k = 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Markov model.</head><label></label><figDesc>Specifically, for every valid assignment A i−2k , . . . , A i−1 of every sub- sequence of senses s i−2k , . . . , s i−1 , we define m(A i−2k , . . . , A i−1 ) as the energy of the best sense sequence up to position i − 1 that is consistent with the assignment A i−2k , . . . , A i−1 . We start with m(A 1 , . . . , A 2k ) = 0 and then recursively compute m in a left-to-right forward process based on the up- date formula: m(A i−2k+1 , . . . , A i ) = min A i−2k m(A i−2k , . . . , A i−1 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Huang et al. (2012), Chen et al. (2014), Neelakan- tan et al. (2014), Li and Jurafsky (2015), Tian et al. (2014) and Bartunov et al. (2016). Here Ours + CBOW denotes our model with a CBOW based energy function and Ours + Skip-gram denotes our model with a Skip-gram based energy function. The results above the thick line are the models based on context clustering methods and the results below the thick line are the probabilistic models including ours. The similarity metrics of context clustering based models are AvgSim and AvgSimC proposed by Reisinger and Mooney (2010). Tian et al. (2014) propose two metrics Model M and Model W which are similar to our HardSim and SoftSim metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : The nearest neighbors of senses of polysemous words</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>The first method is named HardSim and the</figDesc><table>Model 

Similarity 
Metrics 
ρ × 100 

Huang 
AvgSim 
62.8 
Huang 
AvgSimC 
65.7 
Chen 
AvgSim 
66.2 
Chen 
AvgSimC 
68.9 
Neelakantan 
AvgSim 
67.2 
Neelakantan 
AvgSimC 
69.2 
Li 
69.7 
Tian 
Model M 
63.6 
Tian 
Model W 
65.4 
Bartunov 
AvgSimC 
61.2 
Ours + CBOW 
HardSim 
64.3 
Ours + CBOW 
SoftSim 
65.6 
Ours + Skip-gram HardSim 
64.9 
Ours + Skip-gram SoftSim 
66.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Spearman's rank correlation results on the SCWS 

dataset 

second method is named SoftSim. 
</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Breaking sticks and ambiguities with adaptive skip-gram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kondrashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Osokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ai-ku: Using substitute vectors and co-occurrence modeling for word sense induction and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osman</forename><surname>Baskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enis</forename><surname>Sert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkan</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Joint Conference on Lexical and Computational Semantics (*SEM)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="300" to="306" />
		</imprint>
	</monogr>
	<note>Seventh International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">Sbastien</forename><surname>Sencal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frderic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">Luc</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified model for word sense representation and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="257" to="269" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Introduction to the special issue on evaluating word sense disambiguation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Edmonds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="279" to="291" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02276</idno>
		<title level="m">Problems with evaluation of word embeddings using word similarity tasks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Placing search in context: the concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of international conference on World Wide Web</title>
		<meeting>international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning distributed representations of concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth annual conference of the cognitive science society</title>
		<meeting>the eighth annual conference of the cognitive science society</meeting>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ontologically grounded multi-sense representation learning for semantic vector space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Sujay Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="683" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semeval2013 task 13: Word sense induction for graded and non-graded senses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Klapaftis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Joint Conference on Lexical and Computational Semantics (*SEM)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="290" to="299" />
		</imprint>
	</monogr>
	<note>Seventh International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural context embeddings for automatic discovery of word senses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Kageback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devdatt</forename><surname>Dubhashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Do multi-sense embeddings improve natural language understanding? In EMNLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1722" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop at ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Machine Learning</title>
		<meeting>the Twenty-Fourth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semeval-2007 task 07: coarse-grained english all-words task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">C</forename><surname>Litkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orin</forename><surname>Hargraves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Semantic Evaluations</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="30" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient nonparametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1059" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiprototype vector-space models of word meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Autoextend: Extending word embeddings to embeddings for synsets and lexemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schutze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1793" to="1803" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning representation by backpropagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A probabilistic model for learning multi-prototype word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
