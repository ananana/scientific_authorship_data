<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:54+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Golub</surname></persName>
							<email>golubd@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
								<orgName type="institution" key="instit3">LLC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
								<orgName type="institution" key="instit3">LLC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
								<orgName type="institution" key="instit3">LLC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
							<email>l.deng@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
								<orgName type="institution" key="instit3">LLC</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="835" to="844"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We develop a technique for transfer learning in machine comprehension (MC) using a novel two-stage synthesis network (SynNet). Given a high-performing MC model in one domain, our technique aims to answer questions about documents in another domain, where we use no labeled data of question-answer pairs. Using the proposed SynNet with a pretrained model on the SQuAD dataset, we achieve an F1 measure of 46.6% on the challenging NewsQA dataset, approaching performance of in-domain models (F1 measure of 50.0%) and outperforming the out-of-domain baseline by 7.6%, without use of provided annotations. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine comprehension (MC), the ability to an- swer questions over a provided context paragraph, is a key task in natural language processing. The rise of high-quality, large-scale human-annotated datasets for this task <ref type="bibr" target="#b25">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b32">Trischler et al., 2016</ref>) has allowed for the train- ing of data-intensive but expressive models such as deep neural networks ( <ref type="bibr" target="#b29">Seo et al., 2017)</ref>. Moreover, these datasets have the attractive quality that the answer is a short snippet of text within the paragraph, which narrows the search space of possible answer spans.</p><p>However, many of these models rely on large amounts of human-labeled data for training. Yet * Work performed while interning at Microsoft Research.</p><p>† Work performed when the author was at Microsoft Re- search. <ref type="bibr">1</ref> Code will be available at https://github.com/ davidgolub/QuestionGeneration data collection is a time-consuming and expensive task. Moreover, direct application of a MC model trained on one domain to answer questions over paragraphs from another domain may suffer per- formance degradation.</p><p>While understudied, the ability to transfer a MC model to multiple domains is of great practical im- portance. For instance, the ability to quickly use a MC model trained on Wikipedia to bootstrap a question-answering system over customer support manuals or news articles, where there is no labeled data, can unlock a great number of practical appli- cations.</p><p>In this paper, we address this problem in MC through a two-stage synthesis network (SynNet). The SynNet generates synthetic question-answer pairs over paragraphs in a new domain that are then used in place of human-generated annotations to finetune a MC model trained on the original do- main.</p><p>The idea of generating synthetic data to aug- ment insufficient training data has been explored before. For example, for the target task of trans- lation, <ref type="bibr" target="#b28">Sennrich et al. (2016)</ref> present a method to generate synthetic translations given real sen- tences to refine an existing machine translation system.</p><p>However, unlike machine translation, for tasks like MC, we need to synthesize both the question and answers given the context paragraph. More- over, while the question is a syntactically fluent natural language sentence, the answer is mostly a salient semantic concept in the paragraph, e.g., a named entity, an action, or a number, which is of- ten a single word or short phrase. <ref type="bibr">2</ref> Since the an- swer has a very different linguistic structure com- pared to the question, it may be more appropri-  <ref type="figure">Figure 1</ref>: Illustration of the two-stage SynNet. The SynNet is trained to synthesize the answer and the question, given the paragraph. The first stage of the model, an answer synthesis module, uses a bi-directional LSTM to predict IOB tags on the input paragraph, which mark out key semantic concepts that are likely answers. The second stage, a question syn- thesis module, uses a uni-directional LSTM to generate the question, while attending on embeddings of the words in the paragraph and IOB ids. Although multiple spans in the para- graph could be identified as potential answers, we pick one span when generating the question.</p><p>ate to view answers and questions as two different types of data. Hence, the synthesis of a (question, answer) tuple is needed.</p><p>In our approach, we decompose the process of generating question-answer pairs into two steps, answer generation conditioned on the paragraph, and question generation conditioned on the para- graph and answer. We generate the answer first be- cause answers are usually key semantic concepts, while questions can be viewed as a full sentence composed to inquire the concept.</p><p>Using the proposed SynNet, we are able to outperform a strong baseline of directly apply- ing a high-performing MC model trained on an- other domain. For example, when we apply our algorithm using a pretrained model on the Stan- ford Question-Answering Dataset (SQuAD) <ref type="bibr" target="#b25">(Rajpurkar et al., 2016)</ref>, which consists of Wikipedia articles, to answer questions on the NewsQA dataset ( <ref type="bibr" target="#b32">Trischler et al., 2016)</ref>, which consists of CNN/Daily Mail articles, we improve the per- formance of the SQuAD baseline from 39.0% to 46.6% F1 and approach results of previously published work of <ref type="bibr" target="#b32">Trischler et al. (2016)</ref> (50.0% F1), without use of labeled data in the new do- main. Moreover, an error analysis reveals that we achieve higher accuracy over the baseline on all common question types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question Answering</head><p>Question answering is an active area in natural lan- guage processing with ongoing research in many directions <ref type="bibr" target="#b1">(Berant et al., 2013;</ref><ref type="bibr" target="#b12">Hill et al., 2015;</ref><ref type="bibr" target="#b9">Golub and He, 2016;</ref><ref type="bibr" target="#b4">Chen et al., 2016;</ref><ref type="bibr" target="#b11">Hermann et al., 2015)</ref>. Machine comprehension, a form of extractive question answering where the answer is a snippet or multiple snippets of text within a con- text paragraph, has recently attracted a lot of atten- tion in the community. The rise of large-scale hu- man annotated datasets with over 100,000 realistic question-answer pairs such as SQuAD ( <ref type="bibr" target="#b25">Rajpurkar et al., 2016</ref>), NewsQA ( <ref type="bibr" target="#b32">Trischler et al., 2016)</ref>, and MSMARCO ( <ref type="bibr" target="#b22">Nguyen et al., 2016)</ref>, has led to a large number of successful deep learning models ( <ref type="bibr" target="#b19">Lee et al., 2016;</ref><ref type="bibr" target="#b29">Seo et al., 2017;</ref><ref type="bibr" target="#b5">Dhingra et al., 2017;</ref><ref type="bibr" target="#b34">Wang and Jiang, 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semi-Supervised Learning</head><p>Semi-supervised learning has a long history (c.f. <ref type="bibr" target="#b3">Chapelle et al. (2009)</ref> for an overview), and has been applied to many tasks in natural language processing such as dependency parsing ( <ref type="bibr" target="#b17">Koo et al., 2008)</ref>, sentiment analysis ( <ref type="bibr" target="#b39">Yang et al., 2015)</ref>,ma- chine translation <ref type="bibr" target="#b28">(Sennrich et al., 2016)</ref>, and se- mantic parsing <ref type="bibr" target="#b2">(Berant and Liang, 2014;</ref><ref type="bibr" target="#b35">Wang et al., 2015;</ref><ref type="bibr" target="#b14">Jia and Liang, 2016)</ref>. Recent work generated synthetic annotations on unsuper- vised data to boost the performance of both read- ing comprehension and visual question answering models ( <ref type="bibr" target="#b40">Yang et al., 2017;</ref><ref type="bibr" target="#b26">Ren et al., 2015</ref>), but on domains with some form of annotated data. There has also been work on generating high-quality questions ( <ref type="bibr">Yuan et al., 2017;</ref><ref type="bibr" target="#b30">Serban et al., 2016;</ref><ref type="bibr" target="#b18">Labutov et al., 2015)</ref>, but not how to best use them to train a model. In contrast, we use the two-stage SynNet to generate data tuples to directly boost performance of a model on a domain with no an- notations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Transfer Learning</head><p>Transfer learning <ref type="bibr" target="#b23">(Pan and Yang, 2010)</ref> has been successfully applied to numerous domains in ma-chine learning, such as machine translation <ref type="bibr" target="#b44">(Zoph et al., 2016)</ref>, computer vision, <ref type="bibr" target="#b31">(Sharif Razavian et al., 2014)</ref>, and speech recognition ( <ref type="bibr" target="#b6">Doulaty et al., 2015)</ref>. Specifically, object recognition mod- els trained on the large-scale ImageNet challenge ( <ref type="bibr" target="#b27">Russakovsky et al., 2015</ref>) have proven to be ex- cellent feature extractors for diverse tasks such as image captioning (i.e., <ref type="bibr" target="#b21">Lu et al. (2017)</ref>; <ref type="bibr" target="#b7">Fang et al. (2015)</ref>; <ref type="bibr" target="#b15">Karpathy and Fei-Fei (2015)</ref>) and visual question answering (i.e., <ref type="bibr" target="#b43">Zhou et al. (2015)</ref>; <ref type="bibr" target="#b38">Xu and Saenko (2016)</ref>; <ref type="bibr" target="#b8">Fukui et al. (2016)</ref>; ), among others. In a similar fashion, we use a model pretrained on the SQuAD dataset as a generic feature extractor to bootstrap a QA system on NewsQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Transfer Learning Task for MC</head><p>We formalize the task of machine comprehension below. Our MC model takes as input a tokenized question q = {q 0 , q 1 , ...q n }, a context paragraph p = {p 0 , p 1 , ...p n }, where q i , p i are words, and learns a function f (p, q) → {a start , a end } where a start and a end are pointer indices into paragraph p, i.e., the answer a = p astart ...p a end .</p><p>Given a collection of labeled paragraph, ques- tion, answer triples {p, q, a} n i=1 from a particular domain s, i.e., Wikipedia articles, we can learn a MC model f s (p, q) that is able to answer questions in that domain.</p><p>However, when applying the model trained in one domain to answer questions in another, the performance may degrade. On the other hand, la- beling data to train a model in the new domain is expensive and time-consuming.</p><p>In this paper, we propose the task of transferring a MC system f s (p, q) that is trained in a source do- main to answer questions over another target do- main, t. In the target domain t, we are given an unlabeled set p t = {p} k i=1 of k paragraphs. Dur- ing test time, we are given an unseen set of para- graphs, p * , in the target domain, over which we would like to answer questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Two-Stage SynNet</head><p>To bootstrap our model f s we use a SynNet <ref type="figure">(Fig- ure 1</ref>), which consists of answer synthesis and question synthesis modules, to generate data on p t . Our SynNet learns the conditional probability of generating answer a = {a start , a end } and ques- tion q = {q 1 , ...q n } given paragraph p, P (q, a|p).</p><p>We decompose the joint probability distribution P (q, a|p) into a conditional probability distribu- tion P (q|p, a)P (a|p), where we first generate the answer a, followed by generating the question q conditioned on the answer and paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Answer Synthesis Module</head><p>In our answer synthesis module we train a simple IOB tagger to predict whether each word in the paragraph is part of an answer or not.</p><p>More formally, given a set of words in a para- graph p = {p 1 ...p n }, our IOB tagging model learns the conditional probability of labels y 1 ...y n , where y 1 ∈ IOB START , IOB MID , IOB END if a word p i is marked as an answer by the annotator in our train set, NONE otherwise.</p><p>We use a bi-directional Long-Short Term Mem- ory Network (Bi-LSTM) (Hochreiter and Schmid- huber, 1997) for tagging. Specifically, we project each word p i → p * i into a continuous vector space via pretrained GloVe embeddings <ref type="bibr" target="#b24">(Pennington et al., 2014</ref>). We then run a Bi-LSTM over the word embeddings p * 1 , ...p * n to produce a context- dependent word representation h 1 , ...h n , which we feed into two fully connected layers followed by a softmax to produce our tag likelihoods for each word.</p><p>We select all consecutive spans where y = NONE produced by the tagger as our candidate answer chunks, which we feed into our question synthesis module for question generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Question Synthesis Module</head><p>Our question synthesis module learns the conditional probability of generating question q = {q 1 , ...q n } given answer a = a start , a end and paragraph p = p 1 ...p n , P (q 1 , ...q n |p 1 ...p n , a start , a end ). We decompose the joint probability distribution of generating all the question words q 1 , ...q n into gener- ating the question one word at a time, i.e.</p><formula xml:id="formula_0">n i=1 P (q i |p, a, q 1...i−1 ).</formula><p>The model is similar to an encoder-decoder network with attention ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>), which computes the conditional probability P (q i |p 1 ...p n , a start , a end , q 1...i−1 ).</p><p>We run a Bi-LSTM over the paragraph to produce context- dependent word representations h = {h 1 , ...h n }. To model where the answer is in the paragraph, similar to <ref type="bibr" target="#b40">Yang et al. (2017)</ref>, we insert answer information by appending a zero/one feature to the paragraph word embeddings. Then, at each time step i, a decoder network attends to both h and the previously generated question token q i−1 to produce a hidden representation r i . Since paragraphs may often have named entities and rare words not present during training, we incorporate a copy mechanism into our models ( <ref type="bibr" target="#b10">Gu et al., 2016)</ref>.</p><p>We use an architecture motivated by latent pre- dictor networks ( <ref type="bibr">Ling et al., 2016</ref>) to force the model to learn when to copy vs. directly predict the word, without direct supervision of what ac- tion to choose. Specifically, at every time step i, two latent predictors generate the probability of generating word w i , a pointer network C p ( <ref type="bibr" target="#b33">Vinyals et al., 2015</ref>) which can copy a word from the context paragraph, and a vocabulary predictor V p which directly generates a probability distribution of choosing a word w i from a predefined vocab- ulary. The likelihood of choosing predictor k at time step i is proportional to w k r i , and the like- lihood of predicting question token q i is given by</p><formula xml:id="formula_1">q * i = p v l v (w i ) + (1 − p v )l c (w i )</formula><p>, where v rep- resents the vocabulary predictor and c represents the copy predictor, and l(w i ) is the likelihood of the word given by the predictor. 3 For training, since no direct supervision is given as to which predictor to choose, we minimize the cross en- tropy loss of producing the correct question to- kens n j=1 −log(q * j ) by marginalizing out latent variables using a variant of the forward-backward algorithm (see <ref type="bibr">Ling et al. (2016)</ref> for full details).</p><p>During inference, to generate a question q 1 ...q n , we use greedy decoding in the following manner. At time step i, we select the most likely predictor (C p or V p ), followed by the most likely word q i given the predictor. We feed the predicted word as input at the next timestep back into the decoder until we predict the end symbol, END, after which we stop decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Machine Comprehension Model</head><p>Our machine comprehension model f (p, q) → a learns the conditional likelihood of predicting an- swer pointers a = {a start , a end } given paragraph p and question q, P (a|p, q). In our experiments we use the open-source Bi-directional Attention Flow (BiDAF) network (Seo et al., 2017) 4 since it is one of the best-performing models on the SQuAD <ref type="bibr">3</ref> Since we only have two predictors, p c = 1 − p v 4 See https://github.com/allenai/bi-att-flow</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Training Algorithm</head><p>Input : x s = {p s , q s , a s } n i=1 triplets from source domain s; pretrained MC model on s, f s (p, q) → {a start , a end }; paragraphs from target domain t, p m j=1 Output: MC model on target domain, f t (p, q) → {a start , a end } dataset, 5 although we note that our algorithm for data synthesis can be used with any MC model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Algorithm Overview</head><p>Having given an overview of our SynNet and a brief overview of the MC model we describe our training procedure, which is illustrated in Algo- rithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training</head><p>Our approach for transfer learning consists of sev- eral training steps. First, given a series of labeled examples x s = {p s , q s , a s } n i=1 from domain s, paragraphs p m j=1 from domain t, and pretrained MC model f s (p, q), we train the SynNet g s to maximize the likelihood of the question-answer pairs in s.</p><p>Second, we fix our SynNet g s and we sample x t = {p t , q t , a t } k i=1 question-answer pairs on the paragraphs in domain t. Several examples of gen- erated questions can be found in <ref type="table">Table 1</ref>.</p><p>We then transfer the MC model originally learned on the source domain to the target domain t using SGD on the synthetic data. However, since the synthetic data is usually noisy, we alternatively train the MC model with mini-batches from x s and x t , which we call data-regularization. Every k batches from x, we sample 1 batch of synthetic data from x , where k is a hyper-parameter, which we set to 4. Letting the model encounter many ex- amples from source domain s serves to regularize</p><note type="other">Snippet of context paragraph (answer in bold) Generated questions (bold) vs. human questions .</note><p>..At this point, some of these used-luxe models have been around so long that they almost qualify as vin- tage throwback editions. Recently, Consumer Re- port magazine issued its list of best and worst used cars, and divvied them up by price range ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What magazine made best used cars in the USAF?</head><p>Who released a list of best and worst used cars ...A high court in northern India on Friday acquitted a wealthy businessman facing the death sentence for the killing of a teen in a case dubbed "the house of horrors." Moninder Singh Pandher was sentenced to death by a lower court in February. The teen was one of 19 victims -children and ... Where was the first person to be shot ? Where was Forrest killed? <ref type="table">Table 1</ref>: Randomly sampled paragraphs and corresponding synthetic vs. human questions from the NewsQA train set. Human-selected answers from the train set were used as input.</p><p>the distribution of the synthetic data in the target domain with real data from s. We checkpoint fine- tuned model f * s every i mini-batches, i = 1000 in our experiments, and save a copy of the model at each checkpoint.</p><p>At test time, to generate an answer, we feed paragraph p = {p 0 , p 1 , ...p n } and question q through our finetuned MC model f * (p, q) to get P (p i = a start ), P (p i = a end ) for all i ∈ 1...n. We then use dynamic programming ( <ref type="bibr" target="#b29">Seo et al., 2017</ref>) to find the optimal answer span {a start , a end }. To improve the stability of using our model for inference, we average the predicted answer likelihoods from model copies at differ- ent checkpoints prior to running the dynamic pro- gramming algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>We summarize the datasets we use in our experi- ments, parameters for our model architectures, and training details.</p><p>The SQuAD dataset consists of approximately 100,000 question-answer pairs on Wikipedia, 87,600 of which are used for training, 10,570 for development, and an unknown number in a hidden test set. The NewsQA dataset consists of 92,549 train, 5,166 development and 5,165 test questions on CNN/Daily Mail news articles. Both the do- main type (i.e., news) and question types differ between the two datasets. For example, an analy- sis of a randomly generated sample of 1,000 ques- tions from both NewsQA and SQuAD <ref type="bibr" target="#b32">(Trischler et al., 2016)</ref> reveals that approximately 74.1% of questions in SQuAD require word matching or paraphrasing to retrieve the answer, as opposed to 59.7% in NewsQA. As our test metrics, we report two numbers, exact match (EM) and F1 score.</p><p>We train a BIDAF model on the SQuAD train dataset and use a two-stage SynNet to finetune it on the NewsQA train dataset.</p><p>We initialize word-embeddings for the BIDAF model, answer synthesis module, and question synthesis module with 300-dimensional-GloVe vectors ( <ref type="bibr" target="#b24">Pennington et al., 2014</ref>) trained on the 840 Billion Words Common Crawl corpus. We set all embeddings of unknown word tokens to zero.</p><p>For both the answer synthesis and question synthesis module, we use a vocabulary of size 110,179. We use LSTMs with hidden states of size 150 for the answer module vs. those of size 100 for the question module since the answer module is less memory intensive than the question module.</p><p>We train both the answer and question module with Adam ( <ref type="bibr" target="#b16">Kingma and Ba, 2015</ref>) and a learning rate of 1e-2. We train a BIDAF model with the de- fault hyperparameters provided in the open-source repository. To stop training of the question synthe- sis module, after each epoch, we monitor both the loss as well as the quality of questions generated on the SQuAD development set. To stop training of the answer synthesis module, we similarly mon- itor predictions on the SQuAD development set.</p><p>To train the question synthesis module, we only use the questions provided in the SQuAD train set. However, to train the answer synthesis module, we further augment the human-annotated labels of each paragraph with tags from a simple NER system 6 because labels of answers provided in the train set are underspecified, i.e., many words in the paragraph that could be potential answers are not labeled. Therefore, we assume any named entities could also be potential answers of certain ques- tions, in addition to the answers explicitly labeled by annotators.</p><p>To generate question-answer pairs on the NewsQA train set using the SynNet, we first run every paragraph through our answer synthe- sis module. We then randomly sample up to 30 candidate answers extracted by our module, which we feed into the question synthesis module. This results in 250,000 synthetic question-answer pairs that we can use to finetune our MC model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head><p>We report the main results on the NewsQA test set <ref type="table">(Table 2)</ref>, report brief results on SQuAD <ref type="table">(Table 3)</ref>, conduct ablation studies <ref type="table">(Table 4)</ref>, and conduct an error analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Results</head><p>We compare to the best previously published work, which trains BARB ( <ref type="bibr" target="#b32">Trischler et al., 2016)</ref> and Match-LSTM ( <ref type="bibr" target="#b34">Wang and Jiang, 2016)</ref> ar- chitectures, and a BIDAF model we train on NewsQA. Directly applying a BIDAF model trained on SQuAD to predict on NewsQA leads to poor performance with an F1 measure of 39.0%, 13.2% lower than one trained on labeled NewsQA data. Using the 2-stage SynNet already leads to a slight boost in performance (F1 measure of 44.3%), which implies that having exposure to the new domain via question-answer pairs pro- vides important signal for the model during train- ing. When we augment the answers from our an- swer synthesis module with those from a generic NER system to produce questions, we have an ad- ditional 2.3% performance boost. Finally, when we ensemble with the original model, we boost the EM further by 0.2%. Our final system achieves an F1 measure of 46.6%, approaching previously published results of 50.0%. The results demon- strate that using the proposed architecture and training procedure, we can transfer a MC model from one domain to another, without use of anno- tated data.</p><p>We also evaluate the SynNet on the NewsQA- to-SQuAD direction. We directly apply the best setting from the other direction and report the re- sult in <ref type="table">Table 3</ref>. The SynNet improves over the baseline by 1.6% in EM and 0.7% in F1. Lim- ited by space, we leave out ablation studies in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ablation Studies</head><p>To better understand how various components in our training procedure and model impact overall performance we conduct several ablation studies, as summarized in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Answer Synthesis</head><p>We experiment with using the answer chunks given in the train set, A oracle , to generate syn- thetic questions, versus those from an NER sys- tem, A ner . Results in <ref type="table">Table 4</ref>(A) show that us- ing human-annotated answers to generate ques- tions leads to a significant performance boost over using answers from an answer generation module. This supports the hypothesis that the answers hu- mans choose to generate questions for provide im- portant linguistic cues for finetuning the machine comprehension model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Question Synthesis</head><p>To see how copying impacts performance, we ex- plore using the entire paragraph to generate the question vs. only the two sentences before and one sentence after the answer span and report re- sults in <ref type="table">Table 4</ref>(B). On the NewsQA train set, syn- thetic questions that use 2 sentences contain an average of 3.0 context words within 10 words to the left and right of the answer chunk, those that use the entire context have 2.1 context words, and human generated questions only have 1.7 words. Training with generated questions that have a large amount of overlap with words close to the an- swer span (i.e., those that use 2-sentences vs. en- tire context for generation) leads to models that perform worse, especially with synthetic answer spans and no data regularization (35.6% F1 vs. 34.3% F1). One possible reason is that, accord- ing to analysis in <ref type="bibr" target="#b32">Trischler et al. (2016)</ref>  <ref type="table">Table 2</ref>: Main Results. Exact match (EM) and span F1 scores on the NewsQA test set of a BIDAF model finetuned with our SynNet. M sq refers to a baseline BIDAF model trained on SQuAD, A gen , Q gen refers to using answers generated from our SynNet respectively to finetune the model on NewsQA, A ner refers to using answers extracted from a standard NER system to generate questions. M * sq refers to using the baseline SQUAD model in the ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>EM F1 M newsqa 46.3 60.8 M newsqa + S net 47.9 61.5 <ref type="table">Table 3</ref>: NewsQA to SQuAD. Exact match (EM) and span F1 results on SQuAD development set of a NewsQA BIDAF model baseline vs. one fine- tuned on SQuAD using the data generated by a 2-stage SynNet (S net ). quire paraphrase, inference, and synthesis as op- posed to word-matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Model Finetuning</head><p>To see how the quantity of synthetic questions encountered during training impacts performance, we use k = {0, 2, 4} mini-batches from SQuAD for every synthetic mini-batch from NewsQA to finetune our model, and average the prediction of 4 checkpointed models during testing. As we see from the results, letting the model to en- counter data from human annotations, although from another domain, serves as a key form of data- regularization, yielding consistent improvement as k increases. We hypothesize this is because the data distribution of machine-generated questions is different than human-annotated ones; our batch- ing scheme provides a simple way to prevent over- fitting to this distribution.  <ref type="table">Table 4</ref>: Ablation Studies. Exact match (EM) and span F1 results on NewsQA test set of a BIDAF model finetuned with a 2-stage SynNet. In study A, we vary k, the number of mini-batches from SQuAD for every batch in NewsQA. In study B, we set k = 0, and vary the answer type and how much of the paragraph we use for question synthe- sis. 2 − sent refers to using two sentences before answer span, while all refers to using the entire paragraph. A ner refers to using an NER system and A or refers to using the human-annotated an- swers to generate questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Error Analysis</head><p>In this section we provide a qualitative analysis of some of our components to help guide further re- search in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Answer Synthesis</head><p>We randomly sample and present a paragraph with answers extracted by our answer synthesis module <ref type="table" target="#tab_3">(Tables 5 and 6</ref>). Although the module appears to have high precision, i.e., it picks up entities such as the "Atlantic Paranormal Society", it misses clear entities such as "David Schrader", which suggests training a system with full NER/POS tags as la-They are ghost hunters , or , as they prefer to be called , para- normal investigators . " Ghost-Hunters ", which airs a spe- cial live show at 7 p.m. Halloween night , is helping lift the stigma once attached to paranormal investigators . The show has become so popular that the group featured in each episode -Atlantic Paranormal Society -has spawned im- itators across United States and affiliates in countries . TAPS , as the " Hunters" group is informally known , even has its own " Reality Radio" show , magazine , lecture tours , T-shirts -and groupies . " Hunters" has made creepy cool , says David Schrader , a paranormal investigator and co-host of " Radio ", a radio show that investigates paranormal activ- ity.  bels would yield better results, and also explains why augmenting synthetic data generated by Syn- Net with such tags leads to improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Question Synthesis</head><p>We randomly sample synthetic questions gener- ated by our module and present our results in Ta- ble 6. Due to the copy mechanism, our module has the tendency to directly use many words from the paragraph, especially common entities, such as "Oklahoma" in the example. Thus, one way to generate higher-quality questions may be to intro- duce a cost function that promotes diversity during decoding, especially within a single paragraph. In turn, this would expose the RC model to a larger variety of training examples in the new domain, which can lead to better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">Machine Comprehension Model</head><p>We examine the performance over various ques- tion types of a finetuned BIDAF on NewsQA vs. one trained on NewsQA vs. one trained on SQuAD <ref type="figure" target="#fig_1">(Figure 2</ref>). Finetuning with Syn- Net improves performance over all question types given, with the largest performance boost on lo- cation and person-identification questions. Simi- larly, models trained on synthetic questions tend to  <ref type="formula">(436)</ref> what did <ref type="formula">(411)</ref> how many <ref type="formula">(296)</ref> who is <ref type="formula">(238)</ref> what does <ref type="formula">(217)</ref> what was <ref type="formula">(206)</ref> who was <ref type="formula">(162)</ref> where did <ref type="formula">(102)</ref> what are <ref type="formula">(92)</ref> where was <ref type="formula">(89)</ref> Top N-grams approach in-domain performance on numeric and person-identification questions, but still struggle with questions that require higher-order reasoning, i.e. those starting with "what was" or "what did". Designing a question generator that explicitly re- quires such reasoning may be one way to further bridge the gap in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We introduce a two-stage SynNet for the task of transfer learning for machine comprehension, a task which is both challenging and of practical im- portance. With our network and a simple training algorithm where we generate synthetic question- answer pairs on the target domain, we are able to generalize a MC model from one domain to an- other with no annotated data. We present strong results on the NewsQA test set, improving perfor- mance of a baseline BIDAF model by over 7.6% F1. Through ablation studies and error analysis, we provide insights into our methodology on the SynNet and MC models that can help guide fur- ther research in this task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>How many victims were in India ? What was the amount of children murdered ? Joe Pantoliano has met with the Obama and Mc- Cain camps to promote mental health and recov- ery. Pantoliano, founder and president of the eight- month-old advocacy organization No Kidding, Me Too, released a teaser of his new film about various forms of mental illness... Which two groups did Joe Pantoliano meet with? Who did he meet with to discuss the issue? ...Former boxing champion Vernon Forrest , 38 , was shot and killed in southwest Atlanta , Georgia , on July 25 . A grand jury indicted the three suspects - Charman Sinkfield , 30 ; Demario Ware , 20 ; and Jquante...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: NewsQA accuracy of baseline BIDAF model trained on SQuAD (light green), vs. model finetuned with our method (red) vs. one trained from scratch on NewsQA (dark grey).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>, signifi- cantly more questions in the NewsQA dataset re-</figDesc><table>Method 

System 
EM 
F1 

Transfer Learning 
M sq (baseline) 
24.9 39.0 
M sq + A gen + Q gen 
30.6 44.3 
M sq + A gen + A ner + Q gen 
32.8 46.6 
M sq + A gen + A ner + Q gen + M  *  

sq 

33.0 46.6 

Supervised Learning Barb-LSTM on NewsQA (Trischler et al., 2016) 
34.9 50.0 
Match-LSTM on NewsQA (Trischler et al., 2016) 34.1 48.2 
BIDAF on NewsQA 
37.1 52.3 
BIDAF on SQuAD finetuned on NewsQA 
37.3 52.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Sample predictions from our answer syn-
thesis module. 

What is Oklahoma's unemployment rate until Oklahoma City 
? 
What was the manager of the Oklahoma City agency ? 
How many companies are in Oklahoma City ? 
How many workers may Oklahoma have as fair hold ? 
Who said the bureau has already hired civilians to choose 
What was the average hour manager of Oklahoma City ? 
How much would Oklahoma have a year to be held 
What year did Oklahoma 's census build job industry ? 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Predictions from the question synthesis 
module on a subset of a paragraph. 

</table></figure>

			<note place="foot" n="2"> This assumption holds for MC datasets such as SQuAD and NewsQA, but there are exceptions in certain subdomains of MSMARCO.</note>

			<note place="foot" n="1"> Train SynNet g to maximize P (q, a|p) on source s; 2 Generate samples x t = (q, a|p) k i=1 on text in target domain t; 3 Use x s ∪ x t to finetune MC model f s on domain t. For every batch sampled from x t , sample k batches from x s ;</note>

			<note place="foot" n="5"> See https://rajpurkar.github.io/SQuAD-explorer/ for latest results</note>

			<note place="foot" n="6"> https://spacy.io/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Yejin Choi and Luke Zettlemoyer for helpful discussions concerning this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods for Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic parsing via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A thorough examination of the CNN/Daily Mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gated-attention readers for text comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>William W Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Data-selective transfer learning for multidomain speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mortaza</forename><surname>Doulaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Saz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02409</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Hao Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Characterlevel question answering with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The Goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02301</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Data recombination for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Simple semi-supervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><forename type="middle">Carreras</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep questions without deep understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning recurrent span representations for extractive question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01436</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Karl Moritz Hermann, Tomáš Kočisk`Kočisk`y, Andrew Senior, Fumin Wang, and Phil Blunsom. 2016. Latent predictor networks for code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Knowing when to look: Adaptive attention via a visual sentinel for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<title level="m">MS MARCO: A human generated machine reading comprehension dataset</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods for Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods for Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min Joon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generating factoid questions with recurrent neural networks: The 30M factoid question-answer corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">CNN features offthe-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Josephine Sullivan, and Stefan Carlsson</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">NewsQA: A machine comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09830</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07905</idno>
		<title level="m">Machine comprehension using match-LSTM and answer pointer</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Building a semantic parser overnight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-perspective context matching for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">LCCT: a semisupervised model for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Pui</forename><surname>Chow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semi-supervised QA with generative domain-adaptive nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02012</idno>
		<title level="m">Sandeep Subramanian, Saizheng Zhang, and Adam Trischler. 2017. Machine comprehension by text-to-text neural question generation</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Simple baseline for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Transfer learning for low-resource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
