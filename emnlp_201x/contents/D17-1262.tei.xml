<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:54+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Further Investigation into Reference Bias in Monolingual Evaluation of Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingsong</forename><surname>Ma</surname></persName>
							<email>maqingsong@ict.ac.cn †</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology Chinese Academy of Sciences</orgName>
								<orgName type="department" key="dep2">Computing and Info Systems</orgName>
								<orgName type="laboratory">ADAPT Centre Dublin City University</orgName>
								<orgName type="institution">University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology Chinese Academy of Sciences</orgName>
								<orgName type="department" key="dep2">Computing and Info Systems</orgName>
								<orgName type="laboratory">ADAPT Centre Dublin City University</orgName>
								<orgName type="institution">University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology Chinese Academy of Sciences</orgName>
								<orgName type="department" key="dep2">Computing and Info Systems</orgName>
								<orgName type="laboratory">ADAPT Centre Dublin City University</orgName>
								<orgName type="institution">University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology Chinese Academy of Sciences</orgName>
								<orgName type="department" key="dep2">Computing and Info Systems</orgName>
								<orgName type="laboratory">ADAPT Centre Dublin City University</orgName>
								<orgName type="institution">University of Melbourne</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Further Investigation into Reference Bias in Monolingual Evaluation of Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2476" to="2485"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Monolingual evaluation of Machine Translation (MT) aims to simplify human assessment by requiring assessors to compare the meaning of the MT output with a reference translation, opening up the task to a much larger pool of genuinely qualified evaluators. Monolingual evaluation runs the risk, however, of bias in favour of MT systems that happen to produce translations superficially similar to the reference and, consistent with this intuition, previous investigations have concluded monolingual assessment to be strongly biased in this respect. On re-examination of past analyses, we identify a series of potential analytical errors that force some important questions to be raised about the reliability of past conclusions, however. We subsequently carry out further investigation into reference bias via direct human assessment of MT adequacy via quality controlled crowd-sourcing. Contrary to both intuition and past conclusions, results show no significant evidence of reference bias in monolingual evaluation of MT.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Despite it being known for some time now that au- tomatic metrics, such as BLEU ( <ref type="bibr" target="#b12">Papineni et al., 2002</ref>), provide a less than perfect substitute for human assessment <ref type="bibr" target="#b4">(Callison-Burch et al., 2006</ref>), evaluation in MT more often than not still com- prises BLEU scores. Besides increased time and resources required by the alternative, human eval- uation of systems, human assessment of MT faces additional challenges, in particular the fact that human assessors of translation quality tend to be highly inconsistent. In recent Conference on Ma- chine Translation (WMT) shared tasks, for exam- ple, manual evaluators complete a relative ranking (RR) of the output of five alternate MT systems, where they must rank the quality of competing translations from best to worst. Within this set-up, when presented with the same pair of MT output translations, human assessors often disagree with one another's preference, and even their own pre- vious judgment about which translation is better <ref type="bibr" target="#b3">(Callison-Burch et al., 2007;</ref><ref type="bibr" target="#b2">Bojar et al., 2016)</ref>. Low levels of inter-annotator agreement in human evaluation of MT not only cause problems with re- spect to the reliability of MT system evaluations, but unfortunately have an additional knock-on ef- fect with respect to the meta-evaluation of metrics, in providing an unstable gold standard. As such, provision of a fair and reliable human evaluation of MT remains a high priority for empirical evalu- ation.</p><p>Direct assessment (DA) ( <ref type="bibr" target="#b8">Graham et al., 2013</ref><ref type="bibr" target="#b9">Graham et al., , 2014</ref>) is a relatively new human evaluation approach that overcomes previous challenges with respect to lack of reliability of human judges. DA collects assessments of translations separately in the form of both fluency and adequacy on a 0-100 rating scale, and, by combination of repeat judg- ments for translations, produces scores that have been shown to be highly reliable in self-replication experiments ( <ref type="bibr" target="#b11">Graham et al., 2015</ref>). The main com- ponent of DA used to provide a primary ranking of systems is adequacy, where the MT output is assessed via a monolingual similarity of meaning assessment. A reference translation is displayed to the human assessor (rendered in gray) and below it the MT output (in black), with the human judge asked to state the degree to which they agree that The black text adequately expresses the meaning of the gray text in English. <ref type="bibr">1</ref> The motivation behind constructing DA as a monolingual MT evaluation are as follows:</p><p>• Monolingual assessment of MT opens up the annotation task to a larger pool of genuinely qualified human assessors;</p><p>• Crowd-sourced workers are unlikely to make use of information that is not entirely nec- essary for completing a given task; and are therefore unlikely to use the source language input if the reference is also displayed or to make use of the source input inconsistently;</p><p>• Displaying only the source without a refer- ence greatly increases both the difficulty of the task and the time required to complete each annotation, which is too serious a trade- off when we wish to carry out human assess- ment on a very large scale;</p><p>• Varying levels of proficiency in the source language across different human assessors could contribute to inconsistency in bilingual MT evaluations.</p><p>Although DA has been shown to overcome the long-standing challenge of lack of reliability in human evaluation of MT, the possibility still ex- ists that, although scores collected with DA have been shown to be almost perfectly reliable in self- replication experiments, both sets of scores, al- though consistent with each other, could in fact both be biased in the same way. <ref type="bibr" target="#b8">Graham et al. (2013)</ref> include in the design of DA a number of criteria aimed at minimizing such bias: (i) assess- ment of individual translations in isolation from others to avoid a given system being scored un- fairly low due to its translations being assessed more frequently alongside high quality transla- tions <ref type="bibr" target="#b0">(Bojar et al., 2011</ref>); (ii) elicit assessment scores via a Likert-style question without inter- mediate labeling, motivated by medical research showing patients' ratings of their own health to be highly dependent on the exact wording of descrip- tors ( <ref type="bibr" target="#b14">Seymour et al., 1985)</ref>; (iii) accurate qual- ity control by assessing the consistency of judges with reference only to their own rating distribu- tions, to accurately remove inconsistent crowd- sourced data while avoiding removal of data that legitimately diverges from the scoring strategy of a given expert judge; and (iv) score standardization to avoid bias introduced by legitimate variations in scoring strategies.</p><p>Despite efforts to avoid bias in <ref type="bibr" target="#b8">Graham et al. (2013)</ref>, since DA is a monolingual evaluation of MT that operates via comparison of MT output with a reference translation, it is therefore still pos- sible, while avoiding other sources of bias, that DA incurs reference bias where the level of su- perficial similarity of translations with reference translations results in an unfair gain, or indeed an unfair disadvantage for systems that yield trans- lations that legitimately deviate from the surface form of reference translations. Following this in- tuition, <ref type="bibr" target="#b7">Fomicheva and Specia (2016)</ref> carry out an investigation into bias in monolingual evalua- tion of MT and conclude that in a monolingual setting, human assessors of MT are strongly bi- ased by the reference translation. In this paper, we provide further analysis of experiments orig- inally provided in <ref type="bibr" target="#b7">Fomicheva and Specia (2016)</ref>, in addition to further investigation into the degree to which the intuition about reference bias can be supported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Fomicheva and Specia (2016) provide an investi- gation into reference bias in monolingual evalu- ation of MT. 100 Chinese to English MT output translations are assessed by 25 human judges on a five-point scale, in the form of their response (None, Little, Much, Most, or All) to the following question: how much of the meaning of the human translation is also expressed in the machine trans- lation?. Precisely the same 100 translations were assessed by all 25 judges. Human judges were di- vided into five groups of five: Group 1 (G 1 ) was shown the source language input and the MT out- put only and carried out a bilingual assessment, while Groups 2-5 (G 2 -G 5 ) were not shown the source input but instead compared the MT out- put to a human-generated reference translation. A distinct set of reference translations was assigned to each group G 2 -G 5 . Inter-annotator agreement (IAA) was measured for pairs of judges as follows (the total number of judge pairs resulting from each setting is provided in parentheses):</p><p>• SOURCE: a given pair of judges assessed translations in a bilingual setting (all possible pairs within G 1 = 5 2 = 10 pairs);</p><p>• SAME: a given pair of judges assessed trans- lations in a monolingual setting by compari- son with precisely the same reference trans- lation (the sum of all possible pairs result-DIFF SAME SOURCE 0.163 ± 0.01 0.197 ±0.01 0.190 ± 0.02 = 40 pairs);</p><p>• DIFF: a given pair of judges assessed trans- lations in a monolingual setting by com- parison with a distinct reference translation (cross product of judges belonging to the four groups G 2 -G 5 = G 2 ×G 3 ×G 4 ×G 5 = 150 pairs). Reference bias is investigated by comparison of levels of IAA, via Cohen's Kappa (κ) and weighted Kappa coefficients. The hypothesis, al- though not explicitly stated, is that if agreement of human assessors of MT in SAME is higher than that of assessors in DIFF, then the likely cause is reference bias in human assessment scores. Agreement in terms of Cohen's Kappa reported in <ref type="bibr" target="#b7">Fomicheva and Specia (2016)</ref> are reproduced here in <ref type="table" target="#tab_0">Table 1</ref>, where a small increase of 0.034 in av- erage Kappa is shown for pairs of human asses- sors in SAME over that of DIFF. To avoid draw- ing conclusions from a difference that is likely to have occurred simply by chance, confidence inter- vals (CIs) are provided and the non-overlapping CIs for SAME and DIFF shown in <ref type="table" target="#tab_0">Table 1</ref> pro- vide the basis for the conclusion that IAA is sig- nificantly higher for SAME compared to DIFF and subsequently that monolingual evaluation of MT is strongly biased by the reference translation. On examination of the analysis that led to the con- clusion of strong reference bias, we unfortunately discover a series of methodological issues with re- spect to confidence interval estimation, however, that raise doubt about the reliability of this con- clusion. <ref type="bibr">2</ref> A clear indication of the precise approach to CI estimation attempted in Fomicheva and Specia (2016) is unfortunately not explicitly stated but out of the range of methods that exist the approach that is applied most resembles bootstrap resam- pling. Conventionally speaking, bootstrap resam-pling can be applied to CI estimation of a point estimate for a sample, D, of size N , by simulat- ing the variance in the population sampling dis- tribution <ref type="bibr" target="#b6">(Efron and Tibshirani, 1993)</ref>. A stan- dard method of estimating CIs via bootstrap re- sampling is to generate a bootstrap distribution for the statistic of interest made up of M repeat com- putations of it, each time drawing a random sam- ple of size N from D with replacement. Although most similar to bootstrap resampling, the applica- tion in <ref type="bibr" target="#b7">Fomicheva and Specia (2016)</ref> to CI estima- tion of Kappa coefficients diverges in some impor- tant ways from a standard application, however. We therefore provide a comparison of the analy- sis drawn in Fomicheva and Specia (2016) with a standard bootstrap implementation. <ref type="figure" target="#fig_0">Figure 1</ref>(a) shows SAME and DIFF bootstrap distributions, reproduced from code released with <ref type="bibr" target="#b7">Fomicheva and Specia (2016)</ref>, originally yielding non-overlapping CIs that led to the conclusion of strong reference bias. <ref type="bibr">3</ref> Although the level of sta- tistical significance is reported to be 99%, CIs in <ref type="figure" target="#fig_0">Figure 1</ref>(a) show that the proportion of each boot- strap distribution was substantially underestimated leading to overly narrow CI limits for both SAME and DIFF. In contrast, <ref type="figure" target="#fig_0">Figure 1</ref>(b) shows CIs re- sulting from an accurately computed proportion of 95% of the same bootstrap distribution, where even at the lower level of 95% significance (as op- posed to 99%) CIs for SAME and DIFF now over- lap, reversing the conclusion of strong reference bias.</p><p>In addition, CI estimation diverges from boot- strap resampling with respect to the number of bootstrap samples employed. Since there are a to- tal of N N possible distinct bootstrap samples for a given sample D (taking order into account), in a conventional bootstrap implementation a Monte Carlo approximation of size M is employed, and the larger M is, the closer the distribution ap- proaches the true bootstrap distribution <ref type="bibr" target="#b5">(Chernick and LaBudde, 2014)</ref>. In Fomicheva and Specia (2016), CIs are computed via only 50 bootstrap samples, however. 4 <ref type="figure" target="#fig_0">Figure 1(c)</ref> shows the change in location of CIs for a typical M =1,000, as op- posed to M =50 <ref type="figure" target="#fig_0">(Figure 1(b)</ref>).  In summary, <ref type="figure" target="#fig_0">Figure 1</ref>(f) shows all errors with respect to CI estimate in Fomicheva and Specia (2016) corrected, and subsequently CIs for a stan- dard implementation of bootstrap, which can be contrasted to those that led to the original conclu- sion of strong reference bias in <ref type="figure" target="#fig_0">Figure 1(a)</ref>. CIs <ref type="bibr">5</ref> A variant of bootstrap does exist where N is intentionally lowered to appropriately reduce the variance estimate but is only applicable when that of standard bootstrap is known to be over-estimated <ref type="bibr" target="#b5">(Chernick and LaBudde, 2014).</ref> in <ref type="figure" target="#fig_0">Figure 1</ref>(f) for SAME and DIFF now overlap re- vealing that experiments in <ref type="bibr" target="#b7">Fomicheva and Specia (2016)</ref>, thus far do not show any evidence of ref- erence bias.</p><formula xml:id="formula_0">(a) Inacc. BD, M =50, n=20, R=no (b) Acc. BD, M =50, n=20, R=no (c) Acc. BD, M =1000, n=20,</formula><formula xml:id="formula_1">(d) Acc. BD, M =1000, n=20, R=yes (e) Acc. BD, M =50, n=N , R=yes (f) Acc. BD, M =1000, n=N , R=yes</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Measures of Central Tendency</head><p>Even if the correct implementation of bootstrap re- sampling, shown in <ref type="figure" target="#fig_0">Figure 1(f)</ref>, had shown non- overlapping confidence intervals, it would still un- fortunately not have been appropriate to draw a conclusion from this of reference bias, however, due to the fact that significant differences are not investigated for the statistic of interest, the Kappa coefficient, but only for a measure of central ten- dency of two Kappa coefficient distributions, the average Kappa of each Kappa distribution. One reason for avoiding a comparison based on signif- icant differences in average Kappa, as opposed to the Kappa point estimates themselves, is that it is possible for the average of two distributions to be equal, or indeed have a small but non-significant difference, while the underlying distributions dif- fering considerably in several other respects. <ref type="figure" target="#fig_1">Figure 2</ref> shows Kappa coefficient distributions for all pairs of judges in SAME (40 pairs), DIFF (150 pairs) and SOURCE (10 pairs), revealing all distributions to have very similar Kappa coeffi- cient distributions, with the one exception arising for SOURCE, where two of the human annotator pairs had an unusually high agreement level. <ref type="bibr">6</ref> A more informative comparison about levels of agreement in SAME and DIFF examines signifi- cant differences in Kappa point estimates, as op- posed to comparison based on a measure of cen- tral tendency. For this reason, despite there be- ing no significant difference in average Kappa for SAME and DIFF, we also examine the proportion of Kappa point estimates of judge pairs in SAME that are significantly different from agreement lev- els of judge pairs in DIFF, which will provide gen- uine insight into differences in levels of agreement between the two groups. <ref type="table" target="#tab_2">Table 2</ref> shows proportions of all judge pairs with significant differences in Kappa point esti- mates (non-overlapping confidence intervals) for each combination of settings <ref type="bibr" target="#b13">(Revelle, 2014)</ref>. <ref type="bibr">7</ref> The number of significant differences in Kappa point estimates for pairs of judges in SAME and DIFF is only 13%, or, in other words, 87% of judge pairs across SAME and DIFF have no significant difference in agreement levels. <ref type="table" target="#tab_2">Table 2</ref> also in- cludes proportions of significant differences for Kappa point estimates resulting from judges be- longing to a single setting (significance testing all Kappa of SAME with respect to all other Kappa be- longing to SAME, for example), revealing that the proportion of significant differences within SAME (12%) to be very similar to that of SAME × DIFF (13%), and similarly for DIFF (12%), with only a single percentage point difference in both cases in proportions of significant differences. Sub- sequently, even after correcting the measure of central tendency error in <ref type="bibr" target="#b7">Fomicheva and Specia (2016)</ref>, evidence of reference bias can still not be concluded. <ref type="bibr">6</ref> The difference in distributions for SOURCE is exagger- ated to some degree due to the total number of annotator pairs in SOURCE being substantially lower than the other two set- tings (only 10 pairs).</p><p>7 Our re-analysis code is available at https:// github.com/qingsongma/percentage-refBias  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Differences in Ratings</head><p>The effect that reference bias may or may not have on actual 1-5 ratings attributed to translations, is again only reported in terms of a measure of cen- tral tendency, i.e. average ratings, in Fomicheva and Specia <ref type="bibr">(2016)</ref>. The average rating of each group shown a distinct reference translation is re- ported, showing distinct average scores for asses- sors employing a distinct set of reference transla- tions. Due to the fact that each group had a dis- tinct average rating, the conclusion is drawn that MT quality is perceived differently depending on the human translation used as gold-standard. It is however, entirely possible that, the difference in average ratings is in part or even fully caused by   <ref type="figure">Figure 4</ref>: Proportions of 1-5 ratings (1=lowest; 5=highest) for translations when human assessors are shown different reference translations (DIFF), the same reference translation (SAME), the source input versus a reference translation (Source vs Ref.) or the source input (SOURCE) for data in Fomicheva and Specia (2016). </p><formula xml:id="formula_2">SAME(G 5 ) SAME(G 4 ) DIFF(G 2 − G 5 ) SAME(G 3 ) SAME(G 2 ) SOURCE(G 1 )</formula><p>Figure 3: Average rating of human assessors shown the source input (SOURCE), the same ref- erence translation (SAME), or a distinct reference translation (DIFF); range of average ratings pro- vided adjacent to each setting.</p><p>the known lack of consistency across human an- notators in general. Quite a substantial leap is made therefore be- tween the difference in average ratings and the cause of that difference. To investigate this fur- ther, we reproduce the average ratings for asses- sors shown a distinct reference translation, each represented by a green square along the line la- beled "DIFF(G 2 -G 5 )" in <ref type="figure">Figure 3</ref>, where the over- all range in average ratings is 0.76. The extrem- ity of this range is better put into context by com- parison with the average rating of human asses- sors shown the same reference translation, each labeled SAME in <ref type="figure">Figure 3</ref>, where the range of av- erage ratings attributed to human assessors shown the same reference can be as large as 0.97 (G5). Thus, it cannot be concluded from a difference in average ratings for annotators shown distinct ref- erence translations that the cause of this difference is the reference translation.</p><p>However, comparison of ratings based only on averages, again hides detail that an analysis could otherwise benefit from. We therefore examine the distribution of individual ratings attributed to translations, and how well ratings for the same translation correspond when pairs of annotators employ the same or distinct reference translation (or indeed the source input) in <ref type="figure">Figure 4</ref>. <ref type="bibr">8</ref> The rating pattern in <ref type="figure">Figure 4</ref> (a) of judge pairs em- ploying a distinct reference translation compared to those in <ref type="figure">Figure 4 (b)</ref>, where assessors employ the same reference translation, shows agreement at the level of individual ratings to be almost in- distinguishable, showing no evidence of reference bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Alternate Reference Bias Investigation</head><p>Although we can now say that experiments in <ref type="bibr" target="#b7">Fomicheva and Specia (2016)</ref> showed no evidence of reference bias, a further issue lies in the fact that low IAA was incurred throughout the study, and low IAA unfortunately provides no assurance with respect to the reliability of conclusions, even when corrected for analytical errors. In addition, the fact that IAA was itself the measure by which bias was investigated is also likely to exacerbate any prob- lems with respect to reliability of conclusions. We therefore provide our own additional investigation into reference bias in monolingual evaluation of MT. Instead of investigating via IAA, we explore the degree to which unfairly high or low ratings might be assigned to translations with respect to surface similarity or dissimilarity with the refer- ence translation.</p><p>Reference-similarity bias is the attribution of unfairly high scores to translations due to high surface-similarity with the reference translation even though the translation is not high quality. A converse kind of reference bias can also oc- cur, which we call reference-dissimilarity bias, where unfairly low scores are attributed to transla- tions that are superficially dissimilar to the refer- ence translation but are in fact high quality trans- lations. The challenge in investigating reference bias lies in the ability to accurately distinguish be- tween translations that receive unfair scores due to surface-similarity or dissimilarity from those that achieved a fair score due to the translation being genuinely high or low quality.</p><p>To separate genuine high quality translations from those that score unfairly high, we carry out two separate assessments of the same set of trans- lations. Firstly, we carry out a standard monolin- gual MT evaluation that employs a generic ref- erence translation (GEN-REF setting), the scores that potentially encounter reference bias. Sec- ondly, we carry out an additional human evalu- ation of the same translations, where, instead of the generic reference, the human assessor com- pares the MT output with a human post-edit of it (POST-EDIT setting). The latter human assessment is highly unlikely to encounter any form of refer- ence bias because the assessment employs a post- edit of the MT output, which itself will only differ the MT output with respect to the parts of it that are genuinely incorrect. Translations encounter- ing reference-similarity bias can then be identified by a high GEN-REF score combined with a low POST-EDIT score, and vice-versa for reference- dissimilarity, a low GEN-REF score combined with a high POST-EDIT score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Reference Bias Experiments</head><p>Experiments were carried out using the original 100 Chinese to English translations released by <ref type="bibr" target="#b7">Fomicheva and Specia (2016)</ref>, in addition to 70 English to Spanish MT translations (WMT-13 Quality Estimation Task 1.1). <ref type="bibr">9</ref> Professional trans- lators, entirely blind to the purpose of the study, were employed to post-edit the MT outputs used in the POST-EDIT setting, and were shown the source input document and the MT output document only (no reference translations). <ref type="bibr">10</ref> Once post-edits had been created, DA was em- ployed in two separate runs on Amazon Mechani- cal Turk, 11 once for GEN-REF and once for POST- EDIT. Besides employing distinct reference trans- lations in the assessment, all other set-up crite- ria were identical for both evaluation settings, in- cluding the conventional segment-level DA set- ting, where a minimum of 15 human assessments are combined into a mean DA score for a given translation, after strict quality control measures and score standardization have been applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results and Discussion</head><p>Figure 5(a) shows a scatter-plot of DA scores at- tributed to translations for GEN-REF compared to POST-EDIT in the Chinese to English experiment. Translations that encounter reference-dissimilarity bias are expected to appear in the lower-right quadrant of <ref type="figure" target="#fig_4">Figure 5</ref>(a), receiving an unfairly low GEN-REF score combined with a high POST-EDIT score. As can be seen from <ref type="figure" target="#fig_4">Figure 5</ref>(a) only a very small number of translations fall into this quad- rant, all of which are very closely located to adja- cent upper-right and lower-left quadrants. A single translation in <ref type="figure" target="#fig_4">Figure 5</ref>(a) is an outlier in this re- spect, receiving a high POST-EDIT score in combi- nation with a lower than average GEN-REF score, <ref type="bibr">11</ref> https://www.mturk.com possibly indicating reference bias. On closer in- spection, however, the score combination is in fact the result of a mistake in the reference translation. Although the low GEN-REF score was the result of an error in the reference translation, a single trans- lation having this score combination is not suffi- cient evidence to conclude strong reference bias. In future work we would like to investigate the fre- quency of erroneous reference translations in ex- isting MT test sets, although we expect them to be few, accurate statistics would provide a better indi- cation of the degree to which they could negatively impact the accuracy of DA evaluations. Similarly for English to Spanish, the correspon- dence between GEN-REF and POST-EDIT scores for translations are shown in <ref type="figure" target="#fig_5">Figure 6(a)</ref>, where, again, only a small number of translations ap- pear in the bottom-right and upper-left quadrants, all lying very close to adjacent quadrants, again, showing no significant indication of reference bias. A single translation appears to break the trend again, however, receiving a low GEN-REF score combined with a high POST-EDIT score, lo- cated in the lower-right quadrant of <ref type="figure" target="#fig_5">Figure 6(a)</ref>. On closer inspection, the low GEN-REF score is the result of something unexpected, as the MT out- put is in fact an accurate translation while at the same time the generic reference is also correct, but unusually the meaning of the two diverge from each other. <ref type="bibr">12</ref> Again, a single translation receiving this score combination is not sufficient evidence to conclude reference bias to be a significant prob- lem for monolingual evaluation. The lack of ref- erence bias in <ref type="figure" target="#fig_5">Figure 6</ref>(a) can again be contrasted to known reference-biased BLEU scores in <ref type="figure" target="#fig_5">Figure  6</ref>(b) for English to Spanish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we provided an investigation into ref- erence bias in monolingual evaluation of MT. Our review of past investigations reveals potential an- alytical errors and raises questions about the relia- bility of past conclusions of strong reference bias. This motivates our further investigation for Chi- nese to English and English to Spanish MT em- ploying direct human assessment in a monolingual MT evaluation setting. Results showed no signif- icant evidence of reference bias, contrary to prior reports and intuition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) Original bootstrap distribution (BD) and confidence intervals (CI) for average Kappa coefficients when human annotators employ the same reference translation (Same K) or a different reference translation (Diff K) in Fomicheva and Specia (2016) ("Inacc. BD"=inaccurate BD proportion; "Acc. BD"=accurate BD proportion; "M "=number of bootstrap samples; "n"=bootstrap sample size; "R=yes": sampled with replacement; "R=no": sampled without replacement); (b) is (a) with accurate BD proportion; (c) is (b) with conventional M ; (d) is (c) with R=yes; (f) is (d) with N =n (N is the full sample size); (e) is (f) with M =50; (f) corresponds to correct BD with all CI errors corrected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distribution of Kappa coefficients for translations assessed with the same reference translation ("Same K"), different reference translations ("Diff K") and source sentences ("Src K") (Fomicheva and Specia (2016) data set).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: (a) Scatter plot of direct assessment (DA) scores for 100 Chinese to English translations carried out by comparison with a generic reference translation (DA Gen-Ref) or DA with the reference replaced by a human post-edit of the MT output (DA Postedit); (b) sentence-level (smoothed) BLEU scores for the same translations also plotted against DA POST-EDIT; translations and references of (a) and (b) data set of Fomicheva and Specia (2016); post-edits provided by professional translators with access to the source and MT output only. BLEU and DA scores are standardized for ease of comparison in all plots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: (a) Scatter plot of direct assessment (DA) scores for 70 English to Spanish translations carried out by comparison with a generic reference translation (DA Gen-Ref) or DA with the reference replaced by a human post-edit of the MT output (DA Postedit); (b) sentence-level (smoothed) BLEU scores for the same translations also plotted against DA POST-EDIT; translations and generic references for (a) and (b) WMT-13 Quality Estimation Task 1.1 (Bojar et al., 2013) data set; post-edits provided by professional translators with access to the source and MT output only. DA and BLEU scores are standardized for ease of comparison in all plots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 (</head><label>5</label><figDesc>Figure 5(a) is also void of evidence of reference-similarity bias, as only a small number of translations lie in the upper-left quadrant and are all very close to the origin and/or adjacent quadrants. Contrasting Figure 5(a), the correspondence of GEN-REF scores to POST-EDIT scores, with Figure 5(b), the correspondence of known referencebiased BLEU scores, in contrast a large number of BLEU scores for translations do encounter reference bias, as seen by the spread of translations appearing across both the bottom-right and upperleft quadrants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Average Kappa coefficients and 99% con-
fidence intervals reported in Fomicheva and Spe-
cia (2016) 

ing from each individual group G 2 -G 5 = 
5 

2 

+ 
5 

2 

+ 
5 

2 

+ 
5 

2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Percentage of human annotator pairs 
in Fomicheva and Specia (2016) with signifi-
cant differences in Kappa coefficients for pairs of 
annotators shown the same reference translation 
(SAME), different reference translations (DIFF) or 
the source language input only (SOURCE), total 
numbers of annotator comparisons in each case are 
provided within parentheses, numbers of annota-
tor pairs was 10 for SOURCE, 40 for SAME and 
150 for DIFF. 

</table></figure>

			<note place="foot" n="1"> Instructions are translated into a given target language.</note>

			<note place="foot" n="2"> We provide a re-analysis of experiment data specifically with respect to Cohen&apos;s Kappa. All errors outlined for Cohen&apos;s Kappa also lead to the same inaccuracies for weighted Kappa in Fomicheva and Specia (2016), however.</note>

			<note place="foot" n="3"> CIs correspond closely to those of the original (Table 1) but differ by a tiny amount due to the randomness involved in regeneration from the code. 4 We note that M is described as 100 in the publication, but 50 in the released code. Our question raised about the methodology also stands for M =100.</note>

			<note place="foot" n="8"> The sum of percentages in a given row equals 100% in each heat map.</note>

			<note place="foot" n="9"> A single generic reference translation was chosen at random from the Chinese to English data set; only a single reference is available for each translation in the English to Spanish data set. 10 Post-editors were paid at the standard rate.</note>

			<note place="foot" n="12"> Source: A straightforward man; MT: Un hombre sencillo; Reference: Un hombre sincero</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This project has received funding from NSFC Grant No. 61379086 and the European Union Horizon 2020 research and innovation pro-gramme under grant agreement 645452 (QT21) and Science Foundation Ireland in the ADAPT Centre for Digital Content Technology (www. adaptcentre.ie) at Dublin City University funded under the SFI Research Centres Pro-gramme (Grant 13/RC/2106) co-funded under the European Regional Development Fund.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A grain of salt for the WMT manual evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miloš</forename><surname>Ercegovčevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><forename type="middle">F</forename><surname>Zaidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Statistical Machine Translation. Association for Computational Linguistics</title>
		<meeting>the 6th Workshop on Statistical Machine Translation. Association for Computational Linguistics<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<title level="m">Proceedings of the Eighth Workshop on Statistical Machine Translation</title>
		<meeting>the Eighth Workshop on Statistical Machine Translation<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="44" />
		</imprint>
	</monogr>
	<note>Findings of the 2013 Workshop on Statistical Machine Translation</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Findings of the 2016 conference on machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">Jimeno</forename><surname>Yepes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelie</forename><surname>Neveol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Rubino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><surname>Zampieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="131" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">meta-) evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cameron</forename><surname>Fordyce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Schroeder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Statistical Machine Translation</title>
		<meeting>the Second Workshop on Statistical Machine Translation<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="136" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Re-evaluating the role of BLEU in machine translation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Conference European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 11th Conference European Chapter of the Association for Computational Linguistics<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An introduction to bootstrap methods with applications to R</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chernick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert A Labudde</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An Introduction to the Bootstrap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>New York City, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reference bias in monolingual machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Fomicheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="77" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Continuous measurement scales in human evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="33" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Is machine translation getting better over time?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="443" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Can machine translation systems be evaluated by the crowd alone?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<idno type="doi">10.1017/S1351324915000339</idno>
		<ptr target="https://doi.org/10.1017/S1351324915000339" />
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Alistair Moffat, and Justin Zobel</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accurate evaluation of segment-level machine translation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitika</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1183" to="1191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">psych: Procedures for personality and psychological research. Northwestern University</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Revelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evanston. R package version</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An evaluation of length and end-phrase of visual analogue scales in dental pain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><forename type="middle">A</forename><surname>Seymour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><forename type="middle">M</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Ed</forename><surname>Charlton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pain</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="177" to="185" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
