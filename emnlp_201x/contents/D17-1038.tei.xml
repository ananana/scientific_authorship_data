<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to select data for transfer learning with Bayesian Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
							<email>sebastian@ruder.io,b.plank@rug.nl</email>
							<affiliation key="aff1">
								<orgName type="department">Insight Research Centre</orgName>
								<orgName type="institution" key="instit1">National University of Ireland</orgName>
								<orgName type="institution" key="instit2">Galway ♣ Aylien Ltd</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♠</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Center for Language and Cognition</orgName>
								<orgName type="institution">University of Groningen</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to select data for transfer learning with Bayesian Optimization</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="372" to="382"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Domain similarity measures can be used to gauge adaptability and select suitable data for transfer learning, but existing approaches define ad hoc measures that are deemed suitable for respective tasks. Inspired by work on curriculum learning, we propose to learn data selection measures using Bayesian Optimization and evaluate them across models, domains and tasks. Our learned measures outperform existing domain similarity measures significantly on three tasks: sentiment analysis, part-of-speech tagging, and parsing. We show the importance of complementing similarity with diversity, and that learned measures are-to some degree-transferable across models, domains, and even tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural Language Processing (NLP) models suf- fer considerably when applied in the wild. The distribution of the test data is typically very dif- ferent from the data used during training, caus- ing a model's performance to deteriorate substan- tially. Domain adaptation is a prominent approach to transfer learning that can help to bridge this gap; many approaches were suggested so far <ref type="bibr" target="#b11">Daumé III, 2007;</ref><ref type="bibr" target="#b15">Jiang and Zhai, 2007;</ref><ref type="bibr" target="#b20">Ma et al., 2014;</ref><ref type="bibr" target="#b42">Schnabel and Schütze, 2014</ref>). However, most work focused on one-to- one scenarios. Only recently research consid- ered using multiple sources. Such studies are rare and typically rely on specific model transfer ap- proaches <ref type="bibr" target="#b22">(Mansour, 2009;</ref><ref type="bibr" target="#b51">Wu and Huang, 2016)</ref>.</p><p>Inspired by work on curriculum learning <ref type="bibr" target="#b3">(Bengio et al., 2009;</ref><ref type="bibr" target="#b48">Tsvetkov et al., 2016)</ref>, we instead propose-to the best of our knowledge-the first model-agnostic data selection approach to trans- fer learning. Contrary to curriculum learning that aims at speeding up learning (see §6), we aim at learning to select the most relevant data from mul- tiple sources using data metrics. While several measures have been proposed in the past <ref type="bibr" target="#b28">(Moore and Lewis, 2010;</ref><ref type="bibr" target="#b1">Axelrod et al., 2011;</ref><ref type="bibr" target="#b49">Van Asch and Daelemans, 2010;</ref><ref type="bibr" target="#b33">Plank and van Noord, 2011;</ref><ref type="bibr" target="#b38">Remus, 2012)</ref>, prior work is limited in studying metrics mostly in isolation, using only the notion of similarity <ref type="bibr" target="#b2">(Ben-David et al., 2007</ref>) and focus- ing on a single task (see §6). Our hypothesis is that different tasks or even different domains de- mand different notions of similarity. In this paper we go beyond prior work by i) studying a range of similarity metrics, including diversity; and ii) test- ing the robustness of the learned weights across models (e.g., whether a more complex model can be approximated with a simpler surrogate), do- mains and tasks (to delimit the transferability of the learned weights).</p><p>The contributions of this work are threefold. First, we present the first model-independent ap- proach to learn a data selection measure for trans- fer learning. It outperforms baselines across three tasks and multiple domains and is compet- itive with state-of-the-art domain adaptation ap- proaches. Second, prior work on transfer learn- ing mostly focused on similarity. We demonstrate empirically that diversity is as important as- and complements-domain similarity for transfer learning. Finally, we show-for the first time- to what degree learned measures transfer across models, domains and tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: Transfer learning</head><p>Transfer learning generally involves the concepts of a domain and a task <ref type="bibr" target="#b29">(Pan and Yang, 2010)</ref>. A domain D consists of a feature space X and a marginal probability distribution P (X) over X , where X = {x 1 , · · · , x n } ∈ X . For document classification with a bag-of-words, X is the space of all document vectors, x i is the i-th document vector, and X is a sample of documents.</p><p>Given a domain D = {X , P (X)}, a task T con- sists of a label space Y and a conditional probabil- ity distribution P (Y |X) that is typically learned from training data consisting of pairs {x i , y i }, where x i ∈ X and y i ∈ Y .</p><p>Finally, given a source domain D S , a corre- sponding source task T S , as well as a target do- main D T and a target task T T , transfer learn- ing seeks to facilitate the learning of the target conditional probability distribution P (Y T |X T ) in D T with the information gained from D S and T S where D S = D T or T S = T T . We will focus on the scenario where D S = D T assuming that T S = T T , commonly referred to as domain adap- tation. We investigate transfer across tasks in §5.3.</p><p>Existing research in domain adaptation has gen- erally focused on the scenario of one-to-one adap- tation: Given a set of source domains A and a set of target domains B, a model is evaluated based on its ability to adapt between all pairs (a, b) in the Cartesian product A × B where a ∈ A and b ∈ B (Remus, 2012). However, adaptation between two dissimilar domains is often undesirable, as it may lead to negative transfer ( <ref type="bibr" target="#b40">Rosenstein et al., 2005</ref>). Only recently, many-to-one adaptation <ref type="bibr" target="#b22">(Mansour, 2009;</ref><ref type="bibr" target="#b51">Wu and Huang, 2016)</ref> has received some attention, as it replicates the realistic scenario of multiple source domains where performance on the target domain is the foremost objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data selection model</head><p>In order to select training data for adaptation for a task T , existing approaches rank the available n training examples</p><formula xml:id="formula_0">X = {x 1 , x 2 , · · · , x n } of k source domains D = {D 1 , D 2 , · · · , D k } accord-</formula><p>ing to a domain similarity measure S and choose the top m samples for training their algorithm. While this has been shown to work empirically ( <ref type="bibr" target="#b28">Moore and Lewis, 2010;</ref><ref type="bibr" target="#b1">Axelrod et al., 2011;</ref><ref type="bibr" target="#b33">Plank and van Noord, 2011;</ref><ref type="bibr" target="#b49">Van Asch and Daelemans, 2010;</ref><ref type="bibr" target="#b38">Remus, 2012)</ref>, using a pre-existing metric leaves us unable to adapt to the characteris- tics of our task T and target domain D T and fore- goes additional knowledge that may be gleaned from the interaction of different metrics. For this reason, we propose to learn the following linear domain similarity measure S as a linear combina- tion of feature values:</p><formula xml:id="formula_1">S = φ(X) · w (1)</formula><p>where φ(X) ∈ R n×l are the similarity and di- versity features further described in §3.2 for each training example, with l being the number of fea- tures, while w ∈ R l are the weights learned by Bayesian Optimization.</p><p>We aim to learn weights w in order to optimize the objective function J of the respective task T on a small number of validation examples of the corresponding target domain D T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bayesian Optimization for data selection</head><p>As the learned measure S should be agnostic of the particular objective function J, we cannot use gradient-based methods for optimization. Similar to <ref type="bibr" target="#b48">Tsvetkov et al. (2016)</ref>, we use Bayesian Opti- mization ( <ref type="bibr" target="#b10">Brochu et al., 2010)</ref>, which has emerged as an efficient framework to optimize any func- tion. For instance, it has repeatedly found better settings of neural network hyperparameters than domain experts <ref type="bibr" target="#b46">(Snoek et al., 2012)</ref>.</p><p>Given a black-box function f : X → R, Bayesian Optimization aims to find an inputˆxinputˆ inputˆx ∈ arg min x∈X f (x) that globally minimizes f . For this, it requires a prior p(f ) over the function and an acquisition function a p(f ) : X → R that calcu- lates the utility of any evaluation at any x.</p><p>Bayesian Optimization then proceeds itera- tively. At iteration t, 1) it finds the most promising input x t ∈ arg max a p (x) through numerical op- timization; 2) it then evaluates the surrogate func- tion y t ∼ f (x t ) + N (0, σ 2 ) on this input and adds the resulting data point (x t , y t ) to the set of obser- vations O t−1 = (x j , y j ) j=1...t−1 ; 3) finally, it up- dates the prior p(f |O t ) and the acquisition func- tion a p(f |Ot) .</p><p>For data selection, the black-box function f looks as follows: 1) It takes as input a set of weights w that should be evaluated; 2) the train- ing examples of all source domains are then scored and sorted according to Equation 1; 3) the model for the respective task T is trained on the top n samples; 4) the model is evaluated on the valida- tion set according to the evaluation measure J and the value of J is returned.</p><p>Gaussian Processes (GP) are a popular choice for p(f ) due to their descriptive power <ref type="bibr" target="#b36">(Rasmussen, 2006</ref>).</p><p>We use GP with Monte Carlo acquistion and Expected Improvement (EI) <ref type="bibr" target="#b27">(Močkus, 1974)</ref> as acquisition function as this combination has been shown to outperform com- parable approaches <ref type="bibr" target="#b46">(Snoek et al., 2012</ref>). <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Features</head><p>Existing work on data selection for domain adap- tation selects data based on its similarity to the target domain. Several measures have been pro- posed in the literature <ref type="bibr" target="#b49">(Van Asch and Daelemans, 2010;</ref><ref type="bibr" target="#b33">Plank and van Noord, 2011;</ref><ref type="bibr" target="#b38">Remus, 2012)</ref>, but were so far only used in isolation.</p><p>Only selecting training instances with respect to the target domain also fails to account for in- stances that are richer and better suited for knowl- edge acquisition. For this reason, we consider-to our knowledge for the first time-whether intrin- sic qualities of the training data accounting for di- versity are of use for domain adaptation in NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarity</head><p>We use a range of similarity met- rics. Some metrics might be better suited for some tasks, while different measures might cap- ture complementary information. We thus use the following measures as features for learning a more effective domain similarity metric.</p><p>We define similarity features over probability distributions in accordance with existing literature <ref type="bibr" target="#b33">(Plank and van Noord, 2011</ref>). Let P be the rep- resentation of a source training example, while Q is the corresponding target domain representation. Let further M = 1 2 (P + Q), i.e. the average distri- bution between P and Q and let D KL (P ||Q) = n i=1 p i log p i q i , i.e., the KL divergence between the two domains. We do not use D KL as a fea- ture as it is undefined for distributions where some event q i ∈ Q has probability 0, which is common for term distributions. Our features are:</p><p>• Jensen-Shannon divergence <ref type="bibr" target="#b19">(Lin, 1991)</ref>:</p><formula xml:id="formula_2">1 2 [D KL (P ||M ) + D KL (Q||M )]</formula><p>. Jensen- Shannon divergence is a smoothed, symmet- ric variant of D KL that has been successfully used for domain adaptation <ref type="bibr" target="#b33">(Plank and van Noord, 2011;</ref><ref type="bibr" target="#b38">Remus, 2012</ref>).</p><p>• Rényi divergence <ref type="bibr">(Rényi, 1961)</ref>:</p><formula xml:id="formula_3">1 α−1 log( n i=1 p α i q α−1 i</formula><p>). Rényi divergence re- duces to D KL if α = 1. We set α = 0.99 following Van Asch and Daelemans <ref type="bibr">(2010)</ref>. <ref type="bibr">1</ref> We also experimented with FABOLAS ( <ref type="bibr" target="#b17">Klein et al., 2017</ref>), but found its ability to adjust the training set size dur- ing optimization to be inconclusive for our relatively small training sets.</p><p>• Bhattacharyya distance <ref type="bibr" target="#b4">(Bhattacharya, 1943)</ref>: ln(</p><formula xml:id="formula_4">i √ P i Q i )</formula><p>• Cosine similarity <ref type="bibr" target="#b18">(Lee, 2001</ref>): P ·Q P Q . We can treat the distributions alternatively as vec- tors and consider geometrically motivated distance functions such as cosine similarity as well as the following.</p><p>• Euclidean distance <ref type="bibr" target="#b18">(Lee, 2001)</ref>:</p><formula xml:id="formula_5">i (P i − Q i ) 2 .</formula><p>• Variational dist. <ref type="bibr" target="#b18">(Lee, 2001)</ref>:</p><formula xml:id="formula_6">i |P i − Q i |.</formula><p>We consider three different representations for calculating the above domain similarity measures:</p><p>• Term distributions (Plank and van Noord, 2011): t ∈ R |V | where t i is the probability of the i-th word in the vocabulary V .</p><p>• Topic distributions <ref type="bibr" target="#b33">(Plank and van Noord, 2011)</ref>: t ∈ R n where t i is the probability of the i-th topic as determined by an LDA model ( <ref type="bibr" target="#b5">Blei et al., 2003</ref>) trained on the data and n is the number of topics.</p><p>• Word embeddings ( <ref type="bibr" target="#b25">Mikolov et al., 2013)</ref>:</p><formula xml:id="formula_7">1 n i v w i a p(w i )</formula><p>where n is the number of words with embeddings in the document, v w i is the pre-trained embedding of the i-th word, p(w i ) its probability, and a is a smoothing factor used to discount frequent probabilities. A similar weighted sum has recently been shown to outperform supervised approaches for other tasks ( <ref type="bibr" target="#b0">Arora et al., 2017)</ref>. As em- beddings may be negative, we use them only with the latter three geometric features above.</p><p>Diversity For each training example, we calcu- late its diversity based on the words in the exam- ple. Let p i and p j be probabilities of the word types t i and t j in the training data and cos(v t i , v t j ) the cosine similarity between their word embed- dings. We employ measures that have been used in the past for measuring diversity ( <ref type="bibr" target="#b48">Tsvetkov et al., 2016</ref>):</p><p>• Number of word types: #types.</p><p>• Type-token ratio: #types #tokens .</p><p>• Entropy <ref type="bibr" target="#b44">(Shannon, 1948)</ref>:</p><formula xml:id="formula_8">− i p i ln(p i ). • Simpson's index (Simpson, 1949): − i p 2 i .</formula><p>• Rényi entropy <ref type="bibr">(Rényi, 1961)</ref>: <ref type="bibr" target="#b35">(Rao, 1982)</ref>:</p><formula xml:id="formula_9">1 α−1 log( i p α i ) • Quadratic entropy</formula><formula xml:id="formula_10">i,j cos(v t i , v t j )p i p j .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tasks, datasets, and models</head><p>We evaluate our approach on three tasks: senti- ment analysis, part-of speech (POS) tagging, and dependency parsing. We use the n examples with the highest score as determined by the learned data selection measure for training our models. <ref type="bibr">2</ref> We show statistics for all datasets in <ref type="table">Table 1</ref>.</p><p>Sentiment Analysis For sentiment analysis, we evaluate on the Amazon reviews dataset <ref type="bibr" target="#b7">(Blitzer et al., 2006</ref>). We use tf-idf-weighted unigram and bigram features and a linear SVM classifier ( . We set the vocabulary size to 10,000 and the number of training examples n = 1600 to conform with existing approaches (Bollegala et al., 2011) and stratify the training set.</p><p>POS tagging For POS tagging and parsing, we evaluate on the coarse-grained POS data (12 uni- versal POS) of the SANCL 2012 shared task <ref type="bibr" target="#b32">(Petrov and McDonald, 2012)</ref>. Each domain- except for WSJ-contains around 2000-5000 la- beled sentences and more than 100,000 unlabeled sentences. In the case of WSJ, we use its dev and test data as labeled samples and treat the remain- ing sections as unlabeled. We set n = 2000 for POS tagging and parsing to retain enough exam- ples for the most-similar-domain baseline.</p><p>To evaluate the impact of model choice, we compare two models: a Structured Perceptron (in- house implementation with commonly used fea- tures pertaining to tags, words, case, prefixes, as well as prefixes and suffixes) trained for 5 itera- tions with a learning rate of 0.2; and a state-of-the- art Bi-LSTM tagger <ref type="bibr" target="#b34">(Plank et al., 2016</ref>) with word and character embeddings as input. We perform early stopping on the validation set with patience of 2 and use otherwise default hyperparameters 3 as provided by the authors.</p><p>Parsing For parsing, we evaluate the state-of- the-art Bi-LSTM parser by Kiperwasser and Gold- berg (2016) with default hyperparameters. <ref type="bibr">4</ref> We use the same domains as used for POS tagging, i.e., the dependency parsing data with gold POS as made available in the SANCL 2012 shared task. 5 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training details</head><p>In practice, as feature values occupy different ranges, we have found it helpful to apply z- normalisation similar to <ref type="bibr" target="#b48">Tsvetkov et al. (2016)</ref>. We moreover constrain the weights w to <ref type="bibr">[−1, 1]</ref>.</p><p>For each dataset, we treat each domain as target domain and all other domains as source domains. Similar to <ref type="bibr">Bousmalis et al. (2016)</ref>, we chose to use a small number (100) target domain examples as validation set. We optimize each similarity mea- sure using Bayesian Optimization with 300 itera- tions according to the objective measure J of each task (accuracy for sentiment analysis and POS tag- ging; LAS for parsing) with respect to the valida- tion set of the corresponding target domain.</p><p>Unlabeled data is used in addition to calculate the representation of the target domain and to cal- culate the source domain representation for the most similar domain baseline. We train an LDA model ( <ref type="bibr" target="#b5">Blei et al., 2003</ref>) with 50 topics and 10 iter- ations for topic distribution-based representations and use GloVe embeddings ( <ref type="bibr" target="#b30">Pennington et al., 2014</ref>) trained on 42B tokens of Common Crawl data 6 for word embedding-based representations.</p><p>For sentiment analysis, we conduct 10 runs of each feature set for every domain and report mean and variance. For POS tagging and parsing, we observe that variance is low and perform one run while retaining random seeds for reproducibility.  <ref type="table">Table 2</ref>: Accuracy scores for data selection for sentiment analysis domain adaptation on the Amazon reviews dataset <ref type="bibr" target="#b7">(Blitzer et al., 2006</ref>). Best: bold; second-best: underlined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines and features</head><p>We compare the learned measures to three base- lines: i) a random baseline that randomly se- lects n training samples from all source domains (random); ii) the top n examples selected us- ing Jensen-Shannon divergence (JS -examples), which outperformed other measures in previous work <ref type="bibr" target="#b33">(Plank and van Noord, 2011;</ref><ref type="bibr" target="#b38">Remus, 2012)</ref>; iii) n examples randomly selected from the most similar source domain determined by Jensen- Shannon divergence (JS -domain). We addi- tionally compare against training on all available source data (6,000 examples for sentiment analy- sis; 14,700-17,569 examples for POS tagging and parsing depending on the target domain).</p><p>We optimize data selection using Bayesian Op- timization with every feature set: similarity fea- tures respectively based on i) word embeddings, ii) term distributions, and iii) topic distributions; and iv) diversity features. In addition, we investigate how well different representations help each other by using similarity features with the two best- performing representations, term distributions and topic distributions. Finally, we explore whether di- versity and similarity-based features complement each other by in turn using each similarity-based feature set together with diversity features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Sentiment analysis We show results for senti- ment analysis in <ref type="table">Table 2</ref>. First of all, the base- lines show that the sentiment review domains are clearly delimited. Adapting between two similar domains such as Book and DVD is more produc- tive than adaptation between dissimilar domains, e.g. Books and Electronics, as shown in previ- ous work . This explains the strong performance of the most-similar-domain baseline. In contrast, selecting individual exam- ples based on a domain similarity measure per- forms only as good as chance. Thus, when do- mains are more clear-cut, selecting from the clos- est domain is a stronger baseline than selecting from the entire pool of source data.</p><p>If we learn a data selection measure using Bayesian Optimization, we are able to outper- form the baselines with almost all feature sets. Performance gains are considerable for all do- mains with individual feature sets (term simi- larity, word embeddings similarity, diversity and topic similarity), except for Books were improve- ments for some single feature sets are smaller. Term distributions and topic distributions are the best-performing representations for calculat- ing similarity, with term distributions perform- ing slightly better across all domains. Combin- ing term distribution-based and topic distribution- based features only provides marginal gains over the individual feature sets, demonstrating that most of the information is contained in the simi- larity features rather than the representations.</p><p>Diversity features perform comparatively to the best similarity features and outperform them on two domains. Furthermore, the combination of di- versity and similarity features yields another siz- able gain of around 1 percentage point for almost all domains over the best similarity features, which shows that diversity and similarity features capture complementary information. Term distribution and topic distribution-based similarity features in  conjunction with diversity features finally yield the best performance, outperforming the baselines by 2-6 points in absolute accuracy. Finally, we compare data selection to training on all available source data (in this setup, 6,000 in- stances). The result complements the findings of the most-similar baseline: as domains are dissimi- lar, training on all available sources is detrimental. POS tagging Results for POS tagging are given in <ref type="table" target="#tab_3">Table 3</ref>. Using Bayesian Optimization, we are able to outperform the baselines with almost all feature sets, except for a few cases (e.g., diver- sity and word embeddings similarity, topic and term distributions). Overall term distribution- based similarity emerges as the most powerful in- dividual feature. Combining it with diversity does not prove as beneficial as in the sentiment analysis case, however, often yields the second-best results. Notice that for POS tagging/parsing, in con- trast to sentiment analysis, the most-similar do- main baseline is not effective, it often performs only as good as chance, or even hurts. In con- trast, the baseline that selects instances (JS -ex- amples) rather than a domain performs better. This makes sense as in SA topically closer domains ex- press sentiment in more similar ways, while for POS tagging having more varied training instances is intuitively more beneficial. In fact, when in- specting the domain distribution of our approach, we find that the best SA model chooses more in- stances from the closest domain, while for POS tagging instances are more balanced across do- mains. This suggests that the Web treebank do- mains are less clear-cut. In fact, training a model on all sources, which is considerably more and varied data (in this setup, 14-17.5k training in- stances) is beneficial. This is in line with find- ings in machine translation <ref type="bibr" target="#b26">(Mirkin and Besacier, 2014)</ref>, which show that similarity-based selection works best if domains are very different. Results are thus less pronounced for POS tagging, and we leave experimenting with larger n for future work.</p><p>To gain some insight into the optimization pro- cedure, <ref type="figure" target="#fig_0">Figure 1</ref> shows the development accuracy for the Structured Perceptron for an example do- main. The top-right and bottom graphs show the hypothesis space exploration of Bayesian Opti- mization for different single feature sets, while the  <ref type="table">Table 4</ref>: Accuracy scores for cross-model transfer of learned data selection weights for part-of-speech tagging from Structured Perceptron (P proxy ) to Bi-LSTM tagger (B) <ref type="figure" target="#fig_0">(Plank et al., 2016)</ref> on the SANCL 2012 shared task dataset <ref type="bibr" target="#b32">(Petrov and McDonald, 2012)</ref>. Data selection weights are learned using model M S ; Bi-LSTM tagger (B) is then trained using the learned weights. Better than baselines: underlined.</p><p>top-left graph displays the overall best dev accu- racy for different features. We observe again that term similarity is among the best feature sets and results in a larger explored space (more variance), in contrast to the diversity features whose devel- opment accuracy increases less and results in an overall less explored space. Exploration plots for other features/models looks similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parsing</head><p>The results for parsing are given in Ta- ble 3. Diversity features are stronger than for POS tagging and outperform the baselines for all ex- cept the Reviews domain. Similarly to POS tag- ging, term distribution-based similarity as well as its combination with diversity features yield the best results across most domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Transfer across models</head><p>In addition, we are interested how well the met- ric learned for one target domain transfers to other settings. We first investigate its ability to trans- fer to another model. In practice, a metric can be learned using a model that is cheap to evaluate and serves as proxy for a state-of-the-art model, in a way similar to uptraining <ref type="bibr" target="#b31">(Petrov et al., 2010)</ref>. For this, we employ the data selection features learned using the Structured Perceptron model for POS tagging and use them to select data for the Bi-LSTM tagger. The results in <ref type="table">Table 4</ref> indicate that cross-model transfer is indeed possible, with most transferred feature sets achieving similar re- sults or even outperforming features learned with the Bi-LSTM. In particular, transferred diversity significantly outperforms its in-model equivalent. This is encouraging, as it allows to learn a data selection metric using less complex models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Transfer across domains</head><p>We explore whether data selection parameters learned for one target domain transfer to other tar- get domains. For each domain, we use the weights  with the highest performance on the validation set and use them for data selection with the remaining domains as target domains. We conduct 10 runs for the best-performing feature sets for sentiment analysis and report the average accuracy scores in <ref type="table" target="#tab_6">Table 5</ref> (for POS tagging, see <ref type="table" target="#tab_8">Table 6</ref>).</p><p>The transfer of the weights learned with Bayesian Optimization is quite robust in most cases. Feature sets like Similarity or Diversity trained on Books outperform the strong JS -D baseline in all 6 cases, for Electronics and Kitchen in 4/6 cases (off-diagonals for box 2 and 3 in Ta- ble 5). In some cases, the transferred weights even outperform the data selection metric learned for the respective domain, such as on D-&gt;E with sim and sim+div features and by almost 2 pp on E-&gt;D.   <ref type="bibr">Donald, 2012)</ref>. D S : target domain used for learning metric S. Best: bold. In-domain results: gray.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target domains Feature set D S Answers (A) Emails (E) Newsgroups (N) Reviews (R) Weblogs (W) WSJ</head><p>Transferred similarity+diversity features mostly achieve higher performance than other feature sets, but the higher number of parameters runs the risk of overfitting to the domain as can be ob- served with two instances of negative transfer with sim+div features.</p><p>As a reference, we also list the performance of the state-of-the-art multi-domain adaptation ap- proach ( <ref type="bibr" target="#b51">Wu and Huang, 2016</ref>), which shows that task-independent data selection is in fact competi- tive with a task-specific, heuristic state-of-the-art domain adaptation approach. In fact our trans- ferred similarity+diversity feature (E-&gt;D) outper- forms the state-of-the-art ( <ref type="bibr" target="#b51">Wu and Huang, 2016)</ref> on DVD. This is encouraging as previous work <ref type="bibr" target="#b38">(Remus, 2012)</ref> has shown that data selection and domain adaptation can be complementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Transfer across tasks</head><p>We finally investigate whether data selection is task-specific or whether a metric learned on one task can be transferred to another one. For each feature set, we use the learned weights for each do- main in the source task (for sentiment analysis, we use the best weights on the validation set; for POS tagging, we use the Structured Perceptron model) and run experiments with them for all domains in the target task. <ref type="bibr">7</ref> We report the averaged accuracy 7 E.g., for SA-&gt;POS, for each feature set, we obtain one set of weights for each of 4 SA domains, which we use to  scores for transfer across all tasks in <ref type="table" target="#tab_10">Table 7</ref>.</p><p>Transfer is productive between related tasks, i.e. POS tagging and parsing results are similar to those obtained with data selection learned for the particular task. We observe large drops in perfor- mance for transfer between unrelated tasks, such select data for the 6 POS domains, yielding 4 · 6 = 24 results.</p><p>as sentiment analysis and POS tagging, which is expected since these are very different tasks. Be- tween related tasks, the combination of similar- ity and diversity features achieves the most ro- bust transfer and outperforms the baselines in both cases. This suggests that even in the absence of target task data, we only require data of a related task to learn a successful data selection measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>Most prior work on data selection for transfer learning focuses on phrase-based machine transla- tion. Typically language models are leveraged via perplexity or cross-entropy scoring to select tar- get data <ref type="bibr" target="#b28">(Moore and Lewis, 2010;</ref><ref type="bibr" target="#b1">Axelrod et al., 2011;</ref><ref type="bibr" target="#b12">Duh et al., 2013;</ref><ref type="bibr" target="#b26">Mirkin and Besacier, 2014)</ref>. A recent study investigates data selection for neural machine translation (van der <ref type="bibr" target="#b50">Wees et al., 2017)</ref>. Perplexity was also used to select training data for dependency parsing <ref type="bibr">(Søgaard, 2011</ref>), but has been found to be less suitable for tasks such as sentiment analysis <ref type="bibr">(Ruder et al., 2017)</ref>. In general, there are fewer studies on data selection for other tasks, e.g., constituent parsing ( <ref type="bibr" target="#b23">McClosky et al., 2010)</ref>, dependency parsing <ref type="bibr" target="#b33">(Plank and van Noord, 2011;</ref><ref type="bibr" target="#b47">Søgaard, 2011</ref>) and sentiment analysis <ref type="bibr" target="#b38">(Remus, 2012</ref>). Work on predicting task accuracy is related, but can be seen as complementary <ref type="bibr" target="#b37">(Ravi et al., 2008;</ref><ref type="bibr" target="#b49">Van Asch and Daelemans, 2010)</ref>.</p><p>Many domain similarity metrics have been pro- posed.  show that proxy A distance can be used to measure the adaptabil- ity between two domains in order to determine examples for annotation. Van Asch and Daele- mans (2010) find that Rényi divergence outper- forms other metrics in predicting POS tagging ac- curacy, while Plank and van Noord (2011) observe that topic distribution-based representations with Jensen-Shannon divergence perform best for data selection for parsing. Remus (2012) apply Jensen- Shannon divergence to select training examples for sentiment analysis. Finally, <ref type="bibr" target="#b51">Wu and Huang (2016)</ref> propose a similarity metric based on a sen- timent graph. We test previously explored similar- ity metrics and complement them with diversity.</p><p>Very recently interest emerged in curriculum learning ( <ref type="bibr" target="#b3">Bengio et al., 2009</ref>). It is inspired by human active learning by providing easier exam- ples at initial learning stages (e.g., by curriculum strategies such as growing vocabulary size). Cur- riculum learning employs a range of data metrics, but aims at altering the order in which the entire training data is selected, rather than selecting data. In contrast to us, curriculum learning is mostly aimed at speeding up the learning, while we focus on learning metrics for transfer learning. Other related work in this direction include using Re- inforcement Learning to learn what data to select during neural network training <ref type="bibr" target="#b14">(Fan et al., 2017)</ref>.</p><p>There is a long history of research in adaptive data selection, with early approaches grounded in information theory using a Bayesian learning framework <ref type="bibr" target="#b21">(MacKay, 1992)</ref>. It has also been studied extensively as active learning <ref type="bibr" target="#b13">(El-Gamal, 1991)</ref>. Curriculum learning is related to active learning <ref type="bibr" target="#b43">(Settles, 2012)</ref>, whose view is different: active learning aims at finding the most difficult instances to label, examples typically close to the decision boundary. Confidence-based measures are prominent, but as such are less widely appli- cable than our model-agnostic approach.</p><p>The approach most similar to ours is by <ref type="bibr" target="#b48">Tsvetkov et al. (2016)</ref> who use Bayesian Opti- mization to learn a curriculum for training word embeddings. Rather than ordering data (in their case, paragraphs), we use Bayesian Optimization for learning to select relevant training instances that are useful for transfer learning in order to pre- vent negative transfer ( <ref type="bibr" target="#b40">Rosenstein et al., 2005</ref>). To the best of our knowledge there is no prior work that uses this strategy for transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We propose to use Bayesian Optimization to learn data selection measures for transfer learn- ing. Our results outperform existing domain sim- ilarity metrics on three tasks (sentiment analy- sis, POS tagging and parsing), and are competi- tive with a state-of-the-art domain adaptation ap- proach. More importantly, we present the first study on the transferability of such measures, showing promising results to port them across models, domains and related tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Dev accuracy curves of Bayes Optimization for POS tagging on the Reviews domain. Best dev acc for different feature sets (top-left). Best dev acc vs. exploration (top-right, bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>92.55 81.02 91.80 93.25 79.09 92.50 93.26 80.61 92.08 92.12 82.30 92.76 93.03 82.39 91.08 92.54 78.31 JS -examples 92.42 93.16 82.80 91.75 93.77 80.53 92.96 94.29 83.25 92.77 93.32 84.35 94.33 94.92 85.36 92.85 94.08 82.43 JS -domain 90.84 91.13 80.37 91.64 93.16 79.93 92.23 92.67 81.77 92.27 92.67 82.11 93.19 94.34 83.44 91.20 92.99 80.61</figDesc><table>Trg domains → 

Answers 
Emails 
Newsgroups 
Reviews 
Weblogs 
WSJ 
Task → 
POS 
Pars 
POS 
Pars 
POS 
Pars 
POS 
Pars 
POS 
Pars 
POS 
Pars 
Feat ↓ Model → 
P 
B 
BIST 
P 
B 
BIST 
P 
B 
BIST 
P 
B 
BIST 
P 
B 
BIST 
P 
B 
BIST 

Base 
Random 
91.34 Learned measures 

W2v sim 
92.53 93.22 82.74 92.94 94.14 81.18 93.41 94.09 81.62 93.51 93.30 82.98 94.41 94.83 84.30 93.02 94.66 81.57 
Term sim 
93.13 93.43 83.79 92.96 94.04 81.09 93.58 94.55 82.68 93.53 93.73 84.66 94.42 95.09 84.85 93.44 94.11 82.57 
Topic sim 
92.50 93.16 82.87 92.70 94.48 81.43 93.97 94.09 82.07 93.21 93.22 83.98 94.42 93.71 84.98 93.09 94.02 82.90 
Diversity 
92.33 92.58 83.01 93.08 93.56 80.93 94.37 93.97 83.98 93.33 93.05 83.92 94.62 94.94 85.84 93.33 93.44 82.80 
Term+topic sim 
92.80 93.69 82.87 92.70 92.28 81.13 93.57 93.76 82.97 93.56 93.61 84.65 94.41 94.23 84.43 93.07 94.68 82.43 
W2v sim+div 
92.76 92.38 82.34 93.51 94.19 80.77 93.96 94.10 84.26 93.45 93.39 84.47 94.36 94.95 85.53 93.32 93.20 82.32 
Term sim+div 
92.73 93.46 83.72 92.90 93.81 81.60 94.03 93.47 82.80 93.47 93.29 84.62 94.76 95.06 85.44 93.32 93.68 82.87 
Topic sim+div 
92.93 93.62 82.60 92.62 93.93 80.83 93.85 94.06 84.04 93.16 93.59 84.45 94.42 94.45 85.89 93.38 94.23 82.33 
All source data 
94.30 95.16 86.34 94.34 95.90 85.57 95.40 95.90 87.18 94.90 95.03 87.51 95.53 95.79 88.23 94.19 95.64 85.20 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results for data selection for part-of-speech tagging and parsing domain adaptation on the 
SANCL 2012 shared task dataset (Petrov and McDonald, 2012). POS: Part-of-speech tagging. Pars: 
Parsing. POS tagging models: Structured Perceptron (P); Bi-LSTM tagger (B) (Plank et al., 2016). Pars-
ing model: Bi-LSTM parser (BIST) (Kiperwasser and Goldberg, 2016). Evaluation metrics: Accuracy 
(POS tagging); Labeled Attachment Score (parsing). Best: bold; second-best: underlined. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>94.14 Term similarity+diversity 93.46 93.18 93.81 94.29 93.47 94.28 93.29 93.35 95.06 94.67 93.68 93.92</figDesc><table>Target domains 
Answers 
Emails 
Newsgroups 
Reviews 
Weblogs 
WSJ 
Feature set ↓ M S → 
B 
P proxy 
B 
P proxy 
B 
P proxy 
B 
P proxy 
B 
P proxy 
B 
P proxy 
Term similarity 
93.43 93.67 94.04 93.88 94.55 93.77 93.73 93.54 95.09 95.06 94.11 94.30 
Diversity 
92.58 93.19 93.56 94.40 93.97 94.96 93.05 93.52 94.94 94.91 93.44 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Accuracy scores for cross-domain trans-
fer of learned data selection weights on Amazon 
reviews (Blitzer et al., 2006). D S : target domain 
used for learning metric S. B: Book. D: DVD. E: 
Electronics. K: Kitchen. Sim: term distribution-
based similarity. Div: diversity. Best per feature 
set: bold. In-domain results: gray. SDAMS (Wu 
and Huang, 2016) listed as comparison. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Accuracy scores for cross-domain transfer of learned data selection weights for part-of-speech 
tagging with the Structured Perceptron model on the SANCL 2012 shared task dataset (Petrov and Mc-
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Results of cross-task transfer of learned 
data selection weights. T S : task used for learn-
ing metric S. POS: Part-of-speech tagging. Pars: 
Parsing. SA: sentiment analysis. Accuracy scores 
for SA and POS; LAS Attachment Score for pars-
ing. Models: Structured Perceptron (POS tag-
ging); Bi-LSTM parser (Kiperwasser and Gold-
berg, 2016) (Pars). Same features as in Table 5. 
In-task results: gray. Better than base: underlined. 

</table></figure>

			<note place="foot" n="2"> All code is available at https://github.com/ sebastianruder/learn-to-select-data. 3 https://github.com/bplank/bilstm-aux 4 https://github.com/elikip/bist-parser 5 We leave investigating the effect of the adapted taggers on parsing for future work.</note>

			<note place="foot" n="6"> https://nlp.stanford.edu/projects/ glove/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their valu-able feedback. Sebastian is supported by Irish Research Council Grant Number EBPPG/2014/30 and Science Foundation Ireland Grant Num-ber SFI/12/RC/2289. Barbara is supported by NVIDIA corporation and the Computing Center of the University of Groningen.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A Simple But Tough-to-Beat Baseline for Sentence Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain adaptation via pseudo in-domain data selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amittai</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="137" to="144" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On a measure of divergence between two statistical population defined by their population distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><surname>Bhattacharya</surname></persName>
		</author>
		<idno>28.</idno>
	</analytic>
	<monogr>
		<title level="j">Bulletin Calcutta Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="1943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Latent Dirichlet Allocation. Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting-Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page">440</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain Adaptation with Structural Correspondence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP &apos;06)</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP &apos;06)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus for Cross-Domain Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="132" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Dumitru Erhan. 2016. Domain Separation Networks. NIPS</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brochu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N De</forename><surname>Cora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freitas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>In CoRR</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<title level="m">Frustratingly Easy Domain Adaptation. Association for Computational Linguistic (ACL)</title>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="256" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptation Data Selection using Neural Language Models: Experiments in Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-2013</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="678" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The role of priors in active Bayesian learning in the sequential statistical decision framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Maximum Entropy and Bayesian Methods</title>
		<meeting><address><addrLine>Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1991" />
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning What Data to Learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop track-ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Instance Weighting for Domain Adaptation in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007-10" />
			<biblScope unit="page" from="264" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<title level="m">Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations. Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="313" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 20th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the Effectiveness of the Skew Divergence for Statistical Language Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS (Artificial Intelligence and Statistics)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Divergence Measures Based on the Shannon Entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tagging The Web: Building A Robust Web Tagger with Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="144" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Information-Based Objective Functions for Active Data Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="590" to="604" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Domain Adaptation with Multiple Sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems Conference</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic domain adaptation for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<title level="m">Distributed Representations of Words and Phrases and their Compositionality. NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Data selection for compact adapted smt models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shachar</forename><surname>Mirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Besacier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh Conference of the Association for Machine Translation in the Americas (AMTA)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On bayesian methods for seeking the extremum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Močkus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization Techniques IFIP Technical Conference Novosibirsk</title>
		<imprint>
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Intelligent Selection of Language Model Training Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William D</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-2010: 48th Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="220" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Uptraining for accurate deterministic question parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pi-Chuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ringgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Alshawi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="705" to="713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<title level="m">Overview of the 2012 shared task on parsing the web. Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">59</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Effective Measures of Domain Similarity for Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gertjan Van Noord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1566" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Diversity and dissimilarity coefficients: a unified approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Radhakrishna</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical population biology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="43" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Gaussian processes for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Edward Rasmussen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automatic prediction of parser accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="887" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Domain adaptation using Domain Similarity-and Domain Complexity-based Instance Selection for Cross-Domain Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Remus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICDM SENTIRE-2012</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On measures of entropy and information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfréd</forename><surname>Rényi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth Berkeley symposium on mathematical statistics and probability</title>
		<meeting>the fourth Berkeley symposium on mathematical statistics and probability</meeting>
		<imprint>
			<date type="published" when="1961" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">To Transfer or Not To Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">T</forename><surname>Rosenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zvika</forename><surname>Marx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><surname>Pack Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Workshop on Inductive Transfer</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Breslin. 2017. Data Selection Strategies for MultiDomain Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parsa</forename><surname>Ghaffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">G</forename></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1702.02426</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">FLORS: Fast and Simple Domain Adaptation for Part-ofSpeech Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="15" to="26" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Active learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Morgan and Claypool</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Measurement of diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simpson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="1949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Practical Bayesian Optimization of Machine Learning Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems Conference (NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Data Point Selection for CrossLanguage Adaptation of Dependency Parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (HLT &apos;11): Short Papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (HLT &apos;11): Short Papers</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="682" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Using Domain Similarity for Performance Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Van Asch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Daelemans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="31" to="36" />
			<date type="published" when="2010-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dynamic data selection for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Marlies Van Der Wees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sentiment Domain Adaptation with Multiple Sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="301" to="310" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
