<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Humor Recognition and Humor Anchor Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University. Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University. Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University. Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>hovy@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University. Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Humor Recognition and Humor Anchor Extraction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Humor is an essential component in personal communication. How to create computational models to discover the structures behind humor, recognize humor and even extract humor anchors remains a challenge. In this work, we first identify several semantic structures behind humor and design sets of features for each structure, and next employ a computational approach to recognize humor. Furthermore, we develop a simple and effective method to extract anchors that enable humor in a sentence. Experiments conducted on two datasets demonstrate that our humor recognizer is effective in automatically distinguishing between humorous and non-humorous texts and our extracted humor anchors correlate quite well with human annotations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Humor is one of the most interesting and puzzling research areas in the field of natural language understanding. Recently, computers have changed their roles from automatons that can only perform assigned tasks to intelligent agents that dynami- cally interact with people and learn to understand their users. When a computer converses with a human being, if it can figure out the humor in human's language, it can better understand the true meaning of human language, and thereby make better decisions that improve the user experience. Developing techniques that enable computers to understand humor in human conversations and adapt behavior accordingly deserves particular attention.</p><p>The task of Humor Recognition refers to determining whether a sentence in a given context expresses a certain degree of humor. Humor recognition is a challenging natural language problem <ref type="bibr" target="#b0">(Attardo, 1994)</ref>.</p><p>First, a universal definition of humor is hard to achieve, because different people hold different understandings of even the same sentence. Second, humor is always situated in a broader context that sometimes requires a lot of external knowledge to fully understand it. For example, consider the sentence, "The one who invented the door knocker got a No Bell prize" and "Veni, Vidi, Visa: I came, I saw, I did a little shopping". One needs a larger cultural context to figure out the subtle humorous meaning expressed in these two sentences. Last but not least, there are different types of humor <ref type="bibr" target="#b17">(Raz, 2012)</ref>, such as wordplay, irony and sarcasm, but there exist few formal taxonomies of humor characteristics. Thus it is almost impossible to design a general algorithm that can classify all the different types of humor, since even human cannot perfectly classify all of them.</p><p>Although it is impossible to understand univer- sal humor characteristics, one can still capture the possible latent structures behind humor <ref type="bibr" target="#b3">(Bucaria, 2004;</ref><ref type="bibr" target="#b2">Binsted and Ritchie, 1997)</ref>. In this work, we uncover several latent semantic structures behind humor, in terms of meaning incongruity, ambigu- ity, phonetic style and personal affect. In addition to humor recognition, identifying anchors, or which words prompt humor in a sentence, is essential in understanding the phenomenon of humor in language. Here, Anchor Extraction refers to extracting the semantic units (keywords or phrases) that enable the humor in a given sentence. The presence of such anchors plays an important role in generating humor within a sentence or phrase.</p><p>In this work, we formulate humor recognition as a classification task in which we distinguish between humorous and non-humorous instances. Then we explore the semantic structure behind humor from four perspectives: incongruity, am-biguity, interpersonal effect and phonetic style. For each latent structure, we design a set of features to capture the potential indicators of humor. With high classification accuracy, we then extract humor anchors in sentences via a simple and effective method. Both quantitative and qualitative experimental results are provided to validate the classification and anchor extraction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Most existing studies on humor recognition are formulated as a binary classification problem and try to recognize jokes via a set of linguistic features <ref type="bibr" target="#b15">(Purandare and Litman, 2006;</ref><ref type="bibr" target="#b6">Kiddon and Brun, 2011</ref>). For example, <ref type="bibr" target="#b10">Mihalcea and Strapparava (2005)</ref> defined three types of humor- specific stylistic features: Alliteration, Antonym and Adult Slang, and trained a classifier based on these feature representations. Similarly, Zhang and Liu (2014) designed several categories of humor-related features, derived from influential humor theories, linguistic norms, and affective dimensions, and input around fifty features into the Gradient Boosting Regression Tree model for humor recognition. <ref type="bibr" target="#b22">Taylor and Mazlack (2004)</ref> recognized wordplay jokes based on statistical lan- guage recognition techniques, where they learned statistical patterns of text in N-grams and provided a heuristic focus for a location of where wordplay may or may not occur. Similar work can also be found in <ref type="bibr" target="#b23">(Taylor, 2009)</ref>, which described humor detection process through Ontological Semantics by automatically transposing the text into the formatted text-meaning representation to detect humor. In addition to language features, some other studies also utilize spoken or multimodal signals. For example, <ref type="bibr" target="#b15">Purandare and Litman (2006)</ref> analyzed acoustic-prosodic and linguistic features to automatically recognize humor during spoken conversations.</p><p>However, the humor related features in most of those works are not systematically derived or explained.</p><p>One essential component in humor recognition is the construction of negative data instances. Classifiers based on negative samples that lie in a different domain than humor positive instances will have high classification performance, but are not necessarily good classifiers. There are few existing benchmark datasets for humor recognition and most studies select negative instances specifically. For example, <ref type="bibr" target="#b10">Mihalcea and Strapparava (2005)</ref> constructed the set of negative examples by using news title from Reuters news, proverbs and British National Corpus. <ref type="bibr">(Zhang, el. al 2014</ref>) randomly sampled 1500 tweets and then asked annotators to filter out humorous tweets.</p><p>Compared to humor recognition, humor gener- ation has received quite a lot attention in the past decades <ref type="bibr" target="#b21">(Stock and Strapparava, 2005;</ref><ref type="bibr" target="#b18">Ritchie, 2005;</ref><ref type="bibr" target="#b5">Hong and Ong, 2009)</ref>. Most generation work draws on humor theories to account for humor factors, such as the Script-based Semantic Theory of Humor <ref type="bibr" target="#b16">(Raskin, 1985;</ref><ref type="bibr" target="#b8">Labutov and Lipson, 2012)</ref> and employs templates to generate jokes. For example, <ref type="bibr" target="#b13">Ozbal and Strapparava (2012)</ref> created humorous neologism using WordNet and ConceptNet. In detail, their system combined several linguistic resources to generate creative names, more specifically neologisms based on homophonic puns and metaphors. <ref type="bibr" target="#b21">Stock and Strapparava (2005)</ref> introduced HAHACRONYM, a system (an acronym ironic re-analyzer and gen- erator) devoted to produce humorous acronyms mainly by exploiting incongruity theories <ref type="bibr" target="#b20">(Stock and Strapparava, 2003)</ref>.</p><p>In contrast to research on humor recognition and generation, there are few studies that identify the humor anchors that trigger humorous effects in general sentences. A certain type of jokes might have specific structures or characteristics that provide pointers to humor anchors. For example, in the problem of "That's what she said" ( <ref type="bibr" target="#b6">Kiddon and Brun, 2011)</ref>, characteristics that involves the using of nouns that are euphemisms for sexually explicit nouns or structures common in the erotic domain might probably give clues to potential humor anchors. Similarly, in the Knock Knock jokes ( <ref type="bibr" target="#b22">Taylor and Mazlack, 2004)</ref>, wordplay is what leads to the humor. However, the wordplay by itself is not enough to trigger the comic effect, thus not equivalent to the humor anchors for a joke. To address these issues, we introduce a formal definition of humor anchors and design an effective method to extract such anchors in this work. To the best of our knowledge, this is the first study on extracting humor anchors that trigger humor in general sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Preparation</head><p>To perform automatic recognition of humor and humor anchor extraction, a data set consisting of both humorous (positive) and non-humorous (negative) examples is needed. The dataset we use to conduct our humor recognition experiments includes two parts: Pun of the Day 1 and the 16000 One-Liner dataset <ref type="bibr" target="#b10">(Mihalcea and Strapparava, 2005</ref>). The two data sets only contain humorous text.</p><p>In order to acquire negative samples for the humor classification task, we sample negative samples from four resources, including AP News 2 , New York Times, Yahoo! Answer <ref type="bibr">3</ref> and Proverb <ref type="bibr">4</ref> . Such datasets not only enable us to automatically learn computational models for humor recognition, but also provide us with the chances to evaluate the performance of our model. However, directly applying sentences extracted from those four resources and simply treating them as negative instances of humor recognition could result in deceptively high performance of classification, due to the domain differences between positive and negative datasets. For example, the humor sentences in our positive datasets often relate to daily lives, such as "My wife tells me I'm a skeptic, but I don't believe a word she says.". Meanwhile, sentences in news websites sometimes describe scenes related to wars or politics, such as "Judge Thomas P. Griesa of Federal District Court in Manhattan stopped short of issuing sanctions". Such domain differences between descriptive words might make a naive bag of words model perform quite well, without taking into account the deeper semantic structures behind humor. To deal with this issue, we extract our negative instances in a way that tries to minimize such domain differences by (1) selecting negative instances whose words are all contained in our positive instance word dictionary and (2) forcing the text length of non-humorous instances to follow the similar length restriction as humorous examples, i.e. one sentence with an average length of 10-30 words. Here, we assume sentences come from the aforementioned four resources are all non-humorous in nature. <ref type="table">Table 1</ref> provides a detailed statistical description to our datasets. <ref type="table">Table 1</ref>: Statistics on Two Datasets</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset #Positive #Negative Pun of the Day 2423 2403 16000 One Liners 16000 16002</head><p>Effect and (d) Phonetic Style. For each latent structure, a set of features is designed to capture the corresponding indicators of humor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Incongruity Structure</head><p>"Laughter arises from the view of two or more inconsistent, unsuitable, or incongruous parts or circumstances, considered as united in complex object or assemblage, or as acquiring a sort of mutual relation from the peculiar manner in which the mind takes notice of them" <ref type="bibr" target="#b9">(Lefcourt, 2001</ref>). The essence of the laughable is the incongruous, the disconnecting of one idea from another <ref type="bibr" target="#b14">(Paulos, 2008)</ref>. Humor sometimes relies on a certain type of incongruity, such as opposition or contradiction. For example, the following 'clean desk' and 'cluttered desk drawer' example ( <ref type="bibr" target="#b10">Mihalcea and Strapparava, 2005</ref>) presents an incongruous/contrast structure, resulting in a comic effect.</p><p>A clean desk is a sign of a cluttered desk drawer.</p><p>Direct identification of incongruity is hard to achieve, however, it is relatively easier to measure the semantic disconnection in a sentence. Taking advantage of Word2Vec 5 , we extract two types of features to evaluate the meaning distance 6 between content word pairs in a sentence ( <ref type="bibr" target="#b11">Mikolov et al., 2013</ref>):</p><p>• Disconnection: the maximum meaning dis- tance of word pairs in a sentence.</p><p>• Repetition: the minimum meaning distance of word pairs in a sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ambiguity Theory</head><p>Ambiguity <ref type="bibr" target="#b3">(Bucaria, 2004)</ref>, the disambiguation of words with multiple meanings <ref type="bibr" target="#b1">(Bekinschtein et al., 2011)</ref>, is a crucial component of many humor jokes ( <ref type="bibr" target="#b12">Miller and Gurevych, 2015)</ref>. Humor and ambiguity often come together when a listener expects one meaning, but is forced to use another meaning. Ambiguity occurs when the words of the surface sentence structure can be grouped in more than one way, thus yielding more than one associated deep structures, as shown in the example below.</p><p>Did you hear about the guy whose whole left side was cut off? He's all right now.</p><p>The multiple possible meanings of words provide readers with different understandings. To capture the ambiguity contained in a sentence, we utilize the lexical resource WordNet <ref type="bibr" target="#b4">(Fellbaum, 1998)</ref> and capture the ambiguity as follows:</p><p>• Sense Combination: the sense combination in a sentence computed as follows: we first use a POS tagger ( <ref type="bibr" target="#b25">Toutanova et al., 2003)</ref> to identify Noun, Verb, Adj, Adv. Then we consider the possible meanings of such words {w 1 , w 2 · · · w k } via WordNet and calculate the sense combinations as log( k i=1 n w i ). n w i is the total number of senses of word w i .</p><p>• Sense Farmost: the largest Path Similarity <ref type="bibr">7</ref> of any word senses in a sentence.</p><p>• Sense Closest: the smallest Path Similarity of any word senses in a sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Interpersonal Effect</head><p>Besides humor theories and linguistic style mod- eling, one important theory behind humor is its social/hostility focus, especially regarding its interpersonal effect on receivers. That is, humor is essentially associated with sentiment (Zhang and Liu, 2014) and subjectivity <ref type="bibr" target="#b26">(Wiebe and Mihalcea, 2006</ref>). For example, a sentence is likely to be humorous if it contains some words carrying strong sentiment, such as 'idiot' as follows.</p><p>Your village called. They want their Idiot back.</p><p>Each word is associated with positive or neg- ative sentiments and such measurements reflect the emotion expressed by the writer. To identify the word-associated sentiment, we use the word association resource in the work by <ref type="bibr" target="#b27">(Wilson et al., 2005)</ref>, which provides annotations and clues to measure the subjectivity and sentiment associated with words. This enables us to design the following features.</p><p>• Negative (Positive) Polarity: the number of occurrences of all Negative (Positive) words.</p><p>• Weak (Strong) Subjectivity: the number of occurrences of all Weak (Strong) Subjectivity oriented words in a sentence. It is the linguistic expression of people's opinions, evaluations, beliefs or speculations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Phonetic Style</head><p>Many humorous texts play with sounds, creating incongruous sounds or words. Some studies <ref type="bibr" target="#b10">(Mihalcea and Strapparava, 2005</ref>) have shown that the phonetic properties of humorous sentences are at least as important as their content. Many one-liner jokes contain linguistic phenomena such as alliteration, word repetition and rhyme that produce a comic effect even if the jokes are not necessarily meant to be humorous in content.</p><p>What is the difference between a nicely dressed man on a tricycle and a poorly dressed man on a bicycle? A tire.</p><p>An alliteration chain refers to two or more words beginning with the same phones. A rhyme chain is defined as the relationship that words end with the same syllable. To extract this phonetic feature, we take advantage of the CMU Pronouncing Dictionary 8 and design four features as follows:</p><p>• Alliteration: the number of alliteration chains in a sentence, and the maximum length of alliteration chains.</p><p>• Rhyme: the number of rhyme chains and the maximum length of rhyme chains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Humor Anchor Extraction</head><p>In addition to humor recognition, identifying anchors, or which words prompt humor in a sentence, is also essential in understanding humor language phenomena. In this section, we first define what humor anchors are and then describe how to extract such semantic units that enable humor in a given sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Humor Anchor Definition</head><p>The semantic units or humor anchors enable humor in a given sentence, and are reflected in the form of sentence words. However, not every single word can be a humor anchor. For example, I am glad that I know sign language; it is pretty handy. In this one-liner, words such as 'am' and 'is' are not able to enable humor  via themselves. Similarly, 'sign' or 'language' itself are not capable to prompt comic effect. The possible anchors in this example should contain both 'sign language' and 'handy'; it is the combination of these two spans that triggers humor. Therefore, formally defined, a humor anchor is a meaningful, complete, minimal set of word spans in a sentence that potentially enable one of the latent structures of Section 4 to occur. (1) Meaningful means humor anchors are meaningful word spans, not meaningless stop words in a sentence; (2) Completeness shows that all possible humor anchors should be covered by this anchor set and no individual span in this anchor set is capable enough to enable humor; (3) Minimal emphasizes that it is the combination of these anchors together that prompts comic effect; discarding any anchors from this candidate set destroys the humorous effect.</p><note type="other">Anchor Candidates i am i know sign language it is pretty handy Humor sentence: i am glad that i know sign language; it is pretty handy Maximal Decrement Results 0.8, i am glad that I know [sign language]; it is [pretty handy] ßBest 0.3, i am glad that I know [sign language]; it is pretty handy 0.2, i am glad that I</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Anchor Extraction Method</head><p>Based on the humor anchor requirements listed above, we scoped humor anchor candidates to words or phrases that belong to the syntactic categories of Noun, Verb, Noun Phrase, Verb Phrase, ADVP or ADJP. Those properties are acquired via a sentence parse tree. To generate anchor candidates, we parsed each sentence and selected words or phrases that satisfy one or more of the latent structure criteria by first extracting the minimal parse subtrees of NP, VP, ADVP and ADJP and then adding remaining Nouns and Verbs into candidate sets.</p><p>The above anchor generation process provides us with all possible anchors that might enable humor. It satisfies the Meaningful and Com- pleteness requirements. To extract a Minimal set of anchors, we proposed a simple and effective method of Maximal Decrement. Its basic idea is summarized as follows: Each complete sentence has a predicted humor score, which is computed via a humor recognition classifier trained on all data points. This humor recognizer is not limited to any specific classifiers or features as long as it provides good classification accuracy, which guarantees the generalization ability of our anchor extraction method. We next enumerate a subset of anchors from all potential anchors for this sentence. Then, we recompute the predicted humor score by providing the classifier with features associated with the current sentence, after removing that subset of anchors. Note that our designed humor structural features are all word order free, thereby not distinguishing between complete and incomplete sentences. The subset of humor anchor candidates that provides the maximum decrement of humor predicted scores is then returned as the extracted humor anchor set.</p><p>Mathematically, X i is the word set of sentence i. Let f denote the trained classifier on all data instances. f (X i ) is the predicted humor score for sentence i before performing any operations. Denote K i (K i ⊂ X i ) as the subset of words that we need to remove from sentence i. The size of K i should be smaller than a threshold t, |K i | ≤ t. f (X i /K i ) is the recomputed humor score for sentence i after removing K i . Our Maximal Decrement method tries to maximize the following objective by enumerating all possible K i s. The subset K i that gives the maximal decrement is returned as our extracted humor anchors for sentence i. The system overview is shown in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>arg min</p><formula xml:id="formula_0">|K i ≤t| f (X i ) − f (X i /K i )<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiment</head><p>In this section, we validate the performance of different semantic structures we extracted on humor recognition and how the combination of the structures contributes to classification. In addition, both qualitative and quantitative results regarding humor anchor extraction performance are explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Humor Recognition</head><p>We formulate humor recognition as a traditional text classification problem, and apply Random Forest to perform 10 fold cross validation on two datasets. Random Forest is an ensemble of decision trees 9 for classification (regression) that constructs a multitude of decision trees at training time and outputs the class that is the mode of the classes output by individual trees. Unlike single decision trees, which are likely to suffer from high variance or high bias, random forests use averaging to find a natural balance between the two extremes. In addition to the four latent structures behind humor, we also design a set of K Nearest Neighbor (KNN) features that uses the humor classes of the K sentences (K = 5) that are the closest to this sentence in terms of meaning distance in the training data. We use several methods to act as baselines for comparison with our classier. Bag of Words baseline is used to capture a multiset of words in a sentence that might differentiate humor and non-humor. Lan- guage Model baseline assigns a humor/nonhumor probability to words in a sentence via probability distributions. Word2Vec baseline represents the meaning of sentences via <ref type="bibr">Word2Vec (Mikolov et al., 2013</ref>) distributional semantic meaning representation. We implemented an earlier work <ref type="bibr" target="#b10">(Mihalcea and Strapparava, 2005</ref>) that exploits stylistic features including alliteration, autonomy and adult slang and ensembles with bag of words representations, denoted as SaC Ensemble. It is worth mentioning that our datasets are balanced in terms of positive and negative instances, giving a random classification accuracy of 50%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2: Different Latent Structures' Contribu- tion to Humor Recognition</head><p>We first explored how different latent semantic structures affect humor recognition performance and summarize the results in <ref type="figure">Figure 2</ref>. It is evident that Incongruity performs the best among all latent semantic structures in the context of Pun of the Day and both Ambiguity and Phonetic substantially contribute to recognition performance on the 16000 One Liners dataset. The reason behind the differences in performance with Incongruity and with Phonetic lies in the different nature of the corpus. Most puns are well structured and play with contrasting or incongruous meaning. However, humor sentences in the 16000 One Liners often rely on the reader's awareness of attention-catching sounds <ref type="bibr" target="#b10">(Mihalcea and Strapparava, 2005</ref>). This demonstrates that humor characteristics are expressed differently in different contexts and datasets.</p><p>We also investigated how the combination of such semantic structures performs compared with our proposed baselines, as shown in <ref type="table" target="#tab_1">Table 2</ref>   <ref type="bibr">Word2Vec+HCF)</ref> gives the best classification performance because it takes into account both latent structures and semantic word meanings. Such a conclusion is consistent across two datasets. This indicates that our extracted latent semantic structures are effective in capturing humorous meaning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Anchor Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Evaluation</head><p>The above humor recognition classifier provides us with decent accuracy in identifying humor in the text. To better understand which words or semantic units enable humor in sentences, we performed humor anchor extraction as described in Section 5.2. We set the size of the humor anchor set as 3, i.e. t = 3. The classifier that is used to predict the humor score is trained on all data instances. Then all predicted humorous instances are collected and input into the humor anchor extraction component. Based on the Maximal Decrement method, a set of humor anchors is extracted for each instance. <ref type="table" target="#tab_2">Table 3</ref> presents selected extracted humor anchor results, including both successful and unsatisfying extractions. As we can see, extracted humor anchors are quite reasonable in explaining the humor causes or focuses. For example, in the sentence "I used to be a watchmaker; it is a great job and I made my own hours", our method selected 'watchmaker', 'made' and 'hours' as humor anchors. It makes sense because each word is necessary and essential to enable humor. Deleting 'watchmaker' will make the combination of 'made' and 'hours' helpless to the comic effect. To sum up, our extracted anchor extraction works fairly well in identifying the focus and meaning of humor language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Evaluation</head><p>In addition to the above qualitative exploration, we also conducted quantitative evaluations. For each dataset, we randomly sampled 200 sentences. Then for each sentence, 3 annotators are asked to annotate and label the possible humor anchors. To assess the consistency of the labeling in this context, we introduced an Annotation Agreement Ratio (AAR) measurement as follows:</p><formula xml:id="formula_1">AAR(A, B) = 1 N s Ns i=1 |A i ∩ B i | |A i ∪ B i |</formula><p>Here, N s is the total number of sentences. A i and B i are the humor anchor sets of sentence i provided by annotator A and B respectively. The AARs on Pun of the Day and 16000 One Liners datasets are 0.618 and 0.433 respectively, computed by averaging the AAR scores between any two different annotators, which indicate relatively reasonable agreement. As a further step to validate the effectiveness of our anchor extraction method, we also introduced two baselines. The Random Extraction baseline selects humor anchors by sampling words in a sentence randomly. Similarly, POS Extraction baseline generates anchors by narrowing down all the words in a sentence to a set of certain POS, e.g. Noun, Verb, Noun Phrase, Verb Phrase, ADVP and ADJP and then sampling words from this set.</p><p>To evaluate whether our extracted anchors are consistent with human annotation, we used each annotator's extracted anchor list as the ground truth, and compared with anchor list provided by our method. To identify whether two anchors</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Result Category Representative Sentences</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Good</head><p>Did you hear about the guy who got hit in the head with a can of soda? He was lucky it was a soft drink. I was struggling to figure out how lightning works then it struck me. The one who invented the door knocker got a No-bell prize. I used to be a watchmaker; it is a great job and I made my own hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bad</head><p>I wanted to lose weight, so I went to the paint store. I heard I could get thinner there. I used to be a banker but I lost interest   <ref type="table" target="#tab_3">Table 4</ref>, we found that MDE performs quite well under the measurement of human annotation in terms of both ALO and EX settings. This again validates our assumption towards humor anchors and the effectiveness of our anchor extraction method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Discussion</head><p>The above two subsections described the per- formance of both humor recognition and humor anchor extraction tasks. In terms of humor recog- nition, incongruity, ambiguity, personal affect and phonetic style are taken into consideration to assist the identification of humorous language. We focus on discovering generalized structures behind humor, and did not take into account sexual oriented words such as adult slang in modeling humorous language. Based on our results, these four latent structures are effective in capturing humor characteristics and such characteristics are expressed to different extents in different contexts. Note that we can apply any classification methods with our humor latent structures. Once such structures help us acquire high recognition accu- racy, we can perform the generalized Maximal Decrement extraction method to identify anchors in humorous text.</p><p>Both humor recognition and humor anchor extraction suffer from several common issues.</p><p>(1) Phrase Meaning: For example, a humorous sentence "How does the earth get clean? It takes a meteor shower" is predicted as non-humorous, because the recognizer does not fully understand the meaning of 'meteor shower', let alone the comic effect caused by 'earth', 'clean' and 'meteor shower'. For the unsatisfying example in <ref type="table" target="#tab_2">Table 3</ref> "I used to be a banker but I lost interest", anchor extraction would work better if it recognizes 'lost interest' correctly as a basic semantic unit. (2) External Knowledge: For jokes that involve idioms or social phenomena, or need some external knowledge such as "Veni, Vidi, Visa: I came, I saw, I did a little shopping", both humor recognition and anchor extraction fail because a broader and implicit comparison of this sentence and its origin ("Veni, Vidi, Vici: I came, I saw, I conquered... ") is hard to be captured from a sentence. (3) Humor Categorization: Moreover, a fine granularity categorization of humor might aid in understanding humorous language, because humor has different types of manifestations, such as irony, sarcasm, creativity, insult and wordplay. Therefore, more sophisticated techniques in mod- eling phrase meaning, external knowledge, humor types, etc., are needed to better expose and define humor for automatic recognition and extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we focus on understanding hu- morous language through two subtasks: humor recognition and humor anchor extraction. For this purpose, we first designed four semantic structures behind humor. Based on the designed sets of features associated with each structure, we constructed different computational classifiers to recognize humor. Then we proposed a simple and effective Maximal Decrement method to automatically extract anchors that enable humor in a sentence. Experimental results conducted on two datasets demonstrate the effectiveness of our proposed latent structures. The performances of humor recognition and anchor extraction are superior compared to several baselines. In the future, we would like to step further into the discovery of humor characteristics and apply our findings to the process of humor generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>know sign language; it is [pretty handy] 0.1, i am glad that I [know] sign language; it is [pretty handy] …… Maximal Decrement Algorithm · f(X): the predicted score for sentence X · f(X \ K): the predicted score by removing set K from X · For Anchor Subset K (|K|&lt;=t), calculate f(X) -f(X \ K). · Find the K with max decrement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Humor Anchor Extraction Overview. Based on the parsing output of each sentence, we generate its humor anchor candidates. We then apply the Maximal Decrement algorithm to these candidates. The humor anchor subset that gives the maximal decrement is the extracted humor anchors for that sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison of Different Methods of Humor Recognition 

inadequacy of LM also indicates that we can 
alleviate the domain differences and capture the 
real humor. (2) SaC Ensemble is inferior to the 
combination of Word2Vec and HCF because it 
does not involve enough latent structures such as 
Interpersonal Effect and distributional semantics. 
(3) The combination of Word2Vec and HCF 
(</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Representative Extracted Humor Anchors. Highlighted parts are the extracted humor anchors in a sentence. 

are the same, we introduce two measurements: 
Exact (EX) Matching and At-Least-One (ALO) 
Matching. Exact Matching requires the two 
anchors to be exactly the same. For ALO, two 
anchors are considered the same if they have at 
least one word in common. Recall, Precision and 
F1 Score are act as evaluation metrics. We then 
average the three annotators' individual scores to 
get the final extraction performance. 

Metrics 
Recall Precision F1 
Pun of the Day Dataset 
MDE EX 
0.444 0.446 
0.438 
POS EX 
0.166 0.170 
0.165 
Random EX 
0.121 0.116 
0.116 
MDE ALO 
0.782 0.784 
0.756 
POS ALO 
0.364 0.371 
0.360 
Random ALO 0.297 0.287 
0.285 
16000 One Liners Dataset 
MDE EX 
0.314 0.281 
0.288 
POS EX 
0.104 0.110 
0.104 
Random EX 
0.087 0.075 
0.079 
MDE ALO 
0.675 0.638 
0.616 
POS ALO 
0.386 0.363 
0.356 
Random ALO 0.341 0.334 
0.319 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Quantitative Result Comparison of 
Humor Anchor Extraction 

The quantitative evaluation results are summa-
rized in Table 4. Maximal Decrement Extraction 
is denoted as MDE; POS Extraction is denoted 
as POS, and Random Extraction is denoted as 
Random. We report both ALO and EX results 
for MED, POS and Random. From </table></figure>

			<note place="foot" n="4"> Latent Structures behind Humor In this section, we explore the latent semantic structures behind humor in four aspects: (a) Incongruity; (b) Ambiguity; (c) Interpersonal 1 Pun of the Day: http://www.punoftheday. com/ This constructed dataset will be made public. 2 http://hosted.ap.org/dynamic/fronts/HOME?SITE=AP 3 https://answers.yahoo.com/ 4 Manually extracted 654 proverbs from Proverb websites</note>

			<note place="foot" n="5"> https://code.google.com/p/word2vec/ 6 We take the generic Word2Vec vectors without training new vectors for our specific domain. In addition, vectors associated with senses (Kumar Jauhar et al., 2015) might be alternative advantageous in this task.</note>

			<note place="foot" n="7"> Path Similarity: http://www.nltk.org/howto/ wordnet.html</note>

			<note place="foot" n="8"> http://www.speech.cs.cmu.edu/cgi-bin/cmudict</note>

			<note place="foot" n="9"> https://www.kaggle.com/wiki/RandomForests</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The authors would like to thank Li Zhou, Anna Kasunic, the anonymous reviewers, our annotators and all colleagues who have contributed their valuable comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Linguistic theories of humor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Attardo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Walter de Gruyter</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Why clowns taste funny: the relationship between humor and semantic ambiguity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tristan A Bekinschtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">M</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian M</forename><surname>Rodd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Owen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="9665" to="9671" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Computational rules for generating punning riddles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Binsted</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Ritchie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Humor: International Journal of Humor Research</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Lexical and syntactic ambiguity as a source of humor: The case of newspaper headlines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiara</forename><surname>Bucaria</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="279" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatically extracting word relationships as templates for pun generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethel</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Computational Approaches to Linguistic Creativity, CALC &apos;09</title>
		<meeting>the Workshop on Computational Approaches to Linguistic Creativity, CALC &apos;09<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="24" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">That&apos;s what she said: double entendre identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuriy</forename><surname>Brun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="89" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ontologically grounded multi-sense representation learning for semantic vector space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Sujay Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2015 Conference of the North American Chapter</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Humor as circuits in semantic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="150" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Humor: The psychology of living buoyantly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Herbert M Lefcourt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Making computers laugh: Investigations in automatic humor recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="531" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic disambiguation of english puns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="719" to="729" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computational humour for creative naming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gözde</forename><surname>Ozbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Humor</title>
		<imprint>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Mathematics and humor: A study of the logic of humor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulos</forename><surname>John Allen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Humor: Prosody analysis and automatic recognition for f*r*i*e*n*d*s*</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amruta</forename><surname>Purandare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Litman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;06</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;06<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="208" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semantic mechanisms of humor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Victor Raskin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic humor classification on twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Raz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="66" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Computational mechanisms for pun generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><forename type="middle">Ritchie</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th</title>
		<meeting>the 10th</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">European Natural Language Generation Workshop</title>
		<imprint>
			<biblScope unit="page" from="125" to="132" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Getting serious about the development of computational humor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliviero</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="59" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hahacronym: A computational humor system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliviero</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2005 on Interactive poster and demonstration sessions</title>
		<meeting>the ACL 2005 on Interactive poster and demonstration sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="113" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Computationally recognizing wordplay in jokes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mazlack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CogSci</title>
		<meeting>CogSci</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Computational detection of humor: A dream or a nightmare? the ontological semantics approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Julia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="volume">03</biblScope>
			<biblScope unit="page" from="429" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature-rich part-of-speech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter</title>
		<meeting>the 2003 Conference of the North American Chapter<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
	<note>NAACL &apos;03. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Word sense and subjectivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1065" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recognizing contextual polarity in phrase-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing<address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005-10" />
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recognizing humor on twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renxian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naishi</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management</title>
		<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
