<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Can characters reveal your native language? A language-independent approach to native language identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><forename type="middle">Tudor</forename><surname>Ionescu</surname></persName>
							<email>raducu.ionescu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Educational Testing Service</orgName>
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<addrLine>14 Academiei, 660 Rosedale Rd Princeton</addrLine>
									<postCode>08541</postCode>
									<settlement>Bucharest</settlement>
									<region>NJ</region>
									<country>Romania, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Popescu</surname></persName>
							<email>popescunmarius@gmail.com â€ </email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Educational Testing Service</orgName>
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<addrLine>14 Academiei, 660 Rosedale Rd Princeton</addrLine>
									<postCode>08541</postCode>
									<settlement>Bucharest</settlement>
									<region>NJ</region>
									<country>Romania, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoife</forename><surname>Cahill</surname></persName>
							<email>acahill@ets.org</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Educational Testing Service</orgName>
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<addrLine>14 Academiei, 660 Rosedale Rd Princeton</addrLine>
									<postCode>08541</postCode>
									<settlement>Bucharest</settlement>
									<region>NJ</region>
									<country>Romania, USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Can characters reveal your native language? A language-independent approach to native language identification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1363" to="1373"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A common approach in text mining tasks such as text categorization, authorship identification or plagiarism detection is to rely on features like words, part-of-speech tags, stems, or some other high-level linguistic features. In this work, an approach that uses character n-grams as features is proposed for the task of native language identification. Instead of doing standard feature selection, the proposed approach combines several string kernels using multiple kernel learning. Kernel Ridge Regression and Kernel Discriminant Analysis are independently used in the learning stage. The empirical results obtained in all the experiments conducted in this work indicate that the proposed approach achieves state of the art performance in native language identification, reaching an accuracy that is 1.7% above the top scoring system of the 2013 NLI Shared Task. Furthermore , the proposed approach has an important advantage in that it is language independent and linguistic theory neutral. In the cross-corpus experiment, the proposed approach shows that it can also be topic independent, improving the state of the art system by 32.3%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Using words as basic units is natural in textual analysis tasks such as text categorization, author- ship identification or plagiarism detection. Per- haps surprisingly, recent results indicate that meth- ods handling the text at the character level can also be very effective ( <ref type="bibr" target="#b13">Lodhi et al., 2002;</ref><ref type="bibr" target="#b20">Sanderson and Guenter, 2006</ref>; <ref type="bibr" target="#b16">Popescu and Dinu, 2007;</ref><ref type="bibr" target="#b8">Grozea et al., 2009;</ref><ref type="bibr" target="#b19">Popescu, 2011;</ref><ref type="bibr" target="#b17">Popescu and Grozea, 2012)</ref>. By disregarding features of natu- ral language such as words, phrases, or meaning, an approach that works at the character level has an important advantage in that it is language inde- pendent and linguistic theory neutral. This paper presents a state of the art machine learning system for native language identification that works at the character level. The proposed system is inspired by the system of <ref type="bibr" target="#b18">Popescu and Ionescu (2013)</ref>, but includes some variations and improvements. A major improvement is that several string kernels are combined via multiple kernel learning <ref type="bibr">(ShaweTaylor and Cristianini, 2004</ref>). Despite the fact that the (histogram) intersection kernel is very popular in computer vision ( <ref type="bibr" target="#b14">Maji et al., 2008;</ref><ref type="bibr" target="#b24">Vedaldi and Zisserman, 2010)</ref>, it has never been used before in text mining. In this work, the intersection kernel is used for the first time in a text categorization task, alone and in combination with other kernels. The intersection kernel lies somewhere in the middle between the kernel that takes into account only the presence of n-grams and the kernel based on the frequency of n-grams (p-spectrum string kernel).</p><p>Two kernel classifiers are proposed for the learning task, namely Kernel Ridge Regression (KRR) and Kernel Discriminant Analysis (KDA). The KDA classifier is able to avoid the class- masking problem <ref type="bibr" target="#b9">(Hastie and Tibshirani, 2003)</ref>, which may often arise in the context of native language identification. Several experiments are conducted to evaluate the performance of the ap- proach proposed in this work. While multiple ker- nel learning seems to produce a more robust sys- tem, the two kernel classifiers obtained mixed re- sults in the experiments. Overall, the empirical re- sults indicate that the approach proposed in this paper achieves state of the art performance in na- tive language identification, while being both lan-guage independent and linguistic theory neutral. Furthermore, the approach based on string kernels does not need any expert knowledge of words or phrases in the language.</p><p>The paper is organized as follows. Related work is presented in Section 2. Section 3 presents several similarity measures for strings, including string kernels and Local Rank Distance. The learning methods used in the experiments are de- scribed in Section 4. Section 5 presents details about the experiments. Finally, the conclusions are drawn in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Native Language Identification</head><p>The goal of automatic native language identifica- tion (NLI) is to determine the native language of a language learner, based on a piece of writing in a foreign language. This can provide useful in- formation in forensic linguistic tasks <ref type="bibr" target="#b6">(Estival et al., 2007)</ref> or could be used in an educational set- ting to provide contrastive feedback to language learners. Most research has focused on identify- ing the native language of English language learn- ers, though there have been some efforts recently to identify the native language of writing in other languages <ref type="bibr" target="#b15">(Malmasi and Dras, 2014)</ref>.</p><p>In general most approaches to NLI have used multi-way classification with SVMs or similar models along with a range of linguistic features. The seminal paper by <ref type="bibr" target="#b12">Koppel et al. (2005)</ref> intro- duced some of the best-performing features: char- acter, word and part-of-speech n-grams along with features inspired by the work in the area of second- language acquisition such as spelling and gram- matical errors.  or- ganized the first shared task in the field. This al- lowed researchers to compare approaches for the first time on a specifically designed NLI corpus that was much larger than previously available data sets. In the shared task, 29 teams submit- ted results for the test set, and one of the most successful aspects of the competition was that it drew submissions from teams working in a variety of research fields. The submitted systems utilized a wide range of machine learning approaches, combined with several innovative feature contri- butions. The best performing system achieved an overall accuracy of 83.6% on the 11-way classifi- cation of the test set, although there was no signif- icant difference between the top teams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Methods that Work at the Character Level</head><p>In recent years, methods of handling text at the character level have demonstrated impres- sive performance levels in various text analy- sis tasks <ref type="bibr" target="#b13">(Lodhi et al., 2002;</ref><ref type="bibr" target="#b20">Sanderson and Guenter, 2006;</ref><ref type="bibr" target="#b16">Popescu and Dinu, 2007;</ref><ref type="bibr" target="#b8">Grozea et al., 2009;</ref><ref type="bibr" target="#b19">Popescu, 2011;</ref><ref type="bibr" target="#b17">Popescu and Grozea, 2012)</ref>. <ref type="bibr" target="#b13">Lodhi et al. (2002)</ref> used string kernels for document categorization with very good re- sults. String kernels were also successfully used in authorship identification ( <ref type="bibr" target="#b20">Sanderson and Guenter, 2006;</ref><ref type="bibr" target="#b16">Popescu and Dinu, 2007;</ref><ref type="bibr" target="#b17">Popescu and Grozea, 2012)</ref>. For example, the system described in ( <ref type="bibr" target="#b17">Popescu and Grozea, 2012</ref>) ranked first in most problems and overall in the PAN 2012 Traditional Authorship Attribution tasks. Using string kernels makes the corresponding learning method completely language indepen- dent, because the texts will be treated as sequences of symbols (strings). Methods working at the word level or above very often restrict their feature space according to theoretical or empirical princi- ples. For instance, they select only features that re- flect various types of spelling errors or only some type of words, such as function words. These fea- tures prove to be very effective for specific tasks, but it is possible that other good features also ex- ist. String kernels embed the texts in a very large feature space, given by all the substrings of length p, and leave it to the learning algorithm to select important features for the specific task, by highly weighting these features. It is important to note that this approach is also linguistic theory neutral, since it disregards any features of natural language such as words, phrases, or meaning. On the other hand, a method that considers words as features cannot be completely language independent, since the definition of a word is necessarily language- specific. For example, a method that uses only function words as features is not completely lan- guage independent because it needs a list of func- tion words which is specific to a language. When features such as part-of-speech tags are used, as in the work of <ref type="bibr" target="#b11">Jarvis et al. (2013)</ref>, the method re- lies on a part-of-speech tagger which might not be available (yet) for some languages. Furthermore, a way to segment a text into words is not an easy task for some languages, such as Chinese.</p><p>Character n-grams are used by some of the sys- tems developed for native language identification.</p><p>In work where feature ablation results have been reported, the performance with only character n- gram features was modest compared to other types of features <ref type="bibr" target="#b22">(Tetreault et al., 2012)</ref>. Initially, most work limited the character features to unigrams, bigrams and trigrams, perhaps because longer n- grams were considered too expensive to compute or unlikely to improve performance. However, some of the top systems in the 2013 NLI Shared Task were based on longer character n-grams, up to 9-grams ( <ref type="bibr" target="#b11">Jarvis et al., 2013;</ref><ref type="bibr" target="#b18">Popescu and Ionescu, 2013)</ref>. The results presented in this work are obtained using a range of 5-8 n-grams. Com- bining all 5-8 n-grams would generate millions of features, which are indeed expensive to com- pute and represent. The key to avoiding the com- putation of such a large number of features lies in using the dual representation provided by the string kernel. String kernel similarity matrices can be computed much faster and are extremely useful when the number of samples is much lower than the number of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Similarity Measures for Strings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">String Kernels</head><p>The kernel function gives kernel methods the power to naturally handle input data that is not in the form of numerical vectors, e.g. strings.</p><p>The kernel function captures the intuitive notion of similarity between objects in a specific domain and can be any function defined on the respec- tive domain that is symmetric and positive definite. For strings, many such kernel functions exist with various applications in computational biology and computational linguistics <ref type="bibr" target="#b21">(Shawe-Taylor and Cristianini, 2004</ref>).</p><p>Perhaps one of the most natural ways to mea- sure the similarity of two strings is to count how many substrings of length p the two strings have in common. This gives rise to the p-spectrum ker- nel. Formally, for two strings over an alphabet Î£, s, t âˆˆ Î£ * , the p-spectrum kernel is defined as:</p><formula xml:id="formula_0">k p (s, t) = vâˆˆÎ£ p num v (s) Â· num v (t),</formula><p>where num v (s) is the number of occurrences of string v as a substring in s. <ref type="bibr">1</ref> The feature map de-fined by this kernel associates a vector of dimen- sion |Î£| p containing the histogram of frequencies of all its substrings of length p (p-grams) with each string.</p><p>A variant of this kernel can be obtained if the embedding feature map is modified to associate a vector of dimension |Î£| p containing the presence bits (instead of frequencies) of all its substrings of length p with each string. Thus, the character p- grams presence bits kernel is obtained:</p><formula xml:id="formula_1">k 0/1 p (s, t) = vâˆˆÎ£ p in v (s) Â· in v (t),</formula><p>where in v (s) is 1 if string v occurs as a substring in s, and 0 otherwise. In computer vision, the (histogram) intersec- tion kernel has successfully been used for object class recognition from images ( <ref type="bibr" target="#b14">Maji et al., 2008;</ref><ref type="bibr" target="#b24">Vedaldi and Zisserman, 2010)</ref>. In this paper, the intersection kernel is used for the first time as a kernel for strings. The intersection string kernel is defined as follows:</p><formula xml:id="formula_2">k âˆ© p (s, t) = vâˆˆÎ£ p min{num v (s), num v (t)},</formula><p>where num v (s) is the number of occurrences of string v as a substring in s.</p><p>For the p-spectrum kernel, the frequency of a p- gram has a very significant contribution to the ker- nel, since it considers the product of such frequen- cies. On the other hand, the frequency of a p-gram is completely disregarded in the p-grams presence bits kernel. The intersection kernel lies some- where in the middle between the p-grams presence bits kernel and p-spectrum kernel, in the sense that the frequency of a p-gram has a moderate contri- bution to the intersection kernel. More precisely, the following inequality that describes the relation between the three kernels holds:</p><formula xml:id="formula_3">k 0/1 p (s, t) â‰¤ k âˆ© p (s, t) â‰¤ k p (s, t).</formula><p>What is actually more interesting is that the inter- section kernel assigns a high score to a p-gram if it has a high frequency in both strings, since it con- siders the minimum of the two frequencies. The p-spectrum kernel assigns a high score even when the p-gram has a high frequency in only one of the two strings. Thus, the intersection kernel cap- tures something about the correlation between the p-gram frequencies in the two strings, which may lead to a more sensitive similarity between strings.</p><p>Normalized versions of these kernels ensure a fair comparison of strings of different lengths:</p><formula xml:id="formula_4">Ë† k p (s, t) = k p (s, t) k p (s, s) Â· k p (t, t)</formula><p>,</p><formula xml:id="formula_5">Ë† k 0/1 p (s, t) = k 0/1 p (s, t) k 0/1 p (s, s) Â· k 0/1 p (t, t) , Ë† k âˆ© p (s, t) = k âˆ© p (s, t) k âˆ© p (s, s) Â· k âˆ© p (t, t)</formula><p>.</p><p>Taking into account p-grams of different length and summing up the corresponding kernels, new kernels, termed blended spectrum kernels, can be obtained.</p><p>The string kernel implicitly embeds the texts in a high dimensional feature space. Then, a kernel-based learning algorithm implicitly assigns a weight to each feature, thus selecting the fea- tures that are important for the discrimination task. For example, in the case of text categorization the learning algorithm enhances the features rep- resenting stems of content words ( <ref type="bibr" target="#b13">Lodhi et al., 2002</ref>), while in the case of authorship identifica- tion the same learning algorithm enhances the fea- tures representing function words (Popescu and Dinu, 2007).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Local Rank Distance</head><p>A recently introduced distance measure, termed Local Rank Distance , comes from the idea of better adapting rank distance <ref type="bibr" target="#b5">(Dinu, 2003)</ref> to string data, in order to capture a bet- ter similarity between strings, such as DNA se- quences or text. Local Rank Distance (LRD) has already shown promising results in computational biology (  and native language iden- tification ( <ref type="bibr" target="#b18">Popescu and Ionescu, 2013)</ref>.</p><p>In order to describe LRD, the following nota- tions are defined. Given a string x over an al- phabet Î£, and a character a âˆˆ Î£, the length of x is denoted by |x|. Strings are considered to be indexed starting from position 1, that is</p><formula xml:id="formula_6">x = x[1]x[2] Â· Â· Â· x[|x|]. Moreover, x[i : j] denotes its substring x[i]x[i + 1] Â· Â· Â· x[j âˆ’ 1].</formula><p>Local Rank Distance is inspired by rank dis- tance ( <ref type="bibr" target="#b5">Dinu, 2003)</ref>, the main differences being that it uses p-grams instead of single charac- ters, and that it matches each p-gram in the first string with the nearest equal p-gram in the second string. Given a fixed integer p â‰¥ 1, a thresh- old m â‰¥ 1, and two strings x and y over Î£, the Local Rank Distance between x and y, de- noted by âˆ† LRD (x, y), is defined through the fol- lowing algorithmic process. For each position i in x (1 â‰¤ i â‰¤ |x| âˆ’ p + 1), the algorithm searches for that position j in y (1 â‰¤ j â‰¤ |y| âˆ’ p + 1) such that</p><formula xml:id="formula_7">x[i : i + p] = y[j : j + p] and |i âˆ’ j| is minimized.</formula><p>If j exists and |i âˆ’ j| &lt; m, then the offset |i âˆ’ j| is added to the Local Rank Distance. Otherwise, the maximal offset m is added to the Local Rank Distance. An important remark is that LRD does not impose any mathematically developed global constraints, such as matching the i-th occurrence of a p-gram in x with the i-th occurrence of that same p-gram in y. Instead, it is focused on the lo- cal phenomenon, and tries to pair equal p-grams at a minimum offset. To ensure that LRD is a (sym- metric) distance function, the algorithm also has to sum up the offsets obtained from the above pro- cess by exchanging x and y. LRD can be formally defined as follows.</p><p>Definition 1 Let x, y âˆˆ Î£ * be two strings, and let p â‰¥ 1 and m â‰¥ 1 be two fixed integer values. The Local Rank Distance between x and y is defined as:</p><formula xml:id="formula_8">âˆ† LRD (x, y) = âˆ† lef t (x, y) + âˆ† right (x, y),</formula><p>where âˆ† lef t (x, y) and âˆ† right (x, y) are defined as follows:</p><formula xml:id="formula_9">âˆ† lef t (x, y) = |x|âˆ’p+1 i=1 min{|i âˆ’ j| such that 1 â‰¤ j â‰¤ |y| âˆ’ p + 1 and x[i : i + p] = y[j : j + p]} âˆª {m}, âˆ† right (x, y) = |y|âˆ’p+1 j=1 min{|j âˆ’ i| such that 1 â‰¤ i â‰¤ |x| âˆ’ p + 1 and y[j : j + p] = x[i : i + p]} âˆª {m}.</formula><p>Interestingly, the search for matching p-grams is limited within a window of fixed size. The size of this window is determined by the maximum offset parameter m. This parameter must be set a priori and should be proportional to the size of the alpha- bet, the p-grams, and to the lengths of the strings.</p><p>The following example offers a better under- standing of how LRD actually works. LRD is computed between two strings using 2-grams.</p><p>Example 1 Given two strings x = abcaa and y = cabca, a fixed maximal offset m = 3, and a fixed size of p-grams p = 2, âˆ† lef t and âˆ† right are computed as follows:</p><formula xml:id="formula_10">âˆ† lef t (x, y) = |1 âˆ’ 2| + |2 âˆ’ 3| + |3 âˆ’ 4| + 3 = 6, âˆ† right (x, y) = |1 âˆ’ 3| + |2 âˆ’ 1| + |3 âˆ’ 2| + |4 âˆ’ 3| = 5.</formula><p>By summing up the two partial sums, Local Rank Distance is obtained</p><formula xml:id="formula_11">âˆ† LRD (x, y) = âˆ† lef t (x, y) + âˆ† right (x, y) = 11.</formula><p>The maximum LRD value between two strings can be computed as the product between the max- imum offset m and the number of pairs of com- pared p-grams. Thus, LRD can be normalized to a value in the [0, 1] interval. By normalizing, LRD becomes a dissimilarity measure. LRD can be also used as a kernel, since kernel methods are based on similarity. The classical way to transform a distance or dissimilarity measure into a simi- larity measure is by using the Gaussian-like ker- nel <ref type="bibr" target="#b21">(Shawe-Taylor and Cristianini, 2004</ref>):</p><formula xml:id="formula_12">Ë† k LRD p (s, t) = e âˆ’ âˆ† LRD (s, t) 2Ïƒ 2 ,</formula><p>where s and t are two strings and p is the p-grams length. The parameter Ïƒ is usually chosen so that values ofË†kofË† ofË†k(s, t) are well scaled. In the above equation, âˆ† LRD is already normalized to a value in the [0, 1] interval to ensure a fair comparison of strings of different length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning Methods</head><p>Kernel-based learning algorithms work by embed- ding the data into a Hilbert feature space, and searching for linear relations in that space. The embedding is performed implicitly, that is by spec- ifying the inner product between each pair of points rather than by giving their coordinates ex- plicitly. More precisely, a kernel matrix that con- tains the pairwise similarities between every pair of training samples is used in the learning stage to assign a vector of weights to the training sam- ples. Let Î± denote this weight vector. In the test stage, the pairwise similarities between a test sam- ple x and all the training samples are computed. Then, the following binary classification function assigns a positive or a negative label to the test sample:</p><formula xml:id="formula_13">g(x) = n i=1 Î± i Â· k(x, x i ),</formula><p>where x is the test sample, n is the number of training samples, X = {x 1 , x 2 , ..., x n } is the set of training samples, k is a kernel function, and Î± i is the weight assigned to the training sample x i . In the primal form, the same binary classification function can be expressed as:</p><formula xml:id="formula_14">g(x) = w, x,</formula><p>where Â·, Â·Â· denotes the scalar product, x âˆˆ R m is the test sample represented as a vector of features, and w âˆˆ R m is a vector of feature weights that can be computed as follows:</p><formula xml:id="formula_15">w = n i=1 Î± i Â· x i ,</formula><p>given that the kernel function k can be expressed as a scalar product between samples. The advantage of using the dual representation induced by the kernel function becomes clear if the dimension of the feature space m is taken into consideration. Since string kernels are based on character n-grams, the feature space is indeed very high. For instance, using 5-grams based only on the 26 letters of the English alphabet will re- sult in a feature space of 26 5 = 11, 881, 376 fea- tures. However, in the experiments presented in this work the feature space includes 5-grams along with 6-grams, 7-grams and 8-grams. As long as the number of samples n is not greater than the number of features m, it is more efficient to use the dual representation given by the kernel matrix. This fact is also known as the kernel trick (Shawe- <ref type="bibr" target="#b21">Taylor and Cristianini, 2004</ref>).</p><p>Various kernel methods differ in the way they learn to separate the samples. In the case of binary classification problems, kernel-based learning al- gorithms look for a discriminant function, a func- tion that assigns +1 to examples belonging to one class and âˆ’1 to examples belonging to the other class. For the NLI experiments, two binary kernel classifiers are used, namely the SVM ( <ref type="bibr" target="#b3">Cortes and Vapnik, 1995)</ref>, and the KRR. Support Vector Ma- chines try to find the vector of weights that defines the hyperplane that maximally separates the im- ages in the Hilbert space of the training examples belonging to the two classes. Kernel Ridge Re- gression selects the vector of weights that simulta- neously has small empirical error and small norm in the Reproducing Kernel Hilbert Space gener- ated by the kernel function. More details about SVM and KRR can be found in <ref type="bibr" target="#b21">(Shawe-Taylor and Cristianini, 2004</ref>). The important fact is that the above optimization problems are solved in such a way that the coordinates of the embedded points are not needed, only their pairwise inner products which in turn are given by the kernel function.</p><p>SVM and KRR produce binary classifiers, but native language identification is usually a multi- class classification problem. There are many ap- proaches for combining binary classifiers to solve multi-class problems. Typically, the multi-class problem is broken down into multiple binary clas- sification problems using common decomposing schemes such as: one-versus-all and one-versus- one. There are also kernel methods that take the multi-class nature of the problem directly into ac- count, e.g. Kernel Discriminant Analysis. The KDA classifier is able to improve accuracy by avoiding the masking problem <ref type="bibr" target="#b9">(Hastie and Tibshirani, 2003</ref>). In the case of multi-class native language identification, the masking problem may appear when non-native English speakers have ac- quired, as the second language, a different lan- guage to English. For example, an essay written in English produced by a French native speaker that is also proficient in German, could be identified as either French or German.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Sets Description</head><p>In this paper, experiments are carried out on three datasets: a modified version of the ICLEv2 cor- pus ( <ref type="bibr" target="#b7">Granger et al., 2009)</ref>, the ETS Corpus of Non-Native Written English, or TOEFL11 ( , and the TOEFL11-Big corpus as used by <ref type="bibr" target="#b22">Tetreault et al. (2012)</ref>. A summary of the corpora is given in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus</head><p>Languages Documents ICLE 7 770 TOEFL11 11 12, 100 TOEFL11-Big 11 87, 502</p><p>Table 1: Summary of corpora used in the experi- ments.</p><p>The ICLEv2 is a corpus of essays written by highly-proficient non-native college-level students of English. For many years this was the standard corpus used in the task of native language identi- fication. However, the corpus was originally col- lected for the purpose of corpus linguistic inves- tigations, and because of this contains some id- iosyncrasies that make it problematic for the task of NLI ( <ref type="bibr" target="#b1">Brooke and Hirst, 2012</ref>). Therefore, a modified version of the corpus that has been nor- malized as much as possible for topic and charac- ter encoding <ref type="bibr" target="#b22">(Tetreault et al., 2012</ref>) is used. This version of the corpus contains 110 essays each for 7 native languages: Bulgarian, Chinese, Czech, French, Japanese, Russian and Spanish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The ETS Corpus of Non-Native Written English (TOEFL11) was first introduced by Tetreault et al. (2012) and extended for the 2013 Native Language</head><p>Identification Shared Task ( . It was designed to overcome many of the short- comings identified with using the ICLEv2 corpus for this task. The TOEFL11 corpus contains a balanced distribution of essays per prompt (topic) per native language. It also contains information about the language proficiency of each writer. The corpus contains essays written by speakers of the following 11 languages: Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Span- ish, Telugu and Turkish. For the shared task, the 12, 100 essays were split into 9, 900 for training, 1, 100 for development and 1, 100 for testing. <ref type="bibr" target="#b22">Tetreault et al. (2012)</ref> present a corpus, TOEFL11-Big, to investigate the performance of their NLI system on a very large data set. This data set contains the same languages as TOEFL11, but with no overlap in content. It contains a total of over 87 thousand essays written to a total of 76 different prompts. The distribution of L1 per prompt is not as even as for TOEFL11, though all topics are represented for all L1s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Parameter Tuning and Implementation Choices</head><p>In the string kernels approach proposed in this work, documents or essays from this corpus are treated as strings. Therefore, the notions of string or document is used interchangeably throughout this work. Because the approach works at the char- acter level, there is no need to split the texts into words, or to do any NLP-specific preprocessing. The only editing done to the texts was the replac- ing of sequences of consecutive space characters (space, tab, new line, and so on) with a single space character. This normalization was needed in order to prevent the artificial increase or decrease of the similarity between texts, as a result of differ- ent spacing. All uppercase letters were converted to the corresponding lowercase ones. A series of preliminary experiments were con- ducted in order to select the best-performing learn- ing method. In these experiments the string ker- nel was fixed to the p-spectrum normalized ker- nel of length 5 ( Ë† k 5 ), because the goal was to se- lect the best learning method, and not to find the best kernel. The following learning methods were evaluated: one-versus-one SVM, one-versus-all SVM, one-versus-one KRR, one-versus-all KRR, and KDA. A 10-fold cross-validation procedure was carried out on the TOEFL11 training set to evaluate the classifiers. The preliminary results in- dicate that the one-versus-all KRR and the KDA classifiers produce the best results. Therefore, they are selected for the remaining experiments.</p><p>Another set of preliminary experiments were performed to determine the range of n-grams that gives the most accurate results on a 10-fold cross- validation procedure carried out on the TOEFL11 training set. All the n-grams in the range 2-10 were evaluated. Furthermore, experiments with different blended kernels were conducted to see whether combining n-grams of different lengths could improve the accuracy. The best results were obtained when all the n-grams with the length in the range 5-8 were used. Other authors <ref type="bibr" target="#b2">(Bykh and Meurers, 2012;</ref><ref type="bibr" target="#b18">Popescu and Ionescu, 2013</ref>) also report better results by using n-grams with the length in a range, rather than using n-grams of fixed length. Consequently, the results reported in this work are based on blended string kernels based on 5-8 n-grams.</p><p>Some preliminary experiments were also per- formed to establish the type of kernel to be used, namely the blended p-spectrum kernel ( Ë† k 5âˆ’8 ), the blended p-grams presence bits kernel ( Ë† k ). These different kernel representations are obtained from the same data. The idea of combining all these kernels is natural when one wants to improve the perfor- mance of a classifier. When multiple kernels are combined, the features are actually embedded in a higher-dimensional space. As a consequence, the search space of linear patterns grows, which helps the classifier to select a better discriminant function. The most natural way of combining two kernels is to sum them up. Summing up kernels or kernel matrices is equivalent to feature vector concatenation. Another option is to combine ker- nels by kernel alignment ( <ref type="bibr" target="#b4">Cristianini et al., 2001</ref>). Instead of simply summing kernels, kernel align- ment assigns weights for each of the two kernels based on how well they are aligned with the ideal kernel Y Y obtained from training labels. The ker- nels were evaluated alone and in various combina- tions. The best kernels are the blended p-grams presence bits kernel and the blended p-grams in- tersection kernel. The best kernel combinations include the blended p-grams presence bits kernel, the blended p-grams intersection kernel and the kernel based on LRD. Since the kernel based on LRD is slightly slower than the other string ker- nels, the kernel combinations that include it were only evaluated on the TOEFL11 corpus and on the ICLE corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiment on TOEFL11 Corpus</head><p>This section describes the results on the TOEFL11 corpus. Thus, results for the 2013 Closed NLI Shared Task are also included. In the closed shared task the goal is to predict the native language of testing examples, restricted to learning only from the training and the development data. The ad- ditional information from prompts or the English language proficiency level were not used in the proposed approach.</p><p>The regularization parameters were tuned on the development set. In this case, the systems were trained on the entire training set. A 10-fold cross- validation (CV) procedure was done on the train- ing and the development sets. The folds were pro- vided along with the TOEFL11 corpus. Finally, the results of the proposed systems are also re- ported on the NLI Shared Task test set. For test- ing, the systems were trained on both the training set and the development set. The results are sum- marized in <ref type="table">Table 2</ref>.</p><p>The results presented in <ref type="table">Table 2</ref> show that string kernels can reach state of the art accuracy levels for this task. Overall, it seems that KDA is able to obtain better results than KRR. The intersection kernel alone is able to obtain slightly better results than the presence bits kernel. The kernel based on LRD gives significantly lower accuracy rates, but it is able to improve the performance when it is Method Development 10-fold CV Test Ensemble model <ref type="bibr" target="#b22">(Tetreault et al., 2012)</ref> - 80.9% - KRR and string kernels ( <ref type="bibr" target="#b18">Popescu and Ionescu, 2013)</ref> - 82.6% 82.7% SVM and word features ( <ref type="bibr" target="#b11">Jarvis et al., 2013</ref>  87.0% 84.1% 84.8% <ref type="table">Table 2</ref>: Accuracy rates on TOEFL11 corpus of various classification systems based on string kernels compared with other state of the art approaches. The best accuracy rates on each set of experiments are highlighted in bold. The weights a 1 and a 2 from the weighted sums of kernels are computed by kernel alignment.</p><p>combined with the blended p-grams presence bits kernel. In fact, most of the kernel combinations give better results than each of their components.</p><p>The best kernel combination is that of the pres- ence bits kernel and the intersection kernel. Re- sults are quite similar when they are combined ei- ther by summing them up or by kernel alignment. The best performance on the test set (85.3%) is ob- tained by the system that combines these two ker- nels via kernel alignment and learns using KDA. This system is 1.7% better than the state of the art system of Jarvis et al. <ref type="formula">(2013)</ref> based on SVM and word features, this being the top scoring system in the NLI 2013 Shared Task. It is also 2.6% better than the state of the art system based on string ker- nels of <ref type="bibr" target="#b18">Popescu and Ionescu (2013)</ref>. On the cross validation procedure, there are three systems that reach the accuracy rate of 84.1%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Experiment on ICLE Corpus</head><p>The results on the ICLE corpus using a 5-fold cross validation procedure are summarized in <ref type="table" target="#tab_1">Ta- ble 3</ref>. To adequately compare the results with a state of the art system, the same 5-fold cross val- idation procedure used by <ref type="bibr" target="#b22">Tetreault et al. (2012)</ref> was also used in this experiment. <ref type="table" target="#tab_1">Table 3</ref> shows that the results obtained by the presence bits kernel and by the intersection kernel are systematically better than the state of the art system of <ref type="bibr" target="#b22">Tetreault et al. (2012)</ref>. While both KRR and KDA produce accuracy rates that are better than the state of the art accuracy rate, it seems that KRR is slightly bet- ter in this experiment. Again, the idea of com- bining kernels seems to produce more robust sys- tems. The best systems are based on combin- ing the presence bits kernel either with the kernel based on LRD or the intersection kernel. Over- all, the reported accuracy rates are higher than the state of the art accuracy rate. The best perfor- mance (91.3%) is achieved by the KRR classifier based on combining the presence bits kernel with Method 5-fold CV Ensemble model <ref type="bibr" target="#b22">(Tetreault et al., 2012)</ref> 90.1% KRR andË†kandË† andË†k  the kernel based on LRD. This represents an 1.2% improvement over the state of the art accuracy rate of <ref type="bibr" target="#b22">Tetreault et al. (2012)</ref>. Two more systems are able to obtain accuracy rates greater than 91.0%. These are the KRR classifier based on the presence bits kernel (91.2%) and the KDA classifier based on the sum of the presence bits kernel and the in- tersection kernel (91.0%). The overall results on the ICLE corpus show that the string kernels ap- proach can reach state of the art accuracy levels. It is worth mentioning the purpose of this experi- ment was to use the same approach determined to work well in the TOEFL11 corpus. To serve this purpose, the range of n-grams was not tuned on this data set. Furthermore, other classifiers were not tested in this experiment. Nevertheless, better results can probably be obtained by adding these aspects into the equation.</p><formula xml:id="formula_16">0/1 5âˆ’8 91.2% KRR andË†kandË† andË†k âˆ© 5âˆ’8</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>90.5%</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Cross-corpus Experiment</head><p>In this experiment, various systems based on KRR or KDA are trained on the TOEFL11 corpus and tested on the TOEFL11-Big corpus. The kernel based on LRD was not included in this experiment since it is more computationally expensive. There- fore, only the presence bits kernel and the intersec- tion kernel were evaluated on the TOEFL11-Big corpus. The results are summarized in <ref type="table">Table 4</ref>. The same regularization parameters determined to Method Test Ensemble model <ref type="bibr" target="#b22">(Tetreault et al., 2012)</ref> 35.4% KRR andË†kandË† andË†k  <ref type="table">Table 4</ref>: Accuracy rates on TOEFL11-Big corpus of various classification systems based on string kernels compared with a state of the art approach. The systems are trained on the TOEFL11 corpus and tested on the TOEFL11-Big corpus. The best accuracy rate is highlighted in bold. The weights a 1 and a 2 from the weighted sums of kernels are computed by kernel alignment.</p><formula xml:id="formula_17">0/1 5âˆ’8 66.7% KRR andË†kandË† andË†k âˆ© 5âˆ’8</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>67.2%</head><p>work well on the TOEFL11 development set were used.</p><p>The most interesting fact is that all the proposed systems are at least 30% better than the state of the art system. Considering that the TOEFL11-Big corpus contains 87 thousand samples, the 30% im- provement is significant without any doubt. Div- ing into details, it can be observed that the results obtained by KRR are higher than those obtained by KDA. However, both methods perform very well compared to the state of the art. Again, kernel combinations are better than each of their individ- ual kernels alone.</p><p>It is important to mention that the significant performance increase is not due to the learning method (KRR or KDA), but rather due to the string kernels that work at the character level. It is not only the case that string kernels are language in- dependent, but for the same reasons they can also be topic independent. Since the topics (prompts) from TOEFL11 are different from the topics from TOEFL11-Big, it becomes clear that a method that uses words as features is strongly affected, since the distribution of words per topic can be completely different. But mistakes that reveal the native language can be captured by character n- grams that can appear more often even in differ- ent topics. The results indicate that this is also the case of the approach based on string kernels, which seems to be more robust to such topic vari- ations of the data set. The best system has an ac- curacy rate that is 32.3% better than the state of the art system of <ref type="bibr" target="#b22">Tetreault et al. (2012)</ref>. Overall, the empirical results indicate that the string ker- nels approach can achieve significantly better re- sults than other state of the art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>A language-independent approach to native lan- guage identification was presented in this paper.</p><p>The system works at the character level, mak- ing the approach completely language indepen- dent and linguistic theory neutral. The results ob- tained in all the three experiments were very good. The best system presented in this work is based on combining the intersection and the presence string kernels by kernel alignment and on deciding the class label either with KDA or KRR. The best sys- tem is 1.7% above the top scoring system of the 2013 NLI Shared Task. Furthermore, it has an im- pressive generalization capacity, achieving results that are 30% higher than the state of the art method in the cross-corpus experiment.</p><p>Despite the fact that the approach based on string kernels performed so well, it remains to be further investigated why this is the case and why such a simple approach can compete with far more complex approaches that take words, lemmas, syntactic information, or even semantics into ac- count. It seems that there are generalizations to the kinds of mistakes that certain non-native English speakers make that can be captured by n-grams of different lengths. Interestingly, using a range of n-grams generates a large number of features including (but not limited to) stop words, stems of content words, word suffixes, entire words, and even n-grams of short words. Rather than doing feature selection before the training step, which is the usual NLP approach, the kernel classifier selects the most relevant features during training. With enough training samples, the kernel classi- fier does a better job of selecting the right features from a very high feature space. This may be one reason for why the string kernel approach works so well. To gain additional insights into why this technique is working well, the features selected by the classifier as being more discriminating can be analyzed in future work. This analysis would also offer some information about localized lan- guage transfer effects, since the features used by the proposed model are n-grams of lengths 5 to 8. As mentioned before, the features captured by the model typically include stems, function words, word prefixes and suffixes, which have the poten- tial to generalize over purely word-based features. These features would offer insights into two kinds of language transfer effects, namely word choice (lexical transfer) and morphological differences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>All of them are based on KDA and various kernel combinations. The greatest accuracy rate of 84.1% reported for the cross validation procedure is 3.2% above the state of the art system of Tetreault et al. (2012) and 0.4% below the top scoring system of Jarvis et al. (2013). The empirical results obtained in this ex- periment demonstrate that the approach proposed in this paper can reach state of the art accuracy levels. It is worth mentioning that a significance test performed by the organizers of the NLI 2013 Shared Task showed that the top systems that par- ticipated in the competition are not essentially dif- ferent. Further experiments on the ICLE corpus and on the TOEFL11-Big corpus are conducted to determine whether the approach proposed in this paper is significantly better than other state of the art approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Accuracy rates on ICLE corpus of vari-
ous classification systems based on string kernels 
compared with a state of the art approach. The ac-
curacy rates are reported for the same 5-fold CV 
procedure as in (Tetreault et al., 2012). The best 
accuracy rate is highlighted in bold. 

</table></figure>

			<note place="foot" n="1"> Note that the notion of substring requires contiguity. Shawe-Taylor and Cristianini (2004) discuss the ambiguity between the terms substring and subsequence across different domains: biology, computer science.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Beata Beigman Klebanov, Nitin Madnani and Xinhao Wang from ETS for their helpful comments and suggestions. The author also thank the anonymous reviewers for their valuable insights which lead to improve-ments in the presentation of this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TOEFL11: A Corpus of Non-Native English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derrick</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoife</forename><surname>Cahill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<idno>RR-13-24</idno>
	</analytic>
	<monogr>
		<title level="j">Educational Testing Service Research Report</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Brooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
		<title level="m">Robust, Lexicalized Native Language Identification. Proceedings of COLING 2012</title>
		<imprint>
			<date type="published" when="2012-12" />
			<biblScope unit="page" from="391" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Native Language Identification using Recurring n-grams-Investigating Abstraction and Domain Dependence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serhiy</forename><surname>Bykh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Detmar</forename><surname>Meurers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012</title>
		<meeting>COLING 2012</meeting>
		<imprint>
			<publisher>December</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="425" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SupportVector Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On kernel-target alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">AndrÃ©</forename><surname>Elisseeff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaz</forename><forename type="middle">S</forename><surname>Kandola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<publisher>December</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="367" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the classification and aggregation of hierarchies with different constitutive elements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liviu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dinu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fundamenta Informaticae</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="50" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Author profiling for English emails</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Estival</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>Gaustad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son-Bao</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hutchinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of PACLING</title>
		<meeting>PACLING</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylviane</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estelle</forename><surname>Dagneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanny</forename><surname>Meunier</surname></persName>
		</author>
		<title level="m">The International Corpus of Learner English: Handbook and CD-ROM</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>version</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ENCOPLOT: Pairwise Sequence Matching in Linear Time Applied to Plagiarism Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Grozea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Gehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd PAN Workshop. Uncovering Plagiarism, Authorship, and Social Software Misuse</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The Elements of Statistical Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003-07" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>corrected edition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Local Rank Distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tudor</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SYNASC</title>
		<meeting>SYNASC</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="221" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Maximizing classification accuracy in native language identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Jarvis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Bestgen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Pepper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatically Determining an Anonymous Author&apos;s Native Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kfir</forename><surname>Zigdon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ISI</title>
		<meeting>ISI</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="209" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Text classification using string kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huma</forename><surname>Lodhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="419" to="444" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Classification using intersection kernel support vector machines is efficient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Chinese Native Language Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2014-04" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="95" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kernel methods and string kernels for authorship identification: The federalist papers case</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liviu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dinu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RANLP</title>
		<meeting>RANLP</meeting>
		<imprint>
			<date type="published" when="2007-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Grozea</surname></persName>
		</author>
		<title level="m">Kernel methods and string kernels for authorship analysis. CLEF (Online Working Notes/Labs/Workshop)</title>
		<imprint>
			<date type="published" when="2012-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Story of the Characters, the DNA and the Native Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu Tudor</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="270" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Studying translationese at the character level</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RANLP</title>
		<meeting>RANLP</meeting>
		<imprint>
			<date type="published" when="2011-09" />
			<biblScope unit="page" from="634" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Short text authorship attribution via sequence kernels, markov chains and author unmasking: An investigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conrad</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Guenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="482" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Kernel Methods for Pattern Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Taylor</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Native Tongues, Lost and Found: Resources and Empirical Evaluations in Native Language Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoife</forename><surname>Cahill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012</title>
		<meeting>COLING 2012</meeting>
		<imprint>
			<date type="published" when="2012-12" />
			<biblScope unit="page" from="2585" to="2602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A report on the first native language identification shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoife</forename><surname>Cahill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="48" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Efficient additive kernels via explicit feature maps. Proceedings of CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3539" to="3546" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
