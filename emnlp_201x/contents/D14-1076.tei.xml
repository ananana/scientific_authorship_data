<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Multi-documents Summarization by Sentence Compression based on Expanded Constituent Parse Trees</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">The University of Texas at Dallas Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">The University of Texas at Dallas Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
							<email>{feiliu@cs.cmu.edu}</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Research and Technology Center</orgName>
								<orgName type="institution">Robert Bosch LLC Palo Alto</orgName>
								<address>
									<postCode>94304</postCode>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Research and Technology Center</orgName>
								<orgName type="institution">Robert Bosch LLC Palo Alto</orgName>
								<address>
									<postCode>94304</postCode>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Multi-documents Summarization by Sentence Compression based on Expanded Constituent Parse Trees</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="691" to="701"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we focus on the problem of using sentence compression techniques to improve multi-document summariza-tion. We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status-remove or retain. Integer liner programming with discrimina-tive training is used to solve the problem. Under this model, we incorporate various constraints to improve the linguistic quality of the compressed sentences. Then we utilize a pipeline summarization framework where sentences are first compressed by our proposed compression model to obtain top-n candidates and then a sentence selection module is used to generate the final summary. Compared with state-of-the-art algorithms, our model has similar ROUGE-2 scores but better linguistic quality on TAC data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic summarization can be broadly divided into two categories: extractive and abstractive summarization. Extractive summarization focuses on selecting salient sentences from the document collection and concatenating them to form a sum- mary; while abstractive summarization is gener- ally considered more difficult, involving sophisti- cated techniques for meaning representation, con- tent planning, surface realization, etc.</p><p>There has been a surge of interest in recent years on generating compressed document summaries as a viable step towards abstractive summarization. These compressive summaries often contain more information than sentence-based extractive sum- maries since they can remove insignificant sen- tence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strate- gies have been used for compressive summariza- tion. One is a pipeline approach, where sentence- based extractive summarization is followed or pro- ceeded by sentence compression <ref type="bibr" target="#b20">(Lin, 2003;</ref><ref type="bibr" target="#b40">Zajic et al., 2007;</ref><ref type="bibr" target="#b35">Vanderwende et al., 2007;</ref><ref type="bibr" target="#b36">Wang et al., 2013)</ref>. Another line of work uses joint compres- sion and summarization. Such methods have been shown to achieve promising performance <ref type="bibr" target="#b7">(Daumé, 2006;</ref><ref type="bibr" target="#b4">Chali and Hasan, 2012;</ref><ref type="bibr" target="#b1">Almeida and Martins, 2013;</ref><ref type="bibr" target="#b31">Qian and Liu, 2013</ref>), but they are typi- cally computationally expensive.</p><p>In this study, we propose an innovative sen- tence compression model based on expanded con- stituent parse trees. Our model uses integer lin- ear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminative sentence compression model from <ref type="bibr" target="#b25">(McDonald, 2006</ref>) and <ref type="bibr" target="#b5">(Clarke and Lapata, 2008</ref>), but our method uses an expanded constituent parse tree rather than only the leaf nodes in previous work. Therefore we can extract rich features for every node in the con- stituent parser tree. This is an advantage of tree- based compression technique <ref type="bibr" target="#b14">(Knight and Marcu, 2000;</ref><ref type="bibr" target="#b11">Galley and McKeown, 2007;</ref><ref type="bibr" target="#b36">Wang et al., 2013</ref>). Similar to ( <ref type="bibr" target="#b17">Li et al., 2013a</ref>), we use a pipeline summarization framework where multi- ple compression candidates are generated for each pre-selected important sentence, and then an ILP-based summarization model is used to select the final compressed sentences. We evaluate our pro- posed method on the TAC 2008 and 2011 data sets using the standard ROUGE metric <ref type="bibr" target="#b21">(Lin, 2004)</ref> and human evaluation of the linguistic quality. Our results show that using our proposed sentence compression model in the summarization system can yield significant performance gain in linguis- tic quality, without losing much performance on the ROUGE metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Summarization research has seen great develop- ment over the last fifty years <ref type="bibr" target="#b26">(Nenkova and McKeown, 2011</ref>). Compared to the abstractive counter- part, extractive summarization has received con- siderable attention due to its clear problem for- mulation: to extract a set of salient and non- redundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. Su- pervised approaches include the Bayesian classi- fier ( <ref type="bibr" target="#b16">Kupiec et al., 1995)</ref>, maximum entropy <ref type="bibr" target="#b29">(Osborne, 2002</ref>), skip-chain CRF <ref type="bibr" target="#b12">(Galley, 2006</ref>), dis- criminative reranking <ref type="bibr" target="#b0">(Aker et al., 2010</ref>), among others. The extractive summary sentence selec- tion problem can also be formulated in an opti- mization framework. Previous methods include using integer linear programming (ILP) and sub- modular functions to solve the optimization prob- lem ( <ref type="bibr" target="#b13">Gillick et al., 2009;</ref><ref type="bibr" target="#b18">Li et al., 2013b;</ref><ref type="bibr" target="#b19">Lin and Bilmes, 2010)</ref>.</p><p>Compressive summarization receives increas- ing attention in recent years, since it offers a vi- able step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compres- sion processes, or through a pipeline approach that integrates a sentence compression model with a summary sentence pre-selection or post-selection step.</p><p>Many studies have explored the joint sentence compression and selection setting. <ref type="bibr" target="#b24">Martins and Smith (2009)</ref> jointly performed sentence extrac- tion and compression by solving an ILP prob- lem. <ref type="bibr" target="#b2">Berg-Kirkpatrick et al. (2011)</ref> proposed an approach to score the candidate summaries ac- cording to a combined linear model of extrac- tive sentence selection and compression. They trained the model using a margin-based objec- tive whose loss captures the final summary qual- ity. <ref type="bibr" target="#b38">Woodsend and Lapata (2012)</ref> presented an- other method where the summary's informative- ness, succinctness, and grammaticality are learned separately from data but optimized jointly using an ILP setup. <ref type="bibr" target="#b39">Yoshikawa et al. (2012)</ref> incorporated semantic role information in the ILP model.</p><p>Our work is closely related with the pipeline approach, where sentence-based extractive sum- marization is followed or proceeded by sentence compression. There have been many studies on sentence compression, independent of the summa- rization task. <ref type="bibr" target="#b25">McDonald (2006)</ref> firstly introduced a discriminative sentence compression model to directly optimize the quality of the compressed sentences produced. <ref type="bibr" target="#b5">Clarke and Lapata (2008)</ref> improved the above discriminative model by us- ing ILP in decoding, making it convenient to add constraints to preserve grammatical structure. Nomoto (2007) treated the compression task as a sequence labeling problem and used CRF for it. <ref type="bibr" target="#b33">Thadani and McKeown (2013)</ref> presented an approach for discriminative sentence compression that jointly produces sequential and syntactic rep- resentations for output text. <ref type="bibr" target="#b9">Filippova and Altun (2013)</ref> presented a method to automatically build a sentence compression corpus with hundreds of thousands of instances on which deletion-based compression algorithms can be trained.</p><p>In addition to the work on sentence compres- sion as a stand-alone task, prior studies have also investigated compression for the summarization task. <ref type="bibr" target="#b14">Knight and Marcu (2000)</ref> utilized the noisy channel and decision tree method to perform sen- tence compression in the summarization task. <ref type="bibr" target="#b20">Lin (2003)</ref> showed that pure syntactic-based compres- sion may not significantly improve the summariza- tion performance. <ref type="bibr" target="#b40">Zajic et al. (2007)</ref> compared two sentence compression approaches for multi- document summarization, including a 'parse-and- trim' and a noisy-channel approach. <ref type="bibr">Galanis and Androutsopoulos (2010)</ref> used the maximum en- tropy model to generate the candidate compres- sions by removing branches from the source sen- tences. <ref type="bibr" target="#b37">Woodsend and Lapata (2010)</ref> presented a joint content selection and compression model for single-document summarization. They operated over a phrase-based representation of the source document which they obtained by merging infor- mation from PCFG parse trees and dependency graphs. Liu and Liu (2013) adopted the CRF- based sentence compression approach for summa-rizing spoken documents. Unlike the word-based operation, some of these models e.g <ref type="bibr" target="#b14">(Knight and Marcu, 2000;</ref><ref type="bibr" target="#b32">Siddharthan et al., 2004;</ref><ref type="bibr" target="#b34">Turner and Charniak, 2005;</ref><ref type="bibr">Galanis and Androutsopoulos, 2010;</ref><ref type="bibr" target="#b36">Wang et al., 2013)</ref>, are tree-based ap- proaches that operate on the parse trees and thus the compression decision can be made for a con- stituent, instead of a single word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sentence Compression Method</head><p>Sentence compression is a task of producing a summary for a single sentence. The compressed sentence should be shorter, contain important con- tent from the original sentence, and be grammat- ical. In some sense, sentence compression can be described as a 'scaled down version of the text summarization problem' <ref type="bibr" target="#b15">(Knight and Marcu, 2002</ref>). Here similar to much previous work on sentence compression, we just focus on how to re- move/select words in the original sentence without using operation like rewriting sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Discriminative Compression Model by ILP</head><p>McDonald <ref type="formula" target="#formula_3">(2006)</ref> presented a discriminative com- pression model, and Clarke and Lapata (2008) im- proved it by using ILP for decoding. Since our proposed method is based upon this model, in the following we briefly describe it first. Details can be found in <ref type="bibr" target="#b5">(Clarke and Lapata, 2008</ref>). In this model, the following score function is used to evaluate each compression candidate:</p><formula xml:id="formula_0">s(x, y) = |y| j=2 s(x, L(y j−1 ), L(y j ))<label>(1)</label></formula><p>where x = x 1 x 2 , ..., x n represents an original sen- tence and y = y 1 y 2 , ..., y m denotes a compressed sentence. Because the sentence compression prob- lem is defined as a word deletion task, y j must oc- cur in x. Function L(y i ) ∈ [1...n] maps word y i in the compression to the word index in the original sentence x. Note that L(y i ) &lt; L(y i+1 ) is required, that is, each word in x can only occur at most once in compression y. In this model, a first or- der Markov assumption is used for the score func- tion. Decoding this model is to find the combina- tion of bigrams that maximizes the score function in Eq (1). Clarke and Lapata (2008) introduced the following variables and used ILP to solve it:</p><formula xml:id="formula_1">δ i = 1 if x i is in the compression 0 otherwise ∀i ∈ [1..n] α i = 1 if x i starts the compression 0 otherwise ∀i ∈ [1.</formula><p>.n]</p><formula xml:id="formula_2">β i = 1 if x i ends the compression 0 otherwise ∀i ∈ [1..n] γ ij = 1 if x i , x j are in the compression 0 otherwise ∀i ∈ [1..n − 1]∀j ∈ [i + 1..n]</formula><p>Using these variables, the objective function can be defined as:</p><formula xml:id="formula_3">max z = n i=1 α i · s(x, 0, i) + n−1 i=1 n j=i+1 γ ij · s(x, i, j) + n i=1 β i · s(x, i, n + 1)<label>(2)</label></formula><p>The following four basic constraints are used to make the compressed result reasonable:</p><formula xml:id="formula_4">n i=1 α i = 1 (3) δ j − α j − j i=1 γ ij = 0 ∀j ∈ [1..n] (4) δ i − n j=i+1 γ ij − β i = 0 ∀i ∈ [1..n] (5) n i=1 β i = 1<label>(6)</label></formula><p>Formula <ref type="formula">(3)</ref> and <ref type="formula" target="#formula_4">(6)</ref> denote that exactly one word can begin or end a sentence. Formula (4) means if a word is in the compressed sentence, it must either start the compression or follow another word; formula (5) represents if a word is in the compressed sentence, it must either end the sen- tence or be followed by another word.</p><p>Furthermore, discriminative models are used for the score function:</p><formula xml:id="formula_5">s(x, y) = |y| j=2 w · f(x, L(y j−1 ), L(y j ))<label>(7)</label></formula><p>High dimensional features are used and their cor- responding weights are trained discriminatively. Above is the basic supervised ILP formula- tion for sentence compression. Linguistically and semantically motivated constraints can be added in the ILP model to ensure the correct grammar structure in the compressed sentence. For exam- ple, <ref type="bibr" target="#b5">Clarke and Lapata (2008)</ref> forced the introduc- ing term of prepositional phrases and subordinate clauses to be included in the compression if any word from within that syntactic constituent is also included, and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Compression Model based on Expanded Constituent Parse Tree</head><p>In the above ILP model, variables are defined for each word in the sentence, and the task is to pre- dict each word's status. In this paper, we propose to adopt the above ILP framework, but operate di- rectly on the nodes in the constituent parse tree, rather than just the words (leaf nodes in the tree). This way we can remove or retain a chunk of the sentence rather than isolated words, which we ex- pect can improve the readability and grammar cor- rectness of the compressed sentences. The top part of Fig1 is a standard constituent parse tree. For some levels of the tree, the nodes at that same level can not represent a sentence. We extend the parse tree by duplicating non-POS con- stituents so that leaf nodes (words and their corre- sponding POS tags) are aligned at the bottom level as shown in bottom of as Fig1. In the example tree, the solid lines represent relationship of nodes from the original parse tree, the long dot lines denote the extension of the duplication nodes from the up- per level to the lower level, and the nodes at the same level are connected (arrowed lines) to repre- sent that is a sequence. Based on this expanded constituent parse tree, we can consider every level as a 'sentence' and the tokens are POS tags and parse tree labels. We apply the above compression model in Section 3.1 on every level to decide every node's status in the final compressed sentence. In order to make the compressed parsed tree reason- able, we model the relationship of nodes between adjacent levels as following: if the parent node is labeled as removed, all of its children will be re- moved; one node will retain if at least one of its children is kept.</p><p>Therefore, the objective function in the new ILP formulation is:</p><formula xml:id="formula_6">max z = height l=1 ( n l i=1 α l i · s(x, 0, l i ) + n l −1 i=1 n l j=i+1 γ l ij · s(x, l i , l j ) + n l i=1 β l i · s(x, l i , n l + 1) ) (8)</formula><p>where height is the depth for a parse tree (starting from level 1 for the tree), and n l means the length of level l (for example, n 5 = 6 in the example in Fig1). Then every level will have a set of pa- rameters δ l i , α l i , β l i , and γ l ij , and the corresponding constraints as shown in Formula (3) to (6). The re- lationship between nodes from adjacent levels can be expressed as:</p><formula xml:id="formula_7">δ l i ≥ δ (l+1) j<label>(9)</label></formula><formula xml:id="formula_8">δ l i ≤ δ (l+1) j<label>(10)</label></formula><p>in which node j at level (l + 1) is the child of node i at level l. In addition, 1 ≤ l ≤ height − 1, 1 ≤ i ≤ n l and 1 ≤ j ≤ n l+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Linguistically Motivated Constraints</head><p>In our proposed model, we can jointly decide the status of every node in the constituent parse tree at the same time. One advantage is that we can add constraints based on internal nodes or rela- tionship in the parse tree, rather than only using the relationship based on words. In addition to the constraints proposed in <ref type="bibr" target="#b5">(Clarke and Lapata, 2008)</ref>, we introduce more linguistically motivated constraints to keep the compressed sentence more grammatically correct. The following describes the constraints we used based on the constituent parse tree.</p><p>• • If a node's label is 'SBAR', its parent's label is 'VP' and its first child's label is 'WHNP', then if we can find a verb in the left siblings of 'SBAR', this subordinate clause could be an objective clause. Therefore, the found verb node should be included in the compres- sion if the 'SBAR' node is also included, be- cause the node 'SBAR' is the object of that verb. An example is shown in the bottom part of <ref type="figure" target="#fig_2">Fig 2.</ref> The nodes in ellipse should share the same status.</p><p>• If a node's label is 'SBAR', its parent's label is 'VP' and its first child's label is 'WHADVP', then if the first leaf for this node is a wh-word (e.g., 'where, when, why') or 'how', this clause may be an objective clause (when the word is 'why, how, where') or at- tributive clause (when the word is 'where') or adverbial clause (when the word is 'when'). Therefore, similar to above, if a verb or noun is found in the left siblings of 'SBAR', the found verb or noun node should be included in the compression if the 'SBAR' node is also included.</p><p>• If a node's label is 'SBAR' and its parent's la- bel is 'ADJP', then if we can find a 'JJ', 'JJR', or 'JJS' in the left siblings of 'SBAR', the 'SBAR' node should be included in the com- pression if the found 'JJ', 'JJR' or 'JJS' node is also included because the node 'SBAR' is decorated by the adjective.</p><p>• The node with a label of 'PRN' can be re- moved without other constraints.</p><p>We also include some other constraints based on the Stanford dependency parse tree. <ref type="table" target="#tab_0">Table 1</ref> lists the dependency relations we considered.</p><p>• For type I relations, the parent and child node with those relationships should have the same value in the compressed result (both are kept or removed).</p><p>• For type II relations, if the child node in those relations is retained in the compressed sentence, the parent node should be also re- tained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dependency Relation</head><p>Example prt: phrase verb particle They shut down the station. prt(shut,down) prep: prepositional modifier He lives in a small village. prep(lives,in) I pobj: object of a preposition I sat on the chair. pobj(on,chair) nsubj: nominal subject</p><p>The boy is cute. nsubj(cute,boy) cop: copula Bill is big. cop(big,is) partmod: participial modifier Truffles picked during the spring are tasty. partmod(truffles,picked) II nn: noun compound modifier Oil price futures. nn(futures,oil) acomp: adjectival complement She looks very beautiful. acomp(looks,beautiful) pcomp: prepositional complement He felt sad after learning that tragedy. pcomp(after,learning) III ccomp: clausal complement I am certain that he did it. ccomp(certain,did) tmod: temporal modifier Last night I swam in the pool. tmod(swam,night) • For type III relations, if the parent node in these relations is retained, the child node should be kept as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Features</head><p>So far we have defined the decoding process and related constraints used in decoding. These all rely on the score function s(x, y) = w · f(x, L(y j−1 ), L(y j )) for every level in the con- stituent parse tree. We included all the features in- troduced in (Clarke and Lapata, 2008) (those fea- tures are designed for leaves). <ref type="table" target="#tab_2">Table 2</ref> lists the additional features we used in our system.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Features for Every Node</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Learning</head><p>To learn the feature weights during training, we perform ILP decoding on every sentence in the training set, to find the best hypothesis for each node in the expanded constituent parse tree. If the hypothesis is incorrect, we update the feature weights using the structured perceptron learning strategy <ref type="bibr" target="#b6">(Collins, 2002</ref>). The reference label for every node in the expanded constituent parse tree is obtained automatically from the bottom to the top of the tree. Since every leaf node (word) is human annotated (removed or retain), we annotate the internal nodes as removed if all of its children are removed. Otherwise, the node is annotated as retained.</p><p>During perceptron training, a fixed learning rate is used and parameters are averaged to prevent overfitting. In our experiment, we observe sta- ble convergence using the held-out development corpus, with best performance usually obtained around 10-20 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Summarization System</head><p>Similar to ( <ref type="bibr" target="#b17">Li et al., 2013a</ref>), our summarization system is , which consists of three key compo- nents: an initial sentence pre-selection module to select some important sentence candidates; the above compression model to generate n-best com- pressions for each sentence; and then an ILP sum- marization method to select the best summary sen- tences from the multiple compressed sentences.</p><p>The sentence pre-selection model is a simple su- pervised support vector regression (SVR) model that predicts a salience score for each sentence and selects the top ranked sentences for further pro- cessing (compression and summarization). The target value for each sentence during training is the ROUGE-2 score between the sentence and the human written abstracts. We use three common features: (1) sentence position in the document; (2) sentence length; and (3) interpolated n-gram document frequency as introduced in <ref type="bibr" target="#b27">(Ng et al., 2012)</ref>.</p><p>The final sentence selection process follows the ILP method introduced in ( <ref type="bibr" target="#b13">Gillick et al., 2009)</ref>. Word bi-grams are used as concepts, and their doc- ument frequency is used as weights. Since we use multiple compressions for one sentence, an addi- tional constraint is used: for each sentence, only one of its n-best compressions may be included in the summary. For the compression module, using the ILP method described above only finds the best com- pression result for a given sentence. To generate n-best compression candidates, we use an iterative approach -we add one more constraints to prevent it from generating the same answer every time af- ter getting one solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Summarization Data For summarization experi- ments, we use the standard TAC data sets <ref type="bibr">1</ref> , which have been used in the NIST competitions. In par- ticular, we used the TAC 2010 data set as train- ing data for the SVR sentence pre-selection model, TAC 2009 data set as development set for parame- ter tuning, and the TAC 2008 and 2011 data as the test set for reporting the final summarization re- sults. The training data for the sentence compres- sion module in the summarization system is sum- mary guided compression corpus annotated by <ref type="bibr" target="#b17">(Li et al., 2013a</ref>) using TAC2010 data. In the com- pression module, for each word we also used its document level feature. <ref type="bibr">2</ref> Compression Data We also evaluate our com- pression model using the data set from <ref type="bibr" target="#b5">(Clarke and Lapata, 2008)</ref>. It includes 82 newswire arti- cles with manually produced compression for each sentence. We use the same partitions as <ref type="bibr" target="#b24">(Martins and Smith, 2009)</ref>, i.e., 1,188 sentences for training and 441 for testing.</p><p>Data Processing We use Stanford CoreNLP toolkit 3 to tokenize the sentences, extract name en- tity tags, and generate the dependency parse tree. Berkeley Parser ( <ref type="bibr" target="#b30">Petrov et al., 2006</ref>) is adopted to obtain the constituent parse tree for every sen- tence and POS tag for every token. We use Pocket CRF 4 to implement the CRF sentence compres- sion model. SVMlight 5 is used for the summary sentence pre-selection model. Gurobi ILP solver 6 does all ILP decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Summarization Results</head><p>We compare our summarization system against four recent studies, which have reported some of the highest published results on this task. <ref type="bibr">BergKirkpatrick et al. (2011)</ref> introduced a joint model for sentence extraction and compression. Wood- send and Lapata (2012) learned individual sum- mary aspects from data, e.g., informativeness, suc- cinctness, grammaticalness, stylistic writing con- ventions, and jointly optimized the outcome in an ILP framework. <ref type="bibr" target="#b27">Ng et al. (2012)</ref> exploited category-specific information for multi-document summarization. <ref type="bibr" target="#b1">Almeida and Martins (2013)</ref> pro- posed compressive summarization method by dual decomposition and multi-task learning. Our sum- marization framework is the same as ( <ref type="bibr" target="#b17">Li et al., 2013a</ref>), except they used a CRF-based compres- sion model. In addition to the four previous stud- ies, we also report the best achieved results in the TAC competitions. <ref type="table" target="#tab_4">Table 3</ref> shows the summarization results of our method and others. The top part contains the re- sults for TAC 2008 data and bottom part is for TAC 2011 data. We use the ROUGE evaluation metrics <ref type="bibr" target="#b21">(Lin, 2004</ref>), with R-2 measuring the bi- gram overlap between the system and reference summaries and R-SU4 measuring the skip-bigram with the maximum gap length of 4. In addition, we evaluate the linguistic quality (LQ) of the sum- maries for our system and ( <ref type="bibr" target="#b17">Li et al., 2013a</ref>). <ref type="bibr">7</ref> The linguistic quality consists of two parts. One eval- uates the grammar quality within a sentence. For this, annotators marked if a compressed sentence is grammatically correct. Typical grammar errors include lack of verb or subordinate clause. The other evaluates the coherence between sentences, including the order of sentences and irrelevant sen- tences. We invited 3 English native speakers to do this evaluation. They gave every compressed sen- tence a grammar score and a coherence score for System R-2 R-SU4 Gram Cohere TAC'08 Best System 11.03 13.96 n/a n/a ( <ref type="bibr">Berg-Kirk et al., 2011) 11.70 14.38</ref> n/a n/a ( <ref type="bibr">Woodsend et al., 2012) 11.37 14.47</ref> n/a n/a ( <ref type="bibr" target="#b1">Almeida et al.,2013)</ref> 12.30 15.18 n/a n/a ( <ref type="bibr" target="#b17">Li et al., 2013a)</ref> 12. <ref type="bibr">35</ref>   each topic. The score is scaled and ranges from 1 (bad) to 5 (good). Therefore, in table 3, the gram- mar score is the average score for each sentence and coherence score is the average for each topic.</p><p>We measure annotators' agreement in the follow- ing way: we consider the scores from each anno- tator as a distribution and we find that these three distributions are not statistically significantly dif- ferent each other (p &gt; 0.05 based on paired t-test).</p><p>We can see from the table that in general, our system achieves better ROUGE results than most previous work except ( <ref type="bibr" target="#b17">Li et al., 2013a</ref>) on both TAC 2008 and TAC 2011 data. However, our system's linguistic quality is better than ( <ref type="bibr" target="#b17">Li et al., 2013a</ref>). The CRF-based compression model used in ( <ref type="bibr" target="#b17">Li et al., 2013a</ref>) can not well model the grammar. Particularly, our results (ROUGE-2) are statistically significantly (p &lt; 0.05) higher than TAC08 Best system, but are not statistically signif- icant compared with ( <ref type="bibr" target="#b17">Li et al., 2013a</ref>) (p &gt; 0.05). The pattern is similar in TAC 2011 data. Our result (R-2) is statistically significantly (p &lt; 0.05) better than TAC11 Best system, but not statistically (p &gt; 0.05) significantly different from ( <ref type="bibr" target="#b17">Li et al., 2013a</ref>). However, for the grammar and coherence score, our results are statistically significantly (p &lt; 0.05) than ( <ref type="bibr" target="#b17">Li et al., 2013a</ref>). All the above statistics are based on paired t-test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Compression Results</head><p>The results above show that our summarization system is competitive. In this section we focus on the evaluation of our proposed compression method. We compare our compression system against four other models. HedgeTrimmer in <ref type="bibr" target="#b8">Dorr et al. (2003)</ref> applied a variety of linguistically- motivated heuristics to guide the sentences com-</p><formula xml:id="formula_9">System C Rate (%) Uni-F1 Rel-F1</formula><p>HedgeTrimmer 57.64 0.64 0.50 <ref type="bibr" target="#b25">McDonald (2006)</ref> 70.95 0.77 0.55 <ref type="bibr" target="#b24">Martins (2009)</ref> 71.35 0.77 0.56 <ref type="bibr" target="#b36">Wang (2013)</ref> 68.06 0.79 0.59 Our System 71.19 0.77 0.58 <ref type="table">Table 4</ref>: Sentence compression results. The hu- man compression rate of the test set is 69%.</p><p>pression; <ref type="bibr" target="#b25">McDonald (2006)</ref> used the output of two parsers as features in a discriminative model that decomposes over pairs of consecutive words; Mar- tins and Smith (2009) built the compression model in the dependency parse and utilized the relation- ship between the head and modifier to preserve the grammar relationship; <ref type="bibr" target="#b36">Wang et al. (2013)</ref> devel- oped a novel beam search decoder using the tree- based compression model on the constituent parse tree, which could find the most probable compres- sion efficiently. <ref type="table">Table 4</ref> shows the compression results of vari- ous systems, along with the compression ratio (C Rate) of the system output. We adopt the com- pression metrics as used in <ref type="bibr" target="#b24">(Martins and Smith, 2009</ref>) that measures the macro F-measure for the retained unigrams (Uni-F1), and the one used in <ref type="bibr" target="#b5">(Clarke and Lapata, 2008</ref>) that calculates the F1 score of the grammatical relations labeled by <ref type="bibr" target="#b3">(Briscoe and Carroll, 2002</ref>) (Rel-F1). We can see that our proposed compression method performs well, similar to the state-of-the-art systems.</p><p>To evaluate the power of using the expanded parse tree in our model, we conducted another ex- periment where we only consider the bottom level of the constituent parse tree. In some sense, this could be considered as the system in <ref type="bibr" target="#b5">(Clarke and Lapata, 2008</ref>). Furthermore, we use two differ- ent setups: one uses the lexical features (about the words) and the other does not. <ref type="table">Table 5</ref> shows the results using the data in <ref type="bibr" target="#b5">(Clarke and Lapata, 2008)</ref>. For a comparison, we also include the results us- ing the CRF-based compression model (the one used in <ref type="bibr" target="#b28">(Nomoto, 2007;</ref><ref type="bibr" target="#b17">Li et al., 2013a)</ref>). We report results using both the automatically calcu- lated compression metrics and the linguistic qual- ity score. Three English native speaker annotators were asked to judge two aspects of the compressed sentence compared with the gold result: one is the content that looks at whether the important words are kept and the other is the grammar score which evaluates the sentence's readability. Each of these two scores ranges from 1(bad) to 5(good). <ref type="table">Table 5</ref> shows that when using lexical features, our system has statistically significantly (p &lt; 0.05) higher Grammar value and content importance value than the CRF and the leaves only system. When no lexical features are used, default system can achieve statistically significantly (p &lt; 0.01) higher results than the CRF and the leaves only system.</p><p>We can see that using the expanded parse tree performs better than using the leaves only, espe- cially when lexical features are not used. In ad- dition, we observe that our proposed compression method is more generalizable than the CRF-based model. When our system does not use lexical features in the leaves, it achieves better perfor- mance than the CRF-based model. This is impor- tant since such a model is more robust and may be used in multiple domains, whereas a model rely- ing on lexical information may suffer more from domain mismatch. From the table we can see our proposed tree based compression method consis- tently has better linguistic quality. On the other hand, the CRF compression model is the most computationally efficient one among these three compression methods. It is about 200 times faster than our model using the expanded parse tree. Ta- ble 6 shows some examples using different meth- ods. <ref type="bibr">System</ref>   <ref type="table">Table 5</ref>: Sentence compression results: effect of lexical features and expanded parse tree. ILP(I) represents the system using only bottom nodes in constituent parse tree. ILP(II) is our system. Imp means the content importance value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a discriminative ILP sen- tence compression model based on the expanded constituent parse tree, which aims to improve the linguistic quality of the compressed sentences in the summarization task. Linguistically motivated constraints are incorporated to improve the sen- tence quality. We conduct experiments on the TAC Using lexical features Source: Apart from drugs, detectives believe money is laun- dered from a variety of black market deals involving arms and high technology. Human compress: detectives believe money is laundered from a variety of black market deals. CRF result : Apart from drugs detectives believe money is laundered from a black market deals involving arms and technol- ogy. ILP(I) Result: detectives believe money is laundered from a variety of black deals involving arms. ILP(II) Result: detectives believe money is laundered from black mar- ket deals.  2008 and 2011 summarization data sets and show that by incorporating this sentence compression model, our summarization system can yield signif- icant performance gain in linguistic quality with- out losing much ROUGE results. The analysis of the compression module also demonstrates its competitiveness, in particular the better linguistic quality and less reliance on lexical cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No lexical features</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A regular constituent parse tree and its Expanded constituent tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>If a node's label is 'SBAR', its parent's label is 'NP' and its first child's label is 'WHNP' or 'WHPP' or 'IN', then if we can find a noun in the left siblings of 'SBAR', this subordi- nate clause could be an attributive clause or appositive clause. Therefore the found noun node should be included in the compression if the 'SBAR' is also included, because the node 'SBAR' decorates the noun. For exam- ple, the top part of Fig 2 is part of expanded constituent parse tree of sentence 'Those who knew David were all dead.' The nodes in el- lipse should share the same status.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Expanded constituent parse tree for examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Some dependency relations used for extra constraints. All the examples are from (Marneffe and 
Manning, 2002) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Features used in our system besides those 
used in (Clarke and Lapata, 2008). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Summarization results on the TAC 2008 
and 2011 data sets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Examples of original sentences and their 
compressed sentences from different systems. 

</table></figure>

			<note place="foot" n="1"> http://www.nist.gov/tac/data/index.html 2 Document level features for a word include information such as the word&apos;s document frequency in a topic. These features cannot be extracted from a single sentence, as in the standard sentence compression task, and are related to the document summarization task. 3 http://nlp.stanford.edu/software/corenlp.shtml</note>

			<note place="foot" n="4"> http://sourceforge.net/projects/pocket-crf-1/ 5 http://svmlight.joachims.org/ 6 http://www.gurobi.com 7 We chose to evaluate the linguistic quality for this system because of two reasons: one is that we have an implementation of that method; the other more important one is that it has the highest reported ROUGE results among the compared methods.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their de-tailed and insightful comments on earlier drafts of this paper. The work is also partially sup-ported by NSF award IIS-0845484 and DARPA Contract No. FA8750-13-2-0041. Any opinions, findings, and conclusions or recommendations ex-pressed are those of the authors and do not neces-sarily reflect the views of the funding agencies.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-document summarization using a* search and discriminative training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Aker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast and robust compressive summarization with dual decomposition and multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">B</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><forename type="middle">F T</forename><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Jointly learning to extract and compress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust accurate statistical annotation of general text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the effectiveness of using sentence compression models for query-focused multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yllias</forename><surname>Chali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Global inference for sentence compression an integer linear programming approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Practical structured learning techniques for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>University of Southern California</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hedge trimmer: A parse-and-trim approach to headline generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Overcoming the lack of parallel data in sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An extractive supervised two-stage method for sentence compression</title>
	</analytic>
	<monogr>
		<title level="m">Dimitrios Galanis and Ion Androutsopoulos</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Proceedings of NAACL</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lexicalized markov grammars for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Processings of NAACL</title>
		<meeting>essings of NAACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A skip-chain conditional random field for ranking meeting utterances by importance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The icsi/utd summarization system at tac</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Benoit Favre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berndt</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shasha</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TAC</title>
		<meeting>TAC</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Statisticsbased summarization-step one: Sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Summarization beyond sentence extraction: A probabilistic approach to sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="107" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A trainable document summarizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Kupiec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francine</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Document summarization via guided sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP</title>
		<meeting>the EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using supervised bigram-based ilp for extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-document summarization via budgeted maximization of submodular functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving summarization performance by sentence compression-A pilot study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the Sixth International Workshop on Information Retrieval with Asian Language</title>
		<meeting>eeding of the Sixth International Workshop on Information Retrieval with Asian Language</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rouge: a package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards abstractive speech summarization: Exploring unsupervised and supervised approaches for spoken utterance compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Stanford typed dependencies manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Summarization with a joint model for sentence extraction and compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Andre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Integer Linear Programming for Natural Language Processing</title>
		<meeting>the ACL Workshop on Integer Linear Programming for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Discriminative sentence compression with soft syntactic evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Automatic summarization. Foundations and Trends in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploiting category-specific information for multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Ping</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Bysani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew-Lim</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Discriminative sentence compression with conditional random fields. Information Processing and Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadashi</forename><surname>Nomoto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Using maximum entropy for sentence extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 Workshop on Automatic Summarization</title>
		<meeting>the ACL-02 Workshop on Automatic Summarization</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning accurate, compact, and interpretable tree annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING-ACL</title>
		<meeting>COLING-ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast joint compression and summarization via graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Syntactic simplification for improving content selection in multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Advaith</forename><surname>Siddharthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Coling</title>
		<meeting>Coling</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sentence compression with joint structural inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kapil</forename><surname>Thadani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Supervised and unsupervised learning for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenine</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Beyond sumbasic: Taskfocused summarization with sentence simplification and lexical expansion. Information Processing &amp; Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisami</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A sentence compression based framework to query-focused multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hema</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Castelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Radu Florian, and Claire Cardie</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automatic generation of story highlights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Woodsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multiple aspect summarization using integer linear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Woodsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sentence compression with semantic role constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsumasa</forename><surname>Yoshikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-candidate reduction: Sentence compression as a tool for document summarization tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">J</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Processing and Management</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
