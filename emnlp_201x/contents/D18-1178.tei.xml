<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transfer and Multi-Task Learning for Noun-Noun Compound Interpretation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murhaf</forename><surname>Fares</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">University of Oslo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Oepen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">University of Oslo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Velldal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">University of Oslo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Transfer and Multi-Task Learning for Noun-Noun Compound Interpretation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1488" to="1498"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1488</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we empirically evaluate the utility of transfer and multi-task learning on a challenging semantic classification task: semantic interpretation of noun-noun compounds. Through a comprehensive series of experiments and in-depth error analysis, we show that transfer learning via parameter ini-tialization and multi-task learning via parameter sharing can help a neural classification model generalize over a highly skewed distribution of relations. Further, we demonstrate how dual annotation with two distinct sets of relations over the same set of compounds can be exploited to improve the overall accuracy of a neural classifier and its F 1 scores on the less frequent, but more difficult relations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Noun-noun compound interpretation is the task of assigning semantic relations to pairs of nouns (or more generally, pairs of noun phrases in the case of multi-word compounds). For exam- ple, given the nominal compound street protest, the task of compound interpretation is to pre- dict the semantic relation holding between street and protest (a locative relation in this example). Given the frequency of noun-noun compounds in natural language -e.g. 3% of the tokens in the British National Corpus <ref type="bibr" target="#b1">(Burnard, 2000</ref>) are part of noun-noun compounds <ref type="bibr">( ´ O Séaghdha, 2008</ref>) - and its relevance to other natural language pro- cessing (NLP) tasks such as question answering and information retrieval <ref type="bibr" target="#b28">(Nakov, 2008)</ref>, noun- noun compound interpretation has been the focus of much work, in theoretical linguistics <ref type="bibr" target="#b23">(Li, 1972;</ref><ref type="bibr" target="#b7">Downing, 1977;</ref><ref type="bibr" target="#b21">Levi, 1978;</ref><ref type="bibr" target="#b10">Finin, 1980;</ref><ref type="bibr" target="#b39">Ryder, 1994)</ref>, psycholinguistics <ref type="bibr" target="#b11">(Gagné and Shoben, 1997;</ref><ref type="bibr" target="#b24">Marelli et al., 2017)</ref>, and computational lin- guistics <ref type="bibr" target="#b20">(Lauer, 1995;</ref><ref type="bibr" target="#b29">Nakov, 2007;</ref><ref type="bibr" target="#b32">´ O Séaghdha and Copestake, 2009;</ref><ref type="bibr" target="#b12">Girju et al., 2009;</ref><ref type="bibr" target="#b17">Kim and Baldwin, 2013;</ref><ref type="bibr" target="#b6">Dima and Hinrichs, 2015)</ref>. In computational linguistics, noun-noun compound interpretation is, by and large, approached as an automatic classification problem. Hence several machine learning (ML) algorithms and models have been used to learn the semantics of nominal compounds, including Maximum Entropy ( <ref type="bibr" target="#b42">Tratz and Hovy, 2010)</ref>, Support Vector Machines <ref type="bibr" target="#b33">( ´ O Séaghdha and Copestake, 2013)</ref> and Neural Net- works ( <ref type="bibr" target="#b6">Dima and Hinrichs, 2015;</ref><ref type="bibr" target="#b43">Vered and Waterson, 2018</ref>). These models use information from lexical semantics such as WordNet-based features and distributional semantics such as word embed- dings. Nonetheless, noun-noun compound inter- pretation remains one of the more difficult NLP problems because: 1) noun-noun compounding, as a linguistic construction, is very productive and 2) the semantics of noun-noun compounds is not easily derivable from the compounds' constituents ( <ref type="bibr" target="#b43">Vered and Waterson, 2018)</ref>. Our work, in part, contributes to advancing NLP research on noun- noun compound interpretation through the use of transfer and multi-task learning.</p><p>The interest in using transfer learning (TL) and multi-task learning (MTL) in NLP has surged over the past few years, showing 'mixed' results depending on the so-called main and auxiliary tasks involved, model architectures and datasets, among other things <ref type="bibr" target="#b4">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b27">Mou et al., 2016;</ref><ref type="bibr" target="#b41">Søgaard and Goldberg, 2016;</ref><ref type="bibr" target="#b25">Martínez Alonso and Plank, 2017;</ref><ref type="bibr" target="#b0">Bingel and Søgaard, 2017)</ref>. These 'mixed' results, coupled with the fact that neither TL nor MTL has been applied to noun-noun compounds interpretation before, motivate our extensive empirical study on the use of TL and MTL for compound interpreta- tion, not only to supplement existing research on the utility of TL and MTL for semantic NLP tasks in general, but also to determine their benefits for compound interpretation in particular.</p><p>One of the primary motivations for using multi- task learning is to improve generalization by "leveraging the domain-specific information con- tained in the training signals of related tasks" <ref type="bibr" target="#b2">Caruana (1997)</ref>. In this work, we show that TL and MTL can indeed be used as a kind of reg- ularizer to learn to predict infrequent relations given a highly skewed distribution of relations from the noun-noun compound dataset of Fares (2016) which is especially well suited for TL and MTL experimentation as detailed in Section 3.</p><p>Our contributions can be summarized as:</p><p>1. Through careful result analysis, we find that TL and MTL (mainly on the embedding layer) do improve the overall accuracy and the F 1 scores of the less frequent relations in a highly skewed dataset, in comparison to a strong single-task learning baseline.</p><p>2. Even though our work focuses on TL and MTL, to the best of our knowledge, we are the first to report experimental results on the comparatively recent dataset of Fares (2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Noun-Noun Compound Interpretation Exist- ing approaches to noun-noun compound interpre- tation vary depending on the taxonomy of com- pound relations as well as the machine learn- ing models and features used to learn those rela- tions. For example, ´ O Séaghdha (2007) defines a coarse-grained set of relations (viz. six rela- tions based on theoretical work by <ref type="bibr" target="#b21">Levi (1978)</ref>), whereas Tratz and Hovy (2010) assume a con- siderably more fine-grained taxonomy of 43 rela- tions. Others question the very assumption that noun-noun compounds are interpretable using a finite, predefined set of relations <ref type="bibr" target="#b7">(Downing, 1977;</ref><ref type="bibr" target="#b10">Finin, 1980)</ref> and propose alternative paraphrasing- based approaches <ref type="bibr" target="#b29">(Nakov, 2007;</ref><ref type="bibr" target="#b40">Shwartz and Dagan, 2018)</ref>. We here focus on the approaches that cast the interpretation problem as a classi- fication task over a finite predefined set of rela- tions. A wide variety of machine learning models have been already applied to learn this task, in- cluding nearest neighbor classifiers using seman- tic similarity based on lexical resources <ref type="bibr" target="#b16">(Kim and Baldwin, 2005</ref>), kernel-based methods like SVMs using lexical and relational features <ref type="bibr" target="#b32">( ´ O Séaghdha and Copestake, 2009)</ref>, Maximum Entropy models with a relatively large selection of lexical and sur- face form features such as synonyms and affixes <ref type="bibr" target="#b42">(Tratz and Hovy, 2010)</ref> and, most recently, neu- ral networks either solely relying on word embed- dings to represent noun-noun compounds <ref type="bibr" target="#b6">(Dima and Hinrichs, 2015)</ref> or word embeddings and so- called path embeddings (which encode informa- tion about lemmas and part-of-speech tags, inter alia) in a combined paraphrasing and classifica- tion approach <ref type="bibr" target="#b43">(Vered and Waterson, 2018)</ref>. Of the aforementioned studies, <ref type="bibr" target="#b42">Tratz and Hovy (2010)</ref>; <ref type="bibr" target="#b6">Dima and Hinrichs (2015)</ref>; <ref type="bibr" target="#b43">Vered and Waterson (2018)</ref> have all used the same dataset by <ref type="bibr" target="#b42">Tratz and Hovy (2010)</ref>. To the best of our knowledge, TL and MTL have never been applied to com- pound interpretation before, and in the following we therefore review some of the previous work on TL and MTL on other NLP tasks.</p><p>Transfer and Multi-Task Learning A number of recent studies have presented comprehensive experiments on the use of TL and MTL for a vari- ety of NLP tasks including named entity recogni- tion and semantic labeling <ref type="bibr" target="#b25">(Martínez Alonso and Plank, 2017)</ref>, sentence-level sentiment classifica- tion ( <ref type="bibr" target="#b27">Mou et al., 2016)</ref>, super-tagging and chunk- ing ( <ref type="bibr" target="#b0">Bingel and Søgaard, 2017)</ref> and semantic de- pendency parsing ( <ref type="bibr" target="#b36">Peng et al., 2017</ref>). The com- mon thread among the findings of these stud- ies is that the benefits of TL and MTL largely depend on the properties of the tasks at hand, such as the skewedness of the data distribu- tion <ref type="bibr" target="#b25">(Martínez Alonso and Plank, 2017)</ref>, the se- mantic similarity between the source and target tasks ( <ref type="bibr" target="#b27">Mou et al., 2016)</ref>, the learning pattern of the auxiliary and main tasks where "target tasks that quickly plateau" benefit most from "non- plateauing auxiliary tasks" <ref type="bibr" target="#b0">(Bingel and Søgaard, 2017)</ref> and the "structural similarity" between the tasks ( <ref type="bibr" target="#b36">Peng et al., 2017)</ref>. In addition to the dif- ference in the NLP tasks they experiment with, the aforementioned studies assume slightly dif- ferent definitions of TL and MTL (cf. Section 4). Our work is similar in spirit to that of <ref type="bibr" target="#b36">Peng et al. (2017)</ref> in the sense that we use TL and MTL to learn different 'formalisms' (semantic annotations of noun-noun compounds in our case) on the same dataset. However, our experimental setup is more similar to the work by <ref type="bibr" target="#b27">Mou et al. (2016)</ref> in that we experiment with parameter initialization on all the layers of the neural model and simultaneously train one MTL model on two sets of relations (cf. Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Definition and Dataset</head><p>Given a set of labeled pairs of nouns, each a noun- noun compound, the task is simply to learn to classify the semantic relations holding between each pair of compound constituents. The diffi- culty of this task, obviously, depends on the label set used and its distribution, among other things. For all the experiments presented in this paper, we adapt the noun-noun compounds dataset cre- ated by <ref type="bibr" target="#b8">Fares (2016)</ref> which consists of compounds annotated with two different taxonomies of rela- tions; in other words, for each noun-noun com- pound there are two distinct relations, drawing on different linguistic schools. The dataset was de- rived from existing linguistic resources, such as NomBank ( <ref type="bibr" target="#b26">Meyers et al., 2004</ref>) and the Prague Czech-English Dependency Treebank 2.0 ( <ref type="bibr">Hajič et al., 2012, PCEDT)</ref>. Our motivation for using this dataset is twofold: first, dual annotation with rela- tions over the same underlying set of compounds maximally enables TL and MTL perspectives; sec- ond, alignment of two distinct annotation frame- works over the same data facilitates contrastive analysis and comparison across frameworks.</p><p>More specifically, we use a subset of the dataset created by <ref type="bibr" target="#b8">Fares (2016)</ref>, by focusing on type-based instances of so-called two-word com- pounds. <ref type="bibr">1</ref> The original dataset by <ref type="bibr" target="#b8">Fares (2016)</ref> also includes multi-word compounds (i.e. com- pounds consisting of more than two nouns) and more than just one instance per compound type. Furthermore, we define a three-way split of the dataset; <ref type="table">Table 1</ref> presents the number of compound types per split and the vocabulary size of each split (i.e. the number of unique words in each split); the latter is also broken down in terms of words occurring in the right-most position (right constituents) and the left-most position (left con- stituents). <ref type="bibr">2</ref> Overall, the two label sets consists of 35 so-called PCEDT functors and 18 NomBank argument and adjunct relations. As detailed in Section 7.1, these label sets are far from being uni- formly distributed.</p><p>Abstractly, many relations in PCEDT and 1 Two-word compounds consist of two whitespace- separated constituents. A single constituent, however, can be a 'simple' noun (e.g. system) or a hyphenated noun (e.g. land-ownership) leading to compounds like land-ownership system. The representation of compounds with hyphenated constituents is explained in Section 5.1 <ref type="bibr">2</ref> We use the terms left and right constituents, instead of modifier and head nouns, because the dataset does not make explicit the notion of 'headedness'. NomBank describe similar semantic concepts, since they annotate the semantics of the same text. For example, <ref type="bibr" target="#b8">Fares (2016)</ref> reports that the temporal and locative relations in NomBank (ARGM-TMP and ARGM-LOC, respectively) and their counterparts in PCEDT (TWHEN and LOC) exhibit a relatively consistent behavior across frameworks as they annotate many of the same compounds. However, Fares (2016) also points out that some abstractly similar relations do not align well in practice; for example, the func- tor AIM in PCEDT and the modifier argument ARGM-PNC in NomBank express a somewhat similar semantic concept (purpose) but the over- lap between the sets of compounds they annotate in practice is rather small. Nonetheless, it is plau- sible to assume that the semantic similarity in the label sets-whenever it exists-can be exploited in the form of transfer and multi-task learning, not least because the overall distribution of the rela- tions in the two frameworks is different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Transfer vs. Multi-Task Learning</head><p>In this section, we use the notations and definitions by <ref type="bibr" target="#b34">Pan and Yang (2010)</ref> to define our setup for transfer and multi-task learning.</p><p>Our classification task T can be defined in terms of all training pairs (X, Y ) and a probability dis- tribution P (X), where X = x i , . . . , x N ∈ X and Y = y i , . . . , y N ∈ Y; X is the input feature space, Y is the set of all labels and N is the size of the training data. A task's domain D is defined by {X , P (X)}. Our goal is to learn a function f (X) that predicts Y based on the input features X. As- suming two ML tasks, T a and T b , we would train two models (i.e. learn two separate functions f a and f b ) to predict Y a and Y b in a single-task learn- ing setup. However, if T a and T b are related some- how, either explicitly or implicitly, TL and MTL can improve the generalization of either task or both <ref type="bibr" target="#b2">(Caruana, 1997;</ref><ref type="bibr" target="#b34">Pan and Yang, 2010;</ref><ref type="bibr" target="#b27">Mou et al., 2016)</ref>. Two tasks are considered related when their domains, D a and D b , are similar but their label sets are different Y a = Y b or when their domains are different but their label sets are iden- tical, i.e. Y a = Y b ( <ref type="bibr" target="#b34">Pan and Yang, 2010)</ref>. <ref type="bibr">3</ref> As such, noun-noun compound interpretation over the dataset of Fares <ref type="formula">(2016)</ref> is a well suited candi- date for TL and MTL, because the training exam- ples are identical, i.e. X P CEDT = X N omBank , but the label sets are different Y P CEDT = Y N omBank .</p><p>For the sake of clarity, we distinguish between transfer learning and multi-task learning in this paper, even though these two terms are at times used somewhat interchangeably in the literature.</p><p>For example, what we call TL and MTL in this paper are both referred to as transfer learning by <ref type="bibr" target="#b27">Mou et al. (2016)</ref>. We define TL as using the pa- rameters (i.e. weights in neural networks) of one model trained on T a to initialize another model for T b . <ref type="bibr" target="#b27">Mou et al. (2016)</ref> refer to this method as "pa- rameter initialization". <ref type="bibr">4</ref> MTL, on the other hand, here refers to training (parts of) the same model to learn T a and T b , i.e. learning one set of param- eters for both tasks. The idea is to train a sin- gle model simultaneously on the two tasks where one task is considered to introduce inductive bias which would help the model generalize over the main task. Note, however, that this does not nec- essarily mean that we eventually want to use a sin- gle model to predict both label sets in practice (cf. Section 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Neural Classification Models</head><p>Here we present the neural classification models used in our experiments. To isolate the effect of TL and MTL, we first present a single-task learn- ing model, which serves as our baseline model, and then we use the same model to apply TL and MTL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Single-Task Learning Model</head><p>In our single-task learning (STL) setup, we train and fine-tune a feed-forward neural network based on the neural classifier proposed by <ref type="bibr" target="#b6">Dima and Hinrichs (2015)</ref>, which consists of: 1) input layer, 2) embedding layer, 3) hidden layer, and 4) output layer. The input layer is simply two integers spec- ifying the indices of a compound's constituents in the embedding layer where the word embedding vectors are stored; the selected word embedding vectors are then fed to a fully connected hidden layer whose size is the same as the number of di- mensions of the word embedding vectors. Finally, a softmax function is applied on the output layer and the most likely relation is selected.</p><p>The compound's constituents are represented using a 300-dimensional word embedding model trained on an English Wikipedia dump (dated February 2017) and English Gigaword Fifth Edi- tion (Parker et al., 2011) using GloVe ( <ref type="bibr" target="#b37">Pennington et al., 2014</ref>). The embedding model was trained by <ref type="bibr" target="#b9">Fares et al. (2017)</ref> who provide more details on the hyperparameters used to train the embedding model. <ref type="bibr">5</ref> When looking up a word in the embed- ding model, if the word is not found we check if the word is uppercased and look up the same word in lowercase. If a word is hyphenated and is not found in the embedding vocabulary, we split it on the hyphen and average the vectors of its parts (if they exist in the vocabulary). If after these steps the word is still not found, we use a designated vector for unknown words.</p><p>Architecture and Hyperparameters Our choice of hyperparameters is motivated by several rounds of experimentation on the single-task learning model as well as the choices made by <ref type="bibr" target="#b6">Dima and Hinrichs (2015)</ref>.</p><p>The weights of the embedding layer (i.e. the word embeddings) are updated during training in all the models. The optimization function we use in all the models is Adaptive Moment Estimation, known as Adam ( <ref type="bibr" target="#b18">Kingma and Ba, 2015</ref>) with η = 0.001 (the default learning rate). The loss function is negative-log likelihood (aka categorical cross- entropy). We use a Sigmoid activation function on the hidden layer units. All the models are trained using mini-batches of size five. The maximum number of epochs is set to 50, but we also use an early stopping criterion on the model's accuracy on the validation split (i.e. training is interrupted if the validation accuracy doesn't improve over five consecutive epochs). We implement all the mod- els in Keras with TensforFlow as backend. All the TL and MTL models are trained with the same hy- perparameters of the STL model. <ref type="bibr">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Transfer Learning Models</head><p>Transfer learning in our experiments amounts to training an STL model on PCEDT relations, for example, and then using (some of) its weights to initialize another model for NomBank relations. Given the architecture of the neural classifier <ref type="bibr">described</ref>  <ref type="table" target="#tab_2">Table 2</ref>. Note that we do not apply TL (or MTL) on the output layer because it is task-or dataset-specific (Mou et al., 2016).</p><note type="other">in Section 5.1, we identify three ways to implement TL: 1) TL E : Transfer of the embed- ding layer weights, 2) TL H : Transfer of the hid- den layer weights, and 3) TL EH : Transfer of both the embedding and hidden layer weights. Fur- thermore, we distinguish between transfer learn- ing from PCEDT to NomBank and vice versa; that is, either task can be used as main task or auxil- iary task. Hence, we either start by training on NomBank and use the weights of the correspond- ing transfer layer to initialize the PCEDT model or the other way around. In total, this leads to six setups, as shown in</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Multi-Task Learning Models</head><p>In MTL, we train one model simultaneously to learn both PCEDT and NomBank relations, and therefore all the MTL models have two objec- tive functions and two output layers. We im- plement two MTL setups: MTL E , which con- sists of a shared embedding layer but two task- specific hidden layers, and MTL F , which, apart from the output layer, does not have task-specific layers, i.e. both the embedding and hidden layers are shared. We distinguish between the auxiliary and main tasks based on which validation accu- racy (NomBank's or PCEDT's) is monitored by the early stopping criterion. Hence we end up with a total of four MTL models as shown in <ref type="table" target="#tab_3">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head><p>Tables 2 and 3 present the accuracies of the dif- ferent TL and MTL models on the development and test splits in NomBank and PCEDT. The top row in both tables shows the accuracy of the STL model. All the models were trained on the training split only. There are several observations one can draw from these tables. First, the accuracy of the STL models drops when the models are evaluated on the test split, whether on NomBank or PCEDT. Second, all the TL models achieve better accuracy on the test split of NomBank even though transfer learning does not remarkably improve accuracy on   Overall, the accuracy of the STL models drops when evaluated on the test split of NomBank and PCEDT (in comparison to their accuracy on the development split); this might be an indicator of overfitting, especially because we select the model that performs best on the development split in our stopping criterion. Both TL and MTL, on the other hand, improve accuracy on the test splits, even though the same stopping criterion was used for STL, TL and MLT. We interpret this result as im- provement in the models' generalization ability. However, given that these improvements are rel- atively small, we next take a closer look at the re- sults to understand if and how TL and MTL help.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results Analysis</head><p>This section presents a systematic analysis of the performance of the models based on insights from the dataset used in the experiments as well as the classification errors of the models. The discussion in the following sections is based on the results on the test split rather than the development split, primarily because the former is larger in size. <ref type="bibr">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A0</head><p>A1  <ref type="table">Table 4</ref>: Per-label F 1 score on the NomBank test split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Relation Distribution</head><p>To demonstrate the difficulty of the problem at hand, we plot the distribution of the most frequent relations in NomBank and PCEDT across the three data splits in <ref type="figure" target="#fig_0">Figure 1</ref>. We find that almost 71.18% of the relations in the NomBank training split are of type ARG1 (proto-typical patient), and 52.20% of the PCEDT relations are of type RSTR (an un- derspecified adnominal modifier). Such highly skewed distributions of the relations makes learn- ing some of the other relations more difficult, if not impossible in some cases. In fact, of the 15 NomBank relations observed in the test split, five relations are never predicted by any of the STL, TL and MTL models, and of the 26 PCEDT re- lations observed in the test split only six are pre- dicted. That said, the non-predicted relations are extremely infrequent in the training split (e.g. 23 PCEDT functors occur less than 20 times in the training split), and it is therefore questionable if an ML model will be able to learn them under any circumstances.</p><p>From this imbalanced distribution of relations, it immediately follows that accuracy alone, as evaluation measure, is not sufficient to identify the best performing model. Therefore, in the follow- ing section we report, and analyze, the F 1 scores of the predicted NomBank and PCEDT relations across all the STL, TL and MTL models. <ref type="table" target="#tab_6">Tables 4 and 5</ref> show the per-relation F 1 scores for NomBank and PCEDT, respectively. Note that we only include the results for the relations that are actually predicted by at least one of the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Per-Relation F 1 Scores</head><p>We observe several interesting patterns in Ta- bles 4 and 5. First, the MTL F model seems to be confusing for both datasets: it leads to substan- ping criterion based on the validation data can help prevent overfitting on the training data, we still choose a model that achieves the best accuracy on the validation split. In addition, it's unclear if early stopping helps when the validation split is not fully representative of the problem <ref type="bibr" target="#b38">(Prechelt, 2012)</ref>.  tially degraded F 1 scores on four NomBank rela- tions, including the locative modifier ARGM-LOC and manner modifier ARGM-MNR (shortened to LOC and MNR in <ref type="table">Table 4</ref>) which the model is no longer able to predict. The same model has the worst F 1 score, compared to all other models, for two PCEDT relations, REG (which expresses a circumstance) and PAT (patient). Given that the MTL F model achieves the highest accuracy on the NomBank test split (cf. <ref type="table" target="#tab_3">Table 3</ref>), it becomes all the more evident that mere accuracy scores are not enough to judge the utility of TL and MTL for this task (and dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACT TWHEN APP PAT REG RSTR</head><p>Second, with the exception of the MTL F model, all the TL and MTL models consistently improve the F 1 score of all the PCEDT relations except PAT. Most notably, the F 1 scores of the relations TWHEN and ACT see a remarkable boost, com- pared to other PCEDT relations, when only the embedding layer's weights are shared (MTL E ) or transfered (TL E ). This result can be partly ex- plained by looking at the correspondence matrices between NomBank arguments and PCEDT func- tors shown in <ref type="table" target="#tab_7">Tables 7 and 6</ref>, which show how the PCEDT functors map to NomBank arguments in the training split <ref type="table">(Table 6</ref>) and the other way around <ref type="table" target="#tab_7">(Table 7)</ref>. From <ref type="table">Table 6</ref>, we see that 80% of the compounds annotated as TWHEN in PCEDT were annotated as ARGM-TMP in NomBank. In addition, 47% of ACT (Actor) relations map to ARG0 (Proto-Agent) in NomBank, even though this mapping is not as clear as one would have hoped, it is still relatively high if we consider how other PCEDT relations map to ARG0. The cor- respondence matrices also show that the assumed theoretical similarities between the NomBank and PCEDT relations do not always hold. Nonetheless, even such 'imperfect' correspondence can provide a 'training signal' that help the TL and MTL mod- els learn relations such as TWHEN and ACT.</p><p>Since the TL E model outperforms STL on pre- dicting REG by ten absolute points, we inspected all the REG compounds that were correctly classi- fied by the TL E model but were misclassified by the STL model and found that the latter misclassi- fied them as RSTR which indicates that TL from NomBank helps the TL E model recover from the STL's over-generalization in RSTR prediction.</p><p>The two NomBank relations that receive the highest boost in F 1 score (about five absolute points) are ARG0 and ARGM-MNR, but the im- provement in the latter relation corresponds to only one more compound which might well be predicted correctly by chance. Overall, TL and MTL from NomBank to PCEDT is more helpful than the other way around. One way to explain this is considering the first rows in <ref type="table" target="#tab_7">Tables 6 and  7</ref>, where we see that five PCEDT relations (includ- ing the four most frequent ones) map to ARG1 in NomBank in more than 60% of the cases for each relation. This means that the weights learned to predict PCEDT relations offer little or no inductive bias for NomBank relations. Whereas if we con- sider the mapping from NomBank to PCEDT, we see that even though many NomBank arguments map to RSTR in PCEDT the percentages are lower, and hence the mapping is more 'diverse' (i.e. dis- criminative) which seems to help the TL and MTL models learn the less frequent PCEDT relations.</p><p>For completeness, we investigate why the PCEDT functor AIM is never predicted even though it is more frequent than TWHEN (cf. <ref type="figure" target="#fig_0">Figure 1</ref>). We find that AIM is almost always mis- classifed as RSTR by all the models. Furthermore, we discover that AIM and RSTR have the highest lexical overlap in the training set among all other pairs of relations in PCEDT: 78.35% of the left constituents and 73.26% of the right constituents of the compounds annotated as AIM occur in other  <ref type="table">Table 6</ref>: Correspondence matrix between PCEDT functors and NomBank arguments. Slots with '-' mean zero, 0.00 is a very small number but not zero.  compounds annotated as RSTR. This explains why none of the models manage to learn the relation AIM but raises a question about the models' abil- ity to learn relational representations; we pursue this question further in Section 7.3.</p><p>Finally, to clearly demonstrate the benefits of TL and MTL for NomBank and PCEDT, we re- port the F 1 macro-average scores in <ref type="table" target="#tab_8">Table 8</ref> (which is arguably the appropriate evaluation measure for imbalanced classification problems). Note that the relations that are not predicted by any of the models are not included in computing the macro- average. From  8 absolute points increase in macro-average F 1 , in contrast to just 0.65 in the best case on NomBank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Generalization on Unseen Compounds</head><p>Now we turn to analyze the models' ability to generalize over compounds unseen in the training split. Recent work by <ref type="bibr" target="#b5">Dima (2016)</ref> and <ref type="bibr" target="#b43">Vered and Waterson (2018)</ref> suggest that the gains achieved in noun-noun compound interpretation using word embeddings and somewhat similar neural classi- fication models are in fact a by-product of lexical memorization ( <ref type="bibr" target="#b22">Levy et al., 2015)</ref>; in other words, the classification models learn that a specific set of nouns is a strong indicator of a specific rela- tion. Therefore, in order to gauge the role of lexi- cal memorization in our models also, we quantify the number of unseen compounds that the STL, TL and MTL models predict correctly.</p><p>We distinguish between 'partly' and 'com- pletely' unseen compounds. A compound is con- sidered 'partly' unseen if one of its constituents (right or left) is not seen in the training data at all. A completely unseen compound is one whose left and right constituent are not seen in the training data (i.e. completely unseen compounds are the subset of compounds in the test split that have zero lexical overlap with the training split). Overall, al- most 20% of the compounds in the test split have an unseen left constituent, about 16% of the com- pounds have unseen right constituent and 4% are completely unseen. In <ref type="table" target="#tab_11">Table 9</ref>, we compare the performance of the different models on these three groups in terms of the proportion of compounds a model misclassifies in each group.</p><p>From <ref type="table" target="#tab_11">Table 9</ref> we see that TL and MTL reduce the NomBank generalization error in all cases, ex- cept TL H and TL EH on completely unseen com- pounds; the latter leads to higher generalization error. The MTL models lead to the biggest er- ror reduction across the three types of unseen compounds; MTL E leads to about six points er- ror reduction on compounds with unseen right constituent and eleven points on completely un-  seen ones, and MTL F reduces the error on un- seen left constituent by five points. Note, how- ever, that these results have to be read together with the Count row in <ref type="table" target="#tab_11">Table 9</ref> to get a complete picture. For instance, an eleven-point decrease in error on completely unseen compounds amounts to eight compounds. In PCEDT, the largest error reduction on unseen left constituents is 1.14 points which amounts to four compounds, 0.35 (just one compound) on unseen right constituents and 2.7 (or two compounds) on completely unseen com- pounds.</p><p>Since we see larger reductions in the general- ization error in NomBank, we manually inspect the compounds that led to these reductions; i.e. we inspect the distribution of relations in the set of the correctly predicted unseen compounds. The MTL E model reduces the generalization error on completely unseen compounds by a total of eight compounds compared to the STL model, but seven of these compounds are annotated with ARG1 which is the most frequent relation in NomBank. When it comes to the unseen right constituents, the 24 compounds MTL E improves on consist of 18 ARG1 compounds, 5 ARG0 compounds and one ARG2 compound. We see a similar pattern upon inspecting the gains of the TL E model; where most of the improvement arises from predicting more ARG1 and ARG0 correctly.</p><p>The majority of the partly or completely unseen compounds that were misclassified by all models are not of type ARG1 in NomBank or RSTR in PCEDT. This, together with the fact that the cor- rectly predicted unseen compounds are annotated with the most frequent relations, indicate that the classification models rely on lexical memorization to learn the interpretation of compound relations.</p><p>Finally, to complement our understanding of the effect of lexical memorization, we plot the ratio of relation-specific constituents in NomBank and PCEDT in <ref type="figure" target="#fig_3">Figure 2</ref>. We define relation-specific constituents as left or right constituents that only occur with one specific relation in the training split, and their ratio is simply their proportion in the overall set of left or right constituents per rela- tion. Looking at <ref type="figure" target="#fig_3">Figure 2</ref>, we see that NomBank relations have higher ratios of relation-specific constituents in comparison to PCEDT, which ar- guably makes learning the former comparatively easier if the model is only to rely on lexical mem- orization. Furthermore, ARGM-TMP in NomBank and TWHEN in PCEDT stand out from other rela- tions in <ref type="figure" target="#fig_3">Figure 2</ref>, which are also the two relations with the second highest F 1 score in their respec- tive dataset-except in STL on PCEDT (cf. Ta- bles 4 and 5). Lexical memorization is, therefore, the most likely explanation of such relatively high F 1 scores. We also observe some correlation be- tween lower ratios of relation-specific constituents and relatively low F 1 scores, e.g. APP and REG in PCEDT. Based on these observations, we can- not rule out that our models exhibit some degree of lexical memorization effects, even though man- ual result analysis also reveals 'counter-examples' where the models generalize and make correct pre- dictions where lexical memorization is impossi- ble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>Transfer and multi-task learning for NLP currently receive a lot of attention, but for the time be- ing there remains considerable uncertainty about which task properties and experimental settings actually are effective. In this work, we seek to shed light on the utility of TL and MTL per- spectives on the semantic interpretation of noun- noun compounds. Through a comprehensive se- ries of minimally contrasting experiments and in- depth analysis of results and prediction errors, we demonstrate the ability of both TL and MTL to mitigate the challenges of class imbalance and substantially improve prediction of low-frequency relations. In a nutshell, our TL and in particular MTL models make quantitatively and qualitatively better predictions, especially so on the 'hardest' inputs involving at least one constituent not seen in the training data-but clear indicators of remain- ing 'lexical memorization' effects arise from our error analysis of unseen compounds.</p><p>In general, transfer of representations or shar- ing across tasks is most effective at the embed- ding layers, i.e. the model-internal representation of the two compound constituents involved. In multi-task learning, full sharing of the model ar- chitecture across tasks worsens the model's ability to generalize on the less frequent relations.</p><p>We experience the dataset by <ref type="bibr" target="#b8">Fares (2016)</ref> as an interesting opportunity for innovative neural approaches to compound interpretation, as it re- lates this sub-problem to broad-coverage semantic role labeling or semantic dependency parsing in PCEDT and NomBank. In future work, we plan to incorporate other NLP tasks defined over these frameworks to learn noun-noun compound inter- pretation using TL and MTL. Such tasks include semantic role labeling of nominal predicates in NomBank annotations as well as verbal predicates in PropBank ( <ref type="bibr" target="#b19">Kingsbury and Palmer, 2002</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Distribution of NomBank relations (left) and PCEDT relations (right)</figDesc><graphic url="image-1.png" coords="7,72.00,62.80,226.77,148.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Ratio of relation-specific constituents in NomBank (left) and PCEDT (right).</figDesc><graphic url="image-4.png" coords="9,298.77,62.81,226.77,148.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : Accuracy (%) of the transfer learning models.</head><label>2</label><figDesc></figDesc><table>Model 
NomBank 
PCEDT 
Dev 
Test 
Dev 
Test 
STL 
78.15 76.75 
58.80 56.05 
MTLE 77.93 78.45 
59.89 56.96 
MTLF 76.74 78.51 
58.91 56.00 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Accuracy (%) of the MTL models. the development split of the same dataset. The MTL models, especially MTL F , have a negative effect on the development accuracy of NomBank, but we still see the same improvement, as in TL, on the test split. Third, both the TL and MTL mod- els exhibit less consistent effects on PCEDT (on both the development and test splits) compared to NomBank; for example, all the TL models lead to about 1.25 points absolute improvement in ac- curacy on NomBank, whereas in PCEDT TL E is clearly better than the other two TL models (TL E improves over the STL accuracy by 1.37 points).</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>A2 A3 LOC MNR TMP Count 132 1282 153 75 25 25 27 STL 49.82 87.54 45.78 60.81 28.57 29.41 66.67 TLE 55.02 87.98 41.61 60.14 27.91 33.33 63.83 TLH 54.81 87.93 42.51 60.00 25.00 35.29 65.31 TLEH 53.62 87.95 42.70 61.11 29.27 33.33 65.22 MTLE 54.07 88.34 42.86 61.97 30.00 28.57 66.67 MTLF 53.09 88.41 38.14 62.69 00.00 00.00 52.17</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>Count 89 14 118 326 216 900 STL 43.90 42.11 22.78 42.83 20.51 68.81 TLE 49.37 70.97 27.67 41.60 30.77 69.67 TLH 53.99 62.07 25.00 43.01 26.09 68.99 TLEH 49.08 64.52 28.57 42.91 28.57 69.08 MTLE 54.09 66.67 24.05 42.03 27.21 69.31 MTLF 47.80 42.11 25.64 40.73 19.22 68.89</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Per-label F 1 score on the PCEDT test split. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 :</head><label>7</label><figDesc>Correspondence matrix between NomBank arguments and PCEDT functors.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 8 it</head><label>8</label><figDesc>becomes crystal clear that TL and MTL on the embedding layer yield re- markable improvements for PCEDT with about 7-</figDesc><table>Model NomBank PCEDT 
STL 
52.66 
40.15 
TLE 
52.83 
48.34 
TLH 
52.98 
46.52 
TLEH 
53.31 
47.12 
MTLE 
53.21 
47.23 
MTLF 
42.07 
40.73 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Macro-average F 1 score on the test split. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table>Generalization error on the subset of unseen 
compounds in the test split. L: Left constituent. R: 
Right constituent. L&amp;R: Completely unseen. 

</table></figure>

			<note place="foot" n="3"> When the label sets are identical, TL practically becomes a technique for domain adaptation. Though these two terms have also been used interchangeably (Chung et al., 2018). 4 Using pretrained word embeddings as input representation is in a sense a form of unsupervised transfer learning, but in this work we focus on transfer learning based on supervised learning.</note>

			<note place="foot" n="5"> vectors.nlpl.eu/repository 6 github.com/ltgoslo/fun-nom</note>

			<note place="foot" n="7"> One can also argue that result analysis on the test split is stricter than on the validation split. While using an early stop</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Identifying beneficial task relations for multi-task learning in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Bingel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="164" to="169" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Reference guide for the British National Corpus version 1.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lou</forename><surname>Burnard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<title level="m">Multitask Learning. Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supervised and Unsupervised Transfer Learning for Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1585" to="1594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning, ICML</title>
		<meeting>the 25th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the Compositionality and Semantic Interpretation of English Noun Compounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corina</forename><surname>Dima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Representation Learning for NLP</title>
		<meeting>the 1st Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="27" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic Noun Compound Interpretation using Deep Neural Networks and Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corina</forename><surname>Dima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhard</forename><surname>Hinrichs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Computational Semantics</title>
		<meeting>the 11th International Conference on Computational Semantics<address><addrLine>London, UK. As</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="173" to="183" />
		</imprint>
	</monogr>
	<note>sociation for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the Creation and Use of English Compound Nouns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Downing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="810" to="842" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Dataset for Joint Noun-Noun Compound Bracketing and Interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murhaf</forename><surname>Fares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2016 Student Research Workshop</title>
		<meeting>the ACL 2016 Student Research Workshop</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="72" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Word vectors, reuse, and replicability: Towards a community repository of large-text resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murhaf</forename><surname>Fares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Kutuzov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Oepen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Velldal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Nordic Conference on Computational Linguistics</title>
		<meeting>the 21st Nordic Conference on Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="271" to="276" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Semantic Interpretation of Nominal Compounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Wilking Finin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Annual National Conf. on Artificial Intelligence</title>
		<meeting>the First Annual National Conf. on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Influence of Thematic Relations on the Comprehension of Modifier-Noun Combinations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><forename type="middle">L</forename><surname>Gagné</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Shoben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning Memory and Cognition</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="87" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Classification of semantic relations between nominals. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivi</forename><surname>Nastase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="105" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Hajičová</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarmila</forename><surname>Panevová</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Sgall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvie</forename><surname>Cinková</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Fučíková</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jiří Semeck´ySemeck´y</title>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zdeňka</forename><surname>Toman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zdeněkzdeněkˇzdeněkžabokrtsk´zdeněkžabokrtsk´y</forename><surname>Urešová</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Announcing Prague Czech-English Dependency Treebank 2.0</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Language Resources and Evaluation</title>
		<meeting>the 8th International Conference on Language Resources and Evaluation<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="3153" to="3160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic Interpretation of Noun Compounds Using WordNet Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second International Joint Conference on Natural Language Processing</title>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="945" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A lexical semantic approach to interpreting and bracketing English noun compounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="385" to="407" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">From TreeBank to PropBank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Language Resources and Evaluation</title>
		<meeting>the 3rd International Conference on Language Resources and Evaluation<address><addrLine>Las Palmas, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1989" to="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Designing Statistical Language Learners. Experiments on Noun Compounds. Doctoral dissertation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Lauer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<pubPlace>Macquarie University, Sydney, Australia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The syntax and semantics of complex nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith N</forename><surname>Levi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Do Supervised Distributional Methods Really Learn Lexical Inference Relations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Remus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="970" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semantics and the Structure of Compounds in Chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles Na</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972" />
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Compounding as Abstract Operation in Semantic Space: Investigating relational effects through a large-scale, data-driven computational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><forename type="middle">L</forename><surname>Gagné</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Spalding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="207" to="224" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">When is multitask learning effective? Semantic sequence prediction under varying data conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alonso</forename><surname>Héctor Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="44" to="53" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Annotating noun argument structure for NomBank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Macleod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Szekely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronika</forename><surname>Zielinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Language Resources and Evaluation</title>
		<meeting>the 4th International Conference on Language Resources and Evaluation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="803" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">How Transferable are Neural Networks in NLP Applications?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="479" to="489" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Noun Compound Interpretation Using Paraphrasing Verbs: Feasibility Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence: Methodology, Systems, and Applications</title>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="103" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Using the Web as an Implicit Training Set: Application to Noun Compound Syntax and Semantics. Doctoral dissertation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nakov</forename><surname>Preslav Ivanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>EECS Department, University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Annotating and Learning Compound Noun Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Diarmuid´odiarmuid´ Diarmuid´o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Séaghdha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2007 Student Research Workshop</title>
		<meeting>the ACL 2007 Student Research Workshop<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="73" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning Compound Noun Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Diarmuid´odiarmuid´ Diarmuid´o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Séaghdha</surname></persName>
		</author>
		<idno>UCAM-CL-TR- 735</idno>
		<imprint>
			<date type="published" when="2008" />
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge, Computer Laboratory</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Using lexical and relational similarity to classify semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´o</forename><surname>Diarmuid´odiarmuid´</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Copestake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference of the European Chapter</title>
		<meeting>the 12th Conference of the European Chapter<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="621" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Interpreting compound nouns with kernel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´o</forename><surname>Diarmuid´odiarmuid´</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Copestake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="331" to="356" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Survey on Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">English Gigaword Fifth Edition LDC2011T07</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Linguistic Data Consortium</publisher>
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep multitask learning for semantic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2037" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Early stopping-but when?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lutz</forename><surname>Prechelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade: Second Edition</title>
		<editor>Grégoire Montavon,Genevì eve B. Orr, and KlausRobert Müller</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="53" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Ordered chaos: The interpretation of English noun-noun compounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ellen</forename><surname>Ryder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>University of California Press</publisher>
			<biblScope unit="volume">123</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Paraphrase to Explicate: Revealing Implicit Noun-Compound Relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1200" to="1211" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep multi-task learning with low level tasks supervised at lower layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="231" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tratz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="678" to="687" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Olive Oil is Made of Olives, Baby Oil is Made for Babies: Interpreting Noun Compounds using Paraphrases in a Neural Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shwartz</forename><surname>Vered</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Waterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="218" to="224" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
