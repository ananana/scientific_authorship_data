<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adapting Word Embeddings to New Languages with Morphological and Phonological Subword Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditi</forename><surname>Chaudhary</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lori</forename><surname>Levin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Mortensen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adapting Word Embeddings to New Languages with Morphological and Phonological Subword Representations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3285" to="3295"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3285</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Much work in Natural Language Processing (NLP) has been for resource-rich languages, making generalization to new, less-resourced languages challenging. We present two approaches for improving generalization to low-resourced languages by adapting continuous word representations using linguistically motivated subword units: phonemes, morphemes and graphemes. Our method requires neither parallel corpora nor bilingual dictionaries and provides a significant gain in performance over previous methods relying on these resources. We demonstrate the effectiveness of our approaches on Named Entity Recognition for four languages, namely Uyghur, Turkish, Bengali and Hindi, of which Uyghur and Bengali are low resource languages, and also perform experiments on Machine Translation. Exploiting subwords with transfer learning gives us a boost of +15.2 NER F1 for Uyghur and +9.7 F1 for Bengali. We also show improvements in the monolingual setting where we achieve (avg.) +3 F1 and (avg.) +1.35 BLEU.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Continuous word representations have demon- strated utility in state-of-the-art neural models for several NLP tasks, such as named entity recogni- tion (NER; <ref type="bibr" target="#b21">Ma and Hovy (2016)</ref>), machine reading ( <ref type="bibr" target="#b35">Tan et al., 2017)</ref>, sentiment analysis ( <ref type="bibr" target="#b36">Tang et al., 2016;</ref><ref type="bibr" target="#b42">Yu et al., 2018)</ref>, and machine translation (MT; ). While the training of these word vectors does not rely on explicit human su- pervision, their quality is highly contingent on the size and quality of the unlabeled corpora available. There are over 7000 languages in the world <ref type="bibr" target="#b13">(Hammarstr√∂m et al., 2018)</ref>, and corpora with sufficient size and coverage are available for just a handful, making it unclear how these methods will perform in the more common low-resource setting.</p><p>Disheartening though this high dependence on resources sounds, several efforts ( <ref type="bibr" target="#b0">Adams et al., 2017;</ref><ref type="bibr" target="#b12">Haghighi et al., 2008;</ref><ref type="bibr" target="#b4">Bharadwaj et al., 2016;</ref><ref type="bibr" target="#b23">Mayhew et al., 2017</ref>) have shown consid- erable performance gains across different tasks in the low resource setting by transferring knowledge from related high-resource languages. Most exist- ing approaches for learning cross-lingual word em- beddings <ref type="bibr" target="#b33">(Ruder, 2017)</ref> either extend the monolin- gual objective function by adding a cross-lingual regularization objective which is then jointly opti- mized or use mapping-based approaches to align similar words across languages. These post-hoc coordination methods rely on bilingual lexicons or parallel corpora, which are typically of limited quantity and uncertain quality.</p><p>In this paper, we take a different task: fo- cusing instead on the similarity of the surface forms, phonology, or morphology of the two trans- fer languages. Specifically, inspired by <ref type="bibr" target="#b19">Ling et al. (2015)</ref>, who demonstrate the effectiveness of character-level modeling for knowledge shar- ing in multilingual scenarios, we propose two ap- proaches to transfer word embeddings using differ- ent types of linguistically-inspired subword-level information. Both approaches focus on mapping the low resource language embeddings closer to those of the high resource language and are ex- ecuted using two different training regimes. We explore the effect of different subword units- characters, lemmas, inflectional properties, and phonemes-as each one offers a unique linguis- tic insight, discussed more in Section 3. Our pro- posed approaches do require language specific re- sources, but importantly do not depend on cross- lingual resources and achieve considerable perfor- mance gains over existing methods which do.</p><p>We evaluate our proposed approach on two downstream tasks: NER, which deals with detect- ing and classifying Named Entities (NEs) into pre-defined categories ( <ref type="bibr" target="#b27">Nadeau and Sekine, 2007)</ref>, and MT to English. For the purposes of error analy- sis and discussion, we focus on the NER task in particular. NEs are typically noun phrases and oc- cur rarely in the corpus, making the generalization across types and domains difficult. We chose NER as our test bed because word vectors have a di- rect impact on NER model performance-as sug- gested by <ref type="bibr" target="#b33">(Ruder, 2017)</ref> and observed by us in <ref type="table" target="#tab_6">Ta- ble 3</ref>, where the model without any pre-trained em- beddings scores an average of 18 F1 points less. It thus provides a transparent way to measure the ef- fectiveness of different subword units.</p><p>This paper makes the following contributions:</p><p>1. We show that embeddings trained on sub- word representations yield better task per- formance than those trained only on whole words. This is especially true in a transfer set- ting, where subword representations also out- perform a word alignment based method. We further show that embeddings trained on mor- phological representations often outperform those trained only on whole words.</p><p>2. We demonstrate that training embeddings on character-based phonemic representations presents substantial performance advantages over training on orthographic characters in some transfer settings, e.g. when there are script differences across languages. These advantages are in addition to those from mor- phological representations (lemmas and mor- phological properties).</p><p>3. We produce continuous representations for each subword unit, giving researchers the ability to use them in their own tasks as they see fit. The code 1 for training word em- beddings and the embeddings 2 which pro- duced the best results are publicly available. We also release morphological analyzers for Hindi and Bengali 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Skipgram Objective</head><p>The two most popular training objectives for monolingual word embeddings are the skip- gram and continuous-bag-of-words (CBOW), in- troduced by <ref type="bibr" target="#b24">Mikolov et al. (2013a</ref> model attempts to predict the context surrounding a word, given the word itself whereas CBOW pre- dicts the word given its context. Formally, given a corpus having a sequence of words √≠ ¬µ√≠¬±¬§ 1 , √≠ ¬µ√≠¬±¬§ 2 , ‚ãØ , √≠ ¬µ√≠¬±¬§ √≠ ¬µ√≠¬±¬á , the skip-gram model maximizes the following log- likelihood:</p><formula xml:id="formula_0">√≠ ¬µ√≠¬±¬á ‚àë √≠ ¬µ√≠¬±¬ñ=1 ‚àë √≠ ¬µ√≠¬±¬£‚àà√≠ ¬µ√≠¬∞ ¬∂ √≠ ¬µ√≠¬±¬ñ log √≠ ¬µ√≠¬±¬ù(√≠ ¬µ√≠¬±¬£|√≠ ¬µ√≠¬±¬§ √≠ ¬µ√≠¬±¬ñ )<label>(1)</label></formula><p>where √≠ ¬µ√≠¬∞ ¬∂ √≠ ¬µ√≠¬±¬ñ are the context tokens, within a specified window of the focus word √≠ ¬µ√≠¬±¬§ √≠ ¬µ√≠¬±¬ñ and √≠ ¬µ√≠¬±¬ù(√≠ ¬µ√≠¬±¬£|√≠ ¬µ√≠¬±¬§ √≠ ¬µ√≠¬±¬ñ ) is the probability of observing context word √≠ ¬µ√≠¬±¬£ given fo- cus word √≠ ¬µ√≠¬±¬§ √≠ ¬µ√≠¬±¬ñ . The skipgram was originally defined using the softmax function:</p><formula xml:id="formula_1">√≠ ¬µ√≠¬±¬ù(√≠ ¬µ√≠¬±¬£|√≠ ¬µ√≠¬±¬§ √≠ ¬µ√≠¬±¬ñ ) = √≠ ¬µ√≠¬±¬í √≠ ¬µ√≠¬± (√≠ ¬µ√≠¬±¬£,√≠ ¬µ√≠¬±¬§ √≠ ¬µ√≠¬±¬ñ ) ‚àë √≠ ¬µ√≠¬±¬ä √≠ ¬µ√≠¬±¬ó=1 √≠ ¬µ√≠¬±¬í √≠ ¬µ√≠¬± (√≠ ¬µ√≠¬±¬§ √≠ ¬µ√≠¬±¬ñ ,√≠ ¬µ√≠¬±¬ó)<label>(2)</label></formula><p>where √≠ ¬µ√≠¬± is a scoring function mapping √≠ ¬µ√≠¬±¬£ and √≠ ¬µ√≠¬±¬§ √≠ ¬µ√≠¬±¬ñ to ‚Ñù. The summation in the denominator is over the entire vocabulary √≠ ¬µ√≠¬±¬ä which makes this formula- tion computationally inefficient as cost of gradient computation is proportional to √≠ ¬µ√≠¬±¬ä which is quite large (‚àº 10 6 </p><p>where √≠ ¬µ√≠¬±¬Å √≠ ¬µ√≠¬±¬ñ are the negative words sampled ran- domly from vocabulary and √≠ ¬µ√≠¬±¬ô is the log-sigmoid function. The scoring function √≠ ¬µ√≠¬± is a dot product similarity function given by √≠ ¬µ√≠¬± (√≠ ¬µ√≠¬±¬§ √≠ ¬µ√≠¬±¬ñ , √≠ ¬µ√≠¬±¬§ √≠ ¬µ√≠¬±¬ê ) = u ‚ä§ √≠ ¬µ√≠¬±¬§ √≠ ¬µ√≠¬±¬ñ v √≠ ¬µ√≠¬±¬§ √≠ ¬µ√≠¬±¬ê where u √≠ ¬µ√≠¬±¬§ √≠ ¬µ√≠¬±¬ñ and v √≠ ¬µ√≠¬±¬§ √≠ ¬µ√≠¬±¬ê are the embeddings of the focus word and its context word respectively. <ref type="bibr" target="#b25">Mikolov et al. (2013b)</ref>'s model fails to capture internal structure of words and does not general- ize for out of vocabulary words that may share morphemes with in-vocabulary words. The prob- lems of this method are particularly salient for graphemes ‚ü®‚Ä´‚ü©ŸÇÿßÿ±ŸâŸäÿßŸÑŸÖÿßŸäÿØ€á‚Ä¨ phonemes /qarijalmajdu/ morphemes /qari-jal-ma-jdu/ lemma+tag qari+Verb+Pot+Neg+Pres+A3sg gloss 's/he can't care for' <ref type="figure">Figure 1</ref>: Representations of a word in Uyghur morphologically rich languages such as Turkish, Uyghur, Hindi, and Bengali. Although, given a large enough training corpus, most or all morpho- logical forms of a lexeme (of which there may be many) could theoretically learn to have similar vec- tor representations, it will be vastly more data ef- ficient if we can take into account regularities of their form to model morphology explicitly. We ex- plore the following methods for doing so:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Subword Representation</head><p>Orthographic units: <ref type="bibr" target="#b39">Wieting et al. (2016)</ref> and <ref type="bibr" target="#b5">Bojanowski et al. (2016)</ref> show the utility of character-level modeling by representing the focus word √≠ ¬µ√≠¬±¬§ √≠ ¬µ√≠¬±¬ñ as a set of its character ngrams, denoted by u √≠ ¬µ√≠¬±¬§ √≠ ¬µ√≠¬±¬ñ = 1 |√≠ ¬µ√≠¬∞¬∫| ‚àë √≠ ¬µ√≠¬±¬î‚àà√≠ ¬µ√≠¬∞¬∫ x √≠ ¬µ√≠¬±¬î , where √≠ ¬µ√≠¬∞¬∫ is the set of char- acter ngrams and x √≠ ¬µ√≠¬±¬î is the vector representation of ngram √≠ ¬µ√≠¬±¬î. Such representations capture morpho- logical information in a brute-force but principled fashion-words that share the same morpheme are more likely to share the same character ngrams than words that do not.</p><p>Morphological units: Previous work has found that morphological relationships between words can be captured more directly if embeddings are trained on morphological representations ( <ref type="bibr" target="#b20">Luong et al., 2013;</ref><ref type="bibr" target="#b6">Botha and Blunsom, 2014;</ref><ref type="bibr" target="#b8">Cotterell and Sch√ºtze, 2015)</ref>. <ref type="bibr" target="#b2">Avraham and Goldberg (2017)</ref> explicitly model lemmas (stems or citation forms) and morphological properties (the sets of which are sometimes called "tags") for training the word embeddings. Lemmas capture information about the lexical identity of a word and are closely correlated with the semantics of a word; tags cap- ture information about the syntactic context of a word. See <ref type="figure">Figure 1</ref> for an example. We take inspi- ration from the above work in adapting these sub- word units for cross-lingual transfer.</p><p>Phonological units: Subword units other than tags might seem to be of no use in closely-related languages with different scripts (such as Serbian and Croatian). Following <ref type="bibr" target="#b4">Bharadwaj et al. (2016)</ref>, we convert text from its orthographic form into a phoneme ngrams √≠ ¬µ√≠¬±¬• &lt;√≠ ¬µ√≠¬±¬û√≠ ¬µ√≠¬±¬é + √≠ ¬µ√≠¬±¬• √≠ ¬µ√≠¬±¬û√≠ ¬µ√≠¬±¬é√≠ ¬µ√≠¬±¬ü + ... + √≠ ¬µ√≠¬±¬• √≠ ¬µ√≠¬±¬ö√≠ ¬µ√≠¬±¬é√≠ ¬µ√≠¬±¬ó√≠ ¬µ√≠¬±¬ë√≠ ¬µ√≠¬±¬¢&gt; lemma √≠ ¬µ√≠¬±¬• qari morphemes √≠ ¬µ√≠¬±¬• √≠ ¬µ√≠¬±¬â√≠ ¬µ√≠¬±¬í√≠ ¬µ√≠¬±¬ü√≠ ¬µ√≠¬±¬è + √≠ ¬µ√≠¬±¬• √≠ ¬µ√≠¬±¬É√≠ ¬µ√≠¬±¬ú√≠ ¬µ√≠¬±¬° + ... + √≠ ¬µ√≠¬±¬• √≠ ¬µ√≠¬∞¬¥3√≠ ¬µ√≠¬± √≠ ¬µ√≠¬±¬î <ref type="figure">Figure 2</ref>: Vector representations of a word in Uyghur phonemic representation, stated in terms of the In- ternational Phonetic Alphabet (IPA). We then train embeddings on this representation. This means that, roughly speaking, morphemes that sound the same will be represented in the same way across languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Cross-lingual Transfer</head><p>In this section we discuss in detail both our ap- proaches for cross-lingual transfer along with the relevant baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Proposed Approach</head><p>We propose to use phoneme ngrams, represented using IPA, in addition to the lemma and morpho- logical tags, to enable effective transfer across lan- guages.  demonstrate the effectiveness of projecting words from ortho- graphic space to phonemic space as related lan- guages often share similar phonological patterns. More formally, let √≠ ¬µ√≠¬±¬É √≠ ¬µ√≠¬±¬§ be the set of linguistic prop- erties of a word consisting of the phoneme ngrams (I √≠ ¬µ√≠¬±¬î ) , lemma (L) and individual morphological tags (M √≠ ¬µ√≠¬±¬ö ). The focus word is then represented as the average sum of its linguistically motivated sub- word units:</p><formula xml:id="formula_3">v √≠ ¬µ√≠¬±¬§ √≠ ¬µ√≠¬±¬ê = 1 |√≠ ¬µ√≠¬±¬É √≠ ¬µ√≠¬±¬§ √≠ ¬µ√≠¬±¬ê | ‚àë √≠ ¬µ√≠¬±¬ù‚àà√≠ ¬µ√≠¬±¬É √≠ ¬µ√≠¬±¬§ √≠ ¬µ√≠¬±¬ê x √≠ ¬µ√≠¬±¬ù</formula><p>where x √≠ ¬µ√≠¬±¬ù is the vector representation of subword unit √≠ ¬µ√≠¬±¬ù of word √≠ ¬µ√≠¬±¬§ √≠ ¬µ√≠¬±¬ê . The average operation is im- portant to remove any bias towards words having too many or too few subword units. For instance, the Uyghur word in <ref type="figure">Figure 1</ref> is represented using its phoneme-ngrams ranging from 3-grams to 6- grams, lemma and morphological tags as shown in <ref type="figure">Figure 2</ref>. <ref type="bibr" target="#b2">Avraham and Goldberg (2017)</ref> instead encode the different morphological inflections as one tag, so that Verb+Pot+Neg+Pres+A3sg would be encoded as √≠ ¬µ√≠¬±¬• √≠ ¬µ√≠¬±¬â√≠ ¬µ√≠¬±¬í√≠ ¬µ√≠¬±¬ü√≠ ¬µ√≠¬±¬è+√≠ ¬µ√≠¬±¬É√≠ ¬µ√≠¬±¬ú√≠ ¬µ√≠¬±¬°+√≠ ¬µ√≠¬±¬Å√≠ ¬µ√≠¬±¬í√≠ ¬µ√≠¬±¬î+√≠ ¬µ√≠¬±¬É√≠ ¬µ√≠¬±¬ü√≠ ¬µ√≠¬±¬í√≠ ¬µ√≠¬± +√≠ ¬µ√≠¬∞¬¥3√≠ ¬µ√≠¬± √≠ ¬µ√≠¬±¬î . We encode each property in a tag separately to avoid data spar- sity issues and empirically find this approach to perform better.</p><p>We present two training regimes for transferring knowledge from a related language, namely CT-Joint and CT-FineTune by explicitly incorporat- ing the subword units. We hypothesize that having word representations of both languages lying in a similar space will aid the low resource language in leveraging resources from the high resource lan- guage, including annotations for the downstream task. These two regimes are described below:</p><p>CT-Joint: This model explicitly maps the word representations of the two languages into the same space by training simultaneously on both. This is achieved simply by combining the corpora of both the high-resource and the low-resource lan- guage and training jointly using the skip-gram ob- jective, discussed above. The central intuition is as follows: once two related languages are placed in the same phonological and morphological space, they will share many subword units in common and this will make joint training profitable. <ref type="bibr" target="#b9">Duong et al. (2016)</ref> and <ref type="bibr" target="#b10">Gouws et al. (2015)</ref> have previ- ously shown the advantages of joint training and we observe this to be true in our case as well.</p><p>CT-FineTune: This model implicitly maps the word representations of the two languages into the same space. The model attempts this by taking the learned continuous representations of the high resource subword units, referred to by x √≠ ¬µ√≠¬∞¬ª√≠ ¬µ√≠¬±¬ñ √≠ ¬µ√≠¬±¬Ü√≠ ¬µ√≠¬±¬ä√≠ ¬µ√≠¬±¬à , and uses them to initialize the model for the low resource language. The model is first trained using all subword units on the high re- source language and the learned representations are then used for initializing the subword units for the low resource language. To elucidate which pre- trained subword helped the most on the low re- source language, we use the same model for differ- ent experiments, which is trained using all subword units-phoneme-ngrams, lemma and morphologi- cal properties. The linguistic intuition behind CT- FineTune is similar to that behind CT-Joint. This idea of transferring parameters from high resource language has been previously explored by <ref type="bibr" target="#b43">Zoph et al. (2016)</ref> for low resource neural machine trans- lation which showed considerable improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>In this section, we first describe the model setup for training word embeddings followed by details on NER and MT experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation details</head><p>We base our model on the C++ implementation of fasttext 5 ( <ref type="bibr" target="#b5">Bojanowski et al., 2016</ref>) with modifica- tions as described above.</p><p>Data: We represent a word in the training corpus using the format presented by <ref type="bibr" target="#b2">Avraham and Goldberg (2017)</ref>. For instance, the Uyghur word in <ref type="figure">Fig- ure 1</ref> is represented as follows: phoneme ipa: qari- jalmajdu, lemma l:qari, and morphological inflec- tions m:Verb+Pot+Neg+Pres+A3sg. We con- sider phoneme-ngrams ranging from 3-grams to 6- grams and append a special start symbol &lt; and end symbol &gt; to the word. We discard unigrams and bigram ngrams on the assumption that they don't contribute much to the word.</p><p>Linguistic properties: We experiment with dif- ferent subword units for both the transfer set- ting and the monolingual setting. We use the orthography-to-IPA tool <ref type="bibr">Epitran (Mortensen et al., 2018)</ref> to obtain the phonemic representations. The lemmas and morphological properties for a word in context are obtained using a rule-based morpho- logical analyzer in such a fashion as to produce tags similar to the high resource language. For Turkish we use the morphological disambiguator developed by <ref type="bibr" target="#b34">(Shen et al., 2016)</ref>, which in turn is based on an FST-based morphological analyzer de- veloped by <ref type="bibr" target="#b29">Oflazer (1994)</ref>. For Uyghur, we took a (parser combinator based) morphological analyzer that had been developed for a DARPA LORELEI evaluation and modified it to output part-of-speech tags and to use a property set that was as close as possible to that of the Oflazer Turkish analyzer. The analyzer for Turkish produces 116 inflectional properties and for Uyghur we get 54 properties, of which 64% are shared with Turkish. Unfortu- nately, we did not have access to existing mor- phological analyzers for Hindi or Bengali. Many Hindi morphological analyzers exist, but they are not typically released publicly <ref type="bibr" target="#b22">(Malladi and Mannem, 2013;</ref><ref type="bibr" target="#b11">Goyal and Lehal, 2008)</ref>. We developed our own analyzers using a stemmer-like frame- work 6 over a span of few weeks (2-3), which gave 8 unique morphological tags for Hindi and 10 for Bengali (for both languages, noun inflection only) of which just 2 were shared with Hindi.</p><p>Morphologically speaking, we only use inflec-tional properties. For most languages, we consid- ered derivational affixes to be part of the stem, since they change the meaning and grammatical category of the word rather than simply express- ing syntactic information. An exception to this was Turkish, where the available morphological ana- lyzer segments all affixes off from the root. How- ever, even there we confined our use of morpholog- ical properties to inflectional properties. Deriva- tional affixes display scopal behavior; since we wanted to treat the morphological properties of a word as a set, rather than a sequence, we were re- quired to choose this option.</p><p>Hyperparameters: During training, we con- sider context tokens within a window size 3 of the focus word and we sample 5 negative examples from the vocabulary. We chose a window size of only 3 based on the fact that we are working with morphologically rich languages with a rela- tively high information to token ratio (otherwise a window size of 5 may be more appropriate </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>For comparison, we train multilingual embeddings using MultiCCA (Ammar et al., 2016) as our base- line. It employs canonical correlation analysis by projecting multiple languages in the same shared space of one language, also referred to as multi- language space. This method learns linear pro- jections for each language into this common lan- guage space using bilingual lexicons. English is used as a common vector space due to availability of corresponding bilingual lexicons between En- glish and each of our languages. For a fair compar- ison, we run MultiCCA on monolingual embed- dings trained with different subword units. We use 100 dimension ( <ref type="bibr" target="#b5">Bojanowski et al., 2016</ref>) embed- dings for English. For NER, we also compare with <ref type="bibr" target="#b4">Bharadwaj et al. (2016)</ref> who use a neural attention model over phonological features and report the best per- formance for Turkish using transfer from <ref type="bibr">Uzbek and Uyghur, and Mayhew et al. (2017)</ref> who use a cheap translation method to translate training data from high-resource language into the low- resource language and report best NER results for Uyghur, as part of the LORELEI program. Our work differs from these primarily on two fronts: a) it is independent of the downstream task and can easily be adapted across various tasks, and b) it doesn't require parallel corpora or bilingual dictionaries. For our monolingual experiments, we compare our proposed approach with models using subword representations- <ref type="bibr" target="#b5">Bojanowski et al. (2016)</ref> and <ref type="bibr" target="#b2">Avraham and Goldberg (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Named Entity Recognition Task</head><p>We use state-of-the-art NER architecture (Ma and Hovy, 2016) as our model for evaluation. The task is to identify NEs and categorize them into four types. Since this is a supervised model, the per- formance is highly contingent on the quality of la- beled data. F1 scores are used as the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Experiments</head><p>We conduct the two main sets of NER experiments, 1. Transfer experiments on the low resource languages-Uyghur and Bengali-using Turkish and Hindi as the high resource languages respectively. We show results using both our proposed models, CT-Joint and CT-FineTune.</p><p>2. Monolingual experiments on all four lan- guages: Uyghur, Turkish, Bengali and Hindi. We do an ablation study using different com- binations of subword units.</p><p>These language pairs were chosen partly out of convenience-the data were available to us as part of the DARPA LORELEI program-and partly because they satisfied certain deeper desiderata. Turkish and Uyghur are fairly closely related to one another, as are Hindi and Bengali. Despite this re- lationship, the members of both pairs are written   <ref type="bibr">8</ref> , from which we generate train-dev- test splits. Uyghur data was released as part of LoReHLT16 task, organized by NIST 9 under the aegis of DARPA, and training annotations were ac- quired using native speakers as part of the task. For Uyghur we evaluate on an unsequestered set consisting of 199 annotated evaluation documents, released by NIST. For Turkish, Hindi and Ben- gali, we create our own train-dev-test splits (Ta- ble 1). The exact documents from which May- hew et al. (2017) and <ref type="bibr" target="#b4">Bharadwaj et al. (2016)</ref> cre- ated their test set is not apparent. The Uyghur corpus has 27 million tokens and the Turkish cor- pus has about 40 million tokens. Although Ben- gali is widely-spoken and the unlabeled corpus contains more than 140 million tokens, there are very few named entity annotations available, mak- ing it a low-resource language for the purposes of this exercise. To have a fair experimental setup across language pairs, we sub-sample the Bengali and Hindi corpora to have comparable corpus sizes with Uyghur and Turkish respectively. We also up-sample the low resource data for both unla- beled corpora and NER annotations, so the model doesn't become biased towards the high resource language.</p><p>NER model setup: We train the model using 100-dimensional word embeddings, pre-trained using the above discussed strategies, and use hid- den dimension of size 100 for each direction of the LSTM. Stochastic gradient descent was used as the optimizer with a learning rate of 0.015. Dropout of 0.5 was used in the LSTM layer to prevent over-fitting. Uyghur and Turkish were trained for 100 epochs, Bengali and Hindi converged after 70 epochs.    bilingual dictionaries which is possibly why it performs poorly in our low resource setting, where these dictionaries are not of high qual- ity. The advantage of phoneme-ngrams over char-ngrams is quite apparent here. phoneme- ngrams+lemma+morph performs +5.2 F1 points better than char-ngram+lemma+morph for both Uyghur and Bengali, and similar increase is ob- served across other combinations, the only excep- tion being the phoneme case for Uyghur which performed -0.5 F1 with respect to its counterpart word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Results and Discussion</head><p>We find CT-Joint to be consistently better per- forming than CT-FineTune. Interestingly, the per- formance of CT-FineTune model converges to the monolingual performance. We hypothesize that the model forgets the pre-trained subword units as training progresses.</p><p>For CT-FineTune, the column subword units in <ref type="table" target="#tab_4">Table 2</ref> refers to the subword units which were pre- trained on a high resource related language. For example lemma + morph means lemma and morph embeddings are first pre-trained on the resouce- rich language and then used to initialize the respec- tive lemma and morph representations for the low resource language.</p><p>Monolingual Experiments: <ref type="table" target="#tab_6">Table 3</ref> shows our results on all languages. We get +5.8 F1 points for Turkish, +4.8 F1 for Uyghur, +0.8 F1 for Hindi and +0.7 F1 for Bengali over the exist- ing methods. We observe that a combination of character-ngrams, lemma and morphological prop- erties gives the best performance for Uyghur and Bengali. Adding morph hurts in Turkish, in con- trast to Hindi, where it helps. Section 5.3.3 dis- cuses plausible reasons for this.</p><p>We report official NIST scores on the full eval- uation set for Uyghur, as part of LORELEI Offical Retest. Additionally, we compare our results with</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model subword units Uyghur Bengali</head><p>CT-Joint phoneme-ngrams + lemma + morph 23.04 7.88 phoneme-ngrams + lemma 23.24 7.62 phoneme-ngrams 23.25 7.45</p><p>CT-FineTune lemma + morph 23.71 7.58   <ref type="table" target="#tab_7">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Error analysis</head><p>We plot recall curves for all languages. As seen in <ref type="figure" target="#fig_0">Figure 3</ref>, adding subword units boosts the re- call consistently across all languages, more so for Uyghur. For Turkish, lemma performs better than lemma+morph, perhaps because the morpholog- ical analyzer outputs so many redundant proper- ties which reduce the distance between words that are not particularly similar. In contrast, morph helps and lemma hurts in Hindi, perhaps because the morph analyzer outputs only a small num- ber of highly informative properties, but is a poor general-purpose lemmatizer. We analyze our results for Uyghur language, as it was part of the LORELEI challenge and presents a situation close to a real-life application. We base our analysis on the unsequestered set since anno- tations for full test data are not released. There are 1,341 NE's in this set, 396 of which are covered by the word embeddings when trained with just monolingual corpus. One obvious advantage of jointly training with a resource-rich corpus is that coverage of NEs increases, as validated in our case where jointly training with Turkish corpus adds 114 more NEs. The difference is striking-in the monolingual condition, the NEs are widely dispersed, but in the bilingual condition, the NEs cluster together. This suggests that phonologically-mediated transfer through Turkish is resulting in embeddings in which NEs are close to one another, relative to monolingual Uyghur embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Machine Translation Task</head><p>In addition to NER, we test the performance of our proposed approaches on the MT task to test gen- erality of our conclusions. We use XNMT toolkit  to translate sentences from the low-resource language to English. We run sim- ilar transfer and monolingual experiments as done for NER. Due to space limitations, we use select subword combinations for the experiments, details of which can be found in Appendix. BLEU is used as the evaluation metric.</p><p>From <ref type="table" target="#tab_9">Table 6</ref>, we observe that the combina- tion of character-ngrams and lemma performs the best for Uyghur (+0.1) and the combination of character-ngrams, lemma and morph gives the best performance for Bengali (+1.7), over the word baseline, which demonstrates the importance of subword units for low-resource MT as well. One likely reason that the combination of character- ngrams and lemmas consistently show the best per- formance is that, together, they capture lexical sim- ilarity, which is more important to translation than the syntactic information captured by morpholog- ical inflection ("morph"). However, experiments using CT-Joint and CT-FineTune <ref type="table" target="#tab_8">(Table 5)</ref> do not follow the same trend as that of NER. We hy- pothesize that this is because the MT models were trained on a training set that did not have transla- tion pairs from the high resource language. As  note, when training MT systems on a single language pair, it is less necessary for the em- beddings to be coordinated across the languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Word Embedding Models: Most algorithms for learning embeddings take inspiration from lan- guage modeling ( <ref type="bibr" target="#b3">Bengio et al., 2003)</ref>, motivated by distributional hypothesis <ref type="bibr" target="#b14">(Harris, 1954)</ref>, and employ a shallow neural network to map the words into a low dimensional space. <ref type="bibr" target="#b30">Pennington et al. (2014)</ref> built over the above local context window model by combining it with global matrix factor- ization ( <ref type="bibr" target="#b17">Levy and Goldberg, 2014)</ref>. Recently, <ref type="bibr" target="#b31">Peters et al. (2018)</ref> show significant gains across vari- ous tasks by learning word vectors as hidden states of a deep bi-directional language model. This was originally conceived for resource-abundant lan- guages, hence it is as-of-yet unclear how general- izable they are to low-resource settings.</p><p>Modeling subword information: Various methods have validated the importance of mod- eling subword units in downstream tasks. <ref type="bibr" target="#b40">Xu et al. (2016)</ref>; <ref type="bibr" target="#b7">Chen et al. (2015)</ref> experiment at the character level whereas <ref type="bibr" target="#b20">Luong et al. (2013)</ref> use morphemes as a basic unit in recursive neural network (RNN) to get morphologically-aware word representations. <ref type="bibr" target="#b41">Xu and Liu (2017)</ref> in- corporate the morphemes' meanings as part of the word representation to implicitly model the morphological knowledge.</p><p>Transfer learning: Most recent works using transfer in low resource setting are coupled tightly with the downstream task. <ref type="bibr" target="#b15">Jin and Kann (2017)</ref> use morpheme units for cross-lingual transfer in a paradigm completion task using sequence-to- sequence models. <ref type="bibr" target="#b37">Tsai et al. (2016)</ref> employ a language-independent method for NER by ground- ing non-English phrases to English Wikipedia. In- terestingly, <ref type="bibr" target="#b16">Kim et al. (2017)</ref> use separate encoders for modeling language-specific and language- agnostic features for part-of-speech (POS) tagging, and make use of no cross-lingual resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we explored two simple methods for cross-lingual transfer, both of which are task- independent and use transfer learning for leverag- ing subword information from resource-rich lan- guages, especially through phonological and mor- phological representations. CT-Joint and CT- FineTune do not require morphological analyzers, but we have found that even a morphological ana- lyzer built in 2-3 weeks can boost performance and is a worthwhile investment of resources. Prelimi- nary evaluation on a separate task of MT recon- firms the utility of subword units and further re- search will reveal what these learned subword rep- resentations can contribute to other tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Recall for all languages (monolingual) W:Word, NG:Char-ngrams, L:Lemma, M:Morph</figDesc><graphic url="image-1.png" coords="7,85.07,401.83,189.41,126.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 showsFigure 4 :</head><label>44</label><figDesc>Figure 4: Two-dimensional PCA projection of select NEs from word embeddings for Uyghur-CT-Joint model trained with phoneme-ngrams+lemma+morph (blue) and monolingual model trained with charngrams+lemma+morph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Transfer experiments on NER. Metric F1 (out of 100%). Uyghur transfer is from Turkish; Bengali transfer 
is from Hindi 

in different scripts (Roman and Perso-Arabic; De-
vanagari and Bengali). Finally, all four languages 
are morphologically rich, especially Turkish and 
Uyghur. These qualities allow us to showcase the 
value of embeddings with subword units. 
Data Preprocessing: We use data, comprised 
of unlabeled corpora, English bilingual dictionar-
ies, annotations, from the Linguistic Data Consor-
tium (LDC) language packs-Turkish and Hindi 
7 , Bengali </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>NER results for monolingual experiments. Metric F1 (out of 100%) 

Model 
Uyghur* (unseq.) Uyghur* 
Turkish 
Bengali 

Ours 
56.20 
56.00 
68.61 
60.33 
Bharadwaj et al. (2016) 
-
51.2 
66.47 
-
Mayhew et al. (2017) 
51.32 
55.6 
53.44 
45.70 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Comparison with previous work using data released by DARPA LORELEI. Metric F1 (out of 100%) 
*Official NIST scores. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Transfer experiments for MT. Metric: BLEU. Uyghur transfer is from Turkish; Bengali transfer is from 
Hindi 

Model 
subword units 
Uyghur 
Bengali 

Ours 
Char-ngrams + Lemma + Morph 23.59 
7.96 
Char-ngrams + Lemma 
23.91 
7.77 
Char-ngrams + Morph 
23.27 
7.88 

fastText 
Char-ngrams 
23.24 
7.91 

word2vec 
Word 
23.31 
6.64 

Random 
No embedding 
23.51 
6.23 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>MT results for monolingual experiments. Metric: BLEU 

the best results reported on the same LORELEI 
dataset. Results are seen in </table></figure>

			<note place="foot" n="4"> https://code.google.com/archive/p/ word2vec/</note>

			<note place="foot" n="5"> https://github.com/facebookresearch/ fastText/ 6 https://github.com/dmort27/mstem</note>

			<note place="foot" n="7"> LDC2014E115,LDC2017E62,http://www.cfilt. iitb.ac.in/iitb_parallel/ 8 LDC2017E60, LDC2015E13 9 https://www.nist.gov/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is sponsored by Defense Advanced Re-search Projects Agency Information Innovation Office (I2O). Program: Low Resource Languages for Emergent Incidents (LORELEI). Issued by DARPA/I2O under Contract No. HR0011-15-C0114. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official poli-cies, either expressed or implied, of the U.S. Gov-ernment. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cross-lingual word embeddings for low-resource language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Makarucha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="937" to="947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.01925</idno>
		<title level="m">Massively multilingual word embeddings</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The interplay of semantics and morphology in word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oded</forename><surname>Avraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno>abs/1704.01938</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R√©jean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Phonologically aware neural model for named entity recognition in low resource transfer settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Bharadwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mortensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1462" to="1472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Compositional morphology for word representations and language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1899" to="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Joint learning of character and word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan-Bo</forename><surname>Luan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1236" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Morphological word-embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch√ºtze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL</title>
		<meeting><address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1287" to="1292" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.09403</idno>
		<title level="m">Learning crosslingual word embeddings without bilingual corpora</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bilbowa: Fast bilingual distributed representations without word alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="748" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hindi morphological analyzer and generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gupreet</forename><surname>Singh Lehal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 First International Conference on Emerging Trends in Engineering and Technology</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1156" to="1159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning bilingual lexicons from monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="771" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Glottolog 2.0. Max Planck Institute for the Science of Human History</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Hammarstr√∂m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Bank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Forkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Haspelmath</surname></persName>
		</author>
		<idno>Ac- cessed on 2018-05-21</idno>
		<ptr target="http://glottolog.org" />
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>Jena</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploring cross-lingual transfer of morphological knowledge in sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Subword and Character Level Models in NLP</title>
		<meeting>the First Workshop on Subword and Character Level Models in NLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="70" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cross-lingual transfer learning for pos tagging without cross-lingual resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo-Kyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Fosler-Lussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2832" to="2838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Fermandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional LSTM-CNNsCRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Statistical morphological analyzer for hindi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Kumar Malladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashanth</forename><surname>Mannem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Joint Conference on Natural Language Processing</title>
		<meeting><address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1007" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cheap translation for cross-lingual named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Tse</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2536" to="2545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Epitran: Precision G2P for many languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Mortensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Dalmia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Littell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A survey of named entity recognition and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>John Benjamins Publishing Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">XNMT: The extensible neural machine translation toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarguna</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devendra</forename><surname>Singh Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachid</forename><surname>Riad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the Association for Machine Translation in the Americas (AMTA) Open Source Software Showcase</title>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Two-level description of Turkish morphology. Literary and Linguistic Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kemal</forename><surname>Oflazer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="137" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">When and why are pre-trained word embeddings useful for neural machine translation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devendra</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarguna</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="529" to="535" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A survey of cross-lingual embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04902</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The role of context in neural morphological disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinlan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Tagtow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Littell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">S-net: From answer extraction to answer generation for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04815</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sentiment embeddings with applications to sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Knowl. and Data Eng</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="496" to="509" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cross-lingual named entity recognition via wikification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Tse</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="219" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cross-lingual bridges with models of lexical borrowing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="93" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02789</idno>
		<title level="m">Charagram: Embedding words and sentences via character n-grams</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improve Chinese word embeddings by exploiting internal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanhuan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLTNAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Implicitly incorporating morphological information into word embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02481</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Refining word embeddings using intensity scores for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chih</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Robert</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="671" to="681" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Transfer learning for lowresource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02201</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
