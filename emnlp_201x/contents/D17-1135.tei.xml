<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Chinese Zero Pronoun Resolution with Deep Memory Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Chinese Zero Pronoun Resolution with Deep Memory Network</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1309" to="1318"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Existing approaches for Chinese zero pronoun resolution typically utilize only syn-tactical and lexical features while ignoring semantic information. The fundamental reason is that zero pronouns have no descriptive information, which brings difficulty in explicitly capturing their semantic similarities with antecedents. Meanwhile, representing zero pronouns is challenging since they are merely gaps that convey no actual content. In this paper, we address this issue by building a deep memory network that is capable of encoding zero pronouns into vector representations with information obtained from their contexts and potential antecedents. Consequently, our resolver takes advantage of semantic information by using these continuous distributed representations. Experiments on the OntoNotes 5.0 dataset show that the proposed memory network could substantially outperform the state-of-the-art systems in various experimental settings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A zero pronoun (ZP) is a gap in a sentence, which refers to an entity that supplies the neces- sary information for interpreting the gap ( <ref type="bibr" target="#b42">Zhao and Ng, 2007)</ref>. A ZP can be either anaphoric if it corefers to one or more preceding noun phrases (antecedents) in the associated text, or non-anaphoric if there are no such noun phrases. Below is an example of ZPs and their antecedents, where "φ" denotes the ZP.</p><p>[警方] 表示 他们 自杀 的 可能性 很高， 不 过 φ 1 也 不 排除 φ 2 有 他杀 的 可能。 * Email corresponding.</p><p>([The police] said that they are more likely to commit suicide, but φ 1 could not rule out φ 2 the possibility of homicide.)</p><p>In this example, the ZP "φ 1 " is an anaphoric ZP that refers to the antecedent "警方/The police" while the ZP "φ 2 " is non-anaphoric. Unlike overt pronouns, ZPs lack grammatical attributes such as gender and number that have been proven to be essential in pronoun resolution <ref type="bibr" target="#b3">(Chen and Ng, 2014a)</ref>, which makes ZP resolution a more chal- lenging task than overt pronoun resolution.</p><p>Automatic Chinese ZP resolution is typically composed of two steps, i.e., anaphoric zero pro- noun (AZP) identification that identifies whether a ZP is anaphoric; and AZP resolution, which deter- mines antecedents for AZPs. For AZP identifica- tion, state-of-the-art resolvers use machine learn- ing algorithms to build AZP classifiers in a super- vised manner <ref type="bibr">Ng, 2013, 2016</ref>). For AZP resolution, literature approaches include un- supervised methods <ref type="bibr">Ng, 2014b, 2015)</ref>, feature-based supervised models ( <ref type="bibr" target="#b42">Zhao and Ng, 2007;</ref><ref type="bibr" target="#b22">Kong and Zhou, 2010)</ref>, and neural network models <ref type="bibr" target="#b6">(Chen and Ng, 2016)</ref>. Neural network models for AZP resolution are of growing interest for their capacity to learn task-specific represen- tations without extensive feature engineering and to effectively exploit lexical information for ZPs and their candidate antecedents in a more scalable manner than feature-based models.</p><p>Despite these advantages, existing supervised approaches ( <ref type="bibr" target="#b42">Zhao and Ng, 2007;</ref><ref type="bibr">Ng, 2013, 2016)</ref> for AZP resolution typically utilize only syntactical and lexical information through features. They overlook semantic information that is regarded as an important factor in the resolution of common noun phrases <ref type="bibr" target="#b27">(Ng, 2007)</ref>. The fun- damental reason is that ZPs have no descriptive information, which results in difficulty in calcu- lating semantic similarities and relatedness scores between the ZPs and their antecedents. Therefore, the proper representations of ZPs are required so as to take advantage of semantic information when resolving ZPs. However, representing ZPs is chal- lenging because they are merely gaps that convey no actual content.</p><p>One straightforward method to address this is- sue is to represent ZPs with supplemental informa- tion provided by some available components, such as contexts and candidate antecedents. Motivated by <ref type="bibr" target="#b6">Chen and Ng (2016)</ref> who encode a ZP's lex- ical contexts by utilizing its preceding word and governing verb, we notice that a ZP's context can help to describe the ZP itself. As an example of its usefulness, given the sentence "φ taste spicy", people may resolve the ZP "φ" to the candidate an- tecedent "red peppers", but can hardly regard "my shoes" as its antecedent, because they naturally look at the ZP's context "taste spicy" to resolve it ("my shoes" cannot "taste spicy"). Meanwhile, considering that the antecedents of a ZP provide the necessary information for interpreting the gap (ZP), it is a natural way to express a ZP by its po- tential antecedents. However, only some subsets of candidate antecedents are needed to represent a ZP 1 . To achieve this goal, a desirable solution should be capable of explicitly capturing the im- portance of each candidate antecedent and using them to build up the representation for the ZP.</p><p>In this paper, inspired by the recent success of computational models with attention mechanism and explicit memory <ref type="bibr" target="#b34">(Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b37">Tang et al., 2016;</ref><ref type="bibr" target="#b23">Kumar et al., 2015)</ref>, we focus on AZP resolution, proposing the zero pronoun- specific memory network (ZPMN) that is com- petent for representing a ZP with information ob- tained from its contexts and candidate antecedents. These representations provide our system with an ability to take advantage of semantic information when resolving ZPs. Our ZPMN consists of mul- tiple computational layers with shared parameters. With the underlying intuition that not all candidate antecedents are equally relevant for representing the ZP, we develop each computational layer as an attention-based model, which first learns the im- portance of each candidate antecedent and then utilizes this information to calculate the continu-ous distributed representation of the ZP. The at- tention weights over candidate antecedents with respect to the ZP's representation obtained by the last layer are regarded as the ZP coreference clas- sification result. Given that every component is differentiable, the entire model could be efficiently trained end-to-end with gradient descent.</p><p>We evaluate our method on the Chinese portions of the OntoNotes 5.0 corpus by comparing with the baseline systems in different experimental set- tings. Results show that our approach significantly outperforms the baseline algorithms and achieves state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Zero Pronoun-specific Memory Network</head><p>We describe our deep memory network approach for AZP resolution in this section. We first give an overview of our model and then describe its com- ponents. Finally, we present the training and ini- tialization details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">An Overview of the Method</head><p>In this part, we present an overview of the zero pronoun-specific memory network (ZPMN) for AZP resolution. Given an AZP zp, we first ex- tract a set of candidate antecedents. Following <ref type="bibr" target="#b6">Chen and Ng (2016)</ref>, we regard all and only those maximal or modifier noun phrases (NPs) that pre- cede zp in the associated text and are at most two sentences away from it, to be its candidate an- tecedents. Suppose k candidate antecedents are extracted, our task is to determine the correct an- tecedent of zp from its candidate antecedent set A(zp) = {c 1 , c 2 , ..., c k }. Specifically, these candidate antecedents are represented in form of vectors {v c 1 , v c 2 , ..., v c k }, which are stacked and regarded as the external memory mem ∈ R l×k , where l is the dimen- sion of v c . Meanwhile, we represent each word as a continuous and real-valued vector, which is known as word embedding ( <ref type="bibr" target="#b1">Bengio et al., 2003)</ref>. These word vectors can be randomly initialized, or be pre-trained from text corpus with learning al- gorithms <ref type="bibr" target="#b26">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b28">Pennington et al., 2014)</ref>. In this work, we adopt the latter strategy since it can better exploit the semantics of words. All the word vectors are stacked in a word embed- ding matrix L w ∈ R d×|V | , where d is the dimen- sion of the word vector and |V | is the size of the word vocabulary. The embedding of word w is notated as e ∈ R d×1 , which is the column in L w .</p><p>An illustration of ZPMN is given in <ref type="figure" target="#fig_0">Figure 1</ref>, which is inspired by the memory network utilized in question answering ( <ref type="bibr" target="#b34">Sukhbaatar et al., 2015)</ref>. Our model consists of multiple computational lay- ers, each of which contains an attention layer and a linear layer. First, we represent the AZP zp by uti- lizing its contextual information, that is, propos- ing the ZP-centered LSTM that encodes zp into its distributed vector representation (i.e. v zp in <ref type="figure" target="#fig_0">Figure 1</ref>). We then regard v zp as the initial rep- resentation of zp, and feed it as the input to the first computational layer (hop 1). In the first com- putational layer, we calculate the attention weight across the AZP for each candidate antecedent, by which our model adaptively selects important in- formation from the external memory (candidate antecedents). The output of the attention layer and the linear transformation of v zp are summed to- gether as the input of to the next layer (hop 2).</p><p>We stack multiple hops by repeating the same process for multiple times in a similar manner. We call the abstractive information obtained from the external memory the "key extension" of the AZP. Note that the attention and linear layer parame- ters are shared in different hops. Regardless of the number of hops the model employs, they uti- lize the same number of parameters. Finally, after going through all the hops, we regard the atten- tion weight of each candidate antecedent with re- spect to the AZP representation generated by the last hop as the probability that the candidate an- tecedent is the correct antecedent, and predict the highest-scoring (most probable) one to be the an- tecedent of the given AZP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Modeling Zero Pronouns by Contexts</head><p>A vector representation of AZP is required when computing the ZPMN. As aforementioned, a ZP contains no actual content, it is therefore needed to employ some supplemental information to gener- ate its initial representation. To achieve this goal, we develop the ZP-centered LSTM that encodes an AZP into a vector representation by utilizing its contextual information.</p><p>Admittedly, one efficient method to model a variable-length sequence of words (context words) is to utilize a recurrent neural network <ref type="bibr" target="#b9">(Elman, 1991)</ref>. A recurrent neural network (RNN) stores the sequence history in a real-valued history vec- tor, which captures information of the whole se- quence. LSTM <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber, 1997</ref>) is one of the classical variations of RNN that mitigate the gradient vanish problem of RNN. As- suming x = {x 1 , x 2 , ..., x n } is an input sequence, each time step t has an input x t and a hidden state h t . The internal mechanics of the LSTM is defined by:</p><formula xml:id="formula_0">i t = σ(W (i) · [x t ; h t−1 ] + b (i) )</formula><p>(1)</p><formula xml:id="formula_1">f t = σ(W (f ) · [x t ; h t−1 ] + b (f ) )<label>(2)</label></formula><formula xml:id="formula_2">o t = σ(W (o) · [x t ; h t−1 ] + b (o) ) (3) ˜ C t = tanh(W (c) · [x t ; h t−1 ] + b (c) ) (4) C t = i t ˜ C t + f t C t−1 (5) h t = o t tanh(C t ) (6)</formula><p>where is an element-wise product and</p><formula xml:id="formula_3">W (i) , b (i) , W (f ) , b (f ) , W (o) , b (o) , W (c)</formula><p>, and b (c) are the parameters of the LSTM network. Intuitively, the words near an AZP generally contain richer information to express it. To bet- <ref type="figure">Figure 2</ref>: ZP-centered LSTM for encoding the AZP by its context words. w i means the i-th word in the sentence, w zp−i is the i-th last word before the ZP and w zp+i is the i-th word behind the ZP.</p><formula xml:id="formula_4">LSTMp LSTMp LSTMp ...... 1 w 2 w 1 zp w  1 h   2 h   1 zp h   LSTMf LSTMf LSTMf ...... n w 1 n w  1 zp w  n h   1 n h   1 zp h   zp v ZP</formula><p>ter utilize the information of words surrounding the AZP, on the basis of the traditional LSTM, we propose the ZP-centered LSTM to encode the AZPs. A graphical representation of this model is displayed in <ref type="figure">Figure 2</ref>. Specifically, the ZP- centered LSTM contains two standard LSTM neu- ral networks, i.e., the LSTM p that encodes the pre- ceding context of the AZP in a left-to-right man- ner, and the LSTM f that models the following context in the reverse direction. Ideally, the ZP- centered LSTM models the preceding and follow- ing contexts of the AZP separately, so that the words near the AZP are regarded as the last hid- den units and could contribute more in represent- ing the AZP. Afterward, we obtain the represen- tation of the AZP by concatenating the last hid- den vectors of LSTM p and LSTM f , which sum- marizes the useful contextual information centered around the AZP. Averaging or summing the last hidden vectors of LSTM p and LSTM f could also be attempted as alternatives. We regard it as the initial vector representation of the AZP and feed it to the first computational layer to go through the remaining procedures of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Generating the External Memory</head><p>We describe our method for generating the exter- nal memory in this subsection. For a given AZP, a set of noun phrases (NPs) is extracted as its can- didate antecedents. Specifically, we generate the external memory by utilizing these candidate an- tecedents. One way to encode an NP candidate is to utilize its head word embedding <ref type="bibr" target="#b6">(Chen and Ng, 2016)</ref>. However, this method has a major draw- back of not utilizing contextual information that is essential for representing a phrase. Besides, some approaches ( <ref type="bibr" target="#b33">Socher et al., 2013;</ref><ref type="bibr" target="#b35">Sun et al., 2015)</ref> encode a phrase by utilizing the average word em- bedding it contains. We argue that such an aver- aging operation simply treats all the words in a phrase equally, which is inaccurate because some words might be more informative than others.</p><p>A helpful property of LSTM is that it could keep useful history information in the mem- ory cell by exploiting input, output and forget gates to decide how to utilize and update the memory of previous information. Given a se- quence of words {w 1 , w 2 , ..., w n }, previous re- search ( <ref type="bibr" target="#b36">Sutskever et al., 2014</ref>) utilizes the last hid- den vector of LSTM to represent the information of the whole sequence. For word w t in a sequence, its corresponding hidden vector h t can capture useful information before and including w t .</p><formula xml:id="formula_5">... ... ... ... 1 c w  [ ] c m w 1 c w  n w 1 w [ ] 1 c c m c v h h      ' ' [1] 1 c c c v h h      ... ...</formula><p>... Inspired by this, we propose a novel method to produce representations of the candidate an- tecedents by utilizing both their contexts and con- tent words. Specifically, we use the subtraction be- tween LSTM hidden vectors to encode the candi-date antecedents, as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. Given a candidate antecedent c with m words, two stan- dard LSTM neural networks are employed for en- coding c in the forward and backward direction, respectively. For the forward LSTM, we extract a sequence of words related with c in a left-to-right manner, i.e., <ref type="figure" target="#fig_0">{w 1 , w 2 , ..., w c−1 , w c[1]</ref> , </p><note type="other">..., w c[m] }. Subsequently, the forward vector representation of c can be calculated as − → v c = h c[m] − h c−1 , where h c</note><formula xml:id="formula_6">v c = − → v c || ← − v c .</formula><p>This method enables our model to encode a can- didate antecedent by the information both outside and inside the phrase, which provides our model a strong ability to access to sentence-level infor- mation when modeling the candidate antecedents. In this manner, we generate the vector repre- sentations of the candidate antecedents, and re- gard them as the external memory, i.e., mem = {v c 1 , v c 2 , ..., v c k }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Attention Mechanism</head><p>In this part, we introduce our attention mecha- nism. This strategy has been widely used in many nature language processing tasks, such as fac- toid question answering ( ), entailment (Rocktäschel et al., 2015) and disflu- ency detection ( <ref type="bibr" target="#b38">Wang et al., 2016</ref>). The basic idea of attention mechanism is that it assigns a weight/importance to each lower position when computing an upper-level representation (Bah- danau et al., 2015). With the underlying intuition that not all candidate antecedents are equally rel- evant for representing the AZP, we employ the attention mechanism as to dynamically align the more informative candidate antecedents from the external memory, mem = {v c 1 , v c 2 , ..., v c k } with regard to the given AZP, and use them to build up the representation of the AZP.</p><p>As shown in <ref type="bibr" target="#b6">Chen and Ng (2016)</ref>, traditional hand-crafted features are crucial for the resolver's success since they capture the syntactic, positional and other relationships between an AZP and its candidate antecedents. Therefore, to evaluate the importance of each candidate antecedent in a com- prehensive manner, following <ref type="bibr" target="#b6">Chen and Ng (2016)</ref> who encode hand-crafted features as inputs to their network, we integrate a set of features that are uti- lized in <ref type="bibr" target="#b6">Chen and Ng (2016)</ref>, in the form of vec- tor (v (f eature) ) into our attention model. For each multi-valued feature, we convert it into a corre- sponding set of binary-valued features 2 . Specifically, for the t-th candidate antecedent in the memory, v ct , taking the vector representation of the AZP v zp and the corresponding feature vec- tor v (f eature) t as inputs, we compute the attention score as</p><formula xml:id="formula_7">α t = G(v ct , v zp , v (f eature) t</formula><p>). The scoring function G is defined by:</p><formula xml:id="formula_8">s t = tanh(W (att) · [v ct ; v zp ; v (f eature) t ] + b (att) )<label>(7)</label></formula><formula xml:id="formula_9">α t = exp(s t ) k t =1 exp(s t )<label>(8)</label></formula><p>where W (att) and b (att) are the attention parame- ters and k indicates the number of candidate an- tecedents. After obtaining the attention scores for all the candidate antecedents {a 1 , a 2 , ..., a k }, our attention layer outputs a continuous vector vec that is computed as the weighted sum of each piece of memory in mem:</p><formula xml:id="formula_10">vec = k i=1 α i v c i<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Training Details</head><p>We initialize our word embeddings with 100 di- mensional ones produced by the word2vec toolkit ( <ref type="bibr" target="#b26">Mikolov et al., 2013</ref>) on the Chinese portion of the training data from the OntoNotes 5.0 corpus. We randomly initialize the parameters from a uni- form distribution U (−0.03, 0.03) and minimize the training objective using stochastic gradient de- scent with learning rate equals to 0.01. In addition, to regularize the network, we apply L2 regulariza- tion to the network weights and dropout with a rate of 0.5 on the output of each hidden layer.</p><p>The model is trained in a supervised manner by minimizing the cross-entropy error of ZP corefer- ence classification. Suppose the training set con- tains N AZPs {zp 1 , zp 2 , ..., zp N }. Let A(zp i ) de- note the set of candidate antecedents of an AZP zp i , and P (c|zp i ) represents the probability of predicting candidate c as the antecedent of zp i (i.e., the attention weight of candidate antecedent c with respect to the AZP representation generated by the last hop), the loss is given by:</p><formula xml:id="formula_11">loss = − N i=1 c∈A(zp i ) δ(zp i , c)log(P (c|zp i ))</formula><p>(10) where δ(zp, c) is 1 or 0, indicating whether zp and c are coreferent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>Datasets: Following Chen and Ng <ref type="bibr">(2016,</ref><ref type="bibr">2015)</ref>, we run experiments on the Chinese portion of the OntoNotes Release 5.0 dataset 3 used in the <ref type="bibr">CoNLL 2012</ref><ref type="bibr">Shared Task (Pradhan et al., 2012</ref>). The dataset consists of three parts, i.e., a training set, a development set and a test set. Since only the training set and the development set contain ZP coreference annotations, we train our model on the training set and utilize the development set for testing purposes. Meanwhile, we reserve 20% of the training set as a held-out development set for tuning the hyperparameters of our network. The same experimental data setting is utilized in the baseline system <ref type="bibr" target="#b6">(Chen and Ng, 2016)</ref>. <ref type="table" target="#tab_1">Table 1</ref> shows the statistics of our corpus. Besides, doc- uments in the datasets come from six sources, i.e., broadcast news (BN), newswires (NW), broadcast conversations (BC), telephone conversations (TC), web blogs (WB) and magazines (MZ  Evaluation metrics: Same as previous studies on Chinese ZP resolution ( <ref type="bibr" target="#b42">Zhao and Ng, 2007;</ref><ref type="bibr" target="#b6">Chen and Ng, 2016)</ref>, we use three metrics to evaluate the quality of our model: recall, precision and F-score (denoted as R, P and F, respectively).</p><p>Experimental settings: We employ three Chinese ZP resolution systems as our baselines, i.e., <ref type="bibr" target="#b42">Zhao and Ng (2007)</ref>; <ref type="bibr">Ng (2015, 2016)</ref>. Con- sistent with <ref type="bibr">Ng (2015, 2016)</ref>, three ex- perimental settings are designed to evaluate our approach. In Setting 1, we directly employ the gold syntactic parse trees and gold AZPs that are obtained from the OntoNotes dataset. In Setting 2, we utilize gold syntactic parse trees and system (automatically identified) AZPs <ref type="bibr">4</ref> . In Setting 3, we employ system AZP and system syntactic parse trees that obtained through the Berkeley parser 5 , which is the state-of-the-art parsing model. <ref type="table" target="#tab_3">Table 2</ref> shows the experimental results of the base- line systems and our model on entire test set. Our approach is abbreviated to ZPMN (k), where k indicates the number of hops. The best meth- ods in each of the three experimental settings are in bold text. From <ref type="table" target="#tab_3">Table 2</ref>, we can ob- serve that our approach outperforms all previous baseline systems by a substantial margin. Mean- while, among all our models from single hop to six hops, using more computational layers could generally lead to better performance. The best per- formance is achieved by the model with six hops under experimental Setting 1 and 2, and with four hops in experimental Setting 3. Furthermore, the ZPMN (with six hops) significantly outperforms the state-of-the-art baseline system (Chen and Ng, 2016) under three experimental settings by 2.7%, 2.7%, and 3.9% in terms of overall F-score 6 , re- spectively. In all words, our model is an ex- tremely strong performer and substantially outper- forms baseline methods, which demonstrate the efficiency of the proposed zero pronoun-specific memory network. It is well accepted that computational models that are composed of multiple processing layers could learn representations of data with multiple levels of abstraction ( <ref type="bibr" target="#b24">LeCun et al., 2015</ref>). In our approach, multiple computation layers allow the model to learn representations of AZPs with mul- tiple levels of abstraction generated by candidate antecedents. Each layer/hop retrieves important candidate antecedents, and transforms the repre-   sentation at previous level into a representation at a higher, slightly more abstract level. We regard this representation as the "key extension" of the AZP, by which our model learns to encode the AZP in an efficient manner. For per-source results, we conduct experiments by comparing the ZPMN (with six hops) with the state-of-the-art baseline system <ref type="bibr" target="#b6">(Chen and Ng, 2016)</ref> on six sources of test data, as shown in Ta- ble 3. The rows in <ref type="table" target="#tab_4">Table 3</ref> are the experimental re- sults from different sources under the three exper- imental settings. In experimental Settings 1 and 3, ZPMN improves results further across all the six sources of data. Under experimental Setting 2, our model outperforms the baseline system in five of the six sources of data, only slightly under- performs in source TC. All these prove that our approach achieves a considerable improvement in Chinese ZP resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results</head><note type="other">Setting 1 Setting 2 Setting 3 Gold Parse + Gold</note><p>Moreover, to evaluate the effectiveness of our methods for modeling the AZP and candidate an- tecedents proposed in Section 2.2 and 2.3, we compare with three models that are all simpli- fied versions of the ZPMN, namely, ZPCon- textFree where an AZP is initially represented by its governing verb and preceding word; AntCon- tentAvg where the candidate antecedents are en- coded by their averaged content word embed- dings; and AntContentHead where each candi- date antecedent is represented by the embedding of its head word. To make comparison as fair as possible, we keep the other parts of these mod- els unchanged from the ZPMN with six compu- tational layers (hop 6). To minimize the external influence, we run experiments under experimen- tal Setting 1 (gold parse and gold AZPs). <ref type="table" target="#tab_6">Table 4</ref> shows the results.  With an intuition that contexts of an AZP pro- vide more sufficient information than only a few specific of words in expressing the AZP, the per- formance of ZPContextFree is unsurprisingly worse than that of the ZPMN, which reflects the effects of the ZP-centered LSTM proposed to gen- erate the initial representation for the AZP. In addition, the performance of AntContentAvg is relatively low. We attribute this to the model assigning the same importance to all the con- tent words in a phrase, which causes difficulty for the model to capture informative words in a candidate antecedent. Meanwhile, AntContent-Head only models limited information when en- coding candidate antecedents, thereby underper- forms the ZPMN whose external memory con- tains sentence-level information both outside and inside the candidate antecedents. These demon- strate the utility of the method for modeling can- didate antecedents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Attention Model Visualization</head><p>这次 近 50 年 来 印度 发生 的 T 强烈 地b b级 强，φ 波及 范围 N，印度 a国I 尼泊尔 也 受到 了 影响 c</p><p>The earthqua-e that ,5 the 5tro0ge5t o0e occur5 ,0 I0d,a w,th,0 rece0t 50 year5 ha5 a h,gh  mag0,tude, φ ,0f.ue0ce5 a .arge ra0ge of area5, a0d the 0e,ghbour,0g cou0try of I0d,a .,-e Nepa. ,5 a.5o affected. To obtain a better understanding of our deep memory network, we visualize the attention weights of the ZPMN, as is shown in <ref type="figure" target="#fig_3">Figure 4</ref>. We can observe that in the first three hops, the fourth candidate "中国红十字会/Red Cross Soci- ety of China" gains a higher attention weight than the others. Nevertheless, in hop 5 and 6, the atten- tion weight of "这次...强烈的地震/the earthquake that ... in India" increases and the model finally predicts it correctly as the antecedent. This case illustrates the effects of multiple hops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Zero Pronoun Resolution</head><p>Chinese zero pronoun resolution. Early stud- ies utilize heuristic rules to resolve ZPs in Chi- nese <ref type="bibr" target="#b7">(Converse, 2006;</ref><ref type="bibr" target="#b41">Yeh and Chen, 2007)</ref>. More recently, supervised approaches have been vastly explored. <ref type="bibr" target="#b42">Zhao and Ng (2007)</ref> first present a machine learning approach to identify and resolve ZPs. By employing the J48 de- cision tree algorithm, various kinds of features are integrated into their model. <ref type="bibr" target="#b22">Kong and Zhou (2010)</ref> develop a kernel-based approach, employ- ing context-sensitive convolution tree kernels to model syntactic information. <ref type="bibr" target="#b2">Chen and Ng (2013)</ref> further extend the study of <ref type="bibr" target="#b42">Zhao and Ng (2007)</ref> by proposing several novel features and introduc- ing the coreference links between ZPs. Despite the effectiveness of feature engineering, it is labor in- tensive and highly relies on annotated corpus. To handle these weaknesses, <ref type="bibr" target="#b4">Chen and Ng (2014b)</ref> propose an unsupervised method. They first re- cover each ZP into ten overt pronouns and then apply a ranking model to rank the antecedents. <ref type="bibr" target="#b5">Chen and Ng (2015)</ref> propose an end-to-end unsu- pervised probabilistic model, utilizing a salience model to capture discourse information. In recent years, Chen and Ng (2016) develop a deep neural network approach to learn useful task-specific rep- resentations and effectively exploit lexical features through word embeddings. Different from previ- ous studies, in this work, we propose a novel mem- ory network to perform the task. By encoding ZPs and candidate antecedents through the composi- tion of texts based on the representation of words, our model benefits from the semantic information when resolving the ZPs.</p><p>Zero pronoun resolution for other languages. There have been various studies on ZP resolution for other languages besides Chinese. <ref type="bibr" target="#b10">Ferrández and Peral (2000)</ref> propose a set of hand-crafted rules for resolving ZPs in Spanish texts. Recently, supervised approaches have been widely exploited for ZP resolution in <ref type="bibr">Korean (Han, 2006</ref>), Ital- ian ( <ref type="bibr" target="#b16">Iida and Poesio, 2011</ref>) and Japanese ( <ref type="bibr" target="#b21">Isozaki and Hirao, 2003;</ref><ref type="bibr" target="#b14">Iida et al., 2006</ref><ref type="bibr" target="#b15">Iida et al., , 2007</ref><ref type="bibr" target="#b19">Imamura et al., 2009;</ref><ref type="bibr" target="#b32">Sasano and Kurohashi, 2011;</ref><ref type="bibr" target="#b16">Iida and Poesio, 2011;</ref><ref type="bibr" target="#b17">Iida et al., 2015)</ref>. <ref type="bibr" target="#b18">Iida et al. (2016)</ref> propose a multi-column convolutional neu- ral network for Japanese intra-sentential subject zero anaphora resolution, where both the surface word sequence and dependency tree of a target sentence are exploited as clues in their model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Attention and Memory Network</head><p>Attention mechanisms have been widely used in many studies and have achieved promising perfor- mances on a variety of NLP tasks <ref type="bibr" target="#b30">(Rocktäschel et al., 2015;</ref><ref type="bibr">Rush et al., 2015;</ref>. Recently, the memory network has been proposed and applied to question answering task <ref type="bibr">(Weston et al., 2014)</ref>, which is defined to have four compo-nents: input (I), generalization (G), output (O) and response (R). After then, memory networks have been adopted in many other NLP tasks, such as aspect sentiment classification ( <ref type="bibr" target="#b37">Tang et al., 2016)</ref>, dialog systems ( <ref type="bibr" target="#b8">Dodge et al., 2015)</ref>, and informa- tion extraction <ref type="bibr" target="#b40">(Xiaocheng et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this study, we propose a novel zero pronoun- specific memory network that is capable of en- coding zero pronouns into the vector represen- tations with supplemental information obtained from their contexts and candidate antecedents. Consequently, these continuous distributed vec- tors provide our model with an ability to take ad- vantage of the semantic information when resolv- ing zero pronouns. We evaluate our method on the Chinese portion of OntoNotes 5.0 dataset and report substantial improvements over the state-of- the-art systems in various experimental settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the zero pronoun-specific memory network with three computational layers (hops). v zp and v c denote the vector representation of an AZP and its candidate antecedents. The left part in dashed box shows the details of the first hop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration for modeling a candidate antecedent through its context and content words. Candi represents the candidate antecedent. Suppose the candidate antecedent contains m words, w c[j] denotes its j-th word. w i is the i-th word in the sentence, and w c+1(−1) is the word appears immediately after (before) the candidate antecedent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>[m] and h c−1 indicate the hidden vec- tors of the forward LSTM corresponding to w c[m] and w c−1 , respectively. Meanwhile, the back- ward LSTM models a sequence of words that are extracted in the reverse direction, that is, {w n , w n−1 , ..., w c+1 , w c[m] , ..., w c[1] }. We then perform the similar operation, computing the backward representation of c as ← − v c = h c[1] −h c+1 , where h c[1] and h c+1 indicate the hidden vectors of the backward LSTM corresponding to w c[1] and w c+1 . Finally, we concatenate these two vectors together as the ultimate vector representation of c,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example of attention weights in different hops. ZP is denoted as φ. The rows show the attention weights of candidates in each hop. Darker color means higher weight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Documents Sentences Words 
AZPs 
Training 
1,391 
36,487 
756K 
12,111 
Test 
172 
6,083 
110K 
1,713 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Statistics on the training and test corpus.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Experimental results on the test data. ZPMN represents the proposed zero pronoun-specific 
memory network model, and the number beside ZPMN in each row denotes the number of hops. 

Setting 1: Gold Parse + Gold AZP 
Setting 2: Gold Parse + System AZP 
Setting 3: System Parse + System AZP 

Baseline 
ZPMN 
Baseline 
ZPMN 
Baseline 
ZPMN 
R 
P 
F 
R 
P 
F 
R 
P 
F 
R 
P 
F 
R 
P 
F 
R 
P 
F 
NW 
48.8 48.8 48.8 48.8 48.8 48.8 34.5 26.4 29.9 39.5 34.3 36.7 11.9 12.8 12.3 21.0 19.9 20.5 
MZ 
41.4 41.6 41.5 46.3 46.3 46.3 34.0 22.4 27.0 34.6 35.0 34.8 9.3 7.3 8.2 
17.1 15.7 16.4 
WB 
56.3 56.3 56.3 59.8 59.8 59.8 44.7 25.1 32.2 41.2 28.7 33.8 23.9 16.1 19.2 31.3 17.6 22.6 
BN 
55.4 55.4 55.4 58.2 58.6 58.4 36.9 31.9 34.2 43.8 30.0 35.6 22.1 23.2 22.6 35.1 20.7 26.1 
BC 
50.4 51.3 50.8 52.9 53.6 53.2 37.6 25.6 30.5 35.6 29.4 32.2 21.2 14.6 17.3 25.6 15.6 19.4 
TC 
51.9 54.2 53.1 54.8 54.8 54.8 46.3 29.0 35.6 36.9 32.9 34.8 31.4 15.9 21.1 33.2 21.0 25.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Experimental results on each source of test data. The strongest F-score in each row is in bold.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 : Experimental results of different models.</head><label>4</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> A common way to do this task is to first extract a set of candidate antecedents, and then select antecedents from the candidate set. Therefore, only those candidates who are possibly the correct antecedent of the given ZP are suitable for interpreting it.</note>

			<note place="foot" n="2"> If one feature has k different values, we will convert it into k binary features.</note>

			<note place="foot" n="3"> http://catalog.ldc.upenn.edu/LDC2013T19</note>

			<note place="foot" n="4"> In this study, we adopt the learning-based method utilized in (Chen and Ng, 2016) to identify system AZPs, including the location and identification of AZPs. 5 https://github.com/slavpetrov/berkeleyparser 6 All significance tests are paired t-tests, with p &lt; 0.05.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We greatly thank Yiming Cui and Xuxiang Wang for their tremendously helpful discussions. We also thank the anonymous reviewers for their valuable comments. This work was supported by the National High Technology Development 863 Program of China (No.2015AA015407), National Natural Science Foundation of China <ref type="bibr">(No.61472105 and No.61472107</ref>).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rejean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Chinese zero pronoun resolution: Some recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1360" to="1365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Chinese overt pronoun resolution: A bilingual approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1615" to="1621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Chinese zero pronoun resolution: An unsupervised approach combining ranking and integer linear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Chinese zero pronoun resolution: A joint unsupervised discourseaware model rivaling state-of-the-art resolvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">320</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Chinese zero pronoun resolution with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54rd Annual Meeting of the ACL</title>
		<meeting>the 54rd Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pronominal anaphora resolution in chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Susan P Converse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Evaluating prerequisite qualities for learning end-to-end dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06931</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed representations, simple recurrent networks, and grammatical structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="195" to="225" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A computational approach to zero-pronouns in spanish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Ferrández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesús</forename><surname>Peral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 38th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="166" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Korean zero pronouns: analysis and resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Na-Rae</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploiting syntactic patterns as clues in zero-anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="625" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Zero-anaphora resolution by learning rich syntactic pattern features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Asian Language Information Processing (TALIP)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A cross-lingual ilp solution to zero anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="804" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Intrasentential zero anaphora resolution using subject sharing recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chikara</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghoon</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Kloetzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP&apos;15</title>
		<meeting>EMNLP&apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2179" to="2189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Intrasentential subject zero anaphora resolution using multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Hoon</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canasai</forename><surname>Kruengkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Kloetzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discriminative approach to predicateargument structure analysis with zero-anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Imamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniko</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Izumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-IJCNLP</title>
		<meeting>the ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Conference Short Papers</title>
		<imprint>
			<biblScope unit="page" from="85" to="88" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Japanese zero pronoun resolution based on ranking rules and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 conference on Empirical methods in natural language processing</title>
		<meeting>the 2003 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="184" to="191" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A tree kernelbased unified framework for chinese zero anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="882" to="891" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07285</idno>
		<title level="m">Ask me anything: Dynamic memory networks for natural language processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Effective deep memory networks for distant supervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic class induction and coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AcL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="536" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Conll2012 shared task: Modeling multilingual unrestricted coreference in ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL-Shared Task</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kočisk`y, and Phil Blunsom</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.06664</idno>
	</analytic>
	<monogr>
		<title level="m">Reasoning about entailment with neural attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alexander M Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00685</idno>
		<title level="m">Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A discriminative approach to japanese zero anaphora resolution with large-scale lexicalized case frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="758" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling mention, context and entity with neural networks for entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhou</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1333" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Aspect level sentiment classification with deep memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A neural attention model for disfluency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaolei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="278" to="287" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint/>
	</monogr>
<note type="report_type">Bordes. 2014. Memory networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Effective deep memory networks for distant supervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Xiaocheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yongjie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Zero anaphora resolution in chinese with shallow parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Long</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chinese Language and Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="56" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Identification and resolution of chinese zero pronouns: A machine learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
