<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Decomposable Attention Model for Natural Language Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">A Decomposable Attention Model for Natural Language Inference</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2249" to="2255"/>
							<date type="published">November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
					<note>Ankur P. Parikh Google New York, NY Oscar Täckström Google New York, NY Dipanjan Das Google New York, NY Jakob Uszkoreit Google Mountain View, CA</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subprob-lems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural language inference (NLI) refers to the prob- lem of determining entailment and contradiction re- lationships between a premise and a hypothesis. NLI is a central problem in language understanding <ref type="bibr" target="#b8">(Katz, 1972;</ref><ref type="bibr" target="#b1">Bos and Markert, 2005;</ref><ref type="bibr" target="#b14">van Benthem, 2008;</ref><ref type="bibr">MacCartney and Manning, 2009)</ref> and recently the large SNLI corpus of 570K sentence pairs was cre- ated for this task <ref type="bibr" target="#b1">(Bowman et al., 2015</ref>). We present a new model for NLI and leverage this corpus for comparison with prior work.</p><p>A large body of work based on neural networks for text similarity tasks including NLI has been pub- lished in recent years ( <ref type="bibr" target="#b7">Hu et al., 2014;</ref><ref type="bibr">Rocktäschel et al., 2016</ref>; <ref type="bibr" target="#b15">Wang and Jiang, 2016;</ref><ref type="bibr" target="#b16">Yin et al., 2016</ref>, inter alia). The dominating trend in these models is to build complex, deep text representation models, for example, with convolutional networks ( <ref type="bibr" target="#b9">LeCun et al., 1990</ref>, CNNs henceforth) or long short-term mem- ory networks <ref type="bibr">(Hochreiter and Schmidhuber, 1997</ref>, LSTMs henceforth) with the goal of deeper sen- tence comprehension. While these approaches have yielded impressive results, they are often computa- tionally very expensive, and result in models having millions of parameters (excluding embeddings).</p><p>Here, we take a different approach, arguing that for natural language inference it can often suffice to simply align bits of local text substructure and then aggregate this information. For example, consider the following sentences:</p><p>• Bob is in his room, but because of the thunder and lightning outside, he cannot sleep.</p><p>• Bob is awake.</p><p>• It is sunny outside.</p><p>The first sentence is complex in structure and it is challenging to construct a compact representation that expresses its entire meaning. However, it is fairly easy to conclude that the second sentence follows from the first one, by simply aligning Bob with Bob and cannot sleep with awake and recognizing that these are synonyms. Similarly, one can conclude that It is sunny outside contradicts the first sentence, by aligning thunder and lightning with sunny and recognizing that these are most likely incompatible.</p><p>We leverage this intuition to build a simpler and more lightweight approach to NLI within a neural framework; with considerably fewer parameters, our model outperforms more complex existing neural ar- chitectures. In contrast to existing approaches, our approach only relies on alignment and is fully com- putationally decomposable with respect to the input text. An overview of our approach is given in <ref type="figure" target="#fig_0">Fig- ure 1</ref>. Given two sentences, where each word is repre-sented by an embedding vector, we first create a soft alignment matrix using neural attention ( <ref type="bibr">Bahdanau et al., 2015)</ref>. We then use the (soft) alignment to decompose the task into subproblems that are solved separately. Finally, the results of these subproblems are merged to produce the final classification. In ad- dition, we optionally apply intra-sentence attention <ref type="bibr" target="#b2">(Cheng et al., 2016</ref>) to endow the model with a richer encoding of substructures prior to the alignment step.</p><p>Asymptotically our approach does the same total work as a vanilla LSTM encoder, while being triv- ially parallelizable across sentence length, which can allow for considerable speedups in low-latency set- tings. Empirical results on the SNLI corpus show that our approach achieves state-of-the-art results, while using almost an order of magnitude fewer parameters compared to complex LSTM-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our method is motivated by the central role played by alignment in machine translation <ref type="bibr" target="#b9">(Koehn, 2009)</ref> and previous approaches to sentence similarity modeling ( <ref type="bibr">Haghighi et al., 2005;</ref><ref type="bibr" target="#b3">Das and Smith, 2009;</ref><ref type="bibr" target="#b1">Chang et al., 2010;</ref><ref type="bibr" target="#b5">Fader et al., 2013)</ref>, natural language inference <ref type="bibr">(Marsi and Krahmer, 2005;</ref><ref type="bibr" target="#b10">MacCartney et al., 2006;</ref><ref type="bibr">Hickl and Bensley, 2007;</ref><ref type="bibr" target="#b10">MacCartney et al., 2008)</ref>, and semantic parsing ( <ref type="bibr">Andreas et al., 2013)</ref>. The neural counterpart to alignment, atten- tion ( <ref type="bibr">Bahdanau et al., 2015)</ref>, which is a key part of our approach, was originally proposed and has been predominantly used in conjunction with <ref type="bibr">LSTMs (Rocktäschel et al., 2016;</ref><ref type="bibr" target="#b15">Wang and Jiang, 2016)</ref> and to a lesser extent with CNNs ( <ref type="bibr" target="#b16">Yin et al., 2016)</ref>. In contrast, our use of attention is purely based on word embeddings and our method essentially consists of feed-forward networks that operate largely indepen- dently of word order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Let a = (a 1 , . . . , a a ) and b = (b 1 , . . . , b b ) be the two input sentences of length a and b , re- spectively. We assume that each a i , b j ∈ R d is a word embedding vector of dimension d and that each sentence is prepended with a "NULL" token. Our training data comes in the form of labeled pairs  time, we receive a pair of sentences (a, b) and our goal is to predict the correct label y.</p><formula xml:id="formula_0">{a (n) , b (n) , y (n) } N n=1 , where y (n) = (y (n) 1 , . . . , y (n) C ) is</formula><formula xml:id="formula_1">Input representation. Let ¯ a = (¯ a 1 , . . . , ¯ a a ) and ¯ b = ( ¯ b 1 , . . . , ¯ b b )</formula><p>denote the input representation of each fragment that is fed to subsequent steps of the algorithm. The vanilla version of our model simply defines ¯ a := a and ¯ b := b. With this input rep- resentation, our model does not make use of word order. However, we discuss an extension using intra- sentence attention in Section 3.4 that uses a minimal amount of sequence information.</p><p>The core model consists of the following three components (see <ref type="figure" target="#fig_0">Figure 1</ref>), which are trained jointly:</p><p>Attend. First, soft-align the elements of ¯ a and ¯ b using a variant of neural attention ( <ref type="bibr">Bahdanau et al., 2015)</ref> and decompose the problem into the compari- son of aligned subphrases.</p><p>Compare. Second, separately compare each aligned subphrase to produce a set of vectors {v 1,i } a i=1 for a and {v 2,j } b j=1 for b. Each v 1,i is a nonlinear combination of a i and its (softly) aligned subphrase in b (and analogously for v 2,j ).</p><p>Aggregate. Finally, aggregate the sets</p><formula xml:id="formula_2">{v 1,i } a i=1</formula><p>and {v 2,j } b j=1 from the previous step and use the result to predict the labeî y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Attend</head><p>We first obtain unnormalized attention weights e ij , computed by a function F , which decomposes as:</p><formula xml:id="formula_3">e ij := F (¯ a i , ¯ b j ) := F (¯ a i ) T F ( ¯ b j ) .<label>(1)</label></formula><p>This decomposition avoids the quadratic complexity that would be associated with separately applying F a × b times. Instead, only a + b applications of F are needed. We take F to be a feed-forward neural network with ReLU activations <ref type="bibr" target="#b5">(Glorot et al., 2011</ref>).</p><p>These attention weights are normalized as follows:</p><formula xml:id="formula_4">β i := b j=1 exp(e ij ) b k=1 exp(e ik ) ¯ b j , α j := a i=1 exp(e ij ) a k=1 exp(e kj ) ¯ a i .<label>(2)</label></formula><p>Here β i is the subphrase in ¯ b that is (softly) aligned to ¯ a i and vice versa for α j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Compare</head><p>Next, we separately compare the aligned phrases</p><formula xml:id="formula_5">{(¯ a i , β i )} a i=1 and {( ¯ b j , α j )} b j=1</formula><p>using a function G, which in this work is again a feed-forward network:</p><formula xml:id="formula_6">v 1,i := G([¯ a i , β i ]) ∀i ∈ [1, . . . , a ] , v 2,j := G([ ¯ b j , α j ]) ∀j ∈ [1, . . . , b ] .<label>(3)</label></formula><p>where the brackets [·, ·] denote concatenation. Note that since there are only a linear number of terms in this case, we do not need to apply a decomposition as was done in the previous step. Thus G can jointly take into account both ¯ a i , and β i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Aggregate</head><p>We now have two sets of comparison vectors {v 1,i } a i=1 and {v 2,j } b j=1 . We first aggregate over each set by summation:</p><formula xml:id="formula_7">v 1 = a i=1 v 1,i , v 2 = b j=1 v 2,j .<label>(4)</label></formula><p>and feed the result through a final classifier H, that is a feed forward network followed by a linear layer:</p><formula xml:id="formula_8">ˆ y = H([v 1 , v 2 ]) ,<label>(5)</label></formula><p>wherê y ∈ R C represents the predicted (unnormal- ized) scores for each class and consequently the pre- dicted class is given byˆybyˆ byˆy = argmax i ˆ y i . For training, we use multi-class cross-entropy loss with dropout regularization ( <ref type="bibr" target="#b13">Srivastava et al., 2014</ref>):</p><formula xml:id="formula_9">L(θ F , θ G , θ H ) = 1 N N n=1 C c=1 y (n) c log exp(ˆ y c ) C c =1 exp(ˆ y c )</formula><p>.</p><p>Here θ F , θ G , θ H denote the learnable parameters of the functions F, G and H, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Intra-Sentence Attention (Optional)</head><p>In the above model, the input representations are simple word embeddings. However, we can augment this input representation with intra-sentence attention to encode compositional relationships between words within each sentence, as proposed by <ref type="bibr" target="#b2">Cheng et al. (2016)</ref>. Similar to Eqs. 1 and 2, we define</p><formula xml:id="formula_10">f ij := F intra (a i ) T F intra (a j ) ,<label>(6)</label></formula><p>where F intra is a feed-forward network. We then cre- ate the self-aligned phrases</p><formula xml:id="formula_11">a i := a j=1 exp(f ij + d i−j ) a k=1 exp(f ik + d i−k ) a j .<label>(7)</label></formula><p>The distance-sensitive bias terms d i−j ∈ R provides the model with a minimal amount of sequence infor- mation, while remaining parallelizable. These terms are bucketed such that all distances greater than 10 words share the same bias. The input representation for subsequent steps is then defined as</p><formula xml:id="formula_12">¯ a i := [a i , a i ] and analogously ¯ b i := [b i , b i ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Computational Complexity</head><p>We now discuss the asymptotic complexity of our approach and how it offers a higher degree of par- allelism than LSTM-based approaches. Recall that d denotes embedding dimension and means sen- tence length. For simplicity we assume that all hid- den dimensions are d and that the complexity of</p><formula xml:id="formula_13">matrix(d × d)-vector(d × 1) multiplication is O(d 2 ).</formula><p>A key assumption of our analysis is that &lt; d, which we believe is reasonable and is true of the SNLI dataset ( <ref type="bibr" target="#b1">Bowman et al., 2015)</ref> where &lt; 80, whereas recent LSTM-based approaches have used d ≥ 300. This assumption allows us to bound the complexity of computing the 2 attention weights.   Thus the total complexity of the model is O(d 2 + 2 d), which is equal to that of an LSTM with atten- tion. However, note that with the assumption that &lt; d, this becomes O(d 2 ) which is the same com- plexity as a regular LSTM. Moreover, unlike the LSTM, our approach has the advantage of being par- allelizable over , which can be useful at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complexity of LSTMs. The complexity of an</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate our approach on the Stanford Natural Language Inference (SNLI) dataset <ref type="bibr" target="#b1">(Bowman et al., 2015)</ref>. Given a sentences pair (a, b), the task is to predict whether b is entailed by a, b contradicts a, or whether their relationship is neutral.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>The method was implemented in TensorFlow ( <ref type="bibr" target="#b0">Abadi et al., 2015)</ref>.</p><p>Data preprocessing: Following Bowman et al. (2015), we remove examples labeled "-" (no gold label) from the dataset, which leaves 549,367 pairs for training, 9,842 for development, and 9,824 for testing. We use the tokenized sentences from the non-binary parse provided in the dataset and prepend each sentence with a "NULL" token. During training, each sentence was padded up to the maximum length of the batch for efficient training (the padding was explicitly masked out so as not to affect the objec- tive/gradients). For efficient batching in TensorFlow, we semi-sorted the training data to first contain ex- amples where both sentences had length less than 20, followed by those with length less than 50, and then the rest. This ensured that most training batches contained examples of similar length.</p><p>Embeddings: We use 300 dimensional GloVe embeddings ( <ref type="bibr" target="#b12">Pennington et al., 2014</ref>) to represent words. Each embedding vector was normalized to have 2 norm of 1 and projected down to 200 di- mensions, a number determined via hyperparameter tuning. Out-of-vocabulary (OOV) words are hashed to one of 100 random embeddings each initialized to mean 0 and standard deviation 1. All embeddings remain fixed during training, but the projection ma- trix is trained. All other parameter weights (hidden layers etc.) were initialized from random Gaussians with mean 0 and standard deviation 0.01.</p><p>Each hyperparameter setting was run on a sin- gle machine with 10 asynchronous gradient-update threads, using Adagrad (Duchi et al., 2011) for opti- mization with the default initial accumulator value of 0.1. Dropout regularization ( <ref type="bibr" target="#b13">Srivastava et al., 2014</ref>) was used for all ReLU layers, but not for the final linear layer. We additionally tuned the following hyperparameters and present their chosen values in ID Sentence 1 Sentence 2 DA (vanilla) DA (intra att.) SPINN-PI mLSTM Gold A Two kids are standing in the ocean hugging each other. Two kids enjoy their day at the beach.  <ref type="table">Table 3</ref>: Example wins and losses compared to other approaches. DA (Decomposable Attention) refers to our approach while SPINN-PI and mLSTM are previously developed methods (see <ref type="table">Table 1</ref>).</p><formula xml:id="formula_14">N N E E N B A</formula><p>parentheses: network size (2-layers, each with 200 neurons), batch size (4), 1 dropout ratio (0.2) and learning rate (0.05-vanilla, 0.025-intra-attention). All settings were run for 50 million steps (each step indicates one batch) but model parameters were saved frequently as training progressed and we chose the model that did best on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Results in terms of 3-class accuracy are shown in <ref type="table">Table 1</ref>. Our vanilla approach achieves state-of-the- art results with almost an order of magnitude fewer parameters than the LSTMN of <ref type="bibr" target="#b2">Cheng et al. (2016)</ref>. Adding intra-sentence attention gives a considerable improvement of 0.5 percentage points over the ex- isting state of the art. <ref type="table" target="#tab_2">Table 2</ref> gives a breakdown of accuracy on the development set showing that most of our gains stem from neutral, while most losses come from contradiction pairs. <ref type="table">Table 3</ref> shows some wins and losses. Examples A- C are cases where both variants of our approach are correct while both SPINN-PI ( <ref type="bibr" target="#b1">Bowman et al., 2016)</ref> and the mLSTM ( <ref type="bibr" target="#b15">Wang and Jiang, 2016)</ref> are incor- rect. In the first two cases, both sentences contain phrases that are either identical or highly lexically related (e.g. "Two kids" and "ocean / beach") and our approach correctly favors neutral in these cases. In Example C, it is possible that relying on word-order may confuse SPINN-PI and the mLSTM due to how "fountain" is the object of a preposition in the first sentence but the subject of the second.</p><p>The second set of examples (D-F) are cases where <ref type="bibr">1</ref> 16 or 32 also work well and are a bit more stable.</p><p>our vanilla approach is incorrect but mLSTM and SPINN-PI are correct. Example F requires sequen- tial information and neither variant of our approach can predict the correct class. Examples D-E are in- teresting however, since they don't require word or- der information, yet intra-attention seems to help. We suspect this may be because the word embed- dings are not fine-grained enough for the algorithm to conclude that "play/watch" is a contradiction, but intra-attention, by adding an extra layer of composi- tion/nonlinearity to incorporate context, compensates for this. Finally, Examples G-I are cases that all methods get wrong. The first is actually representative of many examples in this category where there is one critical word that separates the two sentences (close vs open in this case) and goes unnoticed by the algorithms. Examples H requires inference about numbers and Example I needs sequence information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented a simple attention-based approach to natural language inference that is trivially paralleliz- able. The approach outperforms considerably more complex neural methods aiming for text understand- ing. Our results suggest that, at least for this task, pairwise comparisons are relatively more important than global sentence-level representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Pictoral overview of the approach, showing the Attend (left), Compare (center) and Aggregate (right) steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>F</head><label></label><figDesc>is evaluated O() times, giving a complexity of O(d 2 ). Each attention weight e ij requires one dot product, resulting in a complexity of O( 2 d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Breakdown of accuracy with respect to classes on SNLI development set. N=neutral, E=entailment, C=contradiction.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Slav Petrov, Tom Kwiatkowski, Yoon Kim, Erick Fonseca, Mark Neumann for useful discussion and Sam Bowman and Shuohang Wang for providing us their model outputs for error analysis.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow. org</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>References [abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<editor>Andreas et al.2013] Jacob Andreas, Andreas Vlachos, and Stephen Clark</editor>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markert2005] Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Markert ; Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL<address><addrLine>Dan Goldwasser, Dan Roth</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Proceedings of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long short-term memory-networks for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Paraphrase identification as probabilistic quasisynchronous recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith2009] Dipanjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACLIJCNLP</title>
		<meeting>ACLIJCNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Duchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Paraphrase-driven learning for open question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Fader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLTNAACL</title>
		<editor>Aria D. Haghighi, Andrew Y. Ng, and Christopher D. Manning</editor>
		<meeting>HLTNAACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Proceedings of AISTATS</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A discourse commitment-based framework for recognizing textual entailment</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing. Association for Computational Linguistics. [Hochreiter and Schmidhuber1997] Sepp Hochreiter and Jürgen Schmidhuber</title>
		<editor>and Bensley2007] Andrew Hickl and Jeremy Bensley</editor>
		<meeting>the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing. Association for Computational Linguistics. [Hochreiter and Schmidhuber1997] Sepp Hochreiter and Jürgen Schmidhuber</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
	<note>Long short-term memory</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semantic theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerrold</forename><forename type="middle">J</forename><surname>Katz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972" />
			<publisher>Harper &amp; Row</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a back-propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IWCS</title>
		<editor>Bill MacCartney and Christopher D. Manning</editor>
		<meeting>the IWCS<address><addrLine>Trond</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
	<note>Advances in NIPS. MacCartney et al.2006] Bill MacCartney</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A phrase-based alignment model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D Manning ;</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maccartney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL workshop on Empirical Modeling of Semantic Equivalence and Entailment</title>
		<meeting>the ACL workshop on Empirical Modeling of Semantic Equivalence and Entailment</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Proceedings of EMNLP. [Marsi and Krahmer2005] Erwin Marsi and Emiel Krahmer</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Natural language inference by tree-based convolution and heuristic matching</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>short papers</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Rocktäschel et al.2016] Tim Rocktäschel</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Kočisk`kočisk`y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
	<note>Reasoning about entailment with neural attention</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 2015. Order-embeddings of images and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Van Benthem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vendrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<publisher>College Publications</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>A brief history of natural logic</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning natural language inference with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang2016] Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ABCNN: Attentionbased convolutional neural network for modeling sentence pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
