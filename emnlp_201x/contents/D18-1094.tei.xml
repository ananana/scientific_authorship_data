<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Hierarchical Neural Attention-based Text Classifier</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koustuv</forename><surname>Sinha</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">McGill University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Montreal Institute of Learning Algorithms</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">McGill University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Montreal Institute of Learning Algorithms</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie</forename><forename type="middle">C K</forename><surname>Cheung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">McGill University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Montreal Institute of Learning Algorithms</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Ruths</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">McGill University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Hierarchical Neural Attention-based Text Classifier</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="817" to="823"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>817</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Deep neural networks have been displaying superior performance over traditional supervised classifiers in text classification. They learn to extract useful features automatically when sufficient amount of data is presented. However, along with the growth in the number of documents comes the increase in the number of categories, which often results in poor performance of the multiclass classifiers. In this work, we use external knowledge in the form of topic category taxonomies to aide the classification by introducing a deep hierarchical neural attention-based classifier. Our model performs better than or comparable to state-of-the-art hierarchical models at significantly lower computational cost while maintaining high interpretability.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A large number of documents are being generated all over the world everyday, and as a result auto- matic text classification has become an essential tool for searching, retrieving, and managing the text ( <ref type="bibr" target="#b0">Allahyari et al., 2017)</ref>. There has been an increasing trend in developing data-driven neural text classifiers <ref type="bibr" target="#b5">(Collobert et al., 2011;</ref><ref type="bibr" target="#b16">Lai et al., 2015;</ref><ref type="bibr" target="#b36">Zhang et al., 2015;</ref><ref type="bibr" target="#b35">Yogatama et al., 2017;</ref><ref type="bibr" target="#b6">Conneau et al., 2017</ref>), due to their ability to han- dle large-scale corpora and their robustness in au- tomatic feature extraction.</p><p>However, text classification has become in- creasingly challenging as the number of categories grows with continually expanding corpus. To alle- viate this problem, one form of the external knowl- edge -class taxonomy -has been introduced to aid the classification in a hierarchical fashion <ref type="bibr" target="#b14">(Koller and Sahami, 1997)</ref>. In general, hierarchi- cal classifiers can be categorized into two broad approaches: local (top-down and bottom-up) and global (or big-bang) ( <ref type="bibr" target="#b27">Silla and Freitas, 2011</ref>). The local approaches create a unique classifier for each parent node in the taxonomy ( <ref type="bibr" target="#b20">Liu et al., 2001;</ref><ref type="bibr" target="#b24">Quinn and Laier, 2006;</ref><ref type="bibr" target="#b32">Vens et al., 2008;</ref><ref type="bibr" target="#b15">Kowsari et al., 2017)</ref>, while global approaches create a sin- gle classifier for the entire taxonomy <ref type="bibr" target="#b28">(Silla Jr and Freitas, 2009)</ref>. <ref type="bibr" target="#b15">Kowsari et al. (2017)</ref> recently proposed a hierar- chical neural-based model called HDLTex, which displayed superior performance over traditional non-neural-based models with a top-down struc- ture. However, HDLTex suffers the inherited dis- advantage of the top-down approach: the number of sub-models grows exponentially with respect to the number of sub-trees. This is especially prob- lematic in HDLTex, as it uses deep networks with a large number of parameters for the sub-models, and the combined model itself grows exponen- tially with the depth of taxonomy.</p><p>In contrast, we propose a unified global deep neural-based classifier that overcomes the prob- lem of exploding models. The backbone of our approach is one encoder-decoder structure that se- quentially predicts the class label of the next level, conditioned on a dynamic document representa- tion obtained based on a variant of an attention mechanism ( <ref type="bibr" target="#b2">Bahdanau et al., 2015)</ref>. The contri- bution of our paper is as follows:</p><p>1. We propose an end-to-end global neural attention-based model for hierarchical clas- sification, which performs better than the state-of-the-art hierarchical classifier at lower computation cost.</p><p>2. We empirically show that the use of hierar- chical taxonomy provides a robust classifier, by comparing with state-of-the-art flat classi- fiers. <ref type="table">Traditional text classification methods focus on se- lecting a good set of features (for example,TF-IDF  (Salton and Buckley</ref> <ref type="bibr">, 1987)</ref>) to represent the doc- uments and employing non-linear classifiers such as SVM ( <ref type="bibr" target="#b7">Dumais et al., 1998;</ref><ref type="bibr" target="#b10">Joachims, 1999;</ref><ref type="bibr" target="#b31">Tong and Koller, 2001</ref>), decision trees <ref type="bibr" target="#b1">(Apté et al., 1994)</ref>, or Naive Bayes ( <ref type="bibr" target="#b21">McCallum et al., 1998;</ref><ref type="bibr" target="#b12">Kim et al., 2006</ref>) methods for text classification. More recent work has employed deep neural net- works to merge feature extraction and classifica- tion into one joint process, where the model pa- rameters can be learned through back-propagation ( <ref type="bibr" target="#b33">Xue et al., 2008;</ref><ref type="bibr" target="#b16">Lai et al., 2015;</ref><ref type="bibr" target="#b36">Zhang et al., 2015)</ref>. A common theme in these convolutional neural networks (CNN)-based or recurrent neural network (RNN)-based approaches is to create a document representation from either the last hid- den state of the RNN or via some pooling opera- tions on all hidden states. Furthermore, the attention mechanism (Bah- danau et al., ) has been adapted for these CNN/RNN structures for text classification ( <ref type="bibr" target="#b19">Lin et al., 2017)</ref>, providing high interpretability and allowing us to inspect which parts of the text are discriminative for a particular sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Literature Review</head><p>In addition, external knowledge has been exam- ined as a way to boost the performance of text clas- sifiers <ref type="bibr" target="#b3">(Collobert and Weston, 2008a;</ref><ref type="bibr" target="#b22">Ngiam et al., 2011;</ref><ref type="bibr" target="#b9">Howard and Ruder, 2018)</ref>. One form of ex- ternal knowledge is built on top of the hierarchical relations of the classes <ref type="bibr" target="#b14">(Koller and Sahami, 1997)</ref>, where a class taxonomy is used to improve the per- formance of the end-level classification <ref type="bibr">1</ref> . Most of the hierarchical classifiers 2 perform classification by navigating through the hierarchy in top-down approaches ( <ref type="bibr" target="#b20">Liu et al., 2001;</ref><ref type="bibr" target="#b24">Quinn and Laier, 2006;</ref><ref type="bibr" target="#b32">Vens et al., 2008)</ref>, where a local classifier is constructed at each parent node. The state-of- the-art hierarchical classifier HDLTex is proposed by <ref type="bibr" target="#b15">Kowsari et al. (2017)</ref>. It combines deep neural networks in the top-down fashion where a sepa- 1 Classifiers that do not take into account the hierarchy and are only concerned with predicting the leaf nodes are termed flat classifiers in this work. <ref type="bibr">2</ref> We use the term "hierarchical classifiers" to refer the models that follow the external taxonomy of class labels, which is substantially different from hierarchical attention networks ( <ref type="bibr" target="#b34">Yang et al., 2016)</ref>. In <ref type="bibr" target="#b34">Yang et al. (2016)</ref>, hierar- chical attention networks refer to the hierarchical nature of their attention mechanism; the model attends to the sentences first and then attends to the words. rate neural network (either CNN or RNN) is built at each parent node to classify its children. <ref type="bibr">w1</ref> w2 w3 w4 w5 w6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>T h e 2 0 1 2 C h ib a e a r th q u a k e o c c u r r e d a lo n g</p><formula xml:id="formula_0">← − h1 − → h1 ← − h2 − → h2 ← − h3 − → h3 ← − h4 − → h4 ← − h6 − → h6 ← − h5 − → h5</formula><p>w7 th e</p><formula xml:id="formula_1">← − h7 − → h7</formula><p>w8 n o r th e a s te r n</p><formula xml:id="formula_2">← − h8 − → h8 w9 c o a s t ← − h9 − → h9 u1 wl1 u2 u3</formula><p>wl2 wl3 l1 l2 l3</p><p>Figure 1: Proposed model architecture</p><p>Our proposed model ( <ref type="figure">Figure 1</ref>) consists of three parts: 1) a bidirectional LSTM encoder (Hochre- iter and Schmidhuber, 1997) that transforms each word into vector representations based on their context. 2) an attention module that helps to gener- ate dynamic document representations across dif- ferent level of classification, 3) multi-layer percep- tron (MLP) classifiers at each level that makes the prediction of classes at that level based on the dy- namically generated document representation and the level masking.</p><p>Our hierarchical classification model can be viewed as a sequence-to-sequence model, where a sequence of word embeddings is used to gen- erate a sequence of hierarchical class labels. In addition, we employ a modified attention module from the traditional attention mechanism used in sequential generation tasks ( <ref type="bibr" target="#b2">Bahdanau et al., 2015;</ref>). Instead of computing at- tention weights conditioned on the hidden state of the decoder at time step i, we condition on the par- ent category embedding c k−1 . This is intuitive in our setting as the document representation should depend on the parent class predicted by the model. Formally, suppose we are given a document with n tokens D = (w 1 , w 2 , ..., w n ) and its cat- egory labels of m levels</p><formula xml:id="formula_3">C = (c 1 , . . . , c m ), c k ∈ {c l k 1 , . . . , c l k s k</formula><p>} where l k indicates the k-th level of the class taxonomy and s k represents the number of classes in level k 3 . A bidirectional LSTM is first used to extract features of the document:</p><formula xml:id="formula_4">− → h t = −−−−→ LST M (w t , − −− → h t−1 ), ← − h t = ←−−−− LST M (w t , ← −− − h t+1 ).<label>(1)</label></formula><p>The encoder's hidden states H = (h 1 , . . . , h n ) are constructed by the concatenation of ( − → h t ) and (</p><formula xml:id="formula_5">← − h t ) as h i = [ − → h t , ← − h t ]</formula><p>. When classifying the class label at level k, we first form contextual word features ¯ H k by concate- nating the previously predicted category embed- ding c k−1 (parent) with each of the encoder's out- puts H = (h 1 , . . . , h n ):</p><formula xml:id="formula_6">¯ H k = H ⊕ c k−1 .<label>(2)</label></formula><p>Then, we transform these n vectors in ¯ H k into n attention scores (scalars) through a series of linear and non-linear transformations: </p><formula xml:id="formula_7">a k = softmax(w s2 tanh(W s 1 ¯ H T k )).<label>(3)</label></formula><formula xml:id="formula_8">D k = W s 3 A k ¯ H k .<label>(4)</label></formula><p>Finally, a two layered multi-layer perceptron (MLP) is employed to classify the category at level k:</p><formula xml:id="formula_9">d k = RELU(W D [D k , d k − 1]), y k = softmax(W k d k )<label>(5)</label></formula><p>Normally, the softmax in Equation 5 is computed over all class labels across the entire taxonomy levels. This is not desirable when the taxonomy is deep and the number of classes is large. We solve this by employing a level masking technique where we mask out all the classes that are not in the current classification level k. The loss is then calculated as the joint cross entropy loss among all levels of the taxonomy: l = m i=1 l i . <ref type="table" target="#tab_0">9  7  Level 2 Categories  70  134  Level 3 Categories  219  NA  Number of documents 381,025 46,985  Mean document length 106.9</ref> 200.7 As deep learning models usually contain a large number of parameters that need to be learned, to prevent over-fitting ( <ref type="bibr" target="#b17">Lawrence et al., 1997;</ref><ref type="bibr" target="#b29">Srivastava et al., 2014</ref>) we usually need a large dataset to train upon. Thus, we curated a bigger dataset with hierarchical labels from Wikipedia meta in- formation provider DBpedia 5 . Compared to WOS, our DBpedia dataset is larger in two aspects: the number of data instances and the number of hier- archical levels <ref type="table" target="#tab_0">(Table 1</ref>). The DBpedia ontology was first used in <ref type="bibr" target="#b36">Zhang et al. (2015)</ref> for flat text classification. We instead use the DBpedia ontol- ogy to construct a dataset with a three-level taxon- omy of classes. In order to ensure enough docu- ments per-class, we only extract leaf-classes with more than 200 documents. We also randomly sub- sample 3,000 documents per category to balance the number of leaf-level categories. This results in 381,025 documents in total, which we split into 90% for training (from which 10% were kept aside for validation) and 10% on testing, on which we report our classification metrics <ref type="bibr">6</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DBpedia WOS Level 1 Categories</head><p>Baselines State-of-the-art flat classifiers such as FastText ( <ref type="bibr" target="#b11">Joulin et al., 2017</ref>), Bi-directional <ref type="bibr">4</ref> The LSHTC dataset ( <ref type="bibr">Partalas et al., 2015</ref>) has been widely used as a benchmark for hierarchical text classifica- tion. However, the raw texts are not available which makes it difficult to extract features for modern neural approaches. Instead, only the tf-idf vectors are provided as inputs with no option to retrieve the original text (even after consulting with the original authors we were unable to procure it).</p><p>5 http://wiki.dbpedia.org/ 6 Our code and data will be released at https:// github.com/koustuvsinha/hier-class  LSTM with max/mean pooling <ref type="bibr" target="#b4">(Collobert and Weston, 2008b;</ref><ref type="bibr" target="#b18">Lee and Dernoncourt, 2016)</ref> and the Structured Self-attentive classifier ( <ref type="bibr" target="#b19">Lin et al., 2017)</ref> are used for the comparison. We no- ticed that using the default hyperparameters of the Structured Self-attentive classifier with high atten- tion hops (m &gt;= 8) performed poorly compared to use just one attention hop (m = 1). There- fore, we reported the results of using one attention hop (m = 1) as our baselines for fair comparison. We also compare our classifier to the state-of-the- art hierarchical classifier HDLTex ( <ref type="bibr" target="#b15">Kowsari et al., 2017</ref>).</p><p>Hyperparameters We use 300-dimensional word embeddings which are randomly initialized and fine-tuned during training. Two-layer Bidirec- tional LSTM with 300 hidden units in each layer are employed. In the multi-head attention mech- anism, we use 4 heads (hops) with 0.1 Frobe- nius norm penalty because it gives the best valida- tion performance. The final fully-connected MLP layer W D has 1200 hidden units. In addition, we add 0.4 dropout on BiLSTM layers and MLP lay- ers to prevent over-fitting. For optimization, we use the standard Adam optimizer ( <ref type="bibr" target="#b13">Kingma and Ba, 2014</ref>) with the learn- ing rate of 0.001, weight decay of 10 −4 and 10 −6 for WOS and DBpedia, respectively. The gra- dients are clipped to 0.5 in order to prevent ex- ploding gradients. All the results are obtained af- ter 25 epochs of training. After every 10 epochs, we reduce the learning rate by half if the valida- tion accuracy is not improving. We employ early- stopping to select the best model. In addition, a weighted loss function is utilized to balance the performance on under-represented classes.</p><p>Hierarchical Evaluation For evaluating hierar- chical models, we present the teacher-forcing re- sult on each level, such as l 1 , l 2 and l 3 . This indicates the per-level classification performance when we provide the true parent class to the classi- fier while predicting the next class. However, this is not desirable as during inference we should not have access to the correct parent class. Hence we also present the Overall score in <ref type="table" target="#tab_2">Table 2</ref>, where the classifier uses its own prediction as the parent class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Our model is significantly better than the existing state-of-the-art hierarchical baseline <ref type="table" target="#tab_2">(Table 2)</ref>. Al- though, we also see that both hierarchical classi- fiers (ours and HDLTex) perform comparably with or slightly worse than the state-of-the-art flat clas- sifiers in terms of accuracy. However, the robust- ness analysis we performed in <ref type="table" target="#tab_4">Table 3</ref> indicates that hierarchical models are more robust in their errors since most of the errors generated by hierar- chical classifiers remain within the correct tree of the parent class, while flat classifiers do worse. For example, on WOS, 88.57% of all classified data by our hierarchical model is within the correct subtree compared to 85.56% for the flat classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classifier</head><p>Correct  Interestingly, the class taxonomy seems to be more beneficial in boosting the performance of hierarchical classifiers on WOS than DBpedia. The hierarchical classifiers perform better on the leaf-node level classification of WOS than that on DBpedia. We observe this behaviour due to the dataset of DBpedia being shorter in average length making it easier to classify for flat classi- fiers, hence hierarchical classifiers overfit on the training data. In addition to the performance improvement on both datasets over HDLTex, our model takes sig- nificantly less time and resources to train, espe- cially when the dataset is large in terms of the in- termediate non-leaf nodes in the output taxonomy. As HDLTex needs to build one sub-classifier for each parent nodes, the number of sub-classifiers grows quickly. For example, there are 80 parent nodes in the taxonomy of the DBpedia dataset and HDLTex needs to build 80 RNNs, where each sub- classifier contains around 67 million parameters. As a consequence, we can barely fit the whole model of HDLTex on our CPU 7 because it re- quires 60 GB RAM to build these 80 deep neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Analysis of Attention The intuition behind build- ing dynamic document representations, using mul- tiple attentions across different hierarchical levels, is to have a re-reading effect over the taxonomy. When we first encounter an article as humans, we tend to read it carefully, but on subsequent reads we can easily identify the key aspects of the ar- ticle. We find in our exploratory experiments the attention vectors behave exactly the same. For the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we propose a light-weight neural- based hierarchical classifier that performs better than or comparable to the state-of-the-art hier- archical model at lower computation cost. Our model employs an adapted version of attention to represent documents dynamically through the hi- erarchy, which provides additional interpretability of the dynamic document representations. In ad- dition, we demonstrate that the robustness of flat text classification can be improved by using ex- ternal knowledge such as a hierarchical taxonomy. As a future direction, we will advance our model to automatically construct the hierarchical taxon- omy in order to improve text classification with a large number of classes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>As one single attention distribution might only fo- cus on a specific component of the semantics in the document, we follow Lin et al.</head><label></label><figDesc>(2017)'s work to perform m hops of attention and form the multi- head attention matrix A k (m × n). To encour- age diversity over the multiple hops of the atten- tion distributions, we employ the Frobenius norm penalty (Lin et al., 2017) P = A k A k − I 2 F to force the attention hops to focus on different as- pects of the semantics. The document representation for level k is ob- tained by multiplying the multi-head attention ma- trix and the contextual word features:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: WOS dataset attention rereading per level. Highlighted words indicate the attented words. Stronger color denote higher focus of attention. We note that the attention spread becomes much more focused in Level 2 compared to its parent Level 1.</figDesc><graphic url="image-2.png" coords="5,301.50,72.77,226.77,131.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Dataset Comparison 

4 Experimental Setup 

Dataset Two datasets are used for our experi-
ments: Web of Science (WOS) and DBpedia. Web 
of Science (WOS) is a hierarchical two-level tax-
onomy dataset that contains 46,985 documents 
collected from Web of Science (Reuters, 2012) by 
Kowsari et al. (2017). Despite its small size, WOS 
is used as a benchmark dataset for hierarchical 
classification as it provides the raw text for deep 
neural models to train on 4 . 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Test accuracy on the WOS and DBpedia datasets. The flat baseline models are trained without the hierarchical 

taxonomy of classes and therefore only have results on the leaf-node classification. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Robustness analysis of taxonomy on the WOS 

dataset. We compare the success rate of our model and the 
BiLSTM flat classifier. The success rate is defined as the 
number of times the predicted class is within the same sub-
tree as the correct parent. We calculate this in two scenar-
ios: 1. when the true parent class is manually provided, or 
teacher-forced (Correct parent), and 2. when the true parent 
class is predicted by our model (Predicted parent) 

</table></figure>

			<note place="foot" n="3"> We suppose wi and ci are word embeddings and class embedding respectively.</note>

			<note place="foot" n="7"> It is not possible to fit the entire model in one GPU as our best GPU has the RAM capacity of 12GB, one needs to have multiple GPU&apos;s and parallel execution for this task. first level, the attention values are more spread out to help our classifier pick various important aspects of the article, but on the subsequent levels, the attention is more focused towards specific keywords for that subclass, as the example shown in Figure 2 8. We perform additional qualitative analysis of attention spread which is provided in Appendix.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Compute Canada and Calcul Quebec for providing the GPU compute resources. We would also express our appreciation to our spon-sors, Pierre Arbour Foundation and FRQNT for supporting Koustuv Sinha and NSERC for sup-porting Yue Dong for this research. <ref type="bibr">8</ref> We use the same visualization script as of <ref type="bibr" target="#b19">Lin et al. (2017)</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A brief survey of text mining: Classification, clustering and extraction techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Allahyari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyedamin</forename><surname>Pouriyeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Assefi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saied</forename><surname>Safaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><forename type="middle">D</forename><surname>Trippe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">B</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krys</forename><surname>Kochut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD Bigdas</title>
		<meeting>KDD Bigdas<address><addrLine>Halifax, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automated learning of decision rules for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chidanand</forename><surname>Apté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Damerau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sholom</forename><forename type="middle">M</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="251" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1107" to="1116" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inductive learning algorithms and representations for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Sahami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh international conference on Information and knowledge management</title>
		<meeting>the seventh international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="148" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Short Papers</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Some effective techniques for naive bayes text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung-Soo</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1457" to="1466" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Hae-Chang Rim, and Sung Hyon Myaeng</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: Amethod for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Conf. Learn. Representations</title>
		<meeting>3rd Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Hierarchically classifying documents using very few words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Sahami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<pubPlace>Stanford InfoLab</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">HDLTex: Hierarchical deep learning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamran</forename><surname>Kowsari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mojtaba</forename><surname>Donald E Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiana</forename><forename type="middle">Jafari</forename><surname>Heidarysafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meimandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><forename type="middle">E</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th IEEE International Conference on Machine Learning and Applications (ICMLA)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="364" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">333</biblScope>
			<biblScope unit="page" from="2267" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lessons in neural network training: Overfitting may be harder than expected</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah Chung</forename><surname>Tsoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI/IAAI</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="540" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequential short-text classification with recurrent and convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="515" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An approach of multihierarchy text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkai</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongzhi</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. ICII 2001-Beijing. 2001 International Conferences on</title>
		<meeting>ICII 2001-Beijing. 2001 International Conferences on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
	<note>Info-tech and Infonet</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A comparison of event models for naive bayes text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI-98 workshop on learning for text categorization</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML11)</title>
		<meeting>the 28th international conference on machine learning (ICML11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aris</forename><surname>Kosmopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Baskiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Artieres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.08581</idno>
		<title level="m">Ion Androutsopoulos, Massih-Reza Amini, and Patrick Galinari. 2015. Lshtc: A benchmark for large-scale text classification</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Method and apparatus for fast lookup of related classification entities in a tree-ordered classification hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">L</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Patent</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Web of science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomson Reuters</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Term weighting approaches in automatic text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
		<respStmt>
			<orgName>Cornell University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A survey of hierarchical classification across different application domains. Data Mining and Knowledge Discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">A</forename><surname>Silla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freitas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="31" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A globalmodel naive bayes approach to the hierarchical prediction of protein functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silla</forename><surname>Carlos N</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">A</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining, 2009. ICDM&apos;09. Ninth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="992" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Support vector machine active learning with applications to text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2001-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Decision trees for hierarchical multi-label classification. Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Celine</forename><surname>Vens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Struyf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leander</forename><surname>Schietgat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sašo</forename><surname>Džeroski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Blockeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">185</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep classification in large-scale text hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Rong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dikan</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 31st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="619" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01898</idno>
		<title level="m">Generative and discriminative text classification with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
