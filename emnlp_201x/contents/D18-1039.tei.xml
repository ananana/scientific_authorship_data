<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contextual Parameter Generation for Universal Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanouil</forename><surname>Antonios Platanios</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning Department</orgName>
								<orgName type="institution">‡ Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning Department</orgName>
								<orgName type="institution">‡ Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning Department</orgName>
								<orgName type="institution">‡ Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning Department</orgName>
								<orgName type="institution">‡ Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Contextual Parameter Generation for Universal Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="425" to="435"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>425</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a simple modification to existing neural machine translation (NMT) models that enables using a single universal model to translate between multiple languages while allowing for language specific parameterization, and that can also be used for domain adaptation. Our approach requires no changes to the model architecture of a standard NMT system, but instead introduces a new component, the contextual parameter generator (CPG), that generates the parameters of the system (e.g., weights in a neural network). This parameter generator accepts source and target language embeddings as input, and generates the parameters for the encoder and the decoder, respectively. The rest of the model remains unchanged and is shared across all languages. We show how this simple modification enables the system to use monolingual data for training and also perform zero-shot translation. We further show it is able to surpass state-of-the-art performance for both the IWSLT-15 and IWSLT-17 datasets and that the learned language embeddings are able to uncover interesting relationships between languages.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural Machine Translation (NMT) directly mod- els the mapping of a source language to a target language without any need for training or tuning any component of the system separately. This has led to a rapid progress in NMT and its successful adoption in many large-scale settings ( <ref type="bibr">Wu et al., 2016;</ref><ref type="bibr" target="#b8">Crego et al., 2016</ref>). The encoder-decoder abstraction makes it conceptually feasible to build a system that maps any source sentence in any lan- guage to a vector representation, and then decodes this representation into any target language. Thus, various approaches have been proposed to extend this abstraction for multilingual MT ( <ref type="bibr" target="#b22">Luong et al., 2016;</ref><ref type="bibr" target="#b9">Dong et al., 2015;</ref><ref type="bibr" target="#b17">Johnson et al., 2017;</ref><ref type="bibr" target="#b14">Ha et al., 2016;</ref><ref type="bibr" target="#b10">Firat et al., 2016a</ref>).</p><p>Prior work in multilingual NMT can be broadly categorized into two paradigms. The first, univer- sal NMT <ref type="bibr" target="#b17">(Johnson et al., 2017;</ref><ref type="bibr" target="#b14">Ha et al., 2016)</ref>, uses a single model for all languages. Univer- sal NMT lacks any language-specific parameter- ization, which is an oversimplification and detri- mental when we have very different languages and limited training data. As verified by our experi- ments, the method of <ref type="bibr" target="#b17">Johnson et al. (2017)</ref> suf- fers from high sample complexity and thus un- derperforms in limited data settings. The univer- sal model proposed by <ref type="bibr" target="#b14">Ha et al. (2016)</ref> requires a new coding scheme for the input sentences, which results in large vocabulary sizes that are diffi- cult to scale. The second paradigm, per-language encoder-decoder ( <ref type="bibr" target="#b22">Luong et al., 2016;</ref><ref type="bibr" target="#b10">Firat et al., 2016a</ref>), uses separate encoders and decoders for each language. This does not allow for sharing of information across languages, which can result in overparameterization and can be detrimental when the languages are similar.</p><p>In this paper, we strike a balance between these two approaches, proposing a model that has the ability to learn parameters separately for each lan- guage, but also share information between simi- lar languages. We propose using a new contextual parameter generator (CPG) which (a) generalizes all of these methods, and (b) mitigates the afore- mentioned issues of universal and per-language encoder-decoder systems. It learns language em- beddings as a context for translation and uses them to generate the parameters of a shared translation model for all language pairs. Thus, it provides these models the ability to learn parameters sepa- rately for each language, but also share informa- tion between similar languages. The parameter generator is general and allows any existing NMT model to be enhanced in this way. <ref type="bibr">1</ref> In addition, it has the following desirable features: 1. Simple: Similar to <ref type="bibr" target="#b17">Johnson et al. (2017)</ref> and <ref type="bibr" target="#b14">Ha et al. (2016)</ref>, and in contrast with <ref type="bibr" target="#b22">Luong et al. (2016)</ref> and <ref type="bibr" target="#b10">Firat et al. (2016a)</ref>, it can be applied to most existing NMT systems with some minor modification, and it is able to ac- commodate attention layers seamlessly. 2. Multilingual: Enables multilingual translation using the same single model as before. 3. Semi-supervised: Can use monolingual data. <ref type="bibr">4</ref>. Scalable: Reduces the number of parameters by employing extensive, yet controllable, shar- ing across languages, thus mitigating the need for large amounts of data, as in <ref type="bibr" target="#b17">Johnson et al. (2017)</ref>. It also allows for the decoupling of lan- guages, avoiding the need for a large shared vo- cabulary, as in <ref type="bibr" target="#b14">Ha et al. (2016)</ref>. 5. Adaptable: Can adapt to support new lan- guages, without requiring complete retraining. 6. State-of-the-art: Achieves better performance than pairwise NMT models and <ref type="bibr" target="#b17">Johnson et al. (2017)</ref>. In fact, our approach can surpass state- of-the-art performance. We first introduce a modular framework that can be used to define and describe most existing NMT systems. Then, in Section 3, we introduce our main contribution, the contextual parameter gen- erator (CPG), in terms of that framework. We also argue that the proposed approach takes us a step closer to a common universal interlingua.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We first define the multi-lingual NMT setting and then introduce a modular framework that can be used to define and describe most existing NMT systems. This will help us distill previous contri- butions and introduce ours.</p><p>Setting. We assume that we have a set of source languages S and a set of target languages T . The total number of languages is L = |S ∪ T |. We also assume we have a set of C ≤ |S| × |T | pairwise parallel corpora, {P 1 , . . . , P C }, each of which contains a set of sentence pairs for a single source-target language combination. The goal of multilingual NMT is to build a model that, when trained using the provided parallel corpora, can learn to translate well between any pair of lan- guages in S×T . The majority of related work only considers pairwise NMT, where |S| = |T | = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">NMT Modules</head><p>Most NMT systems can be decomposed to the fol- lowing modules (also visualized in <ref type="figure">Figure 1</ref>).</p><p>Preprocessing Pipeline. The data preprocessing pipeline handles tokenization, cleaning, normaliz- ing the text data and building a vocabulary, i.e. a two-way mapping from preprocessed sentences to sequences of word indices that will be used for the translation. A commonly used proposal for defin- ing the vocabulary is the byte-pair encoding (BPE) algorithm which generates subword unit vocabu- laries ( <ref type="bibr" target="#b31">Sennrich et al., 2016b)</ref>. This eliminates the notion of out-of-vocabulary words, often resulting in increased translation quality.</p><p>Encoder/Decoder. The encoder takes in in- dexed source language sentences, and produces an intermediate representation that can later be used by a decoder to generate sentences in a target lan- guage. Generally, we can think of the encoder as a function, f (enc) , parameterized by θ (enc) . Simi- larly, we can think of the decoder as another func- tion, f (dec) , parameterized by θ (dec) . The goal of learning to translate can then be defined as finding the values for θ (enc) and θ (dec) that re- sult in the best translations. A large amount of previous work proposes novel designs for the en- coder/decoder module. For example, using atten- tion over the input sequence while decoding <ref type="bibr" target="#b2">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b23">Luong et al., 2015</ref>) provides significant gains in translation performance. <ref type="bibr">2</ref> Parameter Generator. All modules defined so far have previously been used when describing NMT systems and are thus easy to conceptual- ize. However, in previous work, most models are trained for a given language pair, and it is not trivial to extend them to work for multiple pairs of languages. We introduce here the con- cept of the parameter generator, which makes it easy to define and describe multilingual NMT sys- tems. This module is responsible for generating θ (enc) and θ (dec) for any given source and target language. Different parameter generators result in different numbers of learnable parameters and can thus be used to share information across dif- ferent languages. Next, we describe related work, in terms of the parameter generator for NMT:</p><p>• Pairwise: In the simple and commonly used pairwise NMT setting ( <ref type="bibr">Wu et al., 2016;</ref><ref type="bibr" target="#b8">Crego et al., 2016)</ref>, the parameter generator would gen- erate separate parameters, θ (enc) and θ (dec) , for each pair of source-target languages. This re- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ENGLISH VIETNAMESE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vocabulary lookup</head><p>Cám ơn rất nhiều.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder Decoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoupled Pairwise</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLUE TITLES INDICATE DIFFERENT OPTIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coupled</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SOURCE/TARGET SOURCE</head><p>Parameters do not depend on the language embeddings. They are learnable variables. This represents the typical pairwise NMT se�ng where the parameters are different for each language pair.</p><p>TARGET P P P</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Generator</head><p>Figure 1: Overview of an NMT system, under our modular framework. Our main contribution lies in the parameter generator module (i.e., coupled or decoupled -each of the boxes with blue titles is a separate option). Note that g denotes a parameter generator network. In our experiments, we consider linear forms for this network. However, our contribution does not depend on the choices made regarding the rest of the modules; we could still use our parameter generator with different architectures for the encoder and the decoder, as well as using different kinds of vocabularies. As we observed in our experiments, this system fails to perform well when the train- ing data is limited. Finally, we believe that em- bedding languages in the same space as words is not intuitive; in our approach, languages are embedded in a separate space.</p><p>In contrast to all these related systems, we pro- vide a simple, efficient, yet effective alternative - a parameter generator for multilingual NMT, that enables semi-supervised and zero-shot learning. We also learn language embeddings, similar to <ref type="bibr" target="#b17">Johnson et al. (2017)</ref>, but in our case they are sep- arate from the word embeddings and are treated as a context for the translation, in a sense that will become clear in the next section. This notion of context is used to define parameter sharing across various encoders and decoders, and, as we discuss in our conclusion, is even applicable beyond NMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>We propose a new way to share information across different languages and to control the amount of sharing, through the parameter generator module. More specifically, we propose contextual parame- ter generators.</p><p>Contextual Parameter Generator. Let us de- note the source language for a given sentence pair by s and the target language by t . Then, when using the contextual parameter generator, the pa- rameters of the encoder are defined as θ (enc) g (enc) (l s ), for some function g <ref type="bibr">(enc)</ref> , where l s de- notes a language embedding for the source lan- guage s . Similarly, the parameters of the decoder are defined as θ (dec) g (dec) (l t ) for some func- tion g <ref type="bibr">(dec)</ref> , where l t denotes a language embed- ding for the target language t . Our generic for- mulation does not impose any constraints on the functional form of g (enc) and g <ref type="bibr">(dec)</ref> . In this case, we can think of the source language, s , as a con- text for the encoder. The parameters of the en- coder depend on its context, but its architecture is common across all contexts. We can make a simi- lar argument for the decoder, and that is where the name of this parameter generator comes from. We can even go a step further and have a parameter generator that defines θ (enc) g (enc) (l s , l t ) and θ (dec) g (dec) (l s , l t ), thus coupling the encoding and decoding stages for a given language pair. In our experiments we stick to the previous, decou- pled, form, because unlike <ref type="bibr" target="#b17">Johnson et al. (2017)</ref>, it has the potential to lead to an interlingua.</p><p>Concretely, because the encoding and decoding stages are decoupled, the encoder is not aware of the target language while generating it. Thus, we can take an encoded intermediate representation of a sentence and translate it to any target language. This is because, in this case, the intermediate rep- resentation is independent of any target language. This makes for a stronger argument that the inter- mediate representation produced by our encoder could be approaching a universal interlingua, more so than methods that are aware of the target lan- guage when they perform encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Parameter Generator Network</head><p>We refer to the functions g (enc) and g (dec) as pa- rameter generator networks. Even though our pro- posed NMT framework does not rely on a specific choice for g (enc) and g (dec) , here we describe the functional form we used for our experiments. Our goal is to provide a simple form that works, and for which we can reason about. For this reason, we decided to define the parameter generator net- works as simple linear transforms, similar to the factored adaptation model of <ref type="bibr" target="#b25">Michel and Neubig (2018)</ref>, which was only applied to the bias terms of the output softmax:</p><formula xml:id="formula_0">g (enc) (l s ) W (enc) l s ,<label>(1)</label></formula><formula xml:id="formula_1">g (dec) (l t ) W (dec) l t ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">l s , l t ∈ R M , W (enc) ∈ R P (enc) ×M , W (dec) ∈ R P (dec) ×M , M is the language embed- ding size, P (enc)</formula><p>is the number of parameters of the encoder, and P (dec) is the number of parame- ters of the decoder. Another way to interpret this model is that it im- poses a low-rank constraint on the parameters. As opposed to our approach, in the base case of using multiple pairwise models to perform multilingual translation, each model has P = P (enc) + P (dec) learnable parameters for its encoder and decoder. Given that the models are pairwise, for L lan- guages, we have a total of L(L − 1) learnable parameter vectors of size P . On the other hand, using our contextual parameter generator we have a total of L vectors of size M (one for each lan- guage), and a single matrix of size P × M . Then, the parameters of the encoder and the decoder, for a single language pair, are defined as a linear com- bination of the M columns of that matrix.</p><p>Controlled Parameter Sharing. We can further control parameter sharing by observing that the encoder/decoder parameters often have some "nat- ural grouping". For example, in the case of recur- rent neural networks we may have multiple weight matrices, one for each layer, as well as attention- related parameters. Based on this observations, we now propose a way to control how much infor- mation is shared across languages. The language embeddings need to represent all of the language- specific information and thus may need to be large in size. However, when computing the parame- ters of each group, only a small part of that infor- mation is relevant. Let θ (enc) = {θ</p><formula xml:id="formula_3">(enc) j } G j=1 and θ (enc) j ∈ R P (enc) j</formula><p>, where G denotes the number of groups. Then, we define:</p><formula xml:id="formula_4">θ (enc) j W (enc) j P (enc) j l s ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_5">W (enc) j ∈ R P (enc) j</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>×M</head><p>and P (enc) j ∈ R M ×M , with M &lt; M (and similarly for the de- coder parameters). We can see now that P (enc) j is used to extract the relevant information (size M ) for parameter group j, from the larger language embedding (size M ). This allows us to control the parameter sharing across languages in the fol- lowing way: if we want to increase the number of per-language parameters (i.e., the language em- bedding size) we can increase M while keeping M small enough so that the total number of pa- rameters does not explode. This would not have been possible without the proposed low-rank ap-proximation for W <ref type="bibr">(enc)</ref> , that uses the parameter grouping information.</p><p>Alternative Options. Given that our proposed approach does not depend on the specific choice of the parameter generator network, it might be inter- esting to design models that use side-information about the languages that are being used (such as linguistic information about language families and hierarchies). This is outside the scope of this pa- per, but may be an interesting future direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semi-Supervised and Zero-Shot Learning</head><p>The proposed parameter generator also enables semi-supervised learning via back-translation. Concretely, monolingual data can be used to train the shared encoder/decoder networks to translate a sentence from some language to itself (similar to the idea of auto-encoders by <ref type="bibr">Vincent et al. (2008)</ref>). This is possible and can help learning because of the fact that many of the learnable parameters are shared across languages.</p><p>Furthermore, zero-shot translation, where the model translates between language pairs for which it has seen no explicit training data, is also pos- sible. This is because the same per-language pa- rameters are used to translate to and from a given language, irrespective of the language at the other end. Therefore, as long as we train our model us- ing some language pairs that involve a given lan- guage, it is possible to learn to translate in any di- rection involving that language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Potential for Adaptation</head><p>Let us assume that we have trained a model using data for some set of languages, 1 , 2 , . . . , m . If we obtain data for some new language n , we do not have to retrain the whole model from scratch. In fact, we can fix the parameters that are shared across all languages and only learn the embed- ding for the new language (along with the relevant word embeddings if not using a shared vocabu- lary). Assuming that we had a sufficient number of languages in the beginning, this may allow us to obtain reasonable translation performance for the new language, with a minimal amount of training. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Number of Parameters</head><p>For the base case of using multiple pairwise mod- els to perform multilingual translation, each model has P + 2W V parameters, where P = P (enc) + P (dec) , W is the word embedding size, and V is the vocabulary size per language (assumed to be the same across languages, without loss of gen- erality). Given that the models are pairwise, for L languages, we have a total of</p><formula xml:id="formula_6">L(L − 1)(P + 2W V ) = O(L 2 P + 2L 2 W V )</formula><note type="other">learnable param- eters. For our approach, using the linear parame- ter generator network presented in Section 3.1, we have a total of O(P M + LW V ) learnable param- eters. Note that the number of encoder/decoder parameters has no dependence on L now, meaning that our model can easily scale to a large number of languages.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we describe our experimental setup along with our results and key observations.</p><p>Setup. For all our experiments we use as the base NMT model an encoder-decoder network which uses a bidirectional LSTM for the encoder, and a two-layer LSTM with the attention model of <ref type="bibr" target="#b2">Bahdanau et al. (2015)</ref> for the decoder. The word embedding size is set to 512. This is a com- mon baseline model that achieves reasonable per- formance and we decided to use it as-is, without tuning any of its parameters, as extensive hyperpa- rameter search is outside the scope of this paper.</p><p>During training, we use a label smoothing fac- tor of 0.1 ( <ref type="bibr">Wu et al., 2016</ref>) and the AMSGrad op- timizer ( <ref type="bibr" target="#b28">Reddi et al., 2018</ref>) with its default param- eters in TensorFlow, and a batch size of 128 (due to GPU memory constraints). Optimization was stopped when the validation set BLEU score was maximized. The order in which language pairs are used while training was as follows: we always first sample a language pair (uniformly at random), and then sample a batch for that pair (uniformly at ran- dom). <ref type="bibr">4</ref> During inference, we employ beam search with a beam size of 10 and the length normaliza- tion scheme of ( <ref type="bibr">Wu et al., 2016)</ref>. We want to em- phasize that we did not run experiments with other architectures or configurations, and thus this archi- tecture was not chosen because it was favorable to our method, but rather because it was a frequently mentioned baseline in existing literature.</p><p>All experiments were run on a machine with a single Nvidia V100 GPU, and 24 GBs of sys- tem memory. Our most expensive experiment took about 10 hours to complete, which would <ref type="table">Table 1</ref>: Comparison of our proposed approach (shaded rows) with the base pairwise NMT model (PNMT) and the Google multilingual NMT model (GML) for the IWSLT-15 dataset. The Percent Parallel row shows what portion of the parallel corpus is used while training; the rest is being used only as monolingual data. Results are shown for the BLEU and Meteor metrics. CPG* represents the same model as CPG, but trained without using auto-encoding training examples. The best score in each case is shown in bold. cost about $25 on a cloud computing service such as Google Cloud or Amazon Web Services, thus making our results reproducible, even by indepen- dent researchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLEU</head><p>Experimental Settings. The goal of our exper- iments is to show how, by using a simple modi- fication of this model, (i) we can achieve signif- icant improvements in performance, while at the same time (ii) being more data and computation efficient, and (iii) enabling support for zero-shot translation. To that end, we perform three types of experiments:</p><p>1. Supervised: In this experiment, we use full parallel corpora to train our models. Plain pairwise NMT models (PNMT) are compared to the same models modified to use our pro- posed decoupled parameter generator. We use two variants: (i) one which does not use auto- encoding of monolingual data while training (CPG*), and (ii) one which does (CPG). Please refer to Section 3.2 for more details. 2. Low-Resource: Similar to the supervised ex- periments except that we limit the size of the parallel corpora used in training. However, for GML and CPG the full monolingual corpus is used for auto-encoding training. 3. Zero-Shot: In this experiment, our goal is to evaluate how well a model can learn to trans- late between language pairs that it has not seen while training. For example, a model trained using parallel corpora between English and German, and English and French, will be eval- uated in translating from German to French. PNMT can perform zero-shot translation in this setting using pivoting. This means that, in the previous example, we would first translate from German to English and then from English to French (using two pairwise models for a sin- gle translation). However, pivoting is prone to error propagation incurred when chaining mul- tiple imperfect translations. The proposed CPG <ref type="table">Table 2</ref>: Comparison of our proposed approach (shaded rows) with the base pairwise NMT model (PNMT) and the Google multilingual NMT model (GML) for the IWSLT-17 dataset. Results are shown for the BLEU metric only because Meteor does not support It, Nl, and Ro. CPG 8 represents CPG using language embeddings of size 8. The "C4" subscript represents the low-rank version of CPG for controlled parameter sharing (see Section 3.1), using rank 4, etc. The best score in each case is shown in bold. models inherently support zero-shot translation and require no pivoting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLEU</head><p>For the experiments using the CPG model with- out controlled parameter sharing, we use language embeddings of size 8. This is based merely on the fact that this is the largest model size we could fit on one GPU. Whenever possible, we compare against PNMT, GML by <ref type="bibr" target="#b17">Johnson et al. (2017)</ref>, <ref type="bibr">5</ref> and other state-of-the-art results.</p><p>Datasets. We use the following datasets:</p><p>• IWSLT-15: Used for supervised and low- resource experiments only (this dataset does not support zero-shot learning). We report results for Czech (Ch), English (En), French (Fr), Ger- man (De), Thai (Th), and Vietnamese (Vi). This dataset contains ~90,000-220,000 training sentence pairs (depending on the language pair), ~500-900 validation pairs, and ~1,000-1,300 test pairs. • IWSLT-17: Used for supervised and zero-shot experiments. We report results for Dutch (Nl), English (En), German (De), Italian (It), and Romanian (Ro). This dataset contains ~220,000 <ref type="bibr">5</ref> We use our own implementation of GML in order to ob- tain a fair comparison, in terms of the whole MT pipeline. We have modified it to use the same per-language vocabularies that we use for our approaches, as the proposed shared BPE vocabulary fails to perform well for the considered datasets. training sentence pairs (for all language pairs except for the zero-shot ones), ~900 validation pairs, and ~1,100 test pairs. Data Preprocessing. We preprocess our data us- ing a modified version of the Moses tokenizer ( <ref type="bibr" target="#b18">Koehn et al., 2007</ref>) that correctly handles escaped HTML characters. We also perform some Uni- code character normalization and cleaning. While training, we only consider sentences up to length 50. For both datasets, we generate a per-language vocabulary consisting of the most frequently oc- curring words, while ignoring words that appear less than 5 times in the whole corpus, and capping the vocabulary size to 20,000 words.</p><p>Results. Our results for the IWSLT-15 experi- ments are shown in <ref type="table">Table 1</ref>. It is clear that our approach consistently outperforms both the corre- sponding pairwise model and GML. Furthermore, its advantage grows larger in the low-resource setting (up to 5.06 BLEU score difference, or a 2.4× increase), which is expected due to the extensive parameter sharing in our model. For this dataset, there exist some additional published state-of-the-art results not shown in <ref type="table">Tables 1 and  2</ref>. <ref type="bibr" target="#b16">Huang et al. (2018)</ref> report a BLEU score of 28.07 for the En)Vi task, while our model is able to achieve a score of 29.03. Furthermore, <ref type="bibr" target="#b14">Ha et al. (2016)</ref> report a BLEU score of 25.87 for the En)De task, while our model is able to achieve a score of 26.77. <ref type="bibr">6</ref> Our results for the IWSLT-17 experiments are shown in <ref type="table">Table 2</ref>. <ref type="bibr">7</ref> Again, our method consistently outperforms both PNMT and GML, in both the supervised and the zero-shot set- tings. Furthermore, the results indicate that our model performance is robust to different sizes of the language embeddings and the choice of M for controllable parameter sharing. It only underper- forms in the degenerate case where M = 1. It is also worth noting that, in the fully supervised set- ting, GML, the current state-of-the-art in the mul- tilingual setting, underperforms the pairwise mod- els.</p><p>The presented results provide evidence that our proposed approach is able to significantly improve performance, without requiring extensive tuning.</p><p>Language Embeddings. An important aspect of our model is that it learns language embeddings. In <ref type="figure" target="#fig_1">Figure 2</ref> we show pairwise cosine distances be- tween the learned language embeddings for our fully supervised experiments. There are some in- teresting patterns that indicate that the learned lan- guage embeddings are reasonable. For example, we observe that German (De) and Dutch (Nl) are most similar for the IWSLT-17 dataset, with Ital- ian (It) and Romanian (Ro) coming second. Fur- thermore, Romanian and German are the furthest apart for that dataset. These relationships agree with linguistic knowledge about these languages and the families they belong to. We see similar patterns in the IWSLT-15 results but we focus on IWSLT-17 here, because it is a larger, better qual- ity, dataset with more supervised language pairs. These results are encouraging for analyzing such embeddings to discover relationships between lan- guages that were previously unknown. For exam- ple, perhaps surprisingly, French (Fr) and Viet- namese (Vi) appear to be significantly related for the IWSLT-15 dataset results. This is likely due to French influence in Vietnamese because to the occupation of Vietnam by France during the 19 th and 20 th centuries <ref type="bibr" target="#b24">(Marr, 1981)</ref>. <ref type="bibr">6</ref> We were unable to find reported state-of-the-art results for the rest of the language pairs. 7 Note that, our results for IWSLT-17 are not comparable to those of the official challenge report <ref type="bibr" target="#b4">(Cettolo et al., 2017</ref>), as we use less training data, a smaller baseline model, and our evaluation pipeline potentially differs. However, the numbers presented for all methods in this paper are comparable, as they were all obtained using the same baseline model and evaluation pipeline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation and Reproducibility</head><p>Along with this paper we are releasing an imple- mentation of our approach and experiments as part of a new Scala framework for machine transla- tion. 8 It is built on top of TensorFlow Scala (Pla- tanios, 2018) and follows the modular NMT de- sign (described in Section 2.1) that supports var- ious NMT models, including our baselines (e.g., <ref type="bibr" target="#b17">Johnson et al. (2017)</ref>). It also contains data load- ing and preprocessing pipelines that support mul- tiple datasets and languages, and is more efficient than other packages (e.g., tf-nmt 9 ). Further- more, the framework supports various vocabular- ies, among which we provide a new implementa- tion for the byte-pair encoding (BPE) algorithm <ref type="bibr" target="#b31">(Sennrich et al., 2016b</ref>) that is 2 to 3 orders of magnitude faster than the released one. 10 All ex- periments presented in this paper were performed using version 0.1.0 of the framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Interlingual translation <ref type="bibr" target="#b29">(Richens, 1958)</ref> has been the object of many research efforts. For a long time, before the move to NMT, most practical machine translation systems only focused on in- dividual language pairs. Since the success of end-to-end NMT approaches such as the encoder- decoder framework <ref type="bibr" target="#b32">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b2">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b7">Cho et al., 2014</ref>), recent work has tried to extend the framework to multi-lingual translation. An early approach was <ref type="bibr" target="#b9">Dong et al. (2015)</ref> who performed one-to-many translation with a separate attention mechanism for each de- coder. <ref type="bibr" target="#b22">Luong et al. (2016)</ref> extended this idea with a focus on multi-task learning and multiple en- coders and decoders, operating in a single shared vector space. The same architecture is used in <ref type="bibr" target="#b3">(Caglayan et al., 2016</ref>) for translation across mul- tiple modalities. <ref type="bibr">Zoph and Knight (2016)</ref> flipped this idea with a many-to-one translation model, however requiring the presence of a multi-way parallel corpus between all the languages, which is difficult to obtain. <ref type="bibr" target="#b21">Lee et al. (2017)</ref> used a single character-level encoder across multiple languages by training a model on a many-to-one transla- tion task. Closest to our work are more recent approaches, already described in Section 2 ( <ref type="bibr" target="#b10">Firat et al., 2016a;</ref><ref type="bibr" target="#b17">Johnson et al., 2017;</ref><ref type="bibr" target="#b14">Ha et al., 2016)</ref>, that attempt to enforce different kinds of parame- ter sharing across languages. Parameter sharing in multilingual NMT natu- rally enables semi-supervised and zero-shot learn- ing. Unsupervised learning has been previously explored with key ideas such as back-translation ( <ref type="bibr" target="#b30">Sennrich et al., 2016a</ref>), dual learning ( <ref type="bibr" target="#b15">He et al., 2016)</ref>, common latent space learning ( <ref type="bibr" target="#b20">Lample et al., 2018)</ref>, etc. In the vein of multilingual NMT, <ref type="bibr" target="#b1">Artetxe et al. (2018)</ref> proposed a model that uses a shared encoder and multiple decoders with a fo- cus on unsupervised translation. The entire sys- tem uses cross-lingual embeddings and is trained to reconstruct its input using only monolingual data. Zero-shot translation was first attempted in <ref type="bibr" target="#b11">(Firat et al., 2016b</ref>) who performed zero-zhot translation using their pre-trained multi-way mul- tilingual model, fine-tuning it with pseudo-parallel data generated by the model itself. This was recently extended using a teacher-student frame- work ( . Later, zero-shot transla- tion without any additional steps was attempted in <ref type="bibr" target="#b17">(Johnson et al., 2017</ref>) using their shared encoder- decoder network. An iterative training procedure that leverages the duality of translations directly generated by the system for zero-shot learning was proposed by <ref type="bibr" target="#b19">Lakew et al. (2017)</ref>. For extremely low resource languages, <ref type="bibr" target="#b12">Gu et al. (2018)</ref> proposed sharing lexical and sentence-level representations across multiple source languages with a single tar- get language. Closely related is the work of <ref type="bibr" target="#b6">Cheng et al. (2016)</ref> who proposed the joint training of source-to-pivot and pivot-to-target NMT models. <ref type="bibr" target="#b13">Ha et al. (2018)</ref> are probably the first to intro- duce a similar idea to that of having one network (called a hypernetwork) generate the parameters of another. However, in that work, the input to the hypernetwork are structural features of the original network (e.g., layer size and index). <ref type="bibr" target="#b0">Al-Shedivat et al. (2017)</ref> also propose a related method where a neural network generates the parameters of a lin- ear model. Their focus is mostly on interpretabil- ity (i.e., knowing which features the network con- siders important). However, to our knowledge, there is no previous work which proposes hav- ing a network generate the parameters of another deep neural network (e.g., a recurrent neural net- work), using some well-defined context based on the input data. This context, in our case, is the language of the input sentences to the translation model, along with the target translation language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Directions</head><p>We have presented here a novel contextual pa- rameter generation approach to neural machine translation. Our resulting system, which outper- forms other state-of-the-art systems, uses a stan- dard pairwise encoder-decoder architecture. How- ever, it differs from earlier approaches by incor- porating a component that generates the parame- ters to be used by the encoder and the decoder for the current sentence, based on the source and tar- get languages, respectively. We refer to this novel component as the contextual parameter genera- tor. The benefit of this approach is that it dra- matically improves the ratio of the number of pa- rameters to be learned, to the number of training examples available, by leveraging shared structure across different languages. Thus, our approach does not require any extra machinery such as back- translation, dual learning, pivoting, or multilin- gual word embeddings. It rather relies on the sim- ple idea of treating language as a context within which to encode/decode. We also showed that the proposed approach is able to achieve state-of-the- art performance without requiring any tuning. Fi- nally, we performed a basic analysis of the learned language embeddings, which showed that cosine distances between the learned language embed- dings reflect well known similarities among lan- guage pairs such as German and Dutch.</p><p>In the future, we want to extend the concept of the contextual parameter generator to more gen- eral settings, such as translating between different modalities of data (e.g., image captioning). Fur- thermore, based on the discussion of Section 3.3, we hope to develop an adaptable, never-ending learning ( <ref type="bibr" target="#b26">Mitchell et al., 2018)</ref> NMT system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>sults in no parameter sharing across languages, and thus O(ST ) parameters. • Per-Language: In the case of Dong et al. (2015), Luong et al. (2016) and Firat et al. (2016a), the parameter generator would gener- ate separate encoder parameters, θ (enc) , for each source language, and separate decoder parame- ters, θ (dec) , for each target language. This leads to a reduction in the number of learnable pa- rameters for multilingual NMT, from O(ST ) to O(S +T ). On one hand, Dong et al. (2015) train multiple models as a one-to-many multilingual NMT system that translates from one source lan- guage to multiple target languages. On the other hand, Luong et al. (2016) and Firat et al. (2016a) perform many-to-many translation. Luong et al. (2016), however, only report results for a single language pair and do not attempt multilingual translation. Firat et al. (2016a) propose an at- tention mechanism that is shared across all lan- guage pairs. We generalize the idea of multi- way multilingual NMT with the parameter gen- erator network, described later. • Universal: In the case of Ha et al. (2016) and Johnson et al. (2017), the authors propose us- ing a single common set of encoder-decoder parameters for all language pairs. While Ha et al. (2016) embed words in a common se- mantic space across languages, Johnson et al. (2017) learn language embeddings that are in the same space as the word embeddings. Here, the parameter generator would provide the same parameters θ (enc) and θ (dec) for all language pairs. It would also create and keep track of learnable variables representing language em- beddings that are prepended to the encoder input sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Pairwise cosine distance for all language pairs in the IWSLT-15 and IWSLT-17 datasets. Darker colors represent more similar languages.</figDesc></figure>

			<note place="foot" n="1"> In fact, it could likely be applied in other scenarios, such as domain adaptation, as well.</note>

			<note place="foot" n="2"> Note that depending on the vocabulary that is used and on whether it is one shared vocabulary across all languages, or one vocabulary per language, the output projection layer of the decoder (which produces probabilities over words) may be language dependent, or common across all languages. In our experiments, we used separate vocabularies and thus this layer was language-dependent.</note>

			<note place="foot" n="3"> This is due to the small number of parameters that need to be learned in this case. To put this into perspective, in most of our experiments we used language embeddings of size 8.</note>

			<note place="foot" n="4"> We did not observe any &quot;forgetting&quot; effect, because we keep &quot;re-visiting&quot; all language pairs throughout training. It would be interesting to explore other sampling schemes, but it is outside the scope of this paper.</note>

			<note place="foot" n="8"> https://github.com/eaplatanios/symphony-mt 9 https://github.com/tensorflow/nmt 10 https://github.com/rsennrich/subword-nmt</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Otilia Stretcu, Abulhair Saparov, and Maruan Al-Shedivat for the useful feedback they provided in early versions of this paper. This research was supported in part by AFOSR under grant FA95501710218.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maruan</forename><surname>Al-Shedivat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinava</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno>abs/1705.10301</idno>
	</analytic>
	<monogr>
		<title level="j">Contextual Explanation Networks. CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Does Multimodality Help Human and Machine for Translation and Image Captioning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Aransa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mercedes</forename><surname>García-Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Overview of the IWSLT 2017 Evaluation Campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichi</forename><surname>Yoshino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Spoken Language Translation</title>
		<meeting>the 14th International Workshop on Spoken Language Translation</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Teacher-Student Framework for ZeroResource Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor O K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1925" to="1935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Neural Machine Translation with Pivot Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<idno>abs/1611.04928</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Glar Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bahdanau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>Fethi Bougares, Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">SYSTRAN&apos;s Pure Neural Machine Translation Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josep</forename><surname>Maria Crego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungi</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anabel</forename><surname>Rebollo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathy</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Akhanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Brunelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Coquard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Enoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyo</forename><surname>Geiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Johanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ardas</forename><surname>Khalsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raoum</forename><surname>Khiari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongil</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Kobus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Lorieux</surname></persName>
		</author>
		<idno>abs/1610.05540</idno>
		<editor>Leidiana Martins, Dang-Chuan Nguyen, Alexandra Priori, Thomas Riccardi, Natalia Segal, Christophe Servan, Cyril Tiquet, Bo Wang, Jin Yang, Dakun Zhang, Jing Zhou, and Peter Zoldan</editor>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-task learning for multiple language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1723" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="866" to="875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Zero-Resource Translation with Multi-Lingual Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baskaran</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fatos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Yarman-Vural</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Universal Neural Machine Translation for Extremely Low Resource Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor O K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="344" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">HyperNetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Toward Multilingual Neural Machine Translation with Universal Encoder and Decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Le</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Workshop on Spoken Language Translation</title>
		<meeting>the 13th International Workshop on Spoken Language Translation</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dual Learning for Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="820" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards Neural PhraseBased Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sitao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Google&apos;s Multilingual Neural Machine Translation System:Enabling Zero-Shot Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="339" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Moses: Open Source Toolkit for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions</title>
		<meeting>the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving Zero-Shot Translation of Low-Resource Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Surafel M Lakew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Quintino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negri</forename><surname>Lotito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Turchi</forename><surname>Matteo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Spoken Language Translation</title>
		<meeting>the 14th International Workshop on Spoken Language Translation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="113" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised Machine Translation Using Monolingual Corpora Only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fully Character-Level Neural Machine Translation without Explicit Segmentation</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="365" to="378" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-task Sequence to Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attentionbased Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Language and Literacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Marr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vietnamese Tradition on Trial</title>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1920" />
			<biblScope unit="page" from="136" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Extreme Adaptation for Personalized Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="312" to="318" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Never-Ending Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estevam</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavana</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thahir</forename><forename type="middle">P</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ndapa</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Emmanouil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Platanios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Samadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derry</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Wijaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abulhair</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Saparov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Greaves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="103" to="115" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">TensorFlow Scala</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Emmanouil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platanios</surname></persName>
		</author>
		<ptr target="https://github.com/eaplatanios/tensorflow_scala" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the Convergence of Adam and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satyen</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Interlingual Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richard H Richens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="144" to="147" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving Neural Machine Translation Models with Monolingual Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
