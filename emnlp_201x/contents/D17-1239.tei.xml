<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Challenges in Data-to-Document Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University Cambridge</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University Cambridge</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University Cambridge</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Challenges in Data-to-Document Generation</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2253" to="2263"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate human-generated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy-and reconstruction-based extensions lead to noticeable improvements .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past several years, neural text genera- tion systems have shown impressive performance on tasks such as machine translation and summa- rization. As neural systems begin to move toward generating longer outputs in response to longer and more complicated inputs, however, the gener- ated texts begin to display reference errors, inter- sentence incoherence, and a lack of fidelity to the source material. The goal of this paper is to suggest a particular, long-form generation task in which these challenges may be fruitfully explored, to provide a publically available dataset for this task, to suggest some automatic evaluation met- rics, and finally to establish how current, neural text generation methods perform on this task.</p><p>A classic problem in natural-language genera- tion (NLG) <ref type="bibr" target="#b17">(Kukich, 1983;</ref><ref type="bibr" target="#b24">McKeown, 1992;</ref><ref type="bibr" target="#b30">Reiter and Dale, 1997</ref>) involves taking structured data, such as a table, as input, and producing text that adequately and fluently describes this data as output. Unlike machine translation, which aims for a complete transduction of the sentence to be translated, this form of NLG is typically taken to require addressing (at least) two separate chal- lenges: what to say, the selection of an appropriate subset of the input data to discuss, and how to say it, the surface realization of a generation <ref type="bibr" target="#b30">(Reiter and Dale, 1997;</ref><ref type="bibr" target="#b11">Jurafsky and Martin, 2014</ref>). Tra- ditionally, these two challenges have been modu- larized and handled separately by generation sys- tems. However, neural generation systems, which are typically trained end-to-end as conditional lan- guage models ( <ref type="bibr" target="#b27">Mikolov et al., 2010;</ref><ref type="bibr" target="#b35">Sutskever et al., 2011</ref><ref type="bibr" target="#b36">Sutskever et al., , 2014</ref>, blur this distinction.</p><p>In this context, we believe the problem of generating multi-sentence summaries of tables or database records to be a reasonable next-problem for neural techniques to tackle as they begin to consider more difficult NLG tasks. In particu- lar, we would like this generation task to have the following two properties: (1) it is relatively easy to obtain fairly clean summaries and their corre- sponding databases for dataset construction, and (2) the summaries should be primarily focused on conveying the information in the database. This latter property ensures that the task is somewhat congenial to a standard encoder-decoder approach, and, more importantly, that it is reasonable to eval- uate generations in terms of their fidelity to the database.</p><p>One task that meets these criteria is that of gen- erating summaries of sports games from associ- ated box-score data, and there is indeed a long history of NLG work that generates sports game summaries <ref type="bibr" target="#b32">(Robin, 1994;</ref><ref type="bibr" target="#b37">Tanaka-Ishii et al., 1998;</ref><ref type="bibr">Barzilay and Lapata, 2005)</ref>. To this end, we make the following contributions:</p><p>• We introduce a new large-scale corpus con- sisting of textual descriptions of basketball games paired with extensive statistical tables. This dataset is sufficiently large that fully data-driven approaches might be sufficient.</p><p>• We introduce a series of extractive evalua- tion models to automatically evaluate output generation performance, exploiting the fact that post-hoc information extraction is signif- icantly easier than generation itself.</p><p>• We apply a series of state-of-the-art neural methods, as well as a simple templated gener- ation system, to our data-to-document gener- ation task in order to establish baselines and study their generations.</p><p>Our experiments indicate that neural systems are quite good at producing fluent outputs and generally score well on standard word-match met- rics, but perform quite poorly at content selection and at capturing long-term structure. While the use of copy-based models and additional recon- struction terms in the training loss can lead to im- provements in BLEU and in our proposed extrac- tive evaluations, current models are still quite far from producing human-level output, and are sig- nificantly worse than templated systems in terms of content selection and realization. Overall, we believe this problem of data-to-document genera- tion highlights important remaining challenges in neural generation systems, and the use of extrac- tive evaluation reveals significant issues hidden by standard automatic metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data-to-Text Datasets</head><p>We consider the problem of generating descriptive text from database records. Following the notation in <ref type="bibr" target="#b20">Liang et al. (2009)</ref>, let s = {r j } J j=1 be a set of records, where for each r ∈ s we define r.t ∈ T to be the type of r, and we assume each r to be a bi- narized relation, where r.e and r.m are a record's entity and value, respectively. For example, a database recording statistics for a basketball game might have a record r such that r.t = POINTS, r.e = RUSSELL WESTBROOK, and r.m = 50. In this case, r.e gives the player in question, and r.m gives the number of points the player scored. From these records, we are interested in generating de- scriptive text, ˆ y 1:T = ˆ y 1 , . . . , ˆ y T of T words such thatˆythatˆ thatˆy 1:T is an adequate and fluent summary of s. A dataset for training data-to-document systems typically consists of (s, y 1:T ) pairs, where y 1:T is a document consisting of a gold (i.e., human gen- erated) summary for database s.</p><p>Several benchmark datasets have been used in recent years for the text generation task, the most popular of these being WEATHERGOV ( <ref type="bibr" target="#b20">Liang et al., 2009</ref>) and ROBOCUP <ref type="bibr" target="#b1">(Chen and Mooney, 2008)</ref>. Recently, neural generation systems have show strong results on these datasets, with the sys- tem of <ref type="bibr" target="#b25">Mei et al. (2016)</ref> achieving BLEU scores in the 60s and 70s on WEATHERGOV, and BLEU scores of almost 30 even on the smaller ROBOCUP dataset. These results are quite promising, and suggest that neural models are a good fit for text generation. However, the statistics of these datasets, shown in <ref type="table">Table 1</ref>, indicate that these datasets use relatively simple language and record structure. Furthermore, there is reason to believe that WEATHERGOV is at least partially machine- generated <ref type="bibr" target="#b29">(Reiter, 2017)</ref>. More recently, <ref type="bibr" target="#b18">Lebret et al. (2016)</ref> introduced the WIKIBIO dataset, which is at least an order of magnitude larger in terms of number of tokens and record types. How- ever, as shown in <ref type="table">Table 1</ref>, this dataset too only contains short (single-sentence) generations, and relatively few records per generation. As such, we believe that early success on these datasets is not yet sufficient for testing the desired linguistic ca- pabilities of text generation at a document-scale.</p><p>With this challenge in mind, we introduce a new dataset for data-to-document text gen- eration, available at https://github.com/ harvardnlp/boxscore-data. The dataset is intended to be comparable to WEATHERGOV in terms of token count, but to have significantly longer target texts, a larger vocabulary space, and to require more difficult content selection.</p><p>The dataset consists of two sources of arti- cles summarizing NBA basketball games, paired with their corresponding box-and line-score ta- bles. The data statistics of these two sources, RO- TOWIRE and SBNATION, are also shown in Ta- ble 1. The first dataset, ROTOWIRE, uses profes- sionally written, medium length game summaries targeted at fantasy basketball fans. The writing is colloquial, but relatively well structured, and targets an audience primarily interested in game  The Atlanta Hawks defeated the Miami Heat , 103 -95 , at Philips Arena on Wednesday . Atlanta was in desperate need of a win and they were able to take care of a shorthanded Miami team here . Defense was key for the Hawks , as they held the Heat to 42 percent shooting and forced them to commit 16 turnovers . Atlanta also dominated in the paint , winning the rebounding battle , 47 -34 , and outscoring them in the paint 58 -26.The Hawks shot 49 percent from the field and assisted on 27 of their 43 made baskets . This was a near wire -to -wire win for the Hawks , as Miami held just one lead in the first five minutes . Miami <ref type="bibr">( 7 - 15 )</ref> are as beat -up as anyone right now and it 's taking a toll on the heavily used starters . Hassan Whiteside really struggled in this game , as he amassed eight points , 12 rebounds and one blocks on 4 -of -12 shooting ... <ref type="figure">Figure 1</ref>: An example data-record and document pair from the ROTOWIRE dataset. We show a subset of the game's records (there are 628 in total), and a selection from the gold document. The document mentions only a select subset of the records, but may express them in a complicated manner. In addition to capturing the writing style, a generation system should select similar record content, express it clearly, and order it appropriately. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluating Document Generation</head><p>We begin by discussing the evaluation of gener- ated documents, since both the task we introduce and the evaluation methods we propose are moti- vated by some of the shortcomings of current ap- proaches to evaluation. Text generation systems are typically evaluated using a combination of au- tomatic measures, such as BLEU ( <ref type="bibr" target="#b28">Papineni et al., 2002</ref>), and human evaluation. While BLEU is perhaps a reasonably effective way of evaluating short-form text generation, we found it to be un- satisfactory for document generation. In particu- lar, we note that it primarily rewards fluent text generation, rather than generations that capture the most important information in the database, or that report the information in a particularly coherent way. While human evaluation, on the other hand, is likely ultimately necessary for evaluating gener- ations ( <ref type="bibr" target="#b21">Liu et al., 2016;</ref><ref type="bibr" target="#b41">Wu et al., 2016)</ref>, it is much less convenient than using automatic metrics. Fur- thermore, we believe that current text generations are sufficiently bad in sufficiently obvious ways that automatic metrics can still be of use in evalu- ation, and we are not yet at the point of needing to rely solely on human evaluators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Extractive Evaluation</head><p>To address this evaluation challenge, we begin with the intuition that assessing document quality is easier than document generation. In particular, it is much easier to automatically extract informa- tion from documents than to generate documents that accurately convey desired information. As such, simple, high-precision information extrac- tion models can serve as the basis for assessing and better understanding the quality of automatic generations. We emphasize that such an evalua- tion scheme is most appropriate when evaluating generations (such as basketball game summaries) that are primarily intended to summarize informa- tion. While many generation problems do not fall into this category, we believe this to be an interest- ing category, and one worth focusing on because it is amenable to this sort of evaluation.</p><p>To see how a simple information extraction sys- tem might work, consider the document in Fig- ure 1. We may first extract candidate entity (player, team, and city) and value (number and cer- tain string) pairs r.e, r.m that appear in the text, and then predict the type r.t (or none) of each can- didate pair. For example, we might extract the entity-value pair ("Miami Heat", "95") from the first sentence in <ref type="figure">Figure 1</ref>, and then predict that the type of this pair is POINTS, giving us an extracted record r such that (r.e, r.m, r.t) = (MIAMI HEAT, 95, POINTS). Indeed, many relation extraction systems reduce relation extraction to multi-class classification precisely in this way <ref type="bibr" target="#b44">(Zhang, 2004;</ref><ref type="bibr" target="#b45">Zhou et al., 2008;</ref><ref type="bibr" target="#b43">Zeng et al., 2014;</ref><ref type="bibr" target="#b33">dos Santos et al., 2015)</ref>.</p><p>More concretely, given a documentˆydocumentˆ documentˆy 1:T , we consider all pairs of word-spans in each sentence that represent possible entities e and values m. We then model p(r.t | e, m; θ) for each pair, us- ing r.t = to indicate unrelated pairs. We use ar- chitectures similar to those discussed in Collobert et al. (2011) and dos <ref type="bibr" target="#b33">Santos et al. (2015)</ref> to param- eterize this probability; full details are given in the Appendix.</p><p>Importantly, we note that the (s, y 1:T ) pairs typically used for training data-to-document sys- tems are also sufficient for training the informa- tion extraction model presented above, since we can obtain (partial) supervision by simply check- ing whether a candidate record lexically matches a record in s. 1 However, since there may be mul- tiple records r ∈ s with the same e and m but with different types r.t, we will not always be able to determine the type of a given entity-value pair found in the text. We therefore train our clas- sifier to minimize a latent-variable loss: for all document spans e and m, with observed types t(e, m) = {r.t : r ∈ s, r.e = e, r.m = m} (possi- bly {}), we minimize</p><formula xml:id="formula_0">L(θ) = − e,m log t ∈t(e,m) p(r.t = t | e, m; θ).</formula><p>We find that this simple system trained in this way is quite accurate at predicting relations. On the <ref type="bibr">1</ref> Alternative approaches explicitly align the document with the table for this task ( <ref type="bibr" target="#b20">Liang et al., 2009</ref>). ROTOWIRE data it achieves over 90% accuracy on held-out data, and recalls approximately 60% of the relations licensed by the records.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparing Generations</head><p>With a sufficiently precise relation extraction sys- tem, we can begin to evaluate how well an auto- matic generationˆygenerationˆ generationˆy 1:T has captured the information in a set of records s. In particular, since the pre- dictions of a precise information extraction system serve to align entity-mention pairs in the text with database records, this alignment can be used both to evaluate a generation's content selection ("what the generation says"), as well as content placement ("how the generation says it").</p><p>We consider in particular three induced metrics:</p><p>• Content Selection (CS): precision and re- call of unique relations r extracted fromˆy fromˆ fromˆy 1:T that are also extracted from y 1:T . This measures how well the generated document matches the gold document in terms of se- lecting which records to generate.</p><p>• Relation Generation (RG): precision and number of unique relations r extracted fromˆy fromˆ fromˆy 1:T that also appear in s. This measures how well the system is able to generate text con- taining factual (i.e., correct) records.</p><p>• Content Ordering (CO): normalized Damerau-Levenshtein Distance <ref type="bibr">(Brill and Moore, 2000</ref>) 2 between the sequences of records extracted from y 1:T and that extracted fromˆyfromˆ fromˆy 1:T . This measures how well the system orders the records it chooses to discuss.</p><p>We note that CS primarily targets the "what to say" aspect of evaluation, CO targets the "how to say it" aspect, and RG targets both.</p><p>We conclude this section by contrasting the automatic evaluation we have proposed with recently proposed adversarial evaluation ap- proaches, which also advocate automatic metrics backed by classification ( <ref type="bibr">Bowman et al., 2016;</ref><ref type="bibr" target="#b12">Kannan and Vinyals, 2016;</ref>. Un- like adversarial evaluation, which uses a black- box classifier to determine the quality of a gener- ation, our metrics are defined with respect to the predictions of an information extraction system. Accordingly, our metrics are quite interpretable, since by construction it is always possible to deter- mine which fact (i.e., entity-value pair) in the gen- eration is determined by the extractor to not match the database or the gold generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Neural Data-to-Document Models</head><p>In this section we briefly describe the neural gener- ation methods we apply to the proposed task. As a base model we utilize the now standard attention- based encoder-decoder model <ref type="bibr" target="#b36">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b3">Cho et al., 2014;</ref><ref type="bibr">Bahdanau et al., 2015</ref>). We also experiment with several recent extensions to this model, including copy-based generation, and training with a source reconstruction term in the loss (in addition to the standard per-target-word loss).</p><p>Base Model For our base model, we map each record r ∈ s into a vector˜rvector˜ vector˜r by first embedding r.t (e.g., POINTS), r.e (e.g., RUSSELL WESTBROOK), and r.m (e.g., 50), and then applying a 1-layer MLP (similar to <ref type="bibr" target="#b42">Yang et al. (2016)</ref>). <ref type="bibr">3</ref> Our source data-records are then represented as˜sas˜ as˜s = {˜r{˜r j } J j=1 . Giveñ s, we use an LSTM decoder with atten- tion and input-feeding, in the style of <ref type="bibr" target="#b23">Luong et al. (2015)</ref>, to compute the probability of each target word, conditioned on the previous words and on s. The model is trained end-to-end to minimize the negative log-likelihood of the words in the gold text y 1:T given corresponding source material s.</p><p>Copying There has been a surge of recent work involving augmenting encoder-decoder models to copy words directly from the source material on which they condition ( <ref type="bibr" target="#b8">Gu et al., 2016;</ref><ref type="bibr" target="#b9">Gülçehre et al., 2016;</ref><ref type="bibr" target="#b26">Merity et al., 2016;</ref><ref type="bibr" target="#b10">Jia and Liang, 2016;</ref><ref type="bibr" target="#b42">Yang et al., 2016</ref>). These models typically introduce an additional binary variable z t into the per-timestep target word distribution, which indi- cates whether the target wordˆywordˆ wordˆy t is copied from the source or generated:</p><formula xml:id="formula_1">p(ˆ y t | ˆ y 1:t−1 , s) = z∈{0,1} p(ˆ y t , z t = z | ˆ y 1:t−1 , s).</formula><p>In our case, we assume that target words are copied from the value portion of a record r; that is, a copy impliesˆyimpliesˆ impliesˆy t = r.m for some r and t.</p><p>Joint Copy Model The models of <ref type="bibr" target="#b8">Gu et al. (2016)</ref> and <ref type="bibr" target="#b42">Yang et al. (2016)</ref> parameterize the joint distribution table overˆyoverˆ overˆy t and z t directly:</p><formula xml:id="formula_2">p(ˆ y t , z t | ˆ y 1:t−1 , s) ∝      copy(ˆ y t , ˆ y 1:t−1 , s) z t = 1, ˆ y t ∈ s 0 z t = 1, ˆ y t ∈ s gen(ˆ y t , ˆ y 1:t−1 , s) z t = 0,</formula><p>where copy and gen are functions parameterized in terms of the decoder RNN's hidden state that as- sign scores to words, and where the notationˆynotationˆ notationˆy t ∈ s indicates thatˆythatˆ thatˆy t is equal to r.m for some r ∈ s.</p><p>Conditional Copy Model <ref type="bibr">Gülçehre et al. (2016)</ref>, on the other hand, decompose the joint probability as:</p><formula xml:id="formula_3">p(ˆ y t , z t | ˆ y 1:t−1 , s) = p copy (ˆ y t | z t , ˆ y 1:t−1 , s) p(z t | ˆ y 1:t−1 , s) z t =1 p gen (ˆ y t | z t , ˆ y 1:t−1 , s) p(z t | ˆ y 1:t−1 , s) z t =0,</formula><p>where an MLP is used to model p(z t | ˆ y 1:t−1 , s). Models with copy-decoders may be trained to minimize the negative log marginal probability, marginalizing out the latent-variable z t ( <ref type="bibr" target="#b8">Gu et al., 2016;</ref><ref type="bibr" target="#b42">Yang et al., 2016;</ref><ref type="bibr" target="#b26">Merity et al., 2016)</ref>. How- ever, if it is known which target words y t are copied, it is possible to train with a loss that does not marginalize out the latent z t . <ref type="bibr">Gülçehre et al. (2016)</ref>, for instance, assume that any target word y t that also appears in the source is copied, and train to minimize the negative joint log-likelihood of the y t and z t .</p><p>In applying such a loss in our case, we again note that there may be multiple records r such that r.m appears inˆyinˆ inˆy 1:T . Accordingly, we slightly modify the p copy portion of the loss of <ref type="bibr">Gülçehre et al. (2016)</ref> to sum over all matched records. In particular, we model the probability of relations r ∈ s such that r.m = y t and r.e is in the same sentence as r.m. Letting r(y t ) = {r ∈ s : r.m = y t , same−sentence(r.e, r.m)}, we have: p copy (y t | z t , y 1:t−1 , s) = r∈r(yt) p(r | z t , y 1:t−1 , s).</p><p>We note here that the key distinction for our pur- poses between the Joint Copy model and the Con- ditional Copy model is that the latter conditions on whether there is a copy or not, and so in p copy the source records compete only with each other. In the Joint Copy model, however, the source records also compete with words that cannot be copied. As a result, training the Conditional Copy model with the supervised loss of <ref type="bibr">Gülçehre et al. (2016)</ref> can be seen as training with a word-level reconstruc- tion loss, where the decoder is trained to choose the record in s that gives rise to y t .</p><p>Reconstruction Losses Reconstruction-based techniques can also be applied at the document- or sentence-level during training. One simple approach to this problem is to utilize the hidden states of the decoder to try to reconstruct the database. A fully differentiable approach using the decoder hidden states has recently been successfully applied to neural machine translation by <ref type="bibr" target="#b38">Tu et al. (2017)</ref>. Unlike copying, this method is applied only at training, and attempts to learn decoder hidden states with broader coverage of the input data.</p><p>In adopting this reconstruction approach we segment the decoder hidden states h t into T B contiguous blocks of size at most B. Denoting a single one of these hidden state blocks as b i , we attempt to predict each field value in some record r ∈ s from b i . We define p(r.e, r.m | b i ), the prob- ability of the entity and value in record r given b i , to be softmax(f (b i )), where f is a parameterized function of b i , which in our experiments utilize a convolutional layer followed by an MLP; full de- tails are given in the Appendix. We further extend this idea and predict K records in s from b i , rather than one. We can train with the following recon- struction loss for a particular b i :</p><formula xml:id="formula_4">L(θ) = − K k=1 min r∈s log p k (r | b i ; θ) = − K k=1 min r∈s x∈{e,m,t} log p k (r.x | b i ; θ),</formula><p>where p k is the k'th predicted distribution over records, and where we have modeled each com- ponent of r independently. This loss attempts to make the most probable record in s given b i more probable. We found that augmenting the above loss with a term that penalizes the total variation distance (TVD) between the p k to be helpful. <ref type="bibr">4</ref> Both L(θ) and the TVD term are simply added to the standard negative log-likelihood objective at training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Methods</head><p>In this section we highlight a few important de- tails of our models and methods; full details are in the Appendix. For our ROTOWIRE models, the record encoder produces˜rproduces˜ produces˜r j in R 600 , and we use a 2-layer LSTM decoder with hidden states of the same size as the˜rthe˜ the˜r j , and dot-product attention and input-feeding in the style of <ref type="bibr" target="#b23">Luong et al. (2015)</ref>. Unlike past work, we use two identically struc- tured attention layers, one to compute the standard generation probabilities (gen or p gen ), and one to produce the scores used in copy or p copy .</p><p>We train the generation models using SGD and truncated BPTT <ref type="bibr" target="#b6">(Elman, 1990;</ref><ref type="bibr" target="#b27">Mikolov et al., 2010)</ref>, as in language modeling. That is, we split each y 1:T into contiguous blocks of length 100, and backprop both the gradients with respect to the current block as well as with respect to the en- coder parameters for each block.</p><p>Our extractive evaluator consists of an ensem- ble of 3 single-layer convolutional and 3 single- layer bidirectional LSTM models. The convolu- tional models concatenate convolutions with ker- nel widths 2, 3, and 5, and 200 feature maps in the style of <ref type="bibr" target="#b14">(Kim, 2014</ref>). Both models are trained with SGD.</p><p>Templatized Generator In addition to neu- ral baselines, we also use a problem-specific, template-based generator. The template-based generator first emits a sentence about the teams playing in the game, using a templatized sentence taken from the training set:</p><p>The &lt;team1&gt; (&lt;wins1&gt;-&lt;losses1&gt;) de- feated the &lt;team2&gt; (&lt;wins2&gt;-&lt;losses2&gt;) &lt;pts1&gt;-&lt;pts2&gt;.</p><p>Then, 6 player-specific sentences of the following form are emitted (again adapting a simple sentence from the training set):</p><p>&lt;player&gt; scored &lt;pts&gt; points (&lt;fgm&gt;- &lt;fga&gt; FG, &lt;tpm&gt;-&lt;tpa&gt; 3PT, &lt;ftm&gt;- &lt;fta&gt; FT) to go with &lt;reb&gt; rebounds.</p><p>encouraging, rather than penalizing the TVD between the p k , which might make sense if we were worried about ensuring the p k captured different records.</p><p>The 6 highest-scoring players in the game are used to fill in the above template. Finally, a typical end sentence is emitted:</p><p>The &lt;team1&gt;' next game will be at home against the Dallas Mavericks, while the &lt;team2&gt; will travel to play the Bulls.</p><p>Code implementing all models can be found at https://github.com/harvardnlp/ data2text. Our encoder-decoder models are based on OpenNMT ( <ref type="bibr" target="#b15">Klein et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>We found that all models performed quite poorly on the SBNATION data, with the best model achieving a validation perplexity of 33.34 and a BLEU score of 1.78. This poor performance is presumably attributable to the noisy quality of the SBNATION data, and the fact that many docu- ments in the dataset focus on information not in the box-and line-scores. Accordingly, we focus on ROTOWIRE in what follows.</p><p>The main results for the ROTOWIRE dataset are shown in <ref type="table" target="#tab_3">Table 2</ref>, which shows the performance of the models in Section 4 in terms of the metrics defined in Section 3.2, as well as in terms of per- plexity and BLEU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Discussion</head><p>There are several interesting relationships in the development portion of <ref type="table" target="#tab_3">Table 2</ref>. First we note that the Template model scores very poorly on BLEU, but does quite well on the extractive metrics, pro- viding an upper-bound for how domain knowledge could help content selection and generation. All the neural models make significant improvements terms of BLEU score, with the conditional copy- ing with beam search performing the best, even though all the neural models achieve roughly the same perplexity.</p><p>The extractive metrics provide further insight into the behavior of the models. We first note that on the gold documents y 1:T , the extractive model reaches 92% precision. Using the Joint Copy model, generation only has a record gen- eration (RG) precision of 47% indicating that re- lationships are often generated incorrectly. The best Conditional Copy system improves this value to 71%, a significant improvement and potentially the cause of the improved BLEU score, but still far below gold.</p><p>The Utah Jazz ( 38 -26 ) defeated the Houston Rockets ( 38 -26 ) 117 -91 on Wednesday at Energy Solutions Arena in Salt Lake City . The Jazz got out to a quick start in this one , out -scoring the Rockets 31 -15 in the first quarter alone . Along with the quick start , the Rockets were the superior shooters in this game , going 54 percent from the field and 43 percent from the three -point line , while the Jazz went 38 percent from the floor and a meager 19 percent from deep . The Rockets were able to out -rebound the Rockets 49 - 49 , giving them just enough of an advantage to secure the victory in front of their home crowd . The Jazz were led by the duo of Derrick Favors and James Harden . Favors went 2 -for -6 from the field and 0 -for -1 from the three -point line to score a game -high of 15 points , while also adding four rebounds and four assists .... Notably, content selection (CS) and content or- dering (CO) seem to have no correlation at all with BLEU. There is some improvement with CS for the conditional model or reconstruction loss, but not much change as we move to beam search. CO actually gets worse as beam search is utilized, possibly a side effect of generating more records (RG#). The fact that these scores are much worse than the simple templated model indicates that fur- ther research is needed into better copying alone for content selection and better long term content ordering models.</p><p>Test results are consistent with development re- sults, indicating that the Conditional Copy model is most effective at BLEU, RG, and CS, and that reconstruction is quite helpful for improving the joint model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Human Evaluation</head><p>We also undertook two human evaluation studies, using Amazon Mechanical Turk. The first study attempted to determine whether generations con- sidered to be more precise by our metrics were also considered more precise by human raters. To accomplish this, raters were presented with a par- ticular NBA game's box score and line score, as well as with (randomly selected) sentences from summaries generated by our different models for those games. Raters were then asked to count how many facts in each sentence were supported by records in the box or line scores, and how many were contradicted. We randomly selected 20 dis- tinct games to present to raters, and a total of 20 generated sentences per game were evaluated by raters. The left two columns of  average numbers of supporting and contradicting facts per sentence as determined by the raters, for each model. We see that these results are generally in line with the RG and CS metrics, with the Con- ditional Copy model having the highest number of supporting facts, and the reconstruction terms sig- nificantly improving the Joint Copy models.</p><p>Using a Tukey HSD post-hoc analysis of an ANOVA with the number of contradicting facts as the dependent variable and the generating model and rater id as independent variables, we found significant (p &lt; 0.01) pairwise differences in con- tradictory facts between the gold generations and all models except "Copy+Rec+TVD," as well as a significant difference between "Copy+Rec+TVD" and "Copy". We similarly found a significant pair- wise difference between "Copy+Rec+TVD" and "Copy" for number of supporting facts.</p><p>Our second study attempted to determine whether generated summaries differed in terms of how natural their ordering of records (as captured, for instance, by the DLD metric) is. To test this, we presented raters with random summaries gen- erated by our models and asked them to rate the naturalness of the ordering of facts in the sum- maries on a 1-7 Likert scale. 30 random sum- maries were used in this experiment, each rated 3 times by distinct raters. The average Likert rat- ings are shown in the rightmost column of  While it is encouraging that the gold summaries received a higher average score than the gener- ated summaries (and that the reconstruction term again improved the Joint Copy model), a Tukey HSD analysis similar to the one presented above revealed no significant pairwise differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Qualitative Example</head><p>Figure 2 shows a document generated by the Con- ditional Copy model, using a beam of size 5. This particular generation evidently has several nice properties: it nicely learns the colloquial style of the text, correctly using idioms such as "19 per- cent from deep." It is also partially accurate in its use of the records; we highlight in blue when it generates text that is licensed by a record in the associated box-and line-scores. At the same time, the generation also contains major logical errors. First, there are basic copy- ing mistakes, such as flipping the teams' win/loss records. The system also makes obvious seman- tic errors; for instance, it generates the phrase "the Rockets were able to out-rebound the Rock- ets." Finally, we see the model hallucinates fac- tual statements, such as "in front of their home crowd," which is presumably likely according to the language model, but ultimately incorrect (and not supported by anything in the box-or line- scores). In practice, our proposed extractive eval- uation will pick up on many errors in this pas- sage. For instance, "four assists" is an RG error, repeating the Rockets' rebounds could manifest in a lower CO score, and incorrectly indicating the win/loss records is a CS error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>In this section we note additional related work not noted throughout. Natural language generation has been studied for decades <ref type="bibr" target="#b17">(Kukich, 1983;</ref><ref type="bibr" target="#b24">McKeown, 1992;</ref><ref type="bibr" target="#b30">Reiter and Dale, 1997)</ref>, and generat- ing summaries of sports games has been a topic of interest for almost as long <ref type="bibr" target="#b32">(Robin, 1994;</ref><ref type="bibr">TanakaIshii et al., 1998;</ref><ref type="bibr">Barzilay and Lapata, 2005</ref>). Historically, research has focused on both con- tent selection ("what to say") <ref type="bibr" target="#b17">(Kukich, 1983;</ref><ref type="bibr" target="#b24">McKeown, 1992;</ref><ref type="bibr" target="#b30">Reiter and Dale, 1997;</ref><ref type="bibr" target="#b5">Duboue and McKeown, 2003;</ref><ref type="bibr">Barzilay and Lapata, 2005)</ref>, and surface realization ("how to say it") <ref type="bibr" target="#b7">(Goldberg et al., 1994;</ref><ref type="bibr" target="#b31">Reiter et al., 2005</ref>) with earlier work using (hand-built) grammars, and later work using SMT-like approaches <ref type="bibr" target="#b40">(Wong and Mooney, 2007)</ref> or generating from PCFGs <ref type="bibr">(Belz, 2008)</ref> or other formalisms <ref type="bibr" target="#b34">(Soricut and Marcu, 2006;</ref><ref type="bibr" target="#b39">White et al., 2007</ref>). In the late 2000s and early 2010s, a number of systems were proposed that did both ( <ref type="bibr" target="#b20">Liang et al., 2009;</ref><ref type="bibr">Angeli et al., 2010;</ref><ref type="bibr" target="#b13">Kim and Mooney, 2010;</ref><ref type="bibr" target="#b22">Lu and Ng, 2011;</ref><ref type="bibr" target="#b16">Konstas and Lapata, 2013)</ref>.</p><p>Within the world of neural text generation, some recent work has focused on conditioning language models on tables ( <ref type="bibr" target="#b42">Yang et al., 2016)</ref>, and generating short biographies from Wikipedia Tables ( <ref type="bibr" target="#b18">Lebret et al., 2016;</ref><ref type="bibr" target="#b2">Chisholm et al., 2017)</ref>. <ref type="bibr" target="#b25">Mei et al. (2016)</ref> use a neural encoder- decoder approach on standard record-based gen- eration datasets, obtaining impressive results, and motivating the need for more challenging NLG problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Future Work</head><p>This work explores the challenges facing neural data-to-document generation by introducing a new dataset, and proposing various metrics for auto- matically evaluating content selection, generation, and ordering. We see that recent ideas in copying and reconstruction lead to improvements on this task, but that there is a significant gap even be- tween these neural models and templated systems. We hope to motivate researchers to focus further on generation problems that are relevant both to content selection and surface realization, but may not be reflected clearly in the model's perplexity. Future work on this task might include ap- proaches that process or attend to the source records in a more sophisticated way, generation models that attempt to incorporate semantic or reference-related constraints, and approaches to conditioning on facts or records that are not as ex- plicit in the box-and line-scores.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>WIN</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example document generated by the Conditional Copy system with a beam of size 5. Text that accurately reflects a record in the associated box-or line-score is highlighted in blue, and erroneous text is highlighted in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>LOSS PTS FG PCT RB AS . . . TEAM</head><label></label><figDesc></figDesc><table>Heat 
11 
12 
103 
49 
47 
27 
Hawks 
7 
15 
95 
43 
33 
20 

AS RB PT FG FGA CITY . . . 
PLAYER 

Tyler Johnson 
5 
2 
27 
8 
16 
Miami 
Dwight Howard 
4 
17 
23 
9 
11 
Atlanta 
Paul Millsap 
2 
9 
21 
8 
12 
Atlanta 
Goran Dragic 
4 
2 
21 
8 
17 
Miami 
Wayne Ellington 
2 
3 
19 
7 
15 
Miami 
Dennis Schroder 
7 
4 
17 
8 
15 
Atlanta 
Rodney McGruder 
5 
5 
11 
3 
8 
Miami 
Thabo Sefolosha 
5 
5 
10 
5 
11 
Atlanta 
Kyle Korver 
5 
3 
9 
3 
9 
Atlanta 
. . . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 contain the</head><label>3</label><figDesc></figDesc><table>Development 

RG 
CS 
CO 
PPL BLEU 
Beam Model 
P% 
# 
P% 
R% 
DLD% 

Gold 
91.77 12.84 
100 
100 
100 
1.00 
100 
Template 
99.35 
49.7 
18.28 65.52 
12.2 
N/A 
6.87 

B=1 

Joint Copy 
47.55 
7.53 
20.53 22.49 
8.28 
7.46 
10.41 
Joint Copy + Rec 
57.81 
8.31 
23.65 23.30 
9.02 
7.25 
10.00 
Joint Copy + Rec + TVD 
60.69 
8.95 
23.63 24.10 
8.84 
7.22 
12.78 
Conditional Copy 
68.94 
9.09 
25.15 22.94 
9.00 
7.44 
13.31 

B=5 

Joint Copy 
47.00 10.67 16.52 26.08 
7.28 
7.46 
10.23 
Joint Copy + Rec 
62.11 10.90 21.36 26.26 
9.07 
7.25 
10.85 
Joint Copy + Rec + TVD 
57.51 11.41 18.28 25.27 
8.05 
7.22 
12.04 
Conditional Copy 
71.07 12.61 21.90 27.27 
8.70 
7.44 
14.46 

Test 

Template 
99.30 49.61 18.50 64.70 
8.04 
N/A 
6.78 
Joint Copy + Rec (B=5) 
61.23 11.02 21.56 26.45 
9.06 
7.47 
10.88 
Joint Copy + Rec + TVD (B=1) 60.27 
9.18 
23.11 23.69 
8.48 
7.42 
12.96 
Conditional Copy (B=5) 
71.82 12.82 22.17 27.16 
8.68 
7.67 
14.49 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance of induced metrics on gold and system outputs of RotoWire development and test data. Columns indicate 
Record Generation (RG) precision and count, Content Selection (CS) precision and recall, Count Ordering (CO) in normalized 
Damerau-Levenshtein distance, perplexity, and BLEU. These first three metrics are described in Section 3.2. Models com-
pare Joint and Conditional Copy also with addition Reconstruction loss and Total Variation Distance extensions (described in 
Section 4). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table># Supp. # Cont. Order Rat. 

Gold 
2.04 
0.70 
5.19 
Joint Copy 
1.65 
2.31 
3.90 
Joint Copy + Rec 
2.33 
1.83 
4.43 
Joint Copy + Rec +TVD 2.43 
1.16 
4.18 
Conditional Copy 
3.05 
1.48 
4.03 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Average rater judgment of number of box score 
fields supporting (left column) or contradicting (middle col-
umn) a generated sentence, and average rater Likert rating for 
the naturalness of a summary's ordering (right column). All 
generations use B=1. 

</table></figure>

			<note place="foot" n="2"> DLD is a variant of Levenshtein distance that allows transpositions of elements; it is useful in comparing the ordering of sequences that may not be permutations of the same set (which is a requirement for measures like Kendall&apos;s Tau).</note>

			<note place="foot" n="3"> We also include an additional feature for whether the player is on the home-or away-team.</note>

			<note place="foot" n="4"> Penalizing the TVD between the p k might be useful if, for instance, K is too large, and only a smaller number of records can be predicted from bi. We also experimented with</note>

			<note place="foot">Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Józefowicz, and Samy Bengio. 2016. Generating sentences from a continuous space. In CoNLL, pages 10-21. Eric Brill and Robert C Moore. 2000. An improved error model for noisy channel spelling correction. In</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge the support of a Google Research Award. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">of the 38th Annual Meeting on Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="286" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to sportscast: a test of grounded language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning to generate one-sentence biographies from wikidata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chisholm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hachey</surname></persName>
		</author>
		<idno>abs/1702.06235</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><forename type="middle">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Statistical acquisition of content selection rules for natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pablo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Duboue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using natural-language processing to produce weather forecasts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norbert</forename><surname>Driedger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard I</forename><surname>Kittredge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Expert</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="45" to="53" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pointing the unknown words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Data recombination for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Speech and language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James H Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Pearson London</publisher>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adversarial evaluation of dialogue models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2016 Workshop on Adversarial Training</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative alignment and semantic parsing for learning from ambiguous supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joohyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="543" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Opennmt: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno>abs/1701.02810</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A global model for concept-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res.(JAIR)</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="305" to="346" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Design of a knowledge-based report generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Kukich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="page" from="145" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural text generation from structured data with application to the biography domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1203" to="1213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adversarial learning for neural dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno>abs/1701.06547</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning semantic correspondences with less supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2122" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A probabilistic forest-to-string model for language generation from typed lambda calculus expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1611" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Text generation-using discourse strategies and focus constraints to generate natural language text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies in natural language processing</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">What to talk about and how? selective generation using lstms with coarse-to-fine alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="720" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1609.07843</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Building applied natural language generation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="87" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Choosing words in computergenerated weather forecasts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somayajulu</forename><surname>Sripada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Davy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="137" to="169" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Revision-based generation of Natural Language Summaries providing historical Background</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Robin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note>Ph.D. thesis, Citeseer</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<title level="m">Classifying relations by ranking with convolutional neural networks. In ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stochastic language generation using widl-expressions and its application in machine translation and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1105" to="1112" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reactive content selection in the generation of real-time soccer commentary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumiko</forename><surname>Tanaka-Ishii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kôiti</forename><surname>Hasida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itsuki</forename><surname>Noda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING-ACL</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="1282" to="1288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural machine translation with reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3097" to="3103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards broad coverage surface realization with ccg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajakrishnan</forename><surname>Rajkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Using Corpora for NLG: Language Generation and Machine Translation (UCNLG+ MT)</title>
		<meeting>the Workshop on Using Corpora for NLG: Language Generation and Machine Translation (UCNLG+ MT)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="267" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generation by inverting a semantic parser that uses statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="172" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Reference-aware language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<idno>abs/1611.01628</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Weakly-supervised relation classification for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth ACM international conference on Information and knowledge management</title>
		<meeting>the thirteenth ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="581" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Semi-supervised learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhua</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoming</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third International Joint Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
