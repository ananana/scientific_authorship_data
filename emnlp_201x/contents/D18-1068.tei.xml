<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Predicting News Headline Popularity with Syntactic and Semantic Knowledge Using Multi-Task Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hardt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Copenhagen Business School</orgName>
								<orgName type="institution" key="instit1">Bocconi University</orgName>
								<orgName type="institution" key="instit2">Copenhagen University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
							<email>dirk.hovy@unibocconi.it</email>
							<affiliation key="aff0">
								<orgName type="department">Copenhagen Business School</orgName>
								<orgName type="institution" key="instit1">Bocconi University</orgName>
								<orgName type="institution" key="instit2">Copenhagen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sotiris</forename><surname>Lamprinidis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Copenhagen Business School</orgName>
								<orgName type="institution" key="instit1">Bocconi University</orgName>
								<orgName type="institution" key="instit2">Copenhagen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Predicting News Headline Popularity with Syntactic and Semantic Knowledge Using Multi-Task Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="659" to="664"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>659</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Newspapers need to attract readers with headlines , anticipating their readers&apos; preferences. These preferences rely on topical, structural, and lexical factors. We model each of these factors in a multi-task GRU network to predict headline popularity. We find that pre-trained word embeddings provide significant improvements over untrained embeddings, as do the combination of two auxiliary tasks, news-section prediction and part-of-speech tagging. However, we also find that performance is very similar to that of a simple Logistic Regression model over character n-grams. Feature analysis reveals structural patterns of headline popularity , including the use of forward-looking de-ictic expressions and second person pronouns.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The data generated from online news consumption constitutes a rich resource, which allows us to ex- plore the relation between news content and user opinions and behaviors. In order to stay in busi- ness, newspapers need to pay attention to this in- formation. For example, what headlines do users click on, and why? With the volume of news being consumed online today, there is great interest in addressing this problem algorithmically. We col- laborate with a large Danish newspaper, who gave us access to several years' worth of headlines, and the number of clicks generated by readers.</p><p>We aggregate the viewing logs to classify head- lines as popular or unpopular, and build models to predict those classifications. We use an expanded version of the dataset investigated by <ref type="bibr" target="#b7">Hardt and Rambow (2017)</ref>. That work found that bag-of- word models based on headlines did indeed have predictive value concerning viewing behavior, al- though models based on the article body were more accurate. As Hardt and Rambow noted, this is somewhat paradoxical: how can a model based on the article text be better at predicting clicks? After all, the choice to click on an article must be based on the headline alone -the article is only seen after the clicking decision is made. Hardt and Rambow speculate that "it is possible that the headline on its own gives readers a lot of semantic information which we are not capturing with our features, but which the whole article does provide. So human readers can "imagine" the article before they read it and implicitly base their behavior on their expectation." <ref type="bibr" target="#b7">(Hardt and Rambow, 2017)</ref> In other words, readers are able to anticipate the contents of an article in advance from a headline, because of the linguistic and world knowledge that they bring to bear when assessing the headline. If we can incorporate this "future" knowledge into a prediction model, we are likely to improve perfor- mance.</p><p>We test this hypothesis by defining ways to model aspects of the lexical, structural, and top- ical knowledge of human news readers:</p><p>• Lexical -Word Embeddings: we provide our models with pretrained word embeddings from large datasets. This models aspects of the rich lexical information and association that human readers bring to bear in reading a headline.</p><p>• Structural -POS Tagging: part of speech information is a basic component of struc- tural linguistic knowledge, reflected in the structure of common headline templates such as "Can X do Y?" or "You will not believe what happened when X".</p><p>• Topical -Section Prediction: Each article is labeled with a section (sports, politics, etc). We include a task which predicts the section of a headline. This models the ability of a news reader to understand the most salient and interesting topical material in a headline text.</p><p>We use a multi-task learning (MTL) setup <ref type="bibr" target="#b5">(Caruana, 1993)</ref>, which provides a natural frame- work to test the above hypotheses: one of the first uses of MTL was to include the outcome of fu- ture diagnostic tests into a prediction task ( <ref type="bibr" target="#b6">Caruana et al., 1996)</ref>.</p><p>We explore the effect of pretrained word em- beddings, and the effects of auxiliary tasks involv- ing POS tagging and section prediction. We find that the combination of all of these factors results in substantial improvements over the baseline and the previous work, which used a single-task sys- tem. We also build logistic regression models, both for word and character n-grams. The word- based models have the advantage that the predic- tiveness of individual words can be examined.</p><p>While the word n-gram models have perfor- mance comparable to the baseline neural net, the character n-gram model has higher performance, competing with the best MTL result. This finding is in line with the results from <ref type="bibr" target="#b14">Zhang et al. (2015)</ref>.</p><p>Our results indicate that MTL can indeed pro- vide the tools to implement prediction processes that involve expectations about the future. Given the successful integration of two auxiliary tasks, we see this as a promising starting point for fu- ture research. However, the performance parity with the character model underscores the fact that simple model architectures still have a place. Our findings, in line with other current work ( <ref type="bibr" target="#b2">Benton et al., 2017)</ref>, shine light on the question of auxil- iary task selection and their interaction, and high- light that MTL results should be rigorously tested.</p><p>A good predictive model is a powerful diagnos- tic tool for editors, allowing them to select pro- posed headlines. However, journalism is a creative production process, so detection is only part of the application. We also want to be able to give strate- gic advice to headline writers. To this end, we re- port an analysis of common n-gram features in the word-based logistic regression model, that provide some insights into successful headline patterns.</p><p>Contributions We explore an MTL architecture with two auxiliary tasks for headline popularity prediction. We show how aspects of lexical, struc- tural, and topical knowledge are all relevant for headline popularity. The positive results reported here provide a fruitful basis for further develop- ment of MTL models for news data. We also ana- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>News Data The present work is based on a sig- nificantly expanded and cleaned version of the dataset used by <ref type="bibr" target="#b7">Hardt and Rambow (2017)</ref>. This dataset includes Jyllands-Posten articles and logs. Jyllands-Posten is a major Danish newspaper (and became known to an international audience over the cartoon controversy). The data covers a pe- riod from July 2015 through July 2017. We re- moved any articles from before July 2015, when the viewing logs began, since these older articles have unreliable numbers of clicks. The resulting dataset consists of 82,532 articles and a total of 281,005,390 user views. We furthermore extracted the news section each article belongs to (sports, politics, etc.) from the URL.</p><p>We bin the articles by numbers of clicks into 2 bins, thus defining a classification task: is the ar- ticle in the top 50% of clicks or not? The data is divided into 80% training, and 10% each develop- ment and test data. <ref type="figure" target="#fig_0">Figure 1</ref> shows the top headline on the Jyllands- Posten web site for August <ref type="bibr">27,</ref><ref type="bibr">2018</ref>. Our data does not include information such as the position of a headline on the page, and possible associated graphical material.</p><p>Additional Data In addition to the news data from JP, we obtained a corpus of 100 million words of Danish text from the Society for Danish <ref type="bibr">Language and Literature, or DSL (Jørg Asmussen, 2018)</ref>. This corpus was collected from diverse sources over a period from 1990 to 2010. The cor- pus has been automatically annotated for part of speech and lemmatization, and we use this for our POS tagging task. We also downloaded the Dan- ish Wikipedia, which consists of approximately 49 million words of Danish text. We use these cor- pora in conjunction with the JP article texts to in- duce pre-trained Danish word-embeddings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Statement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models</head><p>Our task is to predict which articles get the most user clicks, based on the headline alone. We re- port results using logistic regression and a neural network, using MTL.</p><p>Logistic Regression We define the following features for logistic regression models:</p><p>1. n-chars: sequences of n characters, with n ranging from 2 to 6 in all experiments.</p><p>2. word unigrams: tfidf scores for all word uni- grams 3. word bigrams: tfidf scores for all word bi- grams GRU Neural Network While the task is classi- fication, which could be done with a feed-forward model, we want a sequential architecture, so that we can incorporate POS tagging as an auxiliary task, adding POS output at each time step. Based on good results in recent work ( <ref type="bibr" target="#b10">Lee and Dernoncourt, 2016)</ref>, ( <ref type="bibr" target="#b11">Liu et al., 2016)</ref>, we choose a Recurrent Neural Network architecture and after a series of experiments on the training and valida- tion set, we obtained the best results using GRU (Gated Recurrent Unit) units.</p><p>Each layer k consists of two sets of units, la- beled f w and bw that process the sequence for- wards and backwards respectively, so that infor- mation from the whole sequence is available on every timestep t. The two directions' activa- tions are concatenated and fed to a fully-connected softmax (for multi-class classification) or sigmoid layer (for binary classification) to get the output probability y k t of the task associated with layer k. So that higher level tasks can benefit, we embed the output probabilities using the fully connected label embedding LE layer, a technique used on similar scenarios <ref type="figure" target="#fig_0">(Rønning et al., 2018)</ref>. The em- bedded label gets concatenated with the GRU out- put to get the activation a k t that gets fed in the next layer, or the final fully connected prediction layer, as presented in <ref type="figure" target="#fig_1">figure 2</ref>.</p><p>In the sequential auxiliary task, i.e. POS tag- ging, this is done for every timestep, while for the classification tasks the prediction is made on the final timestep.</p><p>For regularization, we apply dropout on every layer of our network. Auxiliary Tasks In our setup, we use two auxil- iary tasks:</p><p>1. POS tagging: we include POS tagging using the DSL dataset on the first recurrent layer of the GRU.</p><p>2. Section prediction: we include classification into one of the 227 sections of the Jyllands-Posten website. The output for this task is based on the penultimate recurrent layer.</p><p>Hyper-parameters and Training We perform a grid search to find the best hyper-parameters for a single-task model (i.e., without any auxiliary tasks) and then keep those settings for all our ex- periments. We settle on a model with hidden size H = 112 and N k = 3 layers, respectively. The dropout probability p = 0.3 gave best results for both models. We train the model for 10 epochs using Adam optimizer with the default parameters, clipping the gradient updates so that their norm is not higher than 5. We train the different tasks sequentially for each epoch, with the lower level (POS tagging) first and the popularity prediction last. Addition- ally, we decay the learning rate by a factor of 0.9 after each epoch. While this is not common with adaptive methods such as Adam, it performed bet- ter. We stop training if the accuracy on the devel- opment set stops improving. <ref type="table" target="#tab_1">Tables 1 and 2</ref> report accuracy for logistic re- gression and neural classifiers. We also give the best score from Hardt and Rambow (2017) for comparison purposes (note, though, that the data sets are not identical and can therefore not be di- rectly compared). We observe a substantial im- provement over the baseline GRU when incorpo- rating the pre-trained embeddings and both aux- iliary tasks. It seems that pretrained embeddings and MTL act at least partly as regularizers, as these models trained for more epochs without overfit- ting. Interestingly, we observe a similar improve- ment over the word-based logistic regression mod- els with a character n-gram model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis and Discussion</head><p>Our main focus in this paper is on MTL as a frame- work to explore the lexical, structural and topical knowledge involved in users' selection of head- lines. However, recognizing a popular headline and giving advice on how to write one are not the same: we want to provide editors and journalists with insights as to what constructions are likely to attract more eyeballs.</p><p>One way to explore this is to examine individual words and their contribution to predictiveness. Ta- ble 3 displays the top 20 unigrams based on their coefficients in the logistic regression model. For each unigram we provide a translation (if needed) and a comment. We classify several unigrams as Deictic-reference. This follows <ref type="bibr" target="#b4">Blom and Hansen (2015)</ref>, who suggest that headline "clickbait" of- ten relies on forward-looking expressions, such as "This", as in, e.g., "This is how you should eat an avocado". Here, "this" is a referring ex- pression, but the reader understands that the an- tecedent will be found in the article body. Sev- eral of these top unigrams are names that are of specific topical interest in areas such as sports and politics. Others mention topics of more general in- terest (Researchers, dead, found). The second per- son pronoun is also on the list -in general, it was found that second person pronouns are far more predictive of popularity than first or third person pronouns. Finally, several unigrams identify sec- tions of the newspaper of particular interest (car, weather, analysis, and satire).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Prediction of news headline popularity is an in- creasingly important problem, as news consump- tion has moved online. The insights and models described here might well be applicable to related problems of interest: for example, <ref type="bibr" target="#b1">Balakrishnan and Parekh (2014)</ref> and <ref type="bibr" target="#b8">Jaidka et al. (2018)</ref> study the problem of predicting clicks on email subject lines. <ref type="bibr" target="#b13">Subramanian et al. (2018)</ref> show that a regression-based multitask approach can increase performance for the classification prediction of popularity. Their work looks at the popularity of online petitions, but the methodology applies to our subject as well, and ties in with the approaches taken in this project. <ref type="bibr" target="#b2">Benton et al. (2017)</ref> caution that in order to evaluate MTL results properly, we need to take the number of parameters into account. Our results to some extent support this finding, by showing that a simpler linear model can fare equally well on the task.</p><p>The choice of auxiliary tasks greatly influences the performance of MTL architectures, prompt- ing several recent investigations into the selec- tion process <ref type="bibr" target="#b0">(Alonso and Plank, 2017;</ref><ref type="bibr" target="#b3">Bingel and Søgaard, 2017)</ref>. However, it is still unclear whether these tasks serve as mere regularizers, or whether they can also impart some additional in- formation. model input accuracy Hardt and Rambow (2017) word unigrams 61.2 logistic regression word unigrams 65.6 logistic regression word bigrams 65.7 logistic regression character 2-6grams 67.4   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented an exploratory approach to predict- ing newspaper article popularity from headlines alone. Using pre-trained embeddings and a MTL setup, we are able to incorporate rich structural and semantic knowledge into the task and sub- stantially improve performance. While the results are encouraging and allow the exploration of fur- ther auxiliary tasks (for example article word pre- diction), we find that a simple character-based n- gram model performs competitively. These find- ings highlight two aspects: 1) For any application of MTL, this is a strong case for comparing the results to non-deep models. While it is compara- tively easy to show an improvement over the basic STL model, there might be other simple models that are competitive.</p><p>2) The selection of auxil- iary tasks greatly influences the performance, even beyond simple regularization, and in a non-linear way. It does, however, provide us with a tool to test human intuitions about task interactions and the importance of certain problem aspects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of Jyllands-Posten headline as seen by audience</figDesc><graphic url="image-1.png" coords="2,307.28,62.81,218.26,162.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Representation of a single timestep t for a pair of forward-backward units on layer k where h k t−1</figDesc><graphic url="image-2.png" coords="3,325.70,427.70,181.40,163.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Accuracy results for various Logistic Regression models 

model 
input 
auxiliary tasks epoch accuracy 
GRU 3 layers w/ 112 hidden random embeddings 
-
3 
65.2 
GRU 3 layers w/ 112 hidden pretrained embeddings -
5 
66.8 
GRU 3 layers w/ 112 hidden pretrained embeddings POS 
5 
65.7 
GRU 3 layers w/ 112 hidden pretrained embeddings section 
4 
66.8 
GRU 3 layers w/ 112 hidden pretrained embeddings POS+section 
7 
67.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : Accuracy results for various GRU model implementations</head><label>2</label><figDesc></figDesc><table>Unigram 
Translation Comment 
Magnussen 
Name (Sports) 
Trump 
Name (Politics) 
AGF 
Name (Sports) 
Test 
Her 
Here 
Deictic-reference 
død 
dead 
topical 
Wozniac 
Name (Tech) 
Trumps 
Name (Politics) 
Forskere 
Researchers topical 
fundet 
found 
topical 
du 
you 
pronoun 
AGF-traener AGF coach Name (sports) 
Se 
Watch 
Deictic-reference 
Kevin 
Name (Sports) 
Islamisk 
Islamic 
Name (Politics) 
Analyse 
Analysis 
Section 
Sådan 
This 
Deictic-reference 
Satire 
Satire 
Section 
bil 
car 
Section 
DMI 
Weather 
Section 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : Top twenty Unigrams (Logistic Regression)</head><label>3</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Thanks to A. Michele Colombo for help with the data and experiments. We also thank Jyllands-Posten for giving us access to the data, and to DSL for data for embeddings and POS annotations.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">When is multitask learning effective? semantic sequence prediction under varying data conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alonso</forename><surname>Héctor Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to predict subject-line opens for large-scale email marketing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raju</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Parekh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Big Data (Big Data)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="579" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multitask Learning for Mental Health Conditions with Limited Social Media Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Benton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="152" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Identifying beneficial task relations for multi-task learning in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Bingel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="164" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Click bait: Forward-reference as lure in online news headlines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><forename type="middle">Nygaard</forename><surname>Blom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth Reinecke</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Pragmatics</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="87" to="100" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multitask Learning: A Knowledge-Based Source of Inductive Bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Machine Learning</title>
		<meeting>the Tenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using the future to &quot;sort out&quot; the present: Rankprop and multitask learning for medical risk evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumeet</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="959" to="965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting user views in online news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 EMNLP Workshop: Natural Language Processing meets Journalism</title>
		<meeting>the 2017 EMNLP Workshop: Natural Language Processing meets Journalism</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting Email and Article Clickthroughs with Domain-adaptive Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kokil</forename><surname>Jaidka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niyati</forename><surname>Chhaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Web Science</title>
		<meeting>the 10th ACM Conference on Web Science</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Society for Danish Language and Literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jørg</forename><surname>Asmussen</surname></persName>
		</author>
		<ptr target="http://dsl.dk.[Online;ac-cessed28" />
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sequential short-text classification with recurrent and convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.03827</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Recurrent neural network for text classification with multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05101</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sluice Resolution without Hand-crafted Features over Brittle Syntax Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ola</forename><surname>Rønning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Content-based Popularity Prediction of Online Petitions Using a Deep Regression Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivashankar</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
