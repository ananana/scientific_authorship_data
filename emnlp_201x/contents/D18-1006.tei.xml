<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reasoning about Actions and State Changes by Injecting Commonsense Knowledge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tandon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Allen Institute for AI</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavana</forename><surname>Dalvi Mishra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Allen Institute for AI</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Grus</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Allen Institute for AI</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Allen Institute for AI</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Allen Institute for AI</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Allen Institute for AI</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reasoning about Actions and State Changes by Injecting Commonsense Knowledge</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="57" to="66"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>57</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Comprehending procedural text, e.g., a paragraph describing photosynthesis, requires modeling actions and the state changes they produce, so that questions about entities at different timepoints can be answered. Although several recent systems have shown impressive progress in this task, their predictions can be globally inconsistent or highly improbable. In this paper, we show how the predicted effects of actions in the context of a paragraph can be improved in two ways: (1) by incorporating global, commonsense constraints (e.g., a non-existent entity cannot be destroyed), and (2) by biasing reading with preferences from large-scale corpora (e.g., trees rarely move). Unlike earlier methods, we treat the problem as a neural structured prediction task, allowing hard and soft constraints to steer the model away from unlikely predictions. We show that the new model significantly outperforms earlier systems on a benchmark dataset for procedural text comprehension (+8% relative gain), and that it also avoids some of the nonsensical predictions that earlier systems make.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Procedural text is ubiquitous (e.g., scientific proto- cols, news articles, how-to guides, recipes), but is challenging to comprehend because of the dynamic nature of the world being described. Comprehend- ing such text requires a model of the actions de- scribed in the text and the state changes they pro- duce, so that questions about the states of entities at different timepoints can be answered .</p><p>Despite these challenges, substantial progress has been made recently in this task. Recent work -such as EntNet ( <ref type="bibr" target="#b5">Henaff et al., 2017)</ref>, <ref type="bibr">QRN (Seo et al., 2017b</ref>), ProLocal/ProGlobal (Dalvi et al., <ref type="figure">Figure 1</ref>: Poor predictions (in red) made by a prior neu- ral model (ProGlobal) applied to an (abbreviated) para- graph from the ProPara dataset. ProGlobal predicts en- tity locations at each sentence, but the implied move- ments violate commonsense constraints (e.g., an object cannot move from itself (1)) and corpus-based prefer- ences (e.g., it is rare to see turbines move (2)).</p><p>2018), and NPN ( ) -has fo- cused on learning to predict individual entity states at various points in the text, thereby approximating the underlying dynamics of the world. However, while these models can learn to make local pre- dictions with fair accuracy, their results are often globally unlikely or inconsistent. For example, in <ref type="figure">Figure 1</ref>, the neural ProGlobal model from  learns to predict the impossible ac- tion of an object moving from itself (1), and the unlikely action of a turbine changing location (2). We observe similar mistakes in other neural mod- els, indicating that these models have little notion of global consistency. Unsurprisingly, mistakes in local predictions compound as the process be- comes longer, further reducing the plausibility of the overall result.</p><p>To address this challenge, we treat process com- prehension as a structured prediction task and ap- ply hard and soft constraints during reading. Dur- ing training, our model, called ProStruct, learns to search for the most likely action sequence that is consistent with global constraints (e.g., entities cannot be destroyed after they have already been destroyed) and priors from background knowledge (e.g., turbines rarely change location). The model is trained end-to-end, with gradients backpropagating through the search path. We find that this approach significantly outperforms existing approaches on a benchmark dataset for process comprehension, mainly by avoiding the nonsensical predictions that earlier systems make.</p><p>Our contributions are twofold. First, we reformu- late procedural text comprehension in a novel way: as a (neural) structured prediction task. This lets hard and soft constraints steer the model away from unlikely and nonsensical predictions. Second, we present a novel, end-to-end model that integrates these constraints and achieves state-of-the-art per- formance on an existing process comprehension dataset ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work builds off a recent body of work that fo- cuses on using neural networks to explicitly track the states of entities while reading long texts. These works have focused on answering simple common- sense questions <ref type="bibr" target="#b5">(Henaff et al., 2017)</ref>, tracking en- tity states in scientific processes ( , tracking ingredients in cooking recipes , and tracking the emotional reactions and motivations of characters in simple stories ( <ref type="bibr" target="#b14">Rashkin et al., 2018)</ref>. Our work extends these methods and addresses their most common issues by using background knowledge about entities to prune the set of state changes they can experience as the model reads new text.</p><p>Prior to these neural approaches, some earlier systems for process comprehension did make use of world knowledge, and motivated this work. Like us, the system ProRead ( <ref type="bibr" target="#b1">Berant et al., 2014;</ref><ref type="bibr" target="#b15">Scaria et al., 2013</ref>) also treated process comprehension as structure prediction, using an Integer Linear Pro- gramming (ILP) formalism to enforce global con- straints (e.g., if the result of event1 is the agent of event2, then event1 must enable event2). Similarly, <ref type="bibr" target="#b7">Kiddon et al. (2015)</ref> used corpus-based priors to guide extraction of an "action graph" from recipes.</p><p>Our work here can viewed as incorporating these approaches within the neural paradigm.</p><p>Neural methods for structure prediction have been used extensively in other areas of NLP, and we leverage these methods here. In particular we use a neural encoder-decoder architecture with beam search decoding, representative of several current state-of-the-art systems ( <ref type="bibr" target="#b0">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b23">Wiseman and Rush, 2016;</ref><ref type="bibr" target="#b20">Vinyals et al., 2015</ref>). As our model's only supervision signal comes from the final prediction (of state changes), our work is similar to previous work in semantic parsing that extracts structured outputs from text with no inter- mediate supervision ( <ref type="bibr" target="#b9">Krishnamurthy et al., 2017)</ref>.</p><p>State tracking also appears in other areas of AI, such as dialog. A typical dialog state tracking task (e.g., the DSTC competitions) involves gradually uncovering the user's state (e.g., their constraints, preferences, and goals for booking a restaurant), until an answer can be provided. Although this context is somewhat different (the primary goal being state discovery from weak dialog evidence), state tracking techniques originally designed for procedural text have been successfully applied in this context also ( <ref type="bibr" target="#b11">Liu and Perez, 2017)</ref>. Finally, our model learns to search over the best candidate structures using hard constraints and soft KB priors. Previous work in Neural Machine Trans- lation (NMT) has used sets of example-specific lex- ical constraints in beam search decoding to only produce translations that satisfy every constraint in the set ( <ref type="bibr" target="#b6">Hokamp and Liu, 2017)</ref>. In contrast, our work uses a set of global example-free constraints to prune the set of possible paths the search algo- rithm can explore. Simultaneously, a recent body of work has explored encoding soft constraints as an additional loss term in the training objective for dialogue <ref type="bibr" target="#b21">(Wen et al., 2015)</ref>, machine translation ( <ref type="bibr" target="#b19">Tu et al., 2016)</ref>, and recipe generation ( <ref type="bibr" target="#b8">Kiddon et al., 2016)</ref>. Our work instead uses soft constraints to re-rank candidate structures and is not directly encoded in the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Definition</head><p>We first define the general task that we are ad- dressing, before presenting our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">General Formulation</head><p>We define the task as follows. Given:</p><p>• A paragraph of procedural text S = an or- dered set of sentences {s 1 , ..., s T } describing  <ref type="figure">Figure 1</ref> is annotated in ProPara. Each filled row shows the lo- cation of entities between each step ("?" denotes "un- known", "-" denotes "does not exist"). For example, in the last line (state4), the water is at the turbine. a sequence of actions 1 about a given topic (a word or phrase).</p><p>• A set of entities E = {e j } representing the en- tities mentioned in the procedure or process. Each entity e j is denoted by the set of its men- tions in the paragraph, e.g., {leaf, leaves} • A set of properties P = {p k } of entities to be tracked (e.g., location, existence) predict:</p><p>• The state of each entity e j after each sentence s k , where an entity's state is the values of all its properties {p k }. For example, in <ref type="figure" target="#fig_0">Figure 2</ref>, the state of the water after step 2 is {loca- tion(water) = turbine; exists(water) = true}. This task definition covers the tasks used in earlier procedural text comprehension datasets. In bAbI tasks 1-3, a single propert (location) was tracked for a single entity throughout a paragraph ( <ref type="bibr" target="#b22">Weston et al., 2015</ref>). In the state tracking task of , six properties (temperature, shape, etc.) were tracked for each ingredient in the recipe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data</head><p>In our work, we use the ProPara dataset ( ) for both illustration and evalution. ProPara contains 488 paragraphs (3100 sentences) of a particular genre of procedural text, namely sci- ence processes (e.g., how hydroelectricity is gen- erated). The dataset tracks two entity properties, existence and location, for all entities involved in each process, resulting in 81,000 annotations in the dataset. <ref type="figure" target="#fig_0">Figure 2</ref> gives a (simplified) example of the data, visualized as an (entity x sentence) grid, where each column tracks a different entity (time progressing vertically downwards), and each row denotes the entities' state (existence and location) after each sentence. To evaluate the predictions, a set of templated questions whose answers can be computed from the predictions is posed (e.g., "What was destroyed, when and where?").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>We now describe our model, called ProStruct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>We approach the task by predicting the state changes that occur at each step of the text, us- ing a vocabulary (size K) of the possible state change types that can occur given the domain and properties being modeled. For example, for the ProPara dataset, we model K = 4 types of state change: move, create, destroy, and none. move changes an entity's location from one place to another, create from non-existence to a location, and destroy from a location to non-existence. State changes can be parameterized by text spans in the paragraph, e.g., move takes a before and after location parameter. If a parameterized state change is predicted, then the model also must predict its parameter values from the paragraph.</p><p>Previous models for process comprehension make a sequence of local predictions about the entities' states, one sentence at a time, maintaining a (typically neural) state at each sentence. However, none have the ability to reverse earlier predictions should an inconsistency arise later in the sequence. ProStruct overcomes this limitation by reformu- lating the task as structured prediction. To do this, it uses a neural encoder-decoder from the semantic parsing literature ( <ref type="bibr" target="#b9">Krishnamurthy et al., 2017;</ref><ref type="bibr" target="#b24">Yin and Neubig, 2017</ref>) combined with a search proce- dure that integrates soft and hard constraints for finding the best candidate structure.</p><p>For each sentence and entity, the encoder first uses a bidirectional LSTM to encode the sentence and indicator variables identifying which entity is currently being considered ( <ref type="figure" target="#fig_1">Figure 3</ref>). It then pro- duces a (distributed) representation of the action that the sentence describes as being applied to that entity. During decoding, the model decodes each action embedding into a distribution over possi- ble state changes that might result, then performs a search over the space of possible state change sequences. Each node in the space is a partial se- quence of state changes, and each edge is a predic- tion of the next state changes to add to the sequence ( <ref type="figure">Figure 4</ref>).</p><p>During training, the model only follows the path along the gold sequence, and optimizes a loss func- tion that drives up the likelihood of predictions along that path (thus driving down the probabilities for alternative, incorrect paths). At test time, the model does not have access to the gold path, and instead performs a beam search of the space to find the best candidate sequence.</p><p>Most importantly, by mapping the state change prediction problem to structured prediction, we can perform a search over the set of candidate paths that allows us to introduce hard and soft constraints that capture commonsense knowledge. Hard con- straints are used to prune the search space (Equa- tion 4 later), and soft constraints bias the search away from unlikely state changes via an additional term in the scoring function (Equations 5 and 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Encoder</head><p>The encoder operates over every (s t , e j ) ∈ S × E pair to create an encoded representation c t j of the action described by sentence s t , as applied to entity e j . In other words, we can consider the overall action to be represented by |E| embeddings, one for each of the entities in E, encoding the action's effects on each. This novel feature allows us to model different effects on different entities by the same action. For example, a conversion action may simultaneously destroy one entity and create another. <ref type="figure" target="#fig_1">Figure 3</ref> shows the encoder operating on s 4 : "The generator spins, and produces electricity" and e 3 : electricity from <ref type="figure">Figure 1</ref>.</p><p>Without loss of generality, we define an arbitrary sentence in S as s t = {w 0 , ..., w I }. Each word w i in the input sentence is encoded as a vector</p><formula xml:id="formula_0">x i = [v w : v e : v v ]</formula><p>, which is the concatenation of a pre-trained word embedding v w for w i , an indicator variable v e for whether w i is a reference to the specified entity e j , and an indicator variable v v for whether w i is a verb. We use GloVe vectors as pre-trained embeddings ( <ref type="bibr" target="#b13">Pennington et al., 2014</ref>) and a POS tagger to extract verbs <ref type="bibr" target="#b18">(Spacy, 2018)</ref>.</p><p>Then, a BiLSTM is used to encode the word representations extracted above, yielding a contex- tualized vector h i for each embedded word x i that is the concatenated output of the backward and for- ward hidden states produced by the BiLSTM for word w i . An attention over the contextualized em- beddings h i is performed to predict a distribution of weights over the sentence:</p><formula xml:id="formula_1">a i = h i * B * h ev + b (1) c t j = I i=1 a i * h i (2)</formula><p>where a i is the attention weight for each contex- tualized embedding, c t j is the vector encoding the action for the sentence-entity pair (s t , e j ), B and b are learned parameters, and h ev is the concatenation of the contextual embeddings of the hidden states where the entity h e and verb h v are mentioned:</p><formula xml:id="formula_2">h ev = [µ({h i : x i [v e ] = 1}); µ({h i : x i [v v ] = 1}]<label>(3)</label></formula><p>where µ is an average function, and</p><formula xml:id="formula_3">x i [v e ] and x i [v v ]</formula><p>correspond to the entity indicator and verb indicator variables defined above for any word w i , respec- tively. The output vector c t j encodes the action at step s t on entity e j . This vector is computed for all steps and entities, populating a grid of the actions on each entity at each step <ref type="figure" target="#fig_1">(Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Decoder</head><p>To decode the action vectors c t j into their resulting state changes they imply, each is passed through a feedforward layer to generate logit(π j t ), a set of lo- gistic activations over the K possible state changes π j t for entity e j in sentence s t . (For ProPara, there are K = 4 possible state changes: move, create, destroy, none). These logits denote how likely each state change π j t is for entity e j at sentence s t . The decoder then explores the search space of possible state change sequences for the whole para- graph <ref type="figure">(Figure 4</ref>), using these likelihoods to score each visited sequence (Equation 6).</p><p>Let π t be the set of state changes for all entities at time t, i.e., π t = {π j t } j=1..|E| , and let Π t be the sequence of state changes from time 1 to t, i.e.,</p><formula xml:id="formula_4">Π t = [π 1 , ..., π t ]</formula><p>. Each node in the search space is a Π t , and each edge adds a π t+1 to it so that it becomes Π t+1 :</p><formula xml:id="formula_5">Π t π t+1</formula><p>− −− → Π t+1 Given there are K possible values for π j t , the num- ber of possible configurations for π t at time t (i.e., the branching factor during search) is exponential: K |E| , where |E| is the number of entities in the para- graph.</p><p>To explore this exponential number of paths, af- ter every sentence s t , we prune branches Π t → Π t+1 where Π t+1 is impossible according to background knowledge (described in Section 5.1). We define the boolean function over state change sequences:</p><p>allowable(Π) = 1 if hard constraints satisfied = 0 otherwise (4) and prune paths Π t+1 where allowable(Π t+1 ) = 0. For example for ProPara, a state transition such as DESTROY → MOVE is not allowed because a hard constraint prohibits non-existent entities from being moved (Section 5.1).</p><p>While hard constraints remove impossible state change predictions, there may also be other state changes that are implausible with respect to back- ground knowledge. For example, commonsense dictates that it is unlikely (but not impossible) for plants to be destroyed during photosynthesis. Ac- cordingly, our inference procedure should discour- age (but not prohibit) predicting plant destruction when reading about photosynthesis. To discourage unlikely state changes, we make use of soft con- straints that estimate the likelihood of a particular state change associated with an entity, denoted as:</p><formula xml:id="formula_6">P(π j |e j , topic)<label>(5)</label></formula><p>In Section 5.2, we describe how these likelihoods can be estimated from large-scale corpora. We add this bias as an additional term (the second term below) when scoring the addition of π t+1 to the <ref type="figure">Figure 4</ref>: The decoder, illustrated for the ProPara do- main. Each action embedding c t j is first passed through a feedforward layer to generate a distribution over the (here K = 4) possible state changes that could result, for each entity (listed horizontally) at each step (listed vertically downwards). The decoder then explores the space of state-change sequences, using these distribu- tions to guide the search. During end-to-end train- ing, ProStruct follows the correct (green) path, and backpropagates to drive up probabilities along this path. During testing, the system performs a beam search to find the most globally plausible sequence. sequence so far Π t :</p><formula xml:id="formula_7">φ (π t+1 ) = |E| j=1 λ logit(π j t+1 ) + (1 − λ) log P(π j t+1 |e j , topic)<label>(6)</label></formula><p>where λ is a learned parameter controlling the de- gree of bias.</p><p>During search, when making a transition along a path from Π t to a valid Π t+1 , Π t+1 is scored by accumulating normalized scores along the path:</p><formula xml:id="formula_8">φ(Π t+1 ) = φ(Π t ) + φ (π t+1 ) π t+1 ∈Π t+1 φ (π t+1 )<label>(7)</label></formula><p>Continuing state transitions in this manner, when we reach the finished state (i.e., last sentence), our objective is to maximize the score of the state changes produced when reading each sentence. During training, we only materialize a valid node when Π t ∈ Π * t where Π * t is the set of nodes along the gold path.</p><p>We use this constrained decoding to predict the state change sequence. For state changes that take additional parameters, e.g., in the ProPara model a move is parameterized by the before and after locations, we also predict those parameter values during decoding. This is done using standard span prediction layers (inspired by BiDAF, <ref type="bibr" target="#b16">Seo et al. (2017a)</ref>) on top of the encoded input.</p><p>The model is trained to minimize the joint loss of predicting the correct state changes and correct state change parameters for every sentence in the paragraph:</p><formula xml:id="formula_9">L = − T t=1</formula><p>log P(π t )+ At test time, instead of following the gold state change path, we use beam search. After reading any sentence, we explore the top-k states sorted by the score φ (π t ) that satisfy hard constraints. This way, we predict a sequence of state changes that have maximum score while being sensible w.r.t. hard constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Incorporating Commonsense Knowledge</head><p>By formulating procedural text comprehension as a structured prediction task, we can introduce com- monsense knowledge as hard and soft constraints into the model, allowing nonsensical and unlikely predictions to be avoided, and allowing the system to recover from early mistakes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Hard Constraints</head><p>Hard constraints are introduced by defining the (boolean) function over a candidate sequence of state changes: allowable(Π) used in Equation 4.</p><p>While this function can be defined in any way, for the ProPara application we use six constraints. The first three below are based on basic "laws of physics" or commonsense (CS) and are universally applicable: CS-1: An entity must exist before it can be moved or destroyed CS-2: An entity cannot be created if it already exists CS-3: An entity cannot change until it is mentioned in the paragraph</p><p>The next three constraints are observed in the training data: D-1: Maximum number of toggles for an entity be- tween Exists and not Exist ≤ f max_toggles D-2: Max fraction of entities that are changed per sentence ≤ f entities_per_sentence D-3: Max fraction of sentences in which an entity changes ≤ f sentences_per_entity</p><p>The thresholds used in D-1, D-2 and D-3 are hyper- parameters that can be tuned on the dev set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Soft Constraints</head><p>Soft constraints are introduced by defining the prior probabilities used in Equation 6: P(π j |e j , topic) that entity e j undergoes state change π j in a sen- tence of text about topic. These probabilities are used to re-rank the candidate event sequences dur- ing decoding (see Equation 6).</p><p>While any method can be used to estimate these probabilities, we describe our corpus-based ap- proach here. Although it was designed for ProPara, it generalizes easily to other domains, and is it- self a contribution of this work. For a given state change π j , entity e j , and topic, we first gather a corpus of Web sentences mentioning that topic (us- ing Bing search APIs), then count the number of times x that the entity is described as undergoing that state change (e.g., that water is said to MOVE). To determine this frequency, we first convert the sentences into a set of SRL frames (verb + role- argument pairs) using an off-the-shelf SRL labeler. We then use an existing rulebase, derived from VerbNet, that contains rules that map SRL frames to state changes, e.g., e1/ARG0 "absorbs"/VERB e2/ARG1 =⇒ e2 MOVES . Al- though the rules and SRL labels are incomplete and noisy, redundancy in the corpus provides some robustness when estimating the frequency x. Fi- nally, the observed frequency x is converted to a likelihood using a logistic transformation:</p><formula xml:id="formula_10">P(π j |e j , topic) = 1 1 + exp −(x−x 0 ) (9)</formula><p>where, x 0 is a hyperparameter tuned on the dev set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Commonsense Constraints for New Domains</head><p>The commonsense constraints we have used for ProPara are general, covering the large variety of topics contain in ProPara (e.g., electricity, photo- synthesis, earthquakes). However, if one wants to apply ProStruct to other genres of procedural text (e.g., fictional text, newswire articles), or broaden the state change vocabulary, different common- sense constraints may be needed. Note that our model architecture itself is agnostic to the source and quantity of hard and soft constraints. For example, one might leverage commonsense rules from existing ontologies such as SUMO <ref type="bibr" target="#b12">(Niles and Pease, 2001</ref>) or Cyc ( <ref type="bibr" target="#b10">Lenat et al., 1985)</ref> to identify new hard constraints; and our corpus-based method could be extended to cover new state change types should the state change vocabulary be extended.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>We evaluate our model using the ProPara dataset, and compare against several strong baselines pub- lished with the original dataset ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Evaluation setup</head><p>Given a paragraph and set of entities as input, the task is to answer four templated questions, whose answers are deterministically computed from the state change sequence: Q1. What are the inputs to the process? Q2. What are the outputs of the process? Q3. What conversions occur, when and where? Q4. What movements occur, when and where? Inputs are defined as entities that existed at the start of the process, but not at the end. Outputs are entities that did not exist at the start, but did at the end. A conversion is when some entities are destroyed and others created. Finally, a movement is an event where an entity changes location. For each process, as every question can have multiple answers, we compute a separate F1 score for each question by comparing the gold and pre- dicted answers. For Q1 and Q2, this is straightfor- ward as answers are atomic (i.e., individual names of entities). For Q3, as each answer is a 4-tuple (convert-from, convert-to, location, sentence-id), some answers may only be partially correct. To score partial correctness, we pair gold and pre- dicted answers by requiring the sentence-id in each to be the same, and then score each pair by the Ham- ming distance of their tuples. For Q4, each answer is also a 4-tuple (entity, from-location, to-location, sentence-id), and the same procedure is applied. The four F1 scores are then macro-averaged. The total number of items to predict in the train/dev/test partitions is 7043/913/1095.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Baselines</head><p>We compare results using the following process comprehension models: Recurrent Entity Networks (EntNet) <ref type="bibr" target="#b5">(Henaff et al., 2017</ref>) are a state-of-the-art model for the bAbI tasks <ref type="figure" target="#fig_0">(Weston et al., 2015)</ref>. The model uses a dynamic memory to maintain a representation of the world state as sentences are read, with a gated update at each step. These states are decoded to answer questions after each sentence is read. Query Reduction Networks (QRN) ( <ref type="bibr" target="#b17">Seo et al., 2017b</ref>) perform a gated propagation of their hidden state across each time-step. Given a question, the hidden state is used to modify the query to keep pointing to the answer at each step. <ref type="bibr">ProLocal (Dalvi et al., 2018</ref>) predicts the state changes described in individual sentences, and then uses commonsense rules of inertia to propagate state values forwards and backwards in time. <ref type="bibr">ProGlobal (Dalvi et al., 2018</ref>) predicts states of an entity across all time steps. It considers the entire paragraph while predicting states for an entity, and learns to predict location spans at time-step t + 1 based on location span predictions at t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Comparison with Baselines</head><p>We compare our model (which make use of world knowledge) with the four baseline systems on the ProPara dataset. All models were trained on the training partition, and the best model picked based on prediction accuracy on the dev partition. <ref type="table">Table 1</ref> shows the precision, recall, and F1 for all models on the the test partition. ProStruct significantly outperforms the baselines, suggesting that world knowledge helps ProStruct avoid spurious pre- dictions. This hypothesis is supported by the fact that the ProGlobal model has the highest recall and worst precision, indicating that it is over-generating state change predictions. Conversely, the ProLocal model has the highest precision, but its recall is much lower, likely because it makes predictions for individual sentences, and thus has no access to information in surrounding sentences that may suggest a state change is occurring.</p><p>We also examined the role of the constraint rules (both hard and soft) on efficiency. With all rules disabled, the training does not complete even one epoch in more than three hours. Because the num- ber of valid states is exponential in the number of  entities, the training is particularly slow on para- graphs with many entities. In contrast, with all rules enabled, training takes less than 10 minutes per epoch. This illustrates that the constraints are not only contributing to the model scores, but also helping make the search efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Ablations and Analysis</head><p>To explore the impact of world knowledge, we also performed two ablations on the dev set: Re- moving soft constraints (at both training and test time), and a partial ablation of removing hard con- straints at test time only -note that hard constraints cannot be removed during training because model training time becomes prohibitively large without them, thus qualifying this second ablation. <ref type="table" target="#tab_4">Table 4</ref> shows that F1 drops when each type of knowledge is removed, illustrating that they are helping. The smaller drop for hard constraints suggests that they have primarily been incorporated into the network during training due to this ablation being partial. Qualitatively, we compared dev set examples where the predicted event sequence changed, com- paring predictions made without world knowledge to those made with world knowledge. For read- ability, we only show the event type predictions (M,C,D, and N (shown as "-")) and not their from- location/to-location arguments. If a prediction changes from X (without knowledge) to Y (with knowledge), we write this "X → Y". For cases where the prediction changed, we show incorrect predictions in red, and correct predictions in green.</p><p>We first compare predictions made with and without the BK (corpus-based background knowl- edge, the soft constraints). <ref type="table" target="#tab_3">Table 3</ref> shows a para- graph about the process of nuclear-powered elec- tricity generation, in the problematic prediction of the generator moving (M) was predicted in the sec- ond to last sentence. However, the background knowledge contains no examples of generators be- ing moved. As a result, it drives the probability mass away from the move (M) prediction, resulting in a no state change (N) prediction instead. <ref type="table" target="#tab_4">Table 4</ref> shows a second example where, with- out knowledge, no event was predicted for the spark entity. However, BK contains many exam- ples of sparks being created (reflecting text about this topic), shifting the probability mass towards this prediction, resulting in the correct C (create).</p><p>Finally, <ref type="table">Table 5</ref> shows an example of a hard con- straint preventing a nonsensical prediction (namely, electricity is created after it already exists).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Error Analysis</head><p>There are also many cases where incorrect predic- tions are made. The main causes are summarized below, and offer opportunities for future work.</p><p>Implicit reference is a challenge for ProStruct, where an entity affected by an event is not men- tioned until a later sentence in the paragraph. both spark and pressure are created in sentence 3, even though pressure is not mentioned until the subsequent sentence. Recognizing this type of implicit mention is very hard. It is possible that BK could help in such situations, particularly if ignite were often associated with creating pres- sure in the context of a combustion engines, but we did not see such examples in practice.</p><p>A second challenge is coreference, in particular when different entities have similar names. For example, again for combustion, a snippet looks:    <ref type="table">Table 5</ref>: Hard constraints avoid nonsensical predictions.</p><formula xml:id="formula_11">...(2)</formula><p>In this example without CS-2, the electricity is pre- dicted to be created after it already exists (impossible). This mistake is avoided using the constraints.</p><p>fuel is ejected. <ref type="formula" target="#formula_8">(7)</ref> new fuel is injected....</p><p>Here fuel and spent fuel are the same entity, while new fuel is a different entity. Correctly tracking these references is challenging (in this case, ProStruct misidentifies <ref type="formula" target="#formula_8">(7)</ref> as describing an event on the original fuel/spent fuel). A third, related problem is pronoun resolution. For example, in:</p><p>The sound continues to bounce off of things and produce echoes until it is to- tally absorbed or dissipated. the word it confuses ProStruct, and it predicts that the echo (rather than the sound) is destroyed. We observe several such failure cases.</p><p>Finally, we observed BK retrieval failures when there was appropriate background knowl- edge that was expressed in a lexically different way. Consider the example in <ref type="table">Table 6</ref> about oil for- mation. Without BK, the model correctly predicts that sediment is destroyed (D). However, BK has few examples of sediment being destroyed, and so biases the prediction away from this (correct) choice to an incorrect choice. Further examination of BK shows that it does in fact have knowledge about this destruction, but that is expressed using the word deposit instead (e.g., "deposits break down"). A soft (neural) means of accessing BK would help alleviate this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>Answering questions about procedural text remains challenging, requiring models of actions and the Without BK vs. with BK Algae Plankton Sediment Algae and plankton die. D D - The dead algae and plankton ... - - - The sediment breaks down. - - D → M <ref type="table">Table 6</ref>: BK lookup limitation: though BK knows that deposits can be destroyed (broken down), it does not equate this with (synonymous) sediments being de- stroyed, hence biases model away from correct answer. state changes they produce. Predictions made lo- cally throughout the text may together be globally inconsistent or improbable. We have shown how the predicted effects of actions can be improved by treating the task as a structured prediction problem, allowing commonsense knowledge to be injected to avoid an overall inconsistent or improbable set of predictions. In particular, we have shown how two kinds of knowledge can be exploited: hard constraints to exclude impossible and nonsensical state changes, and soft constraints to encourage likely state changes. The resulting system signif- icantly outperforms previous state-of-the-art sys- tems on a challenging dataset, and our ablations and analysis suggest that the knowledge is play- ing an important role. Our code is available at https://github.com/allenai/propara.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: How the (simplified) paragraph in Figure 1 is annotated in ProPara. Each filled row shows the location of entities between each step ("?" denotes "unknown", "-" denotes "does not exist"). For example, in the last line (state4), the water is at the turbine.</figDesc><graphic url="image-2.png" coords="3,72.00,62.81,218.27,120.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The encoder, illustrated for the ProPara domain with the paragraph from Figure 1. During encoding, ProStruct creates an action embedding c t j representing the action at step t on entity e k , for all entities at all steps. The overall action sequence (right-hand box) is the collection of these embeddings, for each entity (listed horizontally) and each step (listed vertically downwards).</figDesc><graphic url="image-3.png" coords="4,72.00,63.80,229.19,184.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>t</head><label></label><figDesc>) are the parameters of state change π j t , and v p jt are the values of those parameters. For example, move is parameterized by before/after lo- cations, and the 2nd loss term refers to the predicted values of those locations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>For example, in the following ProPara paragraph snip- pet about combustion engines: "...(3) A spark ignites fuel...(4) The pres- sure pushes the piston down...."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Ablating world knowledge on the dev set.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>BK improves precision. In a nuclear powered 
electricity generation scenario, BK drives the probabil-
ity mass away from the generator movement, as a gen-
erator does not generally change location. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>BK improves coverage. BK has a strong signal 
that a spark is usually created in combustion engines, 
shifting the probability mass towards spark-creation. 

Without and with constraints 
Electricity Signals 
... 
Electricity enters supply unit. 
M 
-
-
The supply gives electricity to transistors. 
C → D 
-
... 
... 
... 
... 
The energy is used to complete ... 
-
-

</table></figure>

			<note place="foot" n="1"> We use a broad definition of action to mean any event that changes the state of the world (including non-volitional events such as roots absorbing water).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Oren Etzioni for his insightful feedback and encouragement for this work. We are grateful to Paul Allen whose long-term vision continues to inspire our scientific endeavors.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling biological processes for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abby</forename><surname>Vander Linden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brittany</forename><surname>Harding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP&apos;14</title>
		<meeting>EMNLP&apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simulating action dynamics with neural process networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corin</forename><surname>Ennis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">What happened? Leveraging VerbNet to predict the effects of actions in procedural text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Bhavana Dalvi Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tandon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05435</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tracking state changes in procedural text: a challenge dataset and models for process paragraph comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavana</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1595" to="1604" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tracking the world state with recurrent entity networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lexically constrained decoding for sequence generation using grid beam search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hokamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mise en place: Unsupervised interpretation of instructional recipes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thandavam</forename><surname>Ganesa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Ponnuraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP&apos;15</title>
		<meeting>EMNLP&apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="982" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Globally coherent text generation with neural checklist models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP&apos;16</title>
		<meeting>EMNLP&apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="329" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural semantic parsing with type constraints for semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cyc: Using common sense knowledge to overcome brittleness and knowledge acquisition bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Douglas B Lenat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shepherd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">65</biblScope>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dialog state tracking, a machine reading approach using memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards a standard upper ontology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Niles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Pease</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOIS</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling naive psychology of characters in simple commonsense stories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning biological processes with global constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Aju Thalappillil Scaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brittany</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Harding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR&apos;17</title>
		<meeting>ICLR&apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Query-reduction networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Spacy tokenizer API reference page</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spacy</surname></persName>
		</author>
		<ptr target="https://spacy.io/api/annotation#pos-tagging.Accessed" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2018" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Coverage-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantically conditioned lstm-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><forename type="middle">Hao</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698</idno>
		<title level="m">Towards AI-complete question answering: A set of prerequisite toy tasks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Sequence-to-sequence learning as beam-search optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A syntactic neural model for general-purpose code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
