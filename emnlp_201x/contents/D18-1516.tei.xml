<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Sequence Encoders for Temporal Knowledge Graph Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NEC Labs Europe</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastijan</forename><forename type="middle">Dumanči´</forename><surname>Dumanči´c</surname></persName>
							<email>sebastijan.dumancic@cs.kuleuven.be</email>
							<affiliation key="aff1">
								<orgName type="institution">KU Leuven</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NEC Labs Europe</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Sequence Encoders for Temporal Knowledge Graph Completion</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4816" to="4821"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4816 The data sets are available under a permissive BSD-3 license 1 .</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Research on link prediction in knowledge graphs has mainly focused on static multi-relational data. In this work we consider temporal knowledge graphs where relations between entities may only hold for a time interval or a specific point in time. In line with previous work on static knowledge graphs, we propose to address this problem by learning latent entity and relation type representations. To incorporate temporal information, we utilize recurrent neural networks to learn time-aware representations of relation types which can be used in conjunction with existing latent factorization methods. The proposed approach is shown to be robust to common challenges in real-world KGs: the sparsity and hetero-geneity of temporal expressions. Experiments show the benefits of our approach on four temporal KGs.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graphs (KGs) are used to organize, manage, and retrieve structured information. The incompleteness of most real-world KGs has stim- ulated research on predicting missing relations be- tween entities. A KG is of the form G = (E, R), where E is a set of entities and, R is a set of re- lation types or predicates. One can represent G as a set of triples of the form (subject, predicate, object), denoted as (s, p, o). The link prediction problem seeks the most probable completion of a triple (s, p, ?) or (?, p, o) ( <ref type="bibr" target="#b15">Nickel et al., 2016)</ref>. We focus on temporal KGs where some triples are augmented with time information and the link pre- diction problem asks for the most probable com- pletion given time information. More formally, a temporal KG G = (E, R, T ) is a KG where facts can also have the form (subject, predicate, object, timestamp) or (subject, predicate, object, time predicate, timestamp), in addition to (s, p, o) triples. For instance, facts such as <ref type="bibr">(Barack Obama, born, US, 1961)</ref> or <ref type="bibr">(Barack Obama, president, US, occursSince, 2009-01)</ref> express temporal in- formation about the facts associated with Barack Obama. While the former expresses that a relation type occurred at a specific point in time, the latter expresses an (open) time interval using the time predicate "occursSince." The latter example also illustrates a common challenge posed by the het- erogeneity of time expressions due to variations in language and serialization standards.</p><p>Most approaches to link prediction are char- acterized by a scoring function that operates on the entity and relation type embeddings of a triple ( <ref type="bibr" target="#b1">Bordes et al., 2013;</ref><ref type="bibr" target="#b18">Yang et al., 2014;</ref><ref type="bibr" target="#b7">Guu et al., 2015)</ref>. Learning representations that carry temporal information is challenging due to the sparsity and irregularities of time expressions. It is possible, however, to turn time expressions into sequences of tokens expressing said temporal in- formation. Moreover, character-level architectures for language modeling ( <ref type="bibr" target="#b19">Zhang et al., 2015;</ref><ref type="bibr" target="#b11">Kim et al., 2016</ref>) operate on characters as atomic units to derive word embeddings. Inspired by these models, we propose a method to incorporate time information into standard embedding approaches for link prediction. We learn time-aware represen- tations by training a recursive neural network with sequences of tokens representing the time predi- cate and the digits of the timestamp, if they ex- ist. The last hidden state of the recurrent network is combined with standard scoring functions from the KG completion literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Reasoning with temporal information in knowl- edge bases has a long history and has resulted in numerous temporal logics <ref type="bibr" target="#b0">(van Benthem, 1995)</ref>. Several recent approaches extend statistical rela- tional learning frameworks with temporal reason- ing capabilities ( <ref type="bibr" target="#b3">Chekol and Stuckenschmidt, 2018;</ref><ref type="bibr" target="#b4">Dylla et al., 2013)</ref>.</p><p>There is also prior work on incorporating tem- poral information in knowledge graph completion methods. <ref type="bibr" target="#b9">Jiang et al. (2016)</ref> capture the temporal ordering that exists between some relation types as well as additional common-sense constraints to generate more accurate link predictions. <ref type="bibr" target="#b5">Esteban et al. (2016)</ref> introduce a prediction model for link prediction that assumes that changes to a KG are introduced by incoming events. These events are modeled as a separate event graph and used to predict the existence of links in the future. <ref type="bibr" target="#b17">Trivedi et al. (2017)</ref> model the occurrence of a fact as a point process whose intensity function is influenced by the score assigned to the fact by an embedding function. <ref type="bibr" target="#b13">Leblay and Chekol (2018)</ref> develop scoring functions that incorporate time representations into a TransE-type scoring func- tion. Prior work has also incorporated numeri- cal but non-temporal entity information for knowl- edge base completion <ref type="bibr" target="#b6">(Garcia-Duran and Niepert, 2017)</ref>.</p><p>Contrary to all previous approaches, we encode sequences of temporal tokens with an RNN. This facilitates the encoding of relation types with tem- poral tokens such as "since," "until," and the dig- its of timestamps. Moreover, the RNN encoding provides an inductive bias for parameter sharing among similar timestamps (e.g., those occurring in the same century). Finally, our method can be combined with all existing scoring functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Time-Aware Representations</head><p>Embedding approaches for KG completion learn a scoring function f that operates on the embed- dings of the subject e s , the object e o , and the pred- icate e p of the triples. The value of this scoring function on a triple (s, p, o), f (s, p, o), is learned to be proportional to the likelihood of the triples being true. Popular examples of scoring functions are</p><formula xml:id="formula_0">• TRANSE (Bordes et al., 2013) f (s, p, o) = ||e s + e p − e o || 2 .</formula><p>(1) bornIn 1y ep seq (s, p seq , o) : f(e s , e p seq , e o) 9y 8y 6y</p><p>Figure 1: Learning time-aware representations.</p><p>• DISTMULT ( <ref type="bibr" target="#b18">Yang et al., 2014</ref>):</p><formula xml:id="formula_1">f (s, p, o) = (e s • e o )e T p ,<label>(2)</label></formula><p>where e s , e o ∈ R d are the embeddings of the sub- ject and object entities, e p ∈ R d is the embedding of the relation type predicate, and • is the element- wise product. These scoring functions do not take temporal information into account. Given a temporal KG where some triples are augmented with temporal information, we can de- compose a given (possibly incomplete) timestamp into a sequence consisting of some of the follow- ing temporal tokens</p><formula xml:id="formula_2">year 0 · 1 · 2 · 3 · 4 · 5 · 6 · 7 · 8 · 9 month 01 · 02 · 03 · 04 · 05 · 06 · 07 · 08 · 09 · 10 · 11 · 12 day 0 · 1 · 2 · 3 · 4 · 5 · 6 · 7 · 8 · 9</formula><p>Hence, temporal tokens have a vocabulary size of 32. Moreover, for each triple we can extract a sequence of predicate tokens that always consists of the relation type token and, if available, a tem- poral modifier token such as "since" or "until." We refer to the concatenation of the predicate token sequence and (if available) the sequence of tem- poral tokens as the predicate sequence p seq . Now, a temporal KG can be represented as a collection of triples of the form (s, p seq , o), wherein the pred- icate sequence may include temporal information. <ref type="table" target="#tab_0">Table 1</ref> lists some examples of such facts from a temporal KG and their corresponding predicate se- quence. We use the suffix y, m and d to indicate whether the digit corresponds to year, month or day information. It is these sequences of tokens that are used as input to a recurrent neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LSTMs for Time-Encoding Sequences</head><p>A long short-term memory (LSTM) is a neural network architecture particularly suited for mod- eling sequential data. The equations defining an</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fact</head><p>Predicate Sequence (Barack Obama, country, US)</p><p>[country] ( <ref type="bibr">Barack Obama, born, US, 1961)</ref> [born, 1y, 9y, 6y, 1y] (Barack Obama, president, US, since, 2009-01) [president, since, 2y, 0y, 0y, 9y, 01m] </p><formula xml:id="formula_3">i = σ g (h n−1 U i + x n W i ) f = σ g (h n−1 U f + x n W f ) o = σ g (h n−1 U o + x n W o ) g = σ c (h n−1 U g + x n W g ) c n = f • c n−1 + i • g h n = o • σ h (c n ) (3)</formula><p>where i, f , o and g are the input, forget, output and input modulation gates, respectively. c and h are the cell and hidden state, respectively. All vectors are in R h . x n ∈ R d is the representation of the n-th element of a sequence. In this paper we set h = d. σ g , σ c and σ h are activation functions.</p><p>Each token of the input sequence p seq is first mapped to its corresponding d-dimensional em- bedding via a linear layer and the resulting se- quence of embeddings used as input to the LSTM. Each predicate sequence of length N is repre- sented by the last hidden state of the LSTM, that is, e pseq = h N . The predicate sequence repre- sentation, which carries time information, can now be used in conjunction with subject and object em- beddings in standard scoring functions. For in- stance, temporal-aware versions of TRANSE and DISTMULT, which we refer to as TA-TRANSE and TA-DISTMULT, have the following scoring function for triples (s, p seq , o):</p><formula xml:id="formula_4">TA-TRANSE: f (s, p seq , o) = ||e s + e pseq − e o || 2 TA-DISTMULT: f (s, p seq , o) = (e s • e o )e T pseq .</formula><p>All parameters of the scoring functions are learned jointly with the parameters of the LSTMs using stochastic gradient descent.</p><p>The advantages of character level models to en- code time information for link prediction are: (1) the usage of digits and modifiers such as "since" as atomic tokens facilitates the transfer of informa- tion across similar timestamps, leading to higher efficiency (e.g. small vocabulary size); (2) at test time, one can obtain a representation for a times- tamp even though it is not part of the training set;</p><p>(3) the model can use triples with and without tem- poral information as training data. <ref type="figure">Figure 1</ref> illus- trates the generic working of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conducted experiments on four different KG completion data sets where a subset of the facts are augmented with time information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Integrated Crisis Early Warning System (ICEWS) is a repository that contains political events with a specific timestamp. These political events relate entities (e.g. countries, presidents...) to a num- ber of other entities via logical predicates (e.g. 'Make a visit' or 'Express intent to meet or ne- gotiate'). Additional information can be found at http://www.icews.com/. The repository is organized in dumps that contain the events that occurred each year from 1995 to 2015. We cre- ated two temporal KGs out of this repository, i) a short-range version that contains all events in 2014, and ii) a long-range version that contains all events occurring between 2005-2015. We re- fer to these two data sets as <ref type="bibr">ICEWS 2014 and</ref><ref type="bibr">ICEWS 2005-15</ref>, respectively. Due to the large number of entities we selected a subset of the most frequently occurring entities in the graph and all facts where both the subject and object are part of this subset of entities. We split the facts into training, validation and test in a pro- portion of 80%/10%/10%, respectively. The pro- tocol for the creation of these data sets is identi- cal to the onw followed in previous work <ref type="bibr" target="#b1">(Bordes et al., 2013</ref>    ICEWS data sets, YAGO15K does contain tempo- ral modifiers; namely, 'occursSince' and 'occur- sUntil'. Contrary to previous work ( <ref type="bibr" target="#b13">Leblay and Chekol, 2018)</ref>, all facts maintain time information in the same level of granularity as one can find in the original dumps these data sets come from. We also experimented with the temporal facts from the WIKIDATA data set 4 extracted in ( <ref type="bibr" target="#b13">Leblay and Chekol, 2018)</ref>. Only information regarding the year is available for these facts, since the authors discarded information of finer granular- ity. All facts are framed in a time interval (i.e. they contain the temporal modifiers 'occursSince' and 'occursUntil'). Facts annotated with a single point-in-time are associated with that time-point as start and end time. Due to the large number of entities of this data set, which hinders the com- putation of standard KG completion metrics, we selected a subset of the most frequent entities and <ref type="bibr">4</ref> http://staff.aist.go.jp/julien.leblay/datasets kept all facts where both the subject and object are part of this subset of entities. This set of filtered facts was split into training, validation and test in the same proportion as before. <ref type="table" target="#tab_2">Table 2</ref> lists some statistics of the temporal KGs. All four data sets, with their corresponding training, validation, and test splits are available at https://github.com/nle-ml/mmkb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">General Set-up</head><p>We evaluate various methods by their ability to answer completion queries where i) all the argu- ments of a fact are known except the subject entity, and ii) all the arguments of a fact are known except the object entity. For the former we replace the subject by each of the KBs entities E in turn, sort the triples based on the scores returned by the dif- ferent methods, and computed the rank of the cor- rect entity. We repeated the same process for the second completion task and average the results.</p><p>[playsFor, since, temporal_tokens(date)] T-SNE visualization of the embed- dings learned for the predicate sequence p seq = [playsFor, occursSince, date], where date corre- sponds to the date token sequence. This is standard procedure in the KG completion literature. We also report the filtered setting as de- scribed in <ref type="bibr" target="#b1">(Bordes et al., 2013)</ref>. The mean of all computed ranks is the Mean Rank (lower is bet- ter) and the fraction of correct entities ranked in the top n is called hits@n (higher is better). We also compute the Mean Reciprocal Rank (higher is better) which is less susceptible to outliers.</p><p>Recent work ( <ref type="bibr" target="#b13">Leblay and Chekol, 2018</ref>) evalu- ates different approaches for performing link pre- diction in temporal KGs. The approach that learns independent representations for each timestamp and use these representations as translation vec- tors, similarly to ( <ref type="bibr" target="#b1">Bordes et al., 2013)</ref>, leads to the best results. This approach is called VECTOR- BASED TTRANSE, though for the shake of sim- plicity in the paper we refer to it as TTRANSE. We compare our approaches TA-TRANSE and TA- DISTMULT against TTRANSE, and the standard embedding methods TRANSE and DISTMULT. For all approaches, we used ADAM ( <ref type="bibr" target="#b12">Kingma and Ba, 2014</ref>) for parameter learning in a mini-batch setting with a learning rate of 0.001, the cate- gorical cross-entropy ( <ref type="bibr" target="#b10">Kadlec et al., 2017)</ref> as loss function and the number of epochs was set to 500. We validated every 20 epochs and stopped learn- ing whenever the MRR values on the validation set decreased. The batch size was set to 512 and the number of negative samples to 500 for all ex- periments. The embedding size is d=100. We ap- ply dropout ( <ref type="bibr" target="#b16">Srivastava et al., 2014</ref>) for all embed- dings. We validated the dropout from the values {0, 0.4} for all experiments. For TA-TRANSE and TA-DISTMULT, the activation gate σ g is the sig- moid function; σ c and σ h were chosen to be linear activation functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20">50</head><p>Epoch 0.25</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Training Loss</head><p>TransE TA-TransE   <ref type="table" target="#tab_2">Table 2)</ref> and thus enough training examples exist to learn robust timestamp embeddings. TTRANSE's per- formance is similar to that of TA-TRANSE, our time-aware version of TRANSE, in WIKIDATA. Similarly, TTRANSE can learn robust timestamp representations because of the small number of distinct timestamps of this data set. <ref type="figure" target="#fig_1">Figure 3</ref> shows a comparison of the training loss of TRANSE and TA-TRANSE for YAGO15K. Under the same set-up, TA-TRANSE's ability to learn from time information leads to a training loss lower than that of TRANSE. <ref type="figure">Figure 2</ref> shows a t-SNE <ref type="bibr" target="#b14">(Maaten and Hinton, 2008</ref>) visualization of the embeddings learned for the predicate sequence p seq = [playsFor, occursS- ince, date], where date corresponds to the date to- ken sequence. This illustrates that the learned rela- tion type embeddings carry temporal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We propose a digit-level LSTM to learn represen- tations for time-augmented KG facts that can be used in conjunction with existing scoring func- tions for link prediction. Experiments in four tem- poral knowledge graphs show the effectiveness of the approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Training loss in YAGO15K. TATRANSE's ability to learn from time information leads to a lower loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Facts and their corresponding predicate sequence.</head><label>1</label><figDesc></figDesc><table>LSTM are 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>). To create YAGO15K, we used FREEBASE15K (Bordes et al., 2013) (FB15K) as a blueprint. We aligned entities from FB15K to YAGO (Hoffart et al., 2013) with SAMEAS rela- tions contained in a YAGO dump 2 , and kept all facts involving those entities. Finally, we aug- ment this collection of facts with time information from the "yagoDateFacts" 3 dump. Contrary to the</figDesc><table>Data set 
YAGO15K ICEWS '14 ICEWS 05-15 WIKIDATA 
Entities 
15,403 
6,869 
10,094 
11,134 
Relationships 
34 
230 
251 
95 
#Facts 
138,056 
96,730 
461,329 
150,079 
#Distinct TS 
198 
365 
4,017 
328 
Time Span 
1513-2017 
2014 
2005-2015 
25-2020 

Training 
110,441 
72,826 
368,962 
121,422 
[29,381] 
[72,826] 
[368,962] 
[121,422] 

Validation 
13,815 
8,941 
46,275 
14,374 
[3,635] 
[8,941] 
[46,275] 
[14,374] 

Test 
13,800 
8,963 
46,092 
14,283 
[3,685] 
[8,963] 
[46,092] 
[14,283] 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Statistics of the data sets. TS stands for timestamps. The number of facts with time information 
is in brackets. 

YAGO15K 
WIKIDATA 
MRR MR Hits@10 Hits@1 MRR MR Hits@10 Hits@1 
TTRANSE 
32.1 
578 
51.0 
23.0 
48.8 
80 
80.6 
33.9 
TRANSE 
29.6 
614 
46.8 
22.8 
31.6 
50 
65.9 
18.1 

DISTMULT 

27.5 
578 
43.8 
21.5 
31.6 
77 
66.1 
18.1 
TA-TRANSE 
32.1 
564 
51.2 
23.1 
48.4 
79 
80.7 
32.9 
TA-DISTMULT 
29.1 
551 
47.6 
21.6 
70.0 
198 
78.5 
65.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Results (filtered setting) of the temporal knowledge graph completion experiments for the data sets YAGO15K and WIKIDATA. The best results are written bold.</figDesc><table>ICEWS 2014 
ICEWS 2005-15 
MRR MR Hits@10 Hits@1 MRR MR Hits@10 Hits@1 
TTRANSE 
25.5 
148 
60.1 
7.4 
27.1 
181 
61.6 
8.4 
TRANSE 
28.0 
122 
63.7 
9.4 
29.4 
84 
66.3 
9.0 

DISTMULT 

43.9 
189 
67.2 
32.3 
45.6 
90 
69.1 
33.7 
TA-TRANSE 
27.5 
128 
62.5 
9.5 
29.9 
79 
66.8 
9.6 
TA-DISTMULT 
47.7 
276 
68.6 
36.3 
47.4 
98 
72.8 
34.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results (filtered setting) of the temporal knowledge graph completion experiments for the data 
sets ICEWS 2014 and ICEWS 2005-15. The best results are written bold. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 and</head><label>3</label><figDesc></figDesc><table>4 list the results for the KG comple-
tion tasks. TA-TRANSE and TA-DISTMULT sys-
tematically improve TRANSE and DISTMULT in 
MRR, hits@10 and hits@1 in almost all cases. 
Mean rank is a metric that is very susceptible 
to outliers and hence these improvements are not 
consistent. TTRANSE learns independent repre-
sentations for each timestamp contained in the 
training set. At test time, timestamps unseen dur-
ing training are represented by null vectors. This 
explains that TTRANSE is only competitive in 
YAGO15K, wherein the number of distinct times-
tamps is very small (see #Distinct TS in </table></figure>

			<note place="foot" n="2"> /yago-naga/yago3.1/yagoDBpediaInstances.ttl.7z 3 /yago-naga/yago3.1/yagoDateFacts.ttl.7z</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Handbook of logic in artificial intelligence and logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Benthem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">chapter Temporal Logic</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press, Inc</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="241" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Marrying uncertainty and time in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Melisachew Wudage Chekol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joerg</forename><surname>Pirrò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiner</forename><surname>Schoenfisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stuckenschmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="88" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rule Based Temporal Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiner</forename><surname>Melisachew Wudage Chekol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stuckenschmidt</surname></persName>
		</author>
		<idno>1-4:14</idno>
	</analytic>
	<monogr>
		<title level="m">Technical Communications of the 33rd International Conference on Logic Programming</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">58</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A temporal-probabilistic database model for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Dylla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Miliaraki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Theobald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1810" to="1821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predicting the co-evolution of event and knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Esteban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krompa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 19th International Conference on Information Fusion (FUSION)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="98" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Kblrn: End-to-end learning of knowledge base representations with latent, relational, and numerical features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04676</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Traversing knowledge graphs in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01094</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Yago2: A spatially and temporally enhanced knowledge base from wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="28" to="61" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Towards time-aware knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingsong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sui</surname></persName>
		</author>
		<editor>COLING</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Knowledge base completion: Baselines strike back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10744</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2741" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deriving validity time in knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Leblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Melisachew Wudage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chekol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the The Web Conference 2018, WWW &apos;18. International World Wide Web Conferences Steering Committee</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A review of relational machine learning for knowledge graphs. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="11" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Know-evolve: Deep temporal reasoning for dynamic knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakshit</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia. PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3462" to="3471" />
		</imprint>
	</monogr>
	<note>International Convention Centre</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multi-relational semantics using neural-embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4072</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
