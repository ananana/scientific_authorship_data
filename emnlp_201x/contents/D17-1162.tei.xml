<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Grasping the Finer Point: A Supervised Similarity Network for Metaphor Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">The ALTA Institute</orgName>
								<orgName type="institution" key="instit2">University of Cambridge</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♣</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luana</forename><surname>Bulat</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Laboratory</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Grasping the Finer Point: A Supervised Similarity Network for Metaphor Detection</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1537" to="1546"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The ubiquity of metaphor in our everyday communication makes it an important problem for natural language understanding. Yet, the majority of metaphor processing systems to date rely on hand-engineered features and there is still no consensus in the field as to which features are optimal for this task. In this paper, we present the first deep learning architecture designed to capture metaphorical composition. Our results demonstrate that it outperforms the existing approaches in the metaphor identification task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Metaphor is pervasive in our everyday commu- nication, enriching it with sophisticated imagery and helping us to reconcile our experience in the world with our conceptual system <ref type="bibr" target="#b15">(Lakoff and Johnson, 1980)</ref>. In the most influential account of metaphor to date, Lakoff and Johnson explain the phenomenon through the presence of system- atic metaphorical associations between two dis- tinct concepts or domains. For instance, when we talk about "curing juvenile delinquency" or "corruption transmitting through the government ranks", we view the general concept of crime (the target concept) in terms of the properties of a dis- ease (the source concept). Such metaphorical as- sociations are broad generalisations that allow us to project knowledge and inferences across do- mains; and our metaphorical use of language is a reflection of this process.</p><p>Given its ubiquity, metaphorical language poses an important problem for natural language un- derstanding <ref type="bibr" target="#b4">(Cameron, 2003;</ref><ref type="bibr" target="#b25">Shutova and Teufel, 2010)</ref>. A number of approaches to metaphor pro- cessing have thus been proposed, focusing pre- dominantly on classifying linguistic expressions as literal or metaphorical. They experimented with a range of features, including lexical and syntac- tic information <ref type="bibr" target="#b12">(Hovy et al., 2013;</ref><ref type="bibr" target="#b1">Beigman Klebanov et al., 2016</ref>) and higher-level features such as semantic roles <ref type="bibr" target="#b9">(Gedigian et al., 2006</ref>), domain types <ref type="bibr" target="#b6">(Dunn, 2013)</ref>, concreteness <ref type="bibr" target="#b28">(Turney et al., 2011</ref>), imageability ( <ref type="bibr" target="#b26">Strzalkowski et al., 2013)</ref> and WordNet supersenses ( <ref type="bibr" target="#b27">Tsvetkov et al., 2014</ref>). While reporting promising results, all of these ap- proaches used hand-engineered features and re- lied on manually-annotated resources to extract them. In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features ( <ref type="bibr" target="#b23">Shutova and Sun, 2013)</ref> and dense neural word embeddings ( <ref type="bibr" target="#b2">Bracewell et al., 2014;</ref>. Their experiments have demonstrated that corpus-driven lexical representations already en- code information about semantic domains needed to learn the patterns of metaphor usage from lin- guistic data.</p><p>We take this intuition a step further and present the first deep learning architecture designed to capture metaphorical composition. Deep learn- ing methods have already been shown success- ful in many other semantic tasks (e.g. <ref type="bibr" target="#b11">Hermann et al., 2015;</ref><ref type="bibr" target="#b14">Kumar et al., 2015;</ref><ref type="bibr" target="#b31">Zhao et al., 2015)</ref>, which suggests that designing a specialised neu- ral network architecture for metaphor detection will lead to improved performance. In this paper, we present a novel architecture which (1) mod- els the interaction between the source and tar- get domains in the metaphor via a gating func- tion; (2) specialises word representations for the metaphor identification task via supervised train- ing; (3) quantifies metaphoricity via a weighted similarity function that automatically selects the relevant dimensions of similarity. We experi- mented with two types of word representations as inputs to the network: the standard skip-gram word embeddings ( <ref type="bibr" target="#b17">Mikolov et al., 2013a</ref>) and the cognitively-driven attribute-based vectors <ref type="bibr" target="#b3">(Bulat et al., 2017)</ref>, as well as a combination thereof.</p><p>We evaluate our method in the metaphor iden- tification task, focusing on adjective-noun, verb- subject and verb-direct object constructions where the verbs and adjectives can be used metaphori- cally. Our results show that our architecture out- performs both a metaphor agnostic deep learn- ing baseline (a basic feed forward network) and the previous corpus-based approaches to metaphor identification. We also investigate the effects of training data on this task, and demonstrate that with a sufficiently large training set our method also outperforms the best existing systems based on hand-coded lexical knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The majority of approaches to metaphor process- ing cast the problem as classification of linguis- tic expressions as metaphorical or literal. <ref type="bibr" target="#b9">Gedigian et al. (2006)</ref> classified verbs related to MO- TION and CURE within the domain of financial discourse. They used the maximum entropy clas- sifier and the verbs' nominal arguments and their FrameNet roles ( <ref type="bibr" target="#b8">Fillmore et al., 2003)</ref> as features, reporting encouraging results. <ref type="bibr" target="#b6">Dunn (2013)</ref> used a logistic regression classifier and high-level prop- erties of concepts extracted from SUMO ontology, including domain types (ABSTRACT, PHYSICAL, SOCIAL, MENTAL) and event status (PROCESS, STATE, OBJECT). <ref type="bibr" target="#b27">Tsvetkov et al. (2014)</ref> used ran- dom forest classifier and coarse semantic features, such as concreteness, animateness, named entity types and WordNet supersenses. They have shown that the model learned with such coarse semantic features is portable across languages. The work of <ref type="bibr" target="#b12">Hovy et al. (2013)</ref> is notable as they focused on compositional rather than categorical features. They trained an SVM with dependency-tree ker- nels to capture compositional information, using lexical, part-of-speech tag and WordNet super- sense representations of sentence trees. <ref type="bibr" target="#b20">Mohler et al. (2013)</ref> aimed at modelling conceptual infor- mation. They derived semantic signatures of texts as sets of highly-related and interlinked WordNet synsets. The semantic signatures served as fea- tures to train a set of classifiers (maximum en- tropy, decision trees, SVM, random forest) that mapped new metaphors to the semantic signatures of the known ones.</p><p>With the aim of reducing the dependence on manually-annotated lexical resources, other re- search focused on modelling metaphor using corpus-driven information alone.  pointed out that the metaphorical uses of words constitute a large portion of the de- pendency features extracted for abstract concepts from corpora. For example, the feature vec- tor for politics would contain GAME or MECHA- NISM terms among the frequent features. As a result, distributional clustering of abstract nouns with such features identifies groups of diverse concepts metaphorically associated with the same source domain.  exploit this property of co-occurrence vectors to identify new metaphorical mappings starting from a set of ex- amples. <ref type="bibr" target="#b23">Shutova and Sun (2013)</ref> used hierar- chical clustering to derive a network of concepts in which metaphorical associations are learned in an unsupervised way. Do Dinh and Gurevych (2016) investigated metaphors through the task of sequence labelling, detecting metaphor related words in context. <ref type="bibr" target="#b10">Gutiérrez et al. (2016)</ref> inves- tigated metaphorical composition in the composi- tional distributional semantics framework. Their method learns metaphors as linear transformations in a vector space and they demonstrated that it produces superior phrase representations for both metaphorical and literal language, as compared to the traditional "single-sense" compositional distri- butional model. They then used these representa- tions in the metaphor identification task, achieving promising results.</p><p>The more recent approaches of  and <ref type="bibr" target="#b3">Bulat et al. (2017)</ref> used dense skip- gram word embeddings ( <ref type="bibr" target="#b17">Mikolov et al., 2013a</ref>) in- stead of the sparse distributional features.  investigated a set of metaphor identi- fication methods using linguistic and visual fea- tures. They learned linguistic and visual repre- sentations for both words and phrases, using skip- gram and convolutional neural networks <ref type="bibr" target="#b13">(Kiela and Bottou, 2014</ref>) respectively. They then mea- sured the difference between the phrase represen- tation and those of its component words in terms of their cosine similarity, which served as a predic- tor of metaphoricity. They found basic cosine sim- ilarity between the component words in the phrase to be a powerful measure -the neural embeddings of the words were compared with cosine similar- ity and a threshold was tuned on the development set to distinguish between literal and metaphorical phrases. This approach was their best performing linguistic model, outperformed only by a multi- modal system which included both linguistic and visual features.</p><p>Bulat et al. <ref type="formula" target="#formula_5">(2017)</ref> presented a metaphor iden- tification method that uses representations con- structed from human property norms <ref type="bibr" target="#b16">(McRae et al., 2005</ref>). They first learn a mapping from the skip-gram embedding vector space to the prop- erty norm space using linear regression, which al- lows them to generate property norm representa- tions for unseen words. The authors then train an SVM classifier to detect metaphors using these representations as input. <ref type="bibr" target="#b3">Bulat et al. (2017)</ref> have shown that the cognitively-driven property norms outperform standard skip-gram representations in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Supervised Similarity Network</head><p>Our method is inspired by the findings of , who showed that the cosine similarity between neural embeddings of the two words in a phrase is indicative of its metaphoricity. For ex- ample, the phrase 'colourful personality' receives a score:</p><formula xml:id="formula_0">s = cos(x c , x p ) (1)</formula><p>where x c is the embedding for colourful and x p is the embedding for personality. The combined phrase is classified as being metaphorical based on a threshold, which is optimised on a develop- ment dataset. In this paper, we propose several ex- tensions to this general idea, creating a supervised version of the cosine similarity metric which can be optimised on training data to be more suitable for metaphor detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Representation Gating</head><p>Directly comparing the vector representations of both words treats each of the embeddings as an independent unit. In reality, however, word mean- ings vary and adapt based on the context. In case of metaphorical language (e.g. "cure crime"), the source domain properties of the verb (e.g. cure)</p><p>are projected onto the target domain noun (e.g. crime), resulting in the interaction of the two do- mains in the interpretation of the metaphor.</p><p>In order to integrate this idea into the metaphor detection method, we can construct a gating func- tion that modulates the representation of one word based on the other. Given embeddings x 1 and x 2 , the gating values are predicted as a non-linear transformation of x 1 and applied to x 2 through element-wise multiplication:</p><formula xml:id="formula_1">g = σ(W g x 1 ) (2) x 2 = x 2 g (3)</formula><p>where W g is a weight matrix that is optimised dur- ing training, σ is the sigmoid activation function, and represents element-wise multiplication. In an adjective-noun phrase, this architecture allows the network to first look at the adjective, then use its meaning to change the representation of the noun. The sigmoid activation function makes it act as a filter, choosing which information from the original embedding gets through to the rest of the network. While learning a more complex gating function could be beneficial for very large training resources, the filtering approach is more suitable for the annotated metaphor datasets which are rel- atively small in size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Vector Space Mapping</head><p>As the next step, we implement position-specific mappings for the word embeddings. The original method uses word embeddings that have been pre- trained using the distributional skip-gram objec- tive ( <ref type="bibr" target="#b17">Mikolov et al., 2013a)</ref>. While this tunes the vectors for predicting context words, there is no reason to believe that the same space is also opti- mal for the task of metaphor detection. In order to address this shortcoming, we allow the model to learn a mapping from the skip-gram vector space to a new metaphor-specific vector space:</p><formula xml:id="formula_2">z 1 = tanh(W z 1 x 1 )<label>(4)</label></formula><formula xml:id="formula_3">z 2 = tanh(W z 2 x 2 )<label>(5)</label></formula><p>where W z 1 and W z 2 are weight matrices, z 1 and z 2 are the new position-specific word representa- tions. While the original embeddings x 1 and x 2 are pre-trained on a large unannotated corpus, the transformation process is optimised using anno- tated metaphor examples, resulting in word rep- resentations that are more suitable for this task. Furthermore, the adjectives and nouns use sepa- rate mapping weights, which allows the model to better distinguish between the different function- alities of these words. In contrast, the original co- sine similarity is not position-specific and would give the same result regardless of the word order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Weighted Cosine</head><p>If the vectors x 1 and x 2 are normalised to unit length, the cosine similarity between them is equal to their dot product, which in turn is equal to their elementwise multiplication followed by a sum over all elements:</p><formula xml:id="formula_4">cos(x 1 , x 2 ) ∝ i x 1,i x 2,i<label>(6)</label></formula><p>This calculation of cosine similarity can be for- mulated as a small neural network where the two unit-normalised input vectors are directly multi- plied together. This is followed by a single out- put neuron, with all the intermediate weights set to value 1. Such a network would calculate the same sum over the element-wise multiplication, outputting the value of cosine similarity.</p><p>Since there is no reason to assume that all the embedding dimensions are equally important when detecting metaphors, we can explore other strategies for weighting the similarity calculation. <ref type="table">Table 1</ref>: Annotated verb-direct object and verb- subject pairs from MOH.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metaphorical</head><note type="other">Literal absorb cost accommodate guest attack problem attack village attack cancer blur vision breathe life breathe person design excuse deflate mattress deflate economy digest milk leak news land airplane swallow anger swim man</note><p>Rei and Briscoe (2014) used a fixed formula to cal- culate weights for different dimensions of cosine similarity and showed that it helped in recovering hyponym relations. We extend this even further and allow the network to use multiple different weighting strategies which are all optimised dur- ing training. This is done by first creating a vector m, which is an element-wise multiplication of the two word representations:</p><formula xml:id="formula_5">m i = z 1,i z 2,i<label>(7)</label></formula><p>where m i is the i-th element of vector m and z 1,i is the i-th element of vector z 1 . After that, the resulting vector is used as input for a hidden neural layer:</p><formula xml:id="formula_6">d = γ(W d m)<label>(8)</label></formula><p>where W d is a weight matrix and γ is an activation function. If the length of d is 1, all the weights in W d have value 1, and γ is a linear activation, then this formula is equivalent to a regular cosine sim- ilarity. However, we use a larger length for d to capture more features, use tanh as the activation function, and optimise the weights of W d during training, giving the framework more flexibility to customise the model for the task of metaphor de- tection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Prediction and Optimisation</head><p>Based on vector d we can output a prediction for the word pair, showing whether it is literal or metaphorical:</p><formula xml:id="formula_7">y = σ(W y d)<label>(9)</label></formula><p>where W y is a weight matrix, σ is the logistic ac- tivation function, and y is a real-valued prediction with values between 0 and 1.</p><p>We optimise the model based on an annotated training dataset, while minimising the following hinge loss function:</p><formula xml:id="formula_8">E = k q k<label>(10)</label></formula><formula xml:id="formula_9">q k = ( y − y) 2 if | y − y| &gt; 0.4 0, otherwise<label>(11)</label></formula><p>where y is the predicted value, y is the true label, and k iterates over all training examples. Equation 11 optimises the model to minimise the squared error between the predicted and true labels. How- ever, this is only done for training examples where the predicted error is not already close enough to the desired result. The condition | y − y| &gt; 0.4 only updates training examples where the differ- ence from the true label is greater than 0.4. The true labels y can only take values 0 (literal) or 1 (metaphorical), and the threshold 0.4 is chosen so that datapoints that are on the correct side of the decision boundary by more than 0.1 would be ig- nored, which helps reduce overfitting and allows the model to focus on the misclassified examples.</p><p>The diagram of the complete network can be seen in <ref type="figure" target="#fig_0">Figure 1</ref>. We use the 2526-dimensional attribute-based vectors trained by <ref type="bibr" target="#b3">Bulat et al. (2017)</ref>, following <ref type="bibr" target="#b7">Fagarasan et al. (2015)</ref>. These representations were induced by using partial least squares regres- sion to learn a cross-modal mapping function be- tween the word embeddings described above and the <ref type="bibr" target="#b16">McRae et al. (2005)</ref> property-norm semantic space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Word Representations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Datasets</head><p>We evaluate our method using two datasets of phrases manually annotated for metaphoricity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metaphorical</head><p>Literal bloody stupidity bloody nose deep understanding cold weather empty promise dry skin green energy empty can healthy balance frosty morning hot topix hot chocolate muddy thinking gold coin ripe age soft leather sour mood sour cherry warm reception steep hill Since these datasets include examples for different senses (both metaphorical and literal) of the same verbs or adjectives, they allow us to test the ex- tent to which our model is able to discriminate be- tween different word senses, as opposed to merely selecting the most frequent class for a given word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mohammad et al. dataset (MOH) Moham- mad et al. (2016) used WordNet to find verbs</head><p>that had between three and ten senses and ex- tracted the sentences exemplifying them in the cor- responding glosses, yielding a total of 1639 verb uses in sentences. Each of these was annotated for metaphoricity by 10 annotators via the crowd- sourcing platform CrowdFlower 1 . Mohammad et al. selected the verbs that were tagged by at least 70% of the annotators as metaphorical or literal to create their dataset. We extracted verb-direct object and verb-subject relations of the annotated verbs from this dataset, discarding the instances with pronominal or clausal subject or object. This resulted in a dataset of 647 verb-noun pairs (316 metaphorical and 331 literal). Some examples of annotated verb phrases from MOH are presented in <ref type="table">Table 1</ref>.</p><formula xml:id="formula_10">Tsvetkov et al. dataset (TSV) Tsvetkov et al.<label>(2014)</label></formula><p>construct a dataset of adjective-noun pairs annotated for metaphoricity. This is divided into a training set consisting of 884 literal and 884 metaphorical pairs (TSV-TRAIN) and a test set containing 100 literal and 100 metaphorical pairs (TSV-TEST). <ref type="table" target="#tab_0">Table 2</ref> shows a portion of an- notated adjective-noun phrases from TSV-TEST. TSV-TRAIN was collected from publicly available metaphor collections on the web and manually curated by removing duplicates and metaphori- cal phrases that depend on wider context for their interpretation (e.g. drowning students). TSV- TEST was constructed by extracting nouns that co-occur with a list of 1000 frequent adjectives in the TenTen Web Corpus 2 using SketchEngine. The selected adjective-noun pairs were annotated for metaphoricity by 5 annotators with an inter- annotator agreement of κ = 0.76. Since TSV- TRAIN and TSV-TEST were constructed differ- ently, we follow previous work <ref type="bibr" target="#b27">(Tsvetkov et al., 2014;</ref><ref type="bibr" target="#b3">Bulat et al., 2017)</ref> and report performance on TSV-TEST. We randomly separated 200 (out of the 1536) examples from the training set to use for development experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments and Results</head><p>The word representations in our model were ini- tialised with either the 100-dimensional skip-gram embeddings or the 2,526-dimensional attribute vectors (Section 4). These were kept fixed and not updated, which reduces overfitting on the available training examples. For both word representations we use the same embeddings as <ref type="bibr" target="#b3">Bulat et al. (2017)</ref>, which makes the results directly comparable and shows that the improvements are coming from the novel architecture and are not due to a different embedding initialisation. The network was optimised using AdaDelta <ref type="bibr" target="#b30">(Zeiler, 2012)</ref> for controlling adaptive learning rates. The models were evaluated after each full pass over the training data and training was stopped if the F-score on the development set had not improved for 5 epochs. The transformed em- beddings z 1 and z 2 were set to size 300, layer d was set to size 50. The values for these hyperpa- rameters were chosen experimentally using the de- velopment dataset. In order to avoid drawing con- clusions based on outlier results due to random ini- tialisations, we ran each experiment 25 times with random seeds and present the averaged results in this paper. We implemented the framework using Theano (Al-Rfou et al., 2016) and are making the source code publicly available. 3 <ref type="table">Table 3</ref> contains results of different system con- figurations on the TSV dataset. The original F- score by <ref type="bibr" target="#b27">Tsvetkov et al. (2014)</ref> is still the high- est, as they used a range of highly-engineered features that require manual annotation, such as Acc P R F1  <ref type="table">Table 3</ref>: System performance on the Tsvetkov et al. dataset (TSV) in terms of accuracy (Acc), pre- cision (P), recall (R) and F-score (F1).</p><p>the lexical abstractness, imageability scores and the relative number of supersenses for each word in the dataset. Our setup is more similar to the linguistic experiments by , where metaphor detection is performed using pre- trained word embeddings. They also proposed combining the linguistic model with a system us- ing visual word representations and achieved per- formance improvements. Recently, <ref type="bibr" target="#b3">Bulat et al. (2017)</ref> compared different types of embeddings and showed that attribute-based representations can outperform regular skip-gram embeddings.</p><p>As an additional baseline, we report the perfor- mance on metaphor detection using a basic feed- forward network (FFN). In this configuration, the word embeddings x 1 and x 2 are directly connected to the hidden layer d, skipping all the intermedi- ate network structure. The FFN achieves 74.4% F-score on <ref type="bibr">TSV-TEST,</ref> showing that even such a simple model can perform relatively well in a su- pervised setting. Using attribute vectors instead of skip-gram embeddings gives a slight improve- ment, especially on the recall metric, which is con- sistent with the findings by <ref type="bibr" target="#b3">Bulat et al. (2017)</ref>.</p><p>The architecture described in Section 3, which we refer to as a supervised similarity network (SSN), outperforms the baseline and achieves 80.1% F-score using skip-gram embeddings and 80.6% with attribute-based representations. We also created a fusion of these two models where the predictions from both are combined as a weighted average. In this setting, the two net- works are trained in tandem and a real-valued weight, which is also optimised during training, is Acc P R F1   <ref type="formula" target="#formula_5">(2017)</ref> by 5.6% abso- lute, using the same word representations as input. <ref type="table" target="#tab_3">Table 4</ref> contains results of different system ar- chitectures on the MOH dataset.  reported 75% F-score on this dataset with a multimodal system, after randomly separating a subset for testing. Since this corpus contains only 647 annotated examples, we instead evalu- ated the systems using 10-fold cross-validation. The feedforward baseline with skip-gram embed- dings returns an F-score that is close to the lin- guistic configuration of Shutova et al, whereas the best results are achieved by the similarity net- work with skip-gram embeddings. In this setting, the attribute-based representations did not improve performance -this is expected, as the attribute norms by <ref type="bibr" target="#b16">McRae et al. (2005)</ref> are designed for nouns, whereas the MOH dataset is centered on verbs. <ref type="table">Table 5</ref> contains examples from the TSV de- velopment set, together with gold annotations and predicted scores. The system confidently detects literal phrases such as sunny country and meaning- less discussion, along with metaphorical phrases such as unforgiving heights and blind hope. The predicted output disagrees with the annotation on  <ref type="table">Table 5</ref>: Examples from the Tsvetkov develop- ment set, together with the gold label, predicted label, and the predicted score from the best model. cases such as humane treatment and rich program- mer -some of these examples could also be ar- gued as being metaphorical, depending on the spe- cific sense of the words. While the system was rel- atively unsure about the false positives (the scores were close to 0.5), it tended to assign more deci- sive scores to the false negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">The Effects of Training Data</head><p>Results in Section 6 show that performance on the TSV dataset is higher than the MOH dataset, likely due to the former having more examples available for training. Therefore, we ran an additional ex- periment to investigate the effect of dataset size on the performance of metaphor detection. <ref type="bibr" target="#b10">Gutiérrez et al. (2016)</ref> annotated a dataset of adjective-noun phrases as being literal or metaphorical, and we are able to use this as an additional training re- source. While it contains only 23 unique adjec- tives, the total number of phrases reaches 8,592. We remove any phrases that occur in the develop- ment or test data of TSV, then incrementally add the remaining examples to the TSV training data and evaluate on the TSV-TEST. <ref type="figure" target="#fig_2">Figure 2</ref> shows a graph of the system perfor- mance, when increasing the training data at in- tervals of 500. There is a very rapid increase in performance until around 2,000 training points, whereas the existing TSV-TRAIN is limited to 1,336 examples. Providing even more data to the system gives an additional increase that is more gradual. The final performance of the system us-   ing both datasets is 88.3 F-score, which is the highest result reported on the TSV dataset and translates to 36% relative error reduction with re- spect to the same system trained only on the orig- inal dataset.</p><p>We report the exact values in <ref type="table" target="#tab_6">Table 6</ref> for the different training sets. The value on the Tsvetkov training data is different from the result in <ref type="table">Table 3</ref>, which is due to the original attribute embeddings by <ref type="bibr" target="#b3">Bulat et al. (2017)</ref> only containing representa- tions for the vocabulary in the TSV dataset. In or- der to include the data from <ref type="bibr" target="#b10">Gutiérrez et al. (2016)</ref>, we recreated the attribute vectors for a larger vo- cabulary, which results in a slightly different base- line performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Qualitative analysis</head><p>The architecture in Section 3 also acts as a se- mantic composition model, extracting the mean- ing of the phrase by combining the meanings of its component words. Therefore, we performed a qualitative experiment to investigate: (1) how well do traditional compositional methods cap- ture metaphors, without any fine-tuning; and (2) whether the supervised representations still retain their domain-specific semantic information. For this purpose, we construct three vector spaces and visualise some examples from the TSV training set,   <ref type="figure" target="#fig_3">Figure 3</ref> contains examples for three different composition methods: the additive method simply sums the skip-gram embeddings for both words (top); the multiplicative method multiplies the skip-gram embeddings (middle); the final system uses layer m from the SSN model to represent the phrases (bottom).</p><p>The visualisation shows that the additive and multiplicative models are both comparable when it comes to semantic clustering of the phrases, but metaphorical examples are mixed together with literal clusters. The SSN is optimised for metaphor classification and therefore it produces representations with a very clear boundary for metaphoricity. Interestingly, the graph also reveals a misannotated example in the dataset, since 'fiery temper' should be labeled as a metaphor. At the same time, this space also retains the general se- mantic information, as similar phrases with the same label are still positioned close together. Fu- ture work could investigate models of multi-task training where metaphor detection is trained to- gether with an unsupervised objective, allowing the system to take better advantage of unlabeled data while still learning to separate metaphors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In this paper, we introduced the first deep learn- ing architecture designed to capture metaphorical composition and evaluated it on a metaphor iden- tification task.</p><p>Firstly, we demonstrated that the proposed framework outperforms both a metaphor-agnostic baseline (a feed-forward neural network) as well as previous corpus-driven approaches to metaphor identification. The results showed that it is bene- ficial to construct a specialised network architec- ture for metaphor detection, which includes a gat- ing function for capturing the interaction between the source and target domains, word embeddings mapped to a metaphor-specific space, and optimi- sation using a hinge loss function.</p><p>Secondly, our qualitative analysis indicates that our supervised similarity network learns phrase representations with a very clear boundary for metaphoricity, in contrast to traditional composi- tional methods.</p><p>Finally, we show that with a sufficiently large training set our model can also outperform the state-of-the art metaphor identification systems based on hand-coded lexical knowledge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The network architecture for supervised metaphorical phrase classification. The symbol is used to indicate element-wise multiplication.</figDesc><graphic url="image-1.png" coords="3,117.36,62.81,362.85,116.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Following Bulat et al.</head><label></label><figDesc>(2017) we experiment with two types of semantic vectors: skip-gram word embeddings and attribute-based representations. The word embeddings are 100-dimensional and were trained using the standard log-linear skip- gram model with negative sampling of Mikolov et al. (2013b) on Wikipedia for 3 epochs, using a symmetric window of 5 and 10 negative samples per word-context pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance as a function of training set size. The x-axis shows the number of training examples, the y-axis shows F-score on TSV-TEST.</figDesc><graphic url="image-2.png" coords="8,72.00,62.81,218.27,145.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of metaphorical and literal phrases in different vector spaces. Blue circles indicate literal examples, red squares show metaphorical pairs. Top: additive vector space. Middle: multiplicative vector space. Bottom: vectors from layer m in the similarity network.</figDesc><graphic url="image-3.png" coords="8,307.28,62.80,218.27,493.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>using t-SNE (Van Der Maaten and Hinton, 2008).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 : Annotated adjective-noun pairs from TSV-TEST.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>System performance on the Mohammad 
et al. dataset (MOH) in terms of accuracy (Acc), 
precision (P), recall (R) and F-score (F1). 

used to combine them together. This configuration 
achieves 81.1% F-score, indicating that the the 
skip-gram embeddings and attribute vectors cap-
ture somewhat complementary information. Ex-
cluding the system by Tsvetkov et al. (2014) which 
requires hand-annotated features, the proposed 
similarity network outperforms all the previous 
systems, even improving over the multimodal sys-
tem by Shutova et al. (2016) without requiring any 
visual information. The attribute-based SSN also 
improves over Bulat et al. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc>System performance on the Tsvetkov et al. dataset (TSV), using additional training data.</figDesc><table></table></figure>

			<note place="foot" n="1"> www.crowdflower.com</note>

			<note place="foot" n="2"> https://www.sketchengine.co.uk/ententen-corpus/ 3 http://www.marekrei.com/projects/ssn</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Ekaterina Shutova's research is supported by the Leverhulme Trust Early Career Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Angermueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoly</forename><surname>Belikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Belopolsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Bisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><forename type="middle">Bleecher</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Bouchard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Nicolas Boulanger-Lewandowski, and Others</publisher>
		</imprint>
	</monogr>
	<note>arXiv e-prints, abs/1605.0:19</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic classifications for detection of verb metaphors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chee Wee</forename><surname>Beata Beigman Klebanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Dario</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="101" to="106" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A tiered approach to the recognition of metaphor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bracewell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Rink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics and Intelligent Text Processing</title>
		<imprint>
			<biblScope unit="volume">8403</biblScope>
			<biblScope unit="page" from="403" to="414" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modelling metaphor with attribute-based semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luana</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynne</forename><surname>Cameron</surname></persName>
		</author>
		<title level="m">Metaphor in Educational Discourse. Continuum</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">TokenLevel Metaphor Detection using Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik-Lân Do</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Metaphor in NLP</title>
		<meeting>the Fourth Workshop on Metaphor in NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evaluating the premises and results of four metaphor identification systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CICLing&apos;13</title>
		<meeting>CICLing&apos;13<address><addrLine>Samos, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="471" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">From distributional semantics to feature norms: grounding semantic models in human perceptual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luana</forename><surname>Fagarasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><forename type="middle">Maria</forename><surname>Vecchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Computational Semantics (IWCS&apos;15)</title>
		<meeting>the 11th International Conference on Computational Semantics (IWCS&apos;15)<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="52" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Petruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Background to FrameNet. International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="250" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Catching metaphors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gedigian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srini</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Branimir</forename><surname>Ciric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Scalable Natural Language Understanding</title>
		<meeting>the 3rd Workshop on Scalable Natural Language Understanding<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Literal and Metaphorical Senses in Compositional Distributional Semantic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Darío</forename><surname>Gutiérrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Marghetis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">K</forename><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Identifying metaphorical word use with tree kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Whitney</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Metaphor in NLP</title>
		<meeting>the First Workshop on Metaphor in NLP<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="52" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning Image Embeddings using Convolutional Neural Networks for Improved Multi-Modal Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1506.07285</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Metaphors We Live By</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Lakoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
			<publisher>University of Chicago Press</publisher>
			<pubPlace>Chicago</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic feature production norms for a large set of living and nonliving things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Mcrae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>George S Cree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Seidenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcnorgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ICLR 2013</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR<address><addrLine>Scottsdale, AZ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Metaphor as a medium for emotion: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter D</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of *SEM</title>
		<meeting>*SEM</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic signatures for example-based linguistic metaphor detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bracewell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hinote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Metaphor in NLP</title>
		<meeting>the First Workshop on Metaphor in NLP<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Looking for Hyponyms in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="68" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Black Holes and White Rabbits : Metaphor Identification with Visual Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Maillard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised metaphor identification using hierarchical graph factorization clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL 2013</title>
		<meeting>NAACL 2013<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Metaphor identification using verb and noun clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Coling 2010</title>
		<meeting>Coling 2010<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1002" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Metaphor corpus annotated for source-target domain mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Teufel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC 2010</title>
		<meeting>LREC 2010<address><addrLine>Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3255" to="3261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust extraction of metaphor from novel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomek</forename><surname>Strzalkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">Aaron</forename><surname>Broadwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurie</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Yamrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kit</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umit</forename><surname>Boz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Cases</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Elliot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Metaphor in NLP</title>
		<meeting>the First Workshop on Metaphor in NLP<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="67" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Metaphor Detection with Cross-Lingual Model Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Boytsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatole</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014)</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="248" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Literal and metaphorical sense identification through concrete and abstract context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Neuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohai</forename><surname>Assaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="680" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualizing high-dimensional data using tsne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">ADADELTA: An Adaptive Learning Rate Method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.05070</idno>
		<title level="m">Self-adaptive hierarchical sentence model</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
