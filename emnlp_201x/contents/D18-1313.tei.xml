<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Lazy Encoder: A Fine-Grained Analysis of the Role of Morphology in Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
							<email>a.bisazza@liacs.leidenuniv.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">Leiden University</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Tump</surname></persName>
							<email>clara.tump@hotmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Lazy Encoder: A Fine-Grained Analysis of the Role of Morphology in Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2871" to="2876"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2871</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural sequence-to-sequence models have proven very effective for machine translation, but at the expense of model interpretability. To shed more light into the role played by linguistic structure in the process of neural machine translation, we perform a fine-grained analysis of how various source-side morphological features are captured at different levels of the NMT encoder while varying the target language. Differently from previous work, we find no correlation between the accuracy of source morphology encoding and translation quality. We do find that morphological features are only captured in context and only to the extent that they are directly transferable to the target words.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The advent of Neural Machine Translation (NMT) <ref type="bibr" target="#b18">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>) has led to remarkable improvements in machine trans- lation quality <ref type="bibr" target="#b4">(Bentivogli et al., 2016</ref>) but has also produced models that are much less interpretable. In particular, the role played by linguistic features in the process of understanding the source text and rendering it in the target language remains hard to gauge. Acquiring this knowledge is important to inform future research in NMT, especially regard- ing the usefulness of injecting linguistic informa- tion into the NMT model, e.g. by using supervised annotation <ref type="bibr" target="#b16">(Sennrich and Haddow, 2016)</ref>. <ref type="bibr" target="#b8">Hill et al. (2014)</ref> gave a first answer to this question, reporting high accuracies by source-side NMT word embeddings on the well-known anal- ogy task by <ref type="bibr" target="#b14">Mikolov et al. (2013)</ref> which also in- cludes a number of derivational and inflectional transformations in the morphologically poor En- glish language. More recent work ( <ref type="bibr" target="#b17">Shi et al., 2016)</ref> has shown that source sentence representa- tions produced by NMT encoders contain a great deal of syntactic information. <ref type="bibr" target="#b2">Belinkov et al. (2017a)</ref> focused on the word level and examined to what extent part-of-speech and morphological information can be extracted from various NMT word representations. The latter study found that source-side morphology is captured slightly bet- ter by the first recurrent layer than by the word embedding and the final recurrent layer. Another, somewhat surprising finding was that source-side morphology is learned better when translating into an 'easier' target language than into a related one, even if the 'easier' language is morphologically poor.</p><p>In this paper, we also focus on source-side mor- phology but perform a finer-grained analysis of how morphological features are captured by differ- ent components of the NMT encoder while vary- ing the target language. We argue that predicting generic morphological tags where all features are mixed, as done by <ref type="bibr" target="#b2">Belinkov et al. (2017a)</ref>, can only give us a limited insight into the linguistic competence of the model. Hence, we predict mor- phological features independently from one an- other and ask the following questions:</p><p>• Are different morphological features cap- tured by the NMT encoder to substantially different extents and, if yes, why?</p><p>• Are morphological features captured as a word type property (i.e. at the word embed- ding level) or are they mostly computed in context (i.e. at the recurrent state level)?</p><p>• How does source-target language relatedness affect the morphological competence of the NMT encoder?</p><p>More specifically, we look at whether the NMT encoder only learns those morphological features that can be directly transferred to the target words   (such as number) or whether it also learns features that are not directly transferable but can still be useful to correctly parse and infer the meaning of a sentence (such as gender). See example in <ref type="figure" target="#fig_1">Fig. 1</ref>. We focus on French and similarly to previous work ( <ref type="bibr" target="#b17">Shi et al., 2016;</ref><ref type="bibr" target="#b2">Belinkov et al., 2017a</ref>) we use the continuous word representations produced by a trained NMT system to build and evaluate a number of linguistic feature classifiers. Classifier accuracy represents the extent to which a given feature is captured by the NMT encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>We train NMT systems on the following language pairs: French-Italian (F R IT ), French-German (F R DE ), and French-English (F R EN ). We chose these language pairs for their different levels of language relatedness and morphological feature correspondence. Grammatical gender is especially interesting as it is marked in French, Italian and German, but not in English (except for a few pro- nouns). The gender of Italian nouns often cor- responds to that of French because of their com- mon language ancestor, whereas German gender is mostly unrelated from French gender (see ex- ample in <ref type="figure" target="#fig_1">Fig. 1</ref>).</p><p>The continuous word representations produced by the three NMT systems while encoding a cor- pus of French sentences are used to build and eval- uate several specialized classifiers: one per mor- phological feature. If a classifier significantly out- performs the majority baseline, we conclude that the corresponding feature is captured by the NMT encoder. While this methodology is similar to that of previous work <ref type="bibr" target="#b12">(Köhn, 2015;</ref><ref type="bibr">Belinkov et al., 2017a,b;</ref><ref type="bibr" target="#b6">Dalvi et al., 2017</ref>) we make sure that our results are not affected by overfitting by eliminat- ing any vocabulary overlap between the classifier's training and test sets. We find this step crucial to ensure that the redundancy in this type of data does not lead to over-optimistic conclusions. We now provide more details on the experimental setup.</p><p>Parallel corpora. For a fair comparison among target languages, we extract the intersection of the Europarl corpus ( <ref type="bibr" target="#b11">Koehn, 2005</ref>) in our three lan- guage pairs so that the source side data is identi- cal for all NMT systems. Sentences longer than 50 tokens are ignored. This data is then split into an NMT training, validation, and test set of 1.3M, 2.5K, and 2.5K sentence pairs respectively.</p><p>NMT model. The NMT architecture is an at- tentional encoder-decoder model similar to <ref type="bibr" target="#b13">(Luong et al., 2015)</ref> and uses a long short-term mem- ory (LSTM) <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997)</ref> as the recurrent cell. The models have 3 stacked LSTM layers and are trained for 15 epochs. Em- bedding and hidden state sizes are set to 1000. Source and target vocabularies are limited to the 30,000 most frequent words on each side of the training data. <ref type="bibr">1</ref> The NMT models achieve a test BLEU score of 32.6, 25.4 and 39.4 for French- Italian, French-German and French-English re- spectively.</p><p>Continuous word representations. Given a source sentence, the NMT system first encodes it into a sequence of word embeddings (context- independent representations), and then into a se- quence of recurrent states (context-dependent rep- resentations). As we are mostly interested in the impact of context on word representations, we compare the word embeddings against the final layer of the stacked LSTMs (corresponding to lay- ers 0 and 3 in Belinkov et al. (2017a)'s terms) while disregarding the intermediate layers.</p><p>Morphological classification. The continuous word representations are used to train a logis- tic regression classifier 2 for each morphological feature: gender and number for noun and adjec- tives; tense for verbs (with labels: present, fu-ture, imperfect, or simple past). Word labels are taken from the Lefff French morphological lexicon <ref type="bibr" target="#b15">(Sagot, 2010)</ref>  <ref type="bibr">3</ref> . To ensure a fair comparison be- tween context-independent and context-dependent embedding classification, words that are ambigu- ous with respect to a given feature are excluded from the respective classifier's training and test data.</p><p>Classifiers' training/test data. The classifiers are trained on a 50K-sentence subset of the NMT training data and tested on the NMT test sets (2.5K). For each experiment, we extract one vec- tor per token from the NMT encoder. While this is the only possible setup for context-dependent rep- resentations, it leads to a problematic training/test overlap in the word embedding experiment be- cause all occurrences of the same word are associ- ated to exactly the same vector. We find that, due to this overlap, a dummy binary feature assigned to a random half of the vocabulary can be pre- dicted from the word embeddings with very high accuracy (86% for a linear, 98% for a non-linear classifier) leading to over-optimistic conclusions on the linguistic regularities of these representa- tions. To avoid this, we split the vocabulary in two parts of 15K types: the first is used to filter the training samples and the second to filter the test samples. We repeat each experiment five times us- ing five different random vocabulary splits and re- port mean accuracies. This process is applied to all experiments (including those on hidden states) to allow for a fair comparison of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Discussion</head><p>This section presents our results along three di- mensions: context-dependency of the word repre- sentations ( §3.1), different morphological features ( §3.2), and target language impact ( §3.3). Unless explicitly stated, all discussed results are statisti- cally significant (computed using a t-test for a one- tailed hypothesis and independent means).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word embeddings vs recurrent states</head><p>One of our goals was to discover whether morpho- logical features are captured as a word type prop- erty or in context. <ref type="figure" target="#fig_2">Fig. 2</ref>   all target languages. We can see that each fea- ture is clearly captured at the recurrent state level, confirming that source-side morphology is indeed successfully exploited by NMT models. However, at the word embedding level, accuracies are com- parable to the majority class baseline (these dif- ferences are not significant), which implies that the source-side lexicon of our NMT systems does not encode morphology in a systematic way. This might be partly explained by the fact that learning morphological features at the word level is diffi- cult due to data sparsity -indeed the rarest French words in our dataset are observed only 10 times in the training data. However, additional experiments showed that our finding is consistent across differ- ent word frequency bins: that is, even the embed- dings of frequent words do not encode morpholog- ical features better than the majority baseline.</p><p>This result is surprising, considering that our morphological features are usually easy to in- fer from the immediate context of French words (see examples in <ref type="figure" target="#fig_1">Fig.1)</ref> and that morphology was shown to be well captured by monolingual word embeddings in various European languages in- cluding French <ref type="bibr">(Köhn, 2015)</ref>. By contrast, our NMT encoders choose not to store morphology at the word type level, perhaps in order to allocate more capacity to semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Different morphological features</head><p>Secondly, we asked whether the NMT encoder captured different morphological features to dif- ferent extents. For this question, we disregard the word embedding results because none of the fea- tures are significantly captured at this level. <ref type="figure" target="#fig_2">Fig. 2</ref> shows that the mean accuracy of number is the highest, followed by tense and then by gen- der. However, it should be noted that the majority baselines for number and tense are much higher than the one for gender. In both absolute and rela-  tive terms, the best performing feature is number. This can be explained by the fact that number re- mains most often unchanged through translation, and is marked in all target languages -albeit to different extents. On the other hand, tense is de- termined by the semantics but also by language- specific usage, while gender has little semantic value and is mostly assigned to nouns arbitrarily.</p><note type="other">0.9 1.0 Gender Majority class Word embed. LSTM state FR-IT FR-DE FR-EN 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Gender (multitrg.NMT)</note><p>The fact that the results of different morpholog- ical features are so variable confirms the setup of examining each feature independently. <ref type="figure" target="#fig_4">Fig. 3</ref> shows the impact of the target language on the encoded morphology accuracy. We again fo- cus our analysis on the LSTM state level since em- bedding level results are mostly near the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Source-target language relatedness</head><p>Differently from Belinkov et al. (2017a) we do not find that source-side morphology is captured better when translating into the 'easiest' language, which in our case is English, both in terms of mor- phological complexity and BLEU performance. We note that their findings were based on very small, possibly not significant differences, and on the prediction of all morphological features simul- taneously. By contrast, our fine-grained analysis reveals that the impact of target language is signif- icant and even major on only one feature, namely gender, where it agrees with our linguistic intu- ition. Indeed this feature differs from the oth- ers because it varies largely among languages and, when present, is semantically determined only to a very limited extent. F R IT , where source gender is a good predictor of target gender, shows the high- est accuracy; F R EN , where target gender is not marked, shows the lowest; F R DE , where source gender is often confounding for target gender, lies in-between.</p><p>Is language relatedness the main explaining variable? To find that out, we experiment with a modified Italian target language without gen- der marking, i.e. all gender-marked words are re- placed by their masculine form (F R IT * ). This language pair achieves a slightly higher BLEU score than F R IT (33.2 vs 32.6), which can be at- tributed to the smaller target vocabulary. How- ever its source gender accuracy is much worse (see <ref type="figure" target="#fig_4">Fig. 3</ref>), which indicates that the high performance of the F R IT encoder is mostly due to the ubiqui- tous gender marking in the target language, rather than to language relatedness. All this suggests that source morphological features contribute to sen- tence understanding to some degree, but the incen- tive to learn them mostly depends on how directly they can be transferred to the target sentence. Finally, we look at what happens when a sin- gle NMT system is trained in a multitarget fash- ion on our three language pairs. Following the setup of <ref type="bibr" target="#b10">Johnson et al. (2017)</ref>, we prepend a to- target-language tag {2it, 2de, 2en} to the source side of each sentence pair and mix all language pairs in the NMT training data. Results are pre- sented for gender in <ref type="figure" target="#fig_4">Fig. 3 (right)</ref>. <ref type="bibr">4</ref> Note that, while word embeddings are identical for the three language pairs, recurrent states change according to the language tag. In this setup the target lan- guage impact is less visible and gender accuracy at the LSTM state level is overall much higher than that of the mono-target systems (0.77 vs 0.68 on average) whereas BLEU scores are slightly lower (−0.9% on average). While this is only an ini- tial exploration of multilingual NMT systems, our results suggest that this kind of multi-task objec- tive pushes the model to learn linguistic features in a more consistent way <ref type="bibr" target="#b5">(Bjerva, 2017;</ref><ref type="bibr" target="#b7">Enguehard et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have confirmed previous findings that mor- phological features are significantly captured by word-level NMT encoders. However, the features are not captured at the word type level but only at the recurrent state level where word representa- tions are context-dependent. Secondly, there is a visible difference in the extent to which different morphological features are learned: Semantic cat- egories like number and verb tense are well cap- tured in all language pairs, whereas grammatical gender with its only agreement-triggering func- tion, is dramatically affected by the target lan- guage. Source-side gender is encoded well only when it is a good predictor of target gender and when target-side marking is extensive, i.e. when translating from French to Italian.</p><p>Our findings indicate that the importance of lin- guistic structure for the neural translation process is very variable and language-dependent. They also suggest that the NMT encoder is rather 'lazy' when it comes to learning grammatical features of the source words, unless these are directly trans- ferable to their target equivalents.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FR</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Two example French sentences translated to Italian, German, and English. Number (top) is usually carried over to the three target languages, while gender (bottom) is less predictable. Colored fonts mark agreement with the noun in boldface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Classifier accuracy for different morphological features, averaged over target languages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Translation quality (left) and classifier accuracy for each morphological feature in the three different language pairs. IT* denotes a modified Italian language where gender marking is removed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>: Les arbres pl sont hauts. IT: Gli alberi pl sono alti. DE: Die Bäume pl sind hoch. EN:The trees pl are high. FR: La pomme f est petite.</head><label></label><figDesc></figDesc><table>IT: La mela f ` 
e piccola. 

DE: Der Apfelm ist klein. 

EN: The apple is small. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>shows the extent to which the NMT encoder captures different features at the word level (word embeddings) compared to the recurrent state level (LSTM state), averaged over 3 Lexique des Formes Fléchies du Français: http:// alpage.inria.fr/ ˜ sagot/lefff-en.html</figDesc><table>Tense (V) 
Number (N,Adj) Gender (N,Adj) 
0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1.0 

Mean Accuracy 

Majority class 
Word embed. 
LSTM state 

</table></figure>

			<note place="foot" n="1"> Subword/character-level representations are not included in this study since we are interested in the models&apos; ability to learn morphology from word usage, rather than word form. 2 We use linear classifiers since their accuracies can be interpreted as a measure of supervised clustering accuracy, which gives a better insight on the structure of the vector space (Köhn, 2015). Results with a simple multi-layer perceptron were consistent with the findings by the linear classifier, with slightly better performance overall.</note>

			<note place="foot" n="4"> Multitarget results for tense and number did not differ significantly from the corresponding monotarget results.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was partly funded by the Nether-lands Organization for Scientific Research (NWO) under project number 639.021.646. Part of the work was carried out on the DAS computing sys-tem ( <ref type="bibr" target="#b1">Bal et al., 2016</ref>) while the authors were affil-iated at the Informatics Institute of the University of Amsterdam. We thank Ke Tran for providing feedback on the early stages of this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A medium-scale distributed system for computer science research: Infrastructure for the long term</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henri</forename><surname>Bal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dick</forename><surname>Epema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Cees De Laat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Van Nieuwpoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Romein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cees</forename><surname>Seinstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harry</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wijshoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="54" to="63" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What do neural machine translation models learn about morphology?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahim</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="861" to="872" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Evaluating layers of representation in neural machine translation on part-of-speech and semantic tagging tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahim</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural versus phrasebased machine translation quality: a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="257" to="267" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">One Model to Rule them All: Multitask and Multilingual Modelling for Lexical Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Bjerva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>University of Groningen</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding and improving morphological learning in the neural machine translation decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahim</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="151" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring the syntactic abilities of rnns with multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>´ Emile Enguehard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Linzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Not all neural embeddings are born equal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Jean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Workshop on Learning Semantics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Coline Devin, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Vigas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macduff</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="339" to="351" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MT summit</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What&apos;s in an embedding? analyzing word embeddings through multilingual evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Köhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2067" to="2073" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The lefff, a freely available and large-coverage morphological and syntactic lexicon for french</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoˆıtbenoˆıt</forename><surname>Sagot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th international conference on Language Resources and Evaluation</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Linguistic input features improve neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="83" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Does string-based neural mt learn source syntax?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkit</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1526" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
