<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Alignment-Based Compositional Semantics for Instruction Following</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Alignment-Based Compositional Semantics for Instruction Following</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper describes an alignment-based model for interpreting natural language instructions in context. We approach instruction following as a search over plans, scoring sequences of actions conditioned on structured observations of text and the environment. By explicitly modeling both the low-level compositional structure of individual actions and the high-level structure of full plans, we are able to learn both grounded representations of sentence meaning and pragmatic constraints on interpretation. To demonstrate the model&apos;s flexibility, we apply it to a diverse set of benchmark tasks. On every task, we outperform strong task-specific baselines, and achieve several new state-of-the-art results.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In instruction-following tasks, an agent executes a sequence of actions in a real or simulated envi- ronment, in response to a sequence of natural lan- guage commands. Examples include giving nav- igational directions to robots and providing hints to automated game-playing agents. Plans speci- fied with natural language exhibit compositional- ity both at the level of individual actions and at the overall sequence level. This paper describes a framework for learning to follow instructions by leveraging structure at both levels.</p><p>Our primary contribution is a new, alignment- based approach to grounded compositional se- mantics. Building on related logical approaches ( <ref type="bibr" target="#b23">Reddy et al., 2014;</ref><ref type="bibr" target="#b22">Pourdamghani et al., 2014</ref>), we recast instruction following as a pair of nested, structured alignment problems. Given instructions and a candidate plan, the model infers a sequence- to-sequence alignment between sentences and atomic actions. Within each sentence-action pair, the model infers a structure-to-structure alignment between the syntax of the sentence and a graph- based representation of the action.</p><p>At a high level, our agent is a block-structured, graph-valued conditional random field, with align- ment potentials to relate instructions to actions and transition potentials to encode the environment model ( <ref type="figure" target="#fig_5">Figure 3</ref>). Explicitly modeling sequence- to-sequence alignments between text and actions allows flexible reasoning about action sequences, enabling the agent to determine which actions are specified (perhaps redundantly) by text, and which actions must be performed automatically (in or- der to satisfy pragmatic constraints on interpreta- tion). Treating instruction following as a sequence prediction problem, rather than a series of inde- pendent decisions <ref type="bibr" target="#b5">(Branavan et al., 2009;</ref>, makes it possible to use general-purpose planning machinery, greatly in- creasing inferential power.</p><p>The fragment of semantics necessary to com- plete most instruction-following tasks is essen- tially predicate-argument structure, with limited influence from quantification and scoping. Thus the problem of sentence interpretation can reason- ably be modeled as one of finding an alignment be- tween language and the environment it describes. We allow this structure-to-structure alignment- an "overlay" of language onto the world-to be mediated by linguistic structure (in the form of dependency parses) and structured perception (in what we term grounding graphs). Our model thereby reasons directly about the relationship be- tween language and observations of the environ- ment, without the need for an intermediate logi- cal representation of sentence meaning. This, in turn, makes it possible to incorporate flexible fea- ture representations that have been difficult to in- tegrate with previous work in semantic parsing.</p><p>We apply our approach to three established . . . right round the white water but stay quite close 'cause you don't otherwise you're going to be in that stone creek . . .</p><p>Go down the yellow hall. Turn left at the intersection of the yellow and the gray.</p><p>Clear the right column. Then the other column. Then the row. instruction-following benchmarks: the map read- ing task of Vogel and Jurafsky (2010), the maze navigation task of <ref type="bibr" target="#b17">MacMahon et al. (2006)</ref>, and the puzzle solving task of <ref type="bibr" target="#b5">Branavan et al. (2009)</ref>. An example from each is shown in <ref type="figure" target="#fig_1">Figure 1</ref>. These benchmarks exhibit a range of qualitative properties-both in the length and complexity of their plans, and in the quantity and quality of ac- companying language. Each task has been stud- ied in isolation, but we are unaware of any pub- lished approaches capable of robustly handling all three. Our general model outperforms strong, task-specific baselines in each case, achieving relative error reductions of 15-20% over sev- eral state-of-the-art results. Experiments demon- strate the importance of our contributions in both compositional semantics and search over plans. We have released all code for this project at github.com/jacobandreas/instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Existing work on instruction following can be roughly divided into two families: semantic parsers and linear policy estimators.</p><p>Semantic parsers Parser-based approaches ( <ref type="bibr" target="#b8">Chen and Mooney, 2011;</ref><ref type="bibr" target="#b13">Kim and Mooney, 2013</ref>) map from text into a formal language representing commands. These take familiar structured prediction models for semantic parsing <ref type="bibr" target="#b27">(Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b26">Wong and Mooney, 2006</ref>), and train them with task-provided supervision. Instead of attempting to match the structure of a manually-annotated semantic parse, semantic parsers for instruction following are trained to maximize a reward signal provided by black-box execution of the predicted command in the environment. (It is possible to think of response-based learning for question answering ( <ref type="bibr" target="#b15">Liang et al., 2013</ref>) as a special case.) This approach uses a well-studied mechanism for compositional interpretation of language, but is subject to certain limitations. Because the environ- ment is manipulated only through black-box exe- cution of the completed semantic parse, there is no way to incorporate current or future environment state into the scoring function. It is also in general necessary to hand-engineer a task-specific formal language for describing agent behavior. Thus it is extremely difficult to work with environments that cannot be modeled with a fixed inventory of pred- icates (e.g. those involving novel strings or arbi- trary real quantities).</p><p>Much of contemporary work in this family is evaluated on the maze navigation task introduced by <ref type="bibr" target="#b17">MacMahon et al. (2006)</ref>. <ref type="bibr" target="#b10">Dukes (2013)</ref> also in- troduced a "blocks world" task for situated parsing of spatial robot commands.</p><p>Linear policy estimators An alternative fam- ily of approaches is based on learning a pol- icy over primitive actions directly <ref type="bibr" target="#b5">(Branavan et al., 2009;</ref><ref type="bibr" target="#b25">Vogel and Jurafsky, 2010)</ref>. <ref type="bibr">1</ref> Policy- based approaches instantiate a Markov decision process representing the action domain, and ap- ply standard supervised or reinforcement-learning approaches to learn a function for greedily select- ing among actions. In linear policy approximators, natural language instructions are incorporated di- rectly into state observations, and reading order becomes part of the action selection process.</p><p>Almost all existing policy-learning approaches make use of an unstructured parameterization, with a single (flat) feature vector representing all text and observations. Such approaches are thus restricted to problems that are simple enough (and have small enough action spaces) to be effectively characterized in this fashion. While there is a great deal of flexibility in the choice of feature func- tion (which is free to inspect the current and fu- ture state of the environment, the whole instruc- tion sequence, etc.), standard linear policy estima- tors have no way to model compositionality in lan- guage or actions.</p><p>Agents in this family have been evaluated on a variety of tasks, including map reading ( <ref type="bibr" target="#b0">Anderson et al., 1991</ref>) and gameplay ( <ref type="bibr" target="#b5">Branavan et al., 2009</ref>).</p><p>Though both families address the same class of instruction-following problems, they have been applied to a totally disjoint set of tasks. It should be emphasized that there is nothing inherent to policy learning that prevents the use of composi- tional structure, and nothing inherent to general compositional models that prevents more compli- cated dependence on environment state. Indeed, previous work <ref type="bibr" target="#b6">(Branavan et al., 2011;</ref><ref type="bibr" target="#b19">Narasimhan et al., 2015</ref>) uses aspects of both to solve a differ- ent class of gameplay problems. In some sense, our goal in this paper is simply to combine the strengths of semantic parsers and linear policy es- timators for fully general instruction following. As we shall see, however, this requires changes to many aspects of representation, learning and in- ference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Representations</head><p>We wish to train a model capable of following commands in a simulated environment. We do so by presenting the model with a sequence of train- ing pairs (x, y), where each x is a sequence of nat- ural language instructions (x 1 , x 2 , . . . , x m ), e.g.:</p><p>(Go down the yellow hall., Turn left., . . . )</p><p>and each y is a demonstrated action sequence (y 1 , y 2 , . . . , y n ), e.g.:</p><p>(rotate(90), move <ref type="formula">(2)</ref>, . . . ) Given a start state, y can equivalently be char- acterized by a sequence of (state, action, state)  triples resulting from execution of the environ- ment model. An example instruction is shown in <ref type="figure" target="#fig_3">Figure 2a</ref>. An example action, situated in the en- vironment where it occurs, is shown in <ref type="figure" target="#fig_3">Figure 2e</ref>. Our model performs compositional interpreta- tion of instructions by leveraging existing struc- ture inherent in both text and actions. Thus we interpret x i and y j not as raw strings and primitive actions, but rather as structured objects.</p><p>Linguistic structure We assume access to a pre- trained parser, and in particular that each of the instructions Action structure By analogy to the represen- tation of instructions as parse trees, we assume that each (state, action, state) triple (provided by the environment model) can be characterized by a grounding graph. The structure and content of this representation is task-specific. An example grounding graph for the maze navigation task is shown in <ref type="figure" target="#fig_3">Figure 2d</ref>. The example contains a node corresponding to the primitive action move(2) (in the upper left), and several nodes correspond- ing to locations in the environment that are visible after the action is performed.</p><p>Each node in the graph (and, though not de- picted, each edge) is decorated with a list of fea- tures. These features might be simple indica- tors (e.g. whether the primitive action performed was move or rotate), real values (the distance traveled) or even string-valued (English-language names of visible landmarks, if available in the environment description). Formally, a grounding graph consists of a tuple</p><formula xml:id="formula_0">(V, E, L, f V , f E ), with -V a set of vertices -E ∈ V × V a set of (directed) edges -L a space of labels (numbers, strings, etc.) -f V : V → 2 L a vertex feature function -f E : E → 2 L an edge feature function</formula><p>In this paper we have tried to remain agnostic to details of graph construction. Our goal with the grounding graph framework is simply to accom- modate a wider range of modeling decisions than allowed by existing formalisms. Graphs might be constructed directly, given access to a struc- tured virtual environment (as in all experiments in this paper), or alternatively from outputs of a perceptual system. For our experiments, we have remained as close as possible to task representa- tions described in the existing literature. Details for each task can be found in the accompanying software package.</p><p>Graph-based representations are extremely common in formal semantics ( <ref type="bibr" target="#b11">Jones et al., 2012;</ref><ref type="bibr" target="#b23">Reddy et al., 2014</ref>), and the version presented here corresponds to a simple generalization of famil- iar formal methods. Indeed, if L is the set of all atomic entities and relations, f V returns a unique label for every v ∈ V , and f E always returns a vector with one active feature, we recover the existentially-quantified portion of first order logic exactly, and in this form can implement large parts of classical neo-Davidsonian semantics <ref type="bibr" target="#b20">(Parsons, 1990</ref>) using grounding graphs.</p><p>Crucially, with an appropriate choice of L this formalism also makes it possible to go beyond set- theoretic relations, and incorporate string-valued features (like names of entities and landmarks) and real-valued features (like colors and positions) as well.</p><p>Turn left. Go down the yellow hall. Sentences align to a subset of the state-action sequences, with the rest of the states filled in by pragmatic (planning) implication. State-to-state structure represents planning con- straints (environment model) while state-to-text structure rep- resents compositional alignment. All potentials are log-linear and feature-driven.</p><p>Lexical semantics We must eventually combine features provided by parse trees with features pro- vided by the environment. Examples here might include simple conjunctions (word=yellow ∧ rgb=(0.5, 0.5, 0.0)) or more compli- cated computations like edit distance between landmark names and lexical items. Features of the latter kind make it possible to behave correctly in environments containing novel strings or other features unseen during training.</p><p>This aspect of the syntax-semantics inter- face has been troublesome for some logic-based approaches: while past work has used related machinery for selecting lexicon entries <ref type="bibr" target="#b4">(Berant and Liang, 2014</ref>) or for rewriting logical forms ( <ref type="bibr" target="#b14">Kwiatkowski et al., 2013</ref>), the relationship be- tween text and the environment has ultimately been mediated by a discrete (and indeed finite) in- ventory of predicates. Several recent papers have investigated simple grounded models with real- valued output spaces ( <ref type="bibr" target="#b1">Andreas and Klein, 2014;</ref><ref type="bibr" target="#b18">McMahan and Stone, 2015</ref>), but we are unaware of any fully compositional system in recent lit- erature that can incorporate observations of these kinds.</p><p>Formally, we assume access to a joining feature function φ : (2 L × 2 L ) → R d . As with grounding graphs, our goal is to make the general framework as flexible as possible, and for individual exper- iments have chosen φ to emulate modeling deci- sions from previous work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>As noted in the introduction, we approach instruc- tion following as a sequence prediction problem. Thus we must place a distribution over sequences of actions conditioned on instructions. We decom- pose the problem into two components, describing interlocking models of "path structure" and "ac- tion structure". Path structure captures how se- quences of instructions give rise to sequences of actions, while action structure captures the com- positional relationship between individual utter- ances and the actions they specify.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Path structure: aligning utterances to actions</head><p>The high-level path structure in the model is de- picted in <ref type="figure" target="#fig_5">Figure 3</ref>. Our goal here is to permit both under-and over-specification of plans, and to ex- pose a planning framework which allows plans to be computed with lookahead (i.e. non-greedily).</p><p>These goals are achieved by introducing a se- quence of latent alignments between instructions and actions. Consider the multi-step example in <ref type="figure" target="#fig_1">Figure 1b</ref>. If the first instruction go down the yel- low hall were interpreted immediately, we would have a presupposition failure-the agent is facing a wall, and cannot move forward at all. Thus an implicit rotate action, unspecified by text, must be performed before any explicit instructions can be followed.</p><p>To model this, we take the probability of a (text, plan, alignment) triple to be log-proportional to the sum of two quantities:</p><p>1. a path-only score ψ(n; θ) + j ψ(y j ; θ)</p><p>2. a path-and-text score, itself the sum of all pair scores ψ(x i , y j ; θ) licensed by the alignment (1) captures our desire for pragmatic constraints on interpretation, and provides a means of encod- ing the inherent plausibility of paths. We take ψ(n; θ) and ψ(y; θ) to be linear functions of θ.</p><p>(2) provides context-dependent interpretation of text by means of the structured scoring function ψ(x, y; θ), described in the next section. Formally, we associate with each instruction x i a sequence-to-sequence alignment variable a i ∈ 1 . . . n (recalling that n is the number of actions).</p><p>Then we have 2</p><formula xml:id="formula_1">p(y,a|x; θ) ∝ exp ψ(n) + n j=1 ψ(y j ) + m i=1 n j=1 1[a j = i] ψ(x i , y j )<label>(1)</label></formula><p>We additionally place a monotonicity constraint on the alignment variables. This model is globally normalized, and for a fixed alignment is equiva- lent to a linear-chain CRF. In this sense it is analo- gous to IBM Model I ( <ref type="bibr" target="#b7">Brown et al., 1993)</ref>, with the structured potentials ψ(x i , y j ) taking the place of lexical translation probabilities. While alignment models from machine translation have previously been used to align words to fragments of semantic parses ( <ref type="bibr" target="#b26">Wong and Mooney, 2006;</ref><ref type="bibr" target="#b22">Pourdamghani et al., 2014</ref>), we are unaware of such models be- ing used to align entire instruction sequences to demonstrations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action structure: aligning words to percepts</head><p>Intuitively, this scoring function ψ(x, y) should capture how well a given utterance describes an action. If neither the utterances nor the actions had structure (i.e. both could be represented with sim- ple bags of features), we would recover something analogous to the conventional policy-learning ap- proach. As structure is essential for some of our tasks, ψ(x, y) must instead fill the role of a seman- tic parser in a conventional compositional model. Our choice of ψ(x, y) is driven by the following fundamental assumptions: Syntactic relations ap- proximately represent semantic relations. Syntac- tic proximity implies relational proximity. In this view, there is an additional hidden structure-to- structure alignment between the grounding graph and the parsed text describing it. <ref type="bibr">3</ref> Words line up with nodes, and dependencies line up with rela- tions. Visualizations are shown in <ref type="figure" target="#fig_3">Figure 2c</ref> and the zoomed-in portion of <ref type="figure" target="#fig_5">Figure 3</ref>.</p><p>As with the top-level alignment variables, this approach can viewed as a simple relaxation of a familiar model. CCG-based parsers assume that syntactic type strictly determines semantic type, and that each lexical item is associated with a small set of functional forms. Here we simply allow all words to license all predicates, multi- ple words to specify the same predicate, and some edges to be skipped. We instead rely on a scoring function to impose soft versions of the hard con- straints typically provided by a grammar. Related models have previously been used for question an- swering ( <ref type="bibr" target="#b23">Reddy et al., 2014;</ref><ref type="bibr" target="#b21">Pasupat and Liang, 2015)</ref>.</p><p>For the moment let us introduce variables b to denote these structure-to-structure alignments. (As will be seen in the following section, it is straightforward to marginalize over all choices of b. Thus the structure-to-structure alignments are never explicitly instantiated during inference, and do not appear in the final form of ψ(x, y).) For a fixed alignment, we define ψ(x, y, b) according to a recurrence relation. Let x i be the ith word of the sentence, and let y j be the jth node in the ac- tion graph (under some topological ordering). Let c(i) and c(j) give the indices of the dependents of x i and children of y j respectively. Finally, let x ik and y jl denote the associated dependency type or relation. Define a "descendant" function:</p><formula xml:id="formula_2">d(i, j) = (k, l) : k ∈ c(i), l ∈ c(j), (k, l) ∈ b Then, ψ(x i , y j , b) = exp θ φ(x i , y j ) + (k,l)∈d(x,y) θ φ x ik , y jl · ψ(x k , y l , b)</formula><p>This is just an unnormalized synchronous deriva- tion between x and y-at any aligned (node, word) pair, the score for the entire derivation is the score produced by combining that word and node, times the scores at all the aligned descendants. Observe that as long as there are no cycles in the depen- dency parse, it is perfectly acceptable for the rela- tion graph to contain cycles and even self-loops- the recurrence still bottoms out appropriately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Learning and inference</head><p>Given a sequence of training pairs (x, y), we wish to find a parameter setting that maximizes p(y|x; θ). If there were no latent alignments a or b, this would simply involve minimization of a convex objective. The presence of latent vari- ables complicates things. Ideally, we would like Algorithm 1 Computing structure-to-structure alignments x i are words in reverse topological order y j are grounding graph nodes (root last) chart is an m × n array for i = 1 to |x| do for j = 1 to |y| do score ← exp θ φ(x i , y j )</p><formula xml:id="formula_3">for (k, l) ∈ d(i, j) do s ← l∈c(j) exp θ φ(x ik , y jl ) · chart[k, l]</formula><p>score ← score · s end for chart <ref type="bibr">[i, j]</ref> ← score end for end for return chart <ref type="bibr">[n, m]</ref> to sum over the latent variables, but that sum is in- tractable. Instead we make a series of variational approximations: first we replace the sum with a maximization, then perform iterated conditional modes, alternating between maximization of the conditional probability of a and θ. We begin by initializing θ randomly.</p><p>As noted in the preceding section, the vari- able b does not appear in these equations. Con- ditioned on a, the sum over structure-to-structure ψ(x, y) = b ψ(x, y, b) can be performed ex- actly using a simple dynamic program which runs in time O(|x||y|) (assuming out-degree bounded by a constant, and with |x| and |y| the number of words and graph nodes respectively). This is Al- gorithm 1.</p><p>In our experiments, θ is optimized using L- BFGS ( <ref type="bibr" target="#b16">Liu and Nocedal, 1989)</ref>. Calculation of the gradient with respect to θ requires computa- tion of a normalizing constant involving the sum over p(x, y , a) for all y . While in principle the normalizing constant can be computed using the forward algorithm, in practice the state spaces un- der consideration are so large that even this is in- tractable. Thus we make an additional approxima- tion, constructing a set˜Yset˜ set˜Y of alternative actions and taking</p><formula xml:id="formula_4">p(y, a|x) ≈ n j=1 exp ψ(y j )+ m i=1 1[a i =j]ψ(x i ,y i ) ˜ y∈˜Yy∈˜ y∈˜Y exp ψ(˜ y)+ m i=1 1[a i =j]ψ(x i ,˜ y) 1170˜Y</formula><p>1170˜ 1170˜Y is constructed by sampling alternative actions from the environment model. Meanwhile, maxi- mization of a can be performed exactly using the Viterbi algorithm, without computation of normal- izers.</p><p>Inference at test time involves a slightly differ- ent pair of optimization problems. We again per- form iterated conditional modes, here on the align- ments a and the unknown output path y. Max- imization of a is accomplished with the Viterbi algorithm, exactly as before; maximization of y also uses the Viterbi algorithm, or a beam search when this is computationally infeasible. If bounds on path length are known, it is straightforward to adapt these dynamic programs to efficiently con- sider paths of all lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>As one of the main advantages of this approach is its generality, we evaluate on several different benchmark tasks for instruction following. These exhibit great diversity in both environment struc- ture and language use. We compare our full system to recent state-of-the-art approaches to each task. In the introduction, we highlighted two core aspects of our approach to semantics: compositionality (by way of grounding graphs and structure-to-structure alignments) and plan- ning (by way of inference with lookahead and sequence-to-sequence alignments). To evaluate these, we additionally present a pair of ablation ex- periments: no grounding graphs (an agent with an unstructured representation of environment state), and no planning (a reflex agent with no looka- head).</p><p>Map reading Our first application is the map navigation task established by <ref type="bibr" target="#b25">Vogel and Jurafsky (2010)</ref>, based on data collected for a psychological experiment by <ref type="bibr" target="#b0">Anderson et al. (1991)</ref>  <ref type="figure" target="#fig_1">(Figure 1a)</ref>. Each training datum consists of a map with a des- ignated starting position, and a collection of land- marks, each labeled with a spatial coordinate and a string name. Names are not always unique, and landmarks in the test set are never observed dur- ing training. This map is accompanied by a set of instructions specifying a path from the start- ing position to some (unlabeled) destination point. These instruction sets are informal and redundant, involving as many as a hundred utterances. They are transcribed from spoken text, so grammatical errors, disfluencies, etc. are common. This is a P R F1</p><p>Vogel and Jurafsky (2010) 0.46 0.51 0.48 Andreas and <ref type="bibr" target="#b1">Klein (2014)</ref> 0.43 0.51 0.45</p><p>Model [no planning] 0.44 0.46 0.45 Model [no grounding graphs] 0.52 0.52 0.52 Model <ref type="bibr">[full]</ref> 0.51 0.60 0.55 <ref type="table">Table 1</ref>: Evaluation results for the map-reading task. P is pre- cision, R is recall and F1 is F-measure. Scores are calculated with respect to transitions between landmarks appearing in the reference path (for details see <ref type="bibr" target="#b25">Vogel and Jurafsky (2010)</ref>). We use the same train / test split. Some variant of our model achieves the best published results on all three metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Weight</head><p>word=top ∧ side=North</p><formula xml:id="formula_5">1.31 word=top ∧ side=South 0.61 word=top ∧ side=East −0.93 dist=0 4.51 dist=1</formula><p>2.78 dist=4</p><p>1.54 <ref type="table">Table 2</ref>: Learned feature values. The model learns that the word top often instructs the navigator to position itself above a landmark, occasionally to position itself below a landmark, but rarely to the side. The bottom portion of the table shows learned text-independent constraints: given a choice, near destinations are preferred to far ones (so shorter paths are pre- ferred overall).</p><p>prime example of a domain that does not lend it- self to logical representation-grammars may be too rigid, and previously-unseen landmarks and real-valued positions are handled more easily with feature machinery than predicate logic. The map task was previously studied by <ref type="bibr" target="#b25">Vogel and Jurafsky (2010)</ref>, who implemented SARSA with a simple set of features. By combining these features with our alignment model and search pro- cedure, we achieve state-of-the-art results on this task by a substantial margin <ref type="table">(Table 1)</ref>.</p><p>Some learned feature values are shown in Ta- ble 2. The model correctly infers cardinal direc- tions (the example shows the preferred side of a destination landmark modified by the word top). Like Vogel et al., we see support for both allocen- tric references (you are on top of the hill) and ego- centric references (the hill is on top of you). We can also see pragmatics at work: the model learns useful text-independent constraints-in this case, that near destinations should be preferred to far ones.</p><p>Maze navigation The next application we con- sider is the maze navigation task of <ref type="bibr" target="#b17">MacMahon et al. (2006)</ref>  <ref type="figure" target="#fig_1">(Figure 1b)</ref>. Here, a virtual agent is sit-Success (%) <ref type="bibr" target="#b12">Kim and Mooney (2012)</ref> 57.  <ref type="table">Table 3</ref>: Evaluation results for the maze navigation task. "Success" shows the percentage of actions resulting in a cor- rect position and orientation after observing a single instruc- tion. We use the leave-one-map-out evaluation employed by previous work. <ref type="bibr">4</ref> All systems are trained on full action se- quences. Our model outperforms several task-specific base- lines, as well as a baseline with path structure but no action structure.</p><p>uated in a maze (whose hallways are distinguished with various wallpapers, carpets, and the presence of a small set of standard objects), and again given instructions for getting from one point to another. This task has been the subject of focused attention in semantic parsing for several years, resulting in a variety of sophisticated approaches. Despite superficial similarity to the previous navigation task, the language and plans required for this task are quite different. The proportion of instructions to actions is much higher (so redun- dancy much lower), and the interpretation of lan- guage is highly compositional.</p><p>As can be seen in <ref type="table">Table 3</ref>, we outperform a number of systems purpose-built for this naviga- tion task. We also outperform both variants of our system, most conspicuously the variant with- out grounding graphs. This highlights the impor- tance of compositional structure. Recent work by <ref type="bibr" target="#b13">Kim and Mooney (2013)</ref> and <ref type="bibr" target="#b3">Artzi et al. (2014)</ref> has achieved better results; these systems make use of techniques and resources (respectively, dis- criminative reranking and a seed lexicon of hand- annotated logical forms) that are largely orthogo- nal to the ones used here, and might be applied to improve our own results as well.</p><p>Puzzle solving The last task we consider is the Crossblock task studied by <ref type="bibr" target="#b5">Branavan et al. (2009)</ref>  <ref type="figure" target="#fig_1">(Figure 1c)</ref>. Here, again, natural language is used to specify a sequence of actions, in this case the solution to a simple game. The environment is simple enough to be captured with a flat feature  representation, so there is no distinction between the full model and the variant without grounding graphs.</p><p>Unlike the other tasks we consider, Crossblock is distinguished by a challenging associated search problem. Here it is nontrivial to find any sequence that eliminates all the blocks (the goal of the puz- zle). Thus this example allows us measure the ef- fectiveness of our search procedure.</p><p>Results are shown in <ref type="table" target="#tab_2">Table 4</ref>. As can be seen, our model achieves state-of-the-art performance on this task when attempting to match the human- specified plan exactly. If we are purely concerned with task completion (i.e. solving the puzzle, per- haps not with the exact set of moves specified in the instructions) we can measure this directly. Here, too, we substantially outperform a no-text baseline. Thus it can be seen that text induces a useful heuristic, allowing the model to solve a con- siderable fraction of problem instances not solved by na¨ıvena¨ıve beam search.</p><p>The problem of inducing planning heuristics from side information like text is an important one in its own right, and future work might focus specifically on coupling our system with a more sophisticated planner. Even at present, the re- sults in this section demonstrate the importance of lookahead and high-level reasoning in instruction following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have described a new alignment-based com- positional model for following sequences of nat- ural language instructions, and demonstrated the effectiveness of this model on a variety of tasks. A fully general solution to the problem of contextual interpretation must address a wide range of well- studied problems, but the work we have described here provides modular interfaces for the study of a number of fundamental linguistic issues from a machine learning perspective. These include:</p><p>Pragmatics How do we respond to presup- position failures, and choose among possible interpretations of an instruction disambiguated only by context? The mechanism provided by the sequence-prediction architecture we have de- scribed provides a simple answer to this ques- tion, and our experimental results demonstrate that the learned pragmatics aid interpretation of in- structions in a number of concrete ways: am- biguous references are resolved by proximity in the map reading task, missing steps are inferred from an environment model in the maze naviga- tion task, and vague hints are turned into real plans by knowledge of the rules in Crossblock. A more comprehensive solution might explicitly describe the process by which instruction-givers' own be- liefs (expressed as distributions over sequences) give rise to instructions.</p><p>Compositional semantics The graph alignment model of semantics presented here is an expres- sive and computationally efficient generalization of classical logical techniques to accommodate en- vironments like the map task, or those explored in our previous work <ref type="bibr" target="#b1">(Andreas and Klein, 2014</ref>). More broadly, our model provides a compositional approach to semantics that does not require an explicit formal language for encoding sentence meaning. Future work might extend this approach to tasks like question answering, where logic- based approaches have been successful.</p><p>Our primary goal in this paper has been to ex- plore methods for integrating compositional se- mantics and the pragmatic context provided by se- quential structures. While there is a great deal of work left to do, we find it encouraging that this general approach results in substantial gains across multiple tasks and contexts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example tasks handled by our framework. The tasks feature noisy text, over-and under-specification of plans, and challenging search problems.</figDesc><graphic url="image-2.png" coords="2,257.99,192.65,82.28,65.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Go down the yellow hall go down hall the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Structure-to-structure alignment connecting a single sentence (via its syntactic analysis) to the environment state (via its grounding graph). The connecting alignments take the place of a traditional semantic parse and allow flexible, feature-driven linking between lexical primitives and perceptual factors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>x i is represented by a tree-structured dependency parse. An example is shown in Fig- ure 2b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Our model is a conditional random field that describes distributions over state-action sequences conditioned on input text. Each variable's domain is a structured value. Sentences align to a subset of the state-action sequences, with the rest of the states filled in by pragmatic (planning) implication. State-to-state structure represents planning constraints (environment model) while state-to-text structure represents compositional alignment. All potentials are log-linear and feature-driven.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results for the puzzle solving task. "Match" shows 
the percentage of predicted action sequences that exactly 
match the annotation. "Success" shows the percentage of 
predicted action sequences that result in a winning game con-
figuration, regardless of the action sequence performed. Fol-
lowing Branavan et al. (2009), we average across five random 
train / test folds. Our model achieves state-of-the-art results 
on this task. 

</table></figure>

			<note place="foot" n="1"> This is distinct from semantic parsers in which greedy inference happens to have an interpretation as a policy (Vlachos and Clark, 2014).</note>

			<note place="foot" n="2"> Here and the remainder of this paper, we suppress the dependence of the various potentials on θ in the interest of readability. 3 It is formally possible to regard the sequence-tosequence and structure-to-structure alignments as a single (structured) random variable. However, the two kinds of alignments are treated differently for purposes of inference, so it is useful to maintain a notational distinction.</note>

			<note place="foot" n="4"> We specifically targeted the single-sentence version of this evaluation, as an alternative full-sequence evaluation does not align precisely with our data condition.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank S.R.K. Brana-van for assistance with the Crossblock evaluation. The first author is supported by a National Science Foundation Graduate Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The HCRC map task corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><forename type="middle">H</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><forename type="middle">Gurman</forename><surname>Bard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwyneth</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Garrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacqueline</forename><surname>Kowtko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and speech</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="351" to="366" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Grounding language with points and paths in continuous spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Natural Language Learning</title>
		<meeting>the Conference on Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of semantic parsers for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="62" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning compact lexicons for CCG semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1273" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic parsing via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reinforcement learning for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R K</forename><surname>Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harr</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to win by reading manuals in a Monte-Carlo framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R K</forename><surname>Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the Association for Computational Linguistics</title>
		<meeting>the Human Language Technology Conference of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="268" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to interpret natural language navigation instructions from observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Meeting of the Association for the Advancement of Artificial Intelligence</title>
		<meeting>the Meeting of the Association for the Advancement of Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast online lexicon learning for grounded language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David L Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="430" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic annotation of robotic spatial commands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kais</forename><surname>Dukes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language and Technology Conference (LTC)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantics-based machine translation with hyperedge replacement grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bevan</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics</title>
		<meeting>the International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1359" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised PCFG induction for grounded language learning with highly ambiguous supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joohyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="433" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adapting discriminative reranking to grounded language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joohyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scaling semantic parsers with on-the-fly ontology matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="389" to="446" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the limited memory BFGS method for large scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="page" from="503" to="528" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Walk the talk: Connecting language, knowledge, and action in route instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Macmahon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Stankiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Kuipers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Meeting of the Association for the Advancement of Artificial Intelligence</title>
		<meeting>the Meeting of the Association for the Advancement of Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Bayesian model of grounded color semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="103" to="115" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Language understanding for textbased games using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Events in the semantics of English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terence</forename><surname>Parsons</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Aligning english strings with abstract meaning representation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Pourdamghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large-scale semantic parsing without question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="377" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A new corpus and imitation learning framework for contextdependent semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="547" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to follow navigational directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="806" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning for semantic parsing with statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>New York, New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="439" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
