<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discriminative Learning of Open-Vocabulary Object Retrieval and Localization by Negative Phrase Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Hinami</surname></persName>
							<email>hinami@nii.ac.jp, satoh@nii.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National Institute of Infomatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><forename type="middle">&amp;apos;</forename><surname>Ichi Satoh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National Institute of Infomatics</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Discriminative Learning of Open-Vocabulary Object Retrieval and Localization by Negative Phrase Augmentation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2605" to="2615"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2605</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Thanks to the success of object detection technology , we can retrieve objects of the specified classes even from huge image collections. However, the current state-of-the-art object detectors (such as Faster R-CNN) can only handle pre-specified classes. In addition, large amounts of positive and negative visual samples are required for training. In this paper, we address the problem of open-vocabulary object retrieval and localization, where the target object is specified by a textual query (e.g., a word or phrase). We first propose Query-Adaptive R-CNN, a simple extension of Faster R-CNN adapted to open-vocabulary queries, by transforming the text embedding vector into an object classifier and localization regressor. Then, for discriminative training, we then propose negative phrase augmentation (NPA) to mine hard negative samples which are visually similar to the query and at the same time semantically mutually exclusive of the query. The proposed method can retrieve and localize objects specified by a textual query from one million images in only 0.5 seconds with high precision .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Our goal is to retrieve objects from large-scale image database and localize their spatial lo- cations given a textual query. The task of object retrieval and localization has many ap- plications such as spatial position-aware im- age searches ( <ref type="bibr" target="#b10">Hinami et al., 2017)</ref> and it re- cently has gathered much attention from re- searchers. While much of the previous work mainly focused on object instance retrieval wherein the query is an image <ref type="bibr" target="#b30">(Shen et al., 2012;</ref><ref type="bibr" target="#b33">Tao et al., 2014;</ref><ref type="bibr" target="#b34">Tolias et al., 2016)</ref>, re- cent approaches <ref type="bibr" target="#b1">(Aytar and Zisserman, 2014;</ref><ref type="bibr" target="#b11">Hinami and Satoh, 2016</ref>) enable retrieval of more generic concepts such as an object category. Al- though such approaches are built on the recent successes of object detection including that of R-CNN ( ), object detection methods can generally handle only closed sets of categories (e.g., PASCAL 20 classes), which severely limits the variety of queries when they are used as retrieval systems. Open-vocabulary object localization is also a hot topic and many approaches are proposed to solve this prob- lem ( <ref type="bibr" target="#b25">Plummer et al., 2015;</ref><ref type="bibr" target="#b4">Chen et al., 2017)</ref>. However, most of them are not scalable to make them useful for large-scale retrieval.</p><p>We first describe Query-Adaptive R-CNN as an extension of the Faster R-CNN ( <ref type="bibr" target="#b28">Ren et al., 2015)</ref> object detection framework to open-vocabulary object detection simply by adding a component called a detector generator. While Faster R-CNN learns the class-specific linear classifier as learn- able parameters of the neural network, we gen- erate the weight of the classifier adaptively from text descriptions by learning the detector generator <ref type="figure" target="#fig_1">(Fig. 2b)</ref>. All of its components can be trained in an end-to-end manner. In spite of its simple archi-tecture, it outperforms all state-of-the-art methods in the Flickr30k Entities phrase localization task. It can also be used for large-scale retrievals in the manner presented in <ref type="bibr" target="#b11">(Hinami and Satoh, 2016)</ref>.</p><p>However, training a discriminative classifier is harder in the open-vocabulary setting. Closed- vocabulary object detection models such as Faster R-CNN are trained using many negative examples, where a sufficient amount of good-quality nega- tive examples is shown to be important for learn- ing a discriminative classifier <ref type="bibr" target="#b6">(Felzenszwalb et al., 2010;</ref><ref type="bibr" target="#b31">Shrivastava et al., 2016)</ref>. While closed- vocabulary object detection can use all regions without positive labels as negative data, in open- vocabulary detection, it is not guaranteed that a region without a positive label is negative. For ex- ample, as shown in <ref type="figure" target="#fig_0">Fig. 1b</ref>, a region with the anno- tation a man is not always negative for skier. Since training data for open-vocabulary object de- tection is generally composed of images, each having region annotations with free descriptions, it is nearly impossible to do an exhaustive anno- tation throughout the dataset for all possible de- scriptions. Another possible approach is to use the regions without positive labels in the image that contains positive examples, as shown in <ref type="figure" target="#fig_0">Fig. 1c</ref>. Although they can be guaranteed to be positive by carefully annotating the datasets, negative exam- ples are only limited to the objects that cooccur with the learned class.</p><p>To exploit negative data in open-vocabulary ob- ject detection, we use mutually exclusive relation- ships between categories. For example, an object with a label dog is guaranteed to be negative for the cat class because dog and cat are mutually exclusive. In addition, we propose an approach to select hard negative phrases that are difficult to discriminate (e.g., selecting zebra for horse). This approach, called negative phrase augmenta- tion (NPA), significantly improves the discrimina- tive ability of the classifier and improves the re- trieval performance by a large margin.</p><p>Our contributions are as follows. 1) We propose Query-Adaptive R-CNN, an extension of Faster R-CNN to open vocabulary, that is a simple yet strong method of open-vocabulary object detec- tion and that outperforms all state-of-the-art meth- ods in the phrase localization task. 2) We pro- pose negative phrase augmentation (NPA) to ex- ploit hard negative examples when training for open-vocabulary object detection, which makes the classifier more discriminative and robust to distractors in retrieval. Our method can accurately find objects amidst one million images in 0.5 sec- ond.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Phrase localization. Object grounding with nat- ural language descriptions has recently drawn much attention and several tasks and approaches have been proposed for it <ref type="bibr" target="#b9">(Guadarrama et al., 2014;</ref><ref type="bibr" target="#b15">Kazemzadeh et al., 2014;</ref><ref type="bibr" target="#b20">Mao et al., 2016;</ref><ref type="bibr" target="#b25">Plummer et al., 2015</ref>). The most related task to ours is the phrase localization intro- duced by <ref type="bibr" target="#b25">Plummer et al. (Plummer et al., 2015)</ref>, whose goal is to localize objects that corresponds to noun phrases in textual descriptions from an im- age. <ref type="bibr" target="#b4">Chen et al. (Chen et al., 2017</ref>) is the closest to our work in terms of learning region propos- als and performing regression conditioned upon a query. However, most phrase localization meth- ods are not scalable and cannot be used for re- trieval tasks. Some approaches ( <ref type="bibr" target="#b26">Plummer et al., 2017b;</ref><ref type="bibr" target="#b35">Wang et al., 2016a</ref>) learn a common sub- space between the text and image for phrase lo- calization. Instead of learning the subspace be- tween the image and sentence as in standard cross- modal searches, they learn the subspace between a region and a phrase. In particular, <ref type="bibr" target="#b35">Wang et al. (Wang et al., 2016a</ref>) use a deep neural network to learn the joint embedding of images and text; their training uses structure-preserving constraints based on structured matching. Although these ap- proaches can be used for large-scale retrieval, their accuracy is not as good as recent state-of-the-art methods.</p><p>Object retrieval and localization. Object re- trieval and localization have been researched in the context of particular object retrieval ( <ref type="bibr" target="#b30">Shen et al., 2012;</ref><ref type="bibr" target="#b33">Tao et al., 2014;</ref><ref type="bibr" target="#b34">Tolias et al., 2016)</ref>, where a query is given as an image. <ref type="bibr" target="#b1">Aytar et al. (Aytar and Zisserman, 2014)</ref> proposed re- trieval and localization of generic category ob- jects by extending the object detection tech- nique to large-scale retrieval.</p><p>Hinami and Satoh <ref type="bibr" target="#b11">(Hinami and Satoh, 2016</ref>) extended the R- CNN to large-scale retrieval by using approxi- mate nearest neighbor search techniques. How- ever, they assumed that the detector of the cate- gory is given as a query and require many sample images with bounding box annotations in order to learn the detector. Several other approaches have used the external search engines (e.g., Google im- age search) to get training images from textual queries ( <ref type="bibr">Arandjelovi et al., 2012;</ref><ref type="bibr" target="#b3">Chatfield et al., 2015)</ref>. Instead, we generate an object detector di- rectly from the given textual query by using a neu- ral network.</p><p>Parameter prediction by neural network. Query-Adaptive R-CNN generates the weights of the detector from the query instead of learning them by backpropagation. The dynamic filter net- work <ref type="bibr" target="#b5">(De Brabandere et al., 2016</ref>) is one of the first methods that generate neural network pa- rameters dynamically conditioned on an input. Several subsequent approaches use this idea in zero-shot learning ( <ref type="bibr" target="#b2">Ba et al., 2016</ref>) and visual question answering ( <ref type="bibr" target="#b23">Noh et al., 2016)</ref>. <ref type="bibr" target="#b37">Zhang et al. (Zhang et al., 2017)</ref> integrates this idea into the Fast R-CNN framework by dynamically generat- ing the classifier from the text in a similar manner to ( <ref type="bibr" target="#b2">Ba et al., 2016</ref>). We extend this work to the case of large-scale retrieval. The proposed Query- Adaptive R-CNN generates the regressor weights and learn the region proposal network following Faster R-CNN. It enables precise localization with fewer proposals, which makes the retrieval system more memory efficient. In addition, we propose a novel hard negative mining approach, called nega- tive phrase augmentation, which makes the gener- ated classifier more discriminative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Query-Adaptive R-CNN</head><p>Query-adaptive R-CNN is a simple extension of Faster R-CNN to open-vocabulary object detec- tion. While Faster R-CNN detects objects of fixed categories, Query-Adaptive R-CNN detects any objects specified by a textual phrase. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the difference between Faster R-CNN and Query-Adaptive R-CNN. While Faster R- CNN learns a class-specific classifier and regres- sor as parameters of the neural networks, Query- Adaptive R-CNN generates them from the query text by using a detector generator. Query-Adaptive R-CNN is a simple but effective method that sur- passes state-of-the-art phrase localization methods and can be easily extended to the case of large- scale retrieval. Furthermore, its retrieval accu- racy is significantly improved by a novel train- ing strategy called negative phrase augmentation (Sec. 3.2).  While Faster R-CNN learns the classifier of a closed set of categories as learnable parameters of neural net- works, Query-Adaptive R-CNN generates a classifier and regressor adaptively from a query text by learning a detector generator that transforms the text into a clas- sifier and regressor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>The network is composed of two subnetworks: a region feature extractor and detector generator, both of which are trained in an end-to-end man- ner. The region feature extractor takes an im- age as input and outputs features extracted from sub-regions that are candidate objects. Following Faster R-CNN ( <ref type="bibr" target="#b28">Ren et al., 2015</ref>), regions are de- tected using a region proposal network (RPN) and the features of the last layer (e.g., fc7 in VGG net- work) are used as region features. The detector generator takes a text description as an input and outputs a linear classifier and regressor for the de- scription (e.g., if a dog is given, a dog clas- sifier and regressor are output). Finally, a confi- dence and a regressed bounding box are predicted for each region by applying the classifier and re- gressor to the region features.</p><p>Detector generator. The detector generator transforms the given text t into a classifier w c and regressor (w r x , w r y , w r w , w r h ), where w c is the weight of a linear classifier and (w r x , w r y , w r w , w r h ) is the weight of a linear regressor in terms of x, y, width w, and height h, following ). We first transform a text t of variable length into a text embedding vector v. Other phrase localization approaches uses the Fisher vector encoding of word2vec ( <ref type="bibr" target="#b17">Klein et al., 2015;</ref><ref type="bibr" target="#b25">Plummer et al., 2015</ref>) or long-short term memory (LSTM) <ref type="bibr" target="#b4">(Chen et al., 2017</ref>) for the phrase embed- ding. However, we found that the simple mean pooling of word2vec ( <ref type="bibr" target="#b21">Mikolov et al., 2013)</ref> per- forms better than these methods for our model (comparisons given in the supplemental material). The text embedding is then transformed into a de- tector, i.e., w c = G c (v) and (w r x , w r y , w r w , w r h ) = G r (v). Here, we use a linear transformation for G c (i.e., w c = Wv, where W is a projection matrix). For the regressor, we use a multi-layer perceptron with one hidden layer to predict each of (w r x , w r y , w r w , w r h ) = G r (v). We tested var- ious architectures for G r and found that sharing the hidden layer and reducing the dimension of the hidden layer (up to 16) does not adversely affect the performance, while at the same time it sig- nificantly reduces the number of parameters (see Sec. 5.2 for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training with Negative Phrase Augmentation</head><p>All components of Query-Adaptive R-CNN can be jointly trained in an end-to-end manner. The training strategy basically follows that of Faster R-CNN. The differences are shown in <ref type="figure" target="#fig_3">Figure 3</ref>. Faster R-CNN is trained with the fixed closed set of categories <ref type="figure" target="#fig_3">(Fig. 3a)</ref>, where all regions with- out a positive label can be used as negative exam- ples. On the other hand, Query-Adaptive R-CNN is trained using the open-vocabulary phrases an- notated to the regions <ref type="figure" target="#fig_3">(Fig. 3b)</ref>, where sufficient negative examples cannot be used for each phrase compared to Faster R-CNN because a region with- out a positive label is not guaranteed to be negative in open-vocabulary object detection. We solve this problem by proposing negative phrase augmenta- tion (NPA), which enables us to use good quality negative examples by using the linguistic relation- ship (e.g., mutually exclusiveness) and the confu- sion between the categories <ref type="figure" target="#fig_3">(Fig. 3c)</ref>. It signif- icantly improves the discriminative ability of the generated classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Basic Training</head><p>First, we describe the basic training strategy with- out NPA <ref type="figure" target="#fig_3">(Fig. 3b)</ref>. Training a Query-Adaptive R- CNN requires the phrases and their corresponding bounding boxes to be annotated. For the ith image (we use one image as a minibatch), let us assume that C i phrases are associated with the image. The C i phrases can be considered as the classes to train in the minibatch. The labels L i ∈ {0, 1} C i ×nr   are assigned to the region proposals generated by RPN (each of the dotted rectangles in <ref type="figure" target="#fig_3">Fig 3b)</ref>; a positive label is assigned if the box overlaps the ground truth box by more than 0.5 in IoU and neg- ative labels are assigned to other RoIs under the assumption that all positive objects of C i classes are annotated (i.e., regions without annotations are negative within the image). <ref type="bibr">1</ref> We then compute the classification loss by using the training labels and classification scores. <ref type="bibr">2</ref> The loss in terms of RPN and bounding box regression is computed in the same way as Faster R-CNN ( <ref type="bibr" target="#b28">Ren et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Negative Phrase Augmentation</head><p>Here, we address the difficulty of using nega- tive examples in the training of open-vocabulary object detection. As shown in <ref type="figure" target="#fig_0">Fig. 1b</ref> How can we exploit hard examples that are guaranteed to be negative? We can make use of the mutually exclusive relationship between cat- egories: e.g., an object with a dog label is neg- ative for cat because dog and cat are mutu- ally exclusive. There are two ways we can add to a minibatch: add negative images (regions) or negative phrases. Adding negative phrases (as in <ref type="figure" target="#fig_3">Fig. 3c</ref>) is generally better because it involves a much smaller additional training cost than adding images in terms of the both computational cost and GPU memory usage. In addition, to im- prove the discriminative ability of the classifier, we select only hard negative phrases by mining the confusing categories. This approach, called negative phrase augmentation (NPA), is a generic way of exploiting hard negative examples in open- vocabulary object detection and leads to large im- provements in accuracy, as we show in Sec. 5.3.</p><p>Confusion table. We create a confusion ta- ble that associates a category with its hard nega- tive categories, from which negative phrases are picked as illustrated in <ref type="figure" target="#fig_3">Fig. 3c</ref>. To create the en- try for category c, we first generate the candidate list of hard negative categories by retrieving the top 500 scored objects from all objects in the vali- dation set of Visual Genome ( <ref type="bibr" target="#b18">Krishna et al., 2016)</ref> (using c as a query). After that, we remove the mutually non-exclusive category relative to c from the list. Finally, we aggregate the list by category and assign a weight to each category. Each of the registered entries becomes like dog:{cat:0.5, horse:0.3, cow:0.2}. The weight corresponds to the probability of selecting the category in NPA, which is computed based on the number of appear- ances and their ranks in the candidate list. <ref type="bibr">3</ref> Removal of mutually non-exclusive phrases. To remove non-mutually exclusive phrases from the confusion table, we use two approaches that estimate whether the two categories are mutually exclusive or not. 1) The first approach uses the WordNet hierarchy: if two categories have parent- child relationships in WordNet <ref type="bibr" target="#b22">(Miller, 1995)</ref>, they are not mutually exclusive. However, the converse is not necessarily true; e.g., man and skier are not mutually exclusive but do not have the parent-child relationship in the WordNet hi- erarchy. 2) As an alternative approach, we pro- pose to use Visual Genome annotation: if two cat- egories co-occur more often in the Visual Genome dataset ( <ref type="bibr" target="#b18">Krishna et al., 2016</ref>), these categories are considered to be not mutually exclusive. <ref type="bibr">4</ref> These two approaches are complementary, and they im- prove detection performance by removing the mu- tually non-exclusive words (see Sec. 5.3).</p><p>The training pipeline with NPA is as follows:</p><p>(1) Update the confusion table: The confusion ta- ble is updated periodically (after every 10k it- erations in our study). Entries were created for categories that frequently appeared in 10k suc- cessive batches (or the whole training set if the size of the dataset is not large). (2) Add hard negative phrases: Negative phrases are added to each of the C i phrases in a mini- batch. We replace the name of the category in each phrase with its hard negative cate- gory (e.g., generate a running woman for a running man), where the category name is obtained by extracting nouns. A negative phrase is randomly selected from the confusion table on the basis of the assigned probability. <ref type="bibr">3</ref> We compute the weight of each category as the sum of 500 minus the rank for all ranked results in the candidate lists normalized over all categories in order to sum to one. <ref type="bibr">4</ref> We set the ratio at 1% of objects in either category. For example, if there are 1000 objects with the skier la- bel and 20 of those objects are also annotated with man (20/1000=2%), we consider that skier and man are not mu- tually exclusive.</p><p>(3) Add losses: As illustrated in <ref type="figure" target="#fig_3">Fig. 3c</ref>, we only add negative labels to the regions where a posi- tive label is assigned to the original phrase. The classification loss is computed only for the re- gions, which is added to the original loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Large-Scale Object Retrieval</head><p>Query-Adaptive R-CNN can be used for large- scale object retrieval and localization, because it can be decomposed into a query-independent part and a query-dependent part, i.e., a re- gion feature extractor and detector generator. We follow the approach used in large-scale R- CNN ( <ref type="bibr" target="#b11">Hinami and Satoh, 2016</ref>), but we overcome its two critical drawbacks. First, a large-scale R- CNN can only predict boxes included in the region proposals; these are detected offline even though the query is unknown at the time; therefore, to get high recall, a large number of object propos- als should be used, which is memory inefficient. Instead, we generate a regressor as well as a classi- fier, which enables more accurate localization with fewer proposals. Second, a large-scale R-CNN as- sumes that the classifier is given as a query, and learning a classifier requires many samples with bounding annotations. We generate the classifier from a text query directly by using the detector generator of Query-Adaptive R-CNN. The result- ing system is able to retrieve and localize objects from a database with one million images in less than one second. Database indexing. For each image in the database, the region feature extractor extracts re- gion proposals and corresponding features. We create an index for the region features in order to speed up the search. For this, we use the IV- FADC system <ref type="bibr">(Jégou et al., 2011</ref>) in the manner described in <ref type="bibr" target="#b11">(Hinami and Satoh, 2016)</ref>.</p><p>Searching. Given a text query, the detector gen- erator generates a linear classifier and bounding box regressor. The regions with high classifica- tion scores are then retrieved from the database by making an IVFADC-based search. Finally, the re- gressor is applied to the retrieved regions to obtain the accurately localized bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Model: Query-Adaptive R-CNN is based on VGG16 ( <ref type="bibr" target="#b32">Simonyan and Zisserman, 2015)</ref>, as in other work on phrase localization. We first initialized the weights of the VGG and RPN by using Faster R-CNN trained on Microsoft COCO ( <ref type="bibr" target="#b19">Lin et al., 2014)</ref>; the weights were then fine-tuned for each dataset of the evaluation. In the training using Flickr30k Entities, we first pre- trained the model on the Visual Genome dataset using the object name annotations. We used Adam ( <ref type="bibr" target="#b16">Kingma and Ba, 2015</ref>) with a learning rate starting from 1e-5 and ran it for 200k iterations.</p><p>Tasks and datasets: We evaluated our ap- proaches on two tasks: phrase localization and open-vocabulary object detection and retrieval. The phrase localization task was performed on the Flickr30k Entities dataset <ref type="bibr" target="#b25">(Plummer et al., 2015)</ref>. Given an image and a sentence that de- scribes the image, the task was to localize re- gion that corresponds to the phrase in a sentence. Flickr30k datasets contain 44,518 unique phrases, where the number of words of each phrase is 1-8 (2.1 words on average). We followed the evalu- ation protocol of <ref type="bibr" target="#b25">(Plummer et al., 2015</ref>). We did not use Flickr30k Entities for the retrieval task because the dataset is not exhaustively annotated (e.g., not all men appearing in the dataset are anno- tated with man), which makes it difficult to eval- uate with a retrieval metric such as AP, as dis- cussed in <ref type="bibr" target="#b26">Plummer et al. (Plummer et al., 2017b</ref>). Although we cannot evaluate the retrieval perfor- mance directly on the phrase localization task, we can make comparisons with other approaches and show that our method can handle a wide variety of phrases.</p><p>The open-vocabulary object detection and re- trieval task was evaluated in the same way as the standard object detection task. The difference was the assumption that we do not know the target cat- egory at training time in open-vocabulary settings; i.e., the method does not tune in to a specific cate- gory, unlike the standard object detection task. We used the Visual Genome dataset ( <ref type="bibr" target="#b18">Krishna et al., 2016)</ref> and selected the 100 most frequently object categories as queries among its 100k or so cate- gories. <ref type="bibr">5 6</ref> We split the dataset into training, valida- tion, and test sets following ). We also evaluated our approaches on the PASCAL VOC 2007 dataset, which is a widely used dataset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>People Clothing Body Animals Vehicles Instruments Scene Other All   for object detection. <ref type="bibr">7</ref> As metrics, we used top- k precision and average precision (AP), computed from the region-level ranked list as in the standard object detection task. 8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Phrase localization</head><p>Comparison with state-of-the-art. We compared our method with state-of-the-art methods on the Flickr30k Entities phrase localization task. We categorized the methods into two types, i.e., non- scalable and scalable methods (Tab. 1). 1) Non- scalable methods cannot be used for large-scale retrieval because their query-dependent compo- nents are too complex to process a large amount of images online, and 2) Scalable methods can be used for large-scale retrieval because their query- dependent components are easy to scale up (e.g., the L 2 distance computation); these include com- mon subspace-based approaches such as CCA.</p><p>Our method also belongs to the scalable category.</p><p>We used a simple model without a regressor and <ref type="bibr">7</ref> We used the model trained on Visual Genome even for the evaluation on the PASCAL dataset because of the assump- tion that the target category is unknown. <ref type="bibr">8</ref> We did not separately evaluate the detection and retrieval tasks because both can be evaluated with the same metric.</p><p>NPA in the experiments. <ref type="table" target="#tab_4">Table 1</ref> compares Query-Adaptive R-CNN with the state-of-the-art methods. Our model achieved 65.21% in accuracy and outperformed all of the previous state-of-the-art models including the non-scalable or joint localization methods. More- over, it significantly outperformed the scalable methods, which suggests the approach of predict- ing the classifier is better than learning a common subspace for the open-vocabulary detection prob- lem.</p><p>Bounding box regressor. To demonstrate the effectiveness of the bounding box regressor for precise localization, we conducted evaluations with the regressor at different IoU thresholds. As explained in Sec. 3.1, the regressor was generated using G r , which transformed 300-d text embed- dings x into 4096-d regressor weights w r x , w r y , w r w , and w r h . We compared three network archi- tectures for G r : 1) 300-n(-4096) MLP hav- ing a hidden layer with n units that is shared across the four outputs, 2) 300(-n-4096) MLP having a hidden layer that is not shared, and 3) 300(-4096) linear transformation (without a hidden layer). <ref type="table" target="#tab_5">Table 2</ref> shows the results with and without re- gressor. The regressor significantly improved the accuracy with high IoU thresholds, which demon- strates that the regressor improved the localiza- tion accuracy. In addition, the accuracy did not decrease as a result of sharing the hidden layer or reducing the number of units in the hidden layer. This suggests that the regressor lies in a very low-dimensional manifold because the regressor for one concept can be shared by many concepts (e.g., the person regressor can be used for man, woman, girl, boy, etc.). The number of pa- rameters was significantly reduced by these tricks,   to even fewer than in the linear transformation. The accuracy slightly decreased with a threshold of 0.5, because the regressor was not learned prop- erly for the categories that did not frequently ap- pear in the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Open-Vocabulary Object Retrieval</head><p>Main comparison. Open-vocabulary object de- tection and retrieval is a much more difficult task than phrase localization, because we do not know how many objects are present in an image. We used NPA to train our model. As explained in Sec.  region-based CCA ( <ref type="bibr" target="#b26">Plummer et al., 2017b</ref>), which is scalable and shown to be effective for phrase lo- calization; for a fair comparison, the subspace was learned using the same dataset as ours. An approx- imate search was not used to evaluate the actual performance at open-vocabulary object detection. <ref type="table" target="#tab_7">Table 3</ref> compares different training strate- gies. NPA significantly improved the perfor- mance: more than 25% relative improvement for all metrics. Removing mutually non-exclusive words also contributed the performance: WN and VG both improved performance (5.8% and 6.9% relative AP gain, respectively). Performance im- proved even further by combining them (11.8% relative AP gain), which shows they are comple- mentary. AP was much improved by NPA for the PASCAL dataset as well (47% relative gain). However, the performance was still much poorer than those of the state-of-the-art object detection methods <ref type="bibr" target="#b27">(Redmon and Farhadi, 2017;</ref><ref type="bibr" target="#b28">Ren et al., 2015)</ref>, which suggests that there is a large gap be- tween open-vocabulary and closed-vocabulary ob- ject detection.</p><p>Detailed results of NPA. To investigate the ef- fect of NPA, we show the AP with and with- out NPA for individual categories in <ref type="figure">Figure 5</ref>, which are sorted by relative AP improvement. It shows that AP improved especially for ani- mals (elephant, cow, horse, etc.) and per- son (skier, surfer, girl), which are visually similar within the same upper category. <ref type="table" target="#tab_9">Table 4</ref> shows the most confused category and its total count in the top 100 search results for each query, which shows what concept is confusing for each query and how much the confusion is reduced by NPA. <ref type="bibr">9</ref> This shows that visually similar categories resulted in false positive without NPA, while their number was suppressed by training with NPA. The reason is that these confusing categories were added for negative phrases in NPA, and the net- work learned to reject them. <ref type="figure" target="#fig_4">Figure 4</ref> shows the qualitative search results for each query with and without NPA (and CCA as a baseline), which also showed that NPA can discriminate confusing cat- egories (e.g., horse and zebra). These re- sults clearly demonstrate that NPA significantly improves the discriminative ability of classifiers by adding hard negative categories.</p><p>Large-scale experiments. Finally, we evalu- ated the scalability of our method on a large im- age database. We used one million images from the ILSVRC 2012 training set for this evaluation. <ref type="table" target="#tab_11">Table 5</ref> show the speed and memory. The mean <ref type="bibr">9</ref> For each query, we scored all the objects in the Visual Genome testing set and counted the false alarms in the top 100 scored objects.  and standard deviation of speed are computed over 20 queries in PASCAL VOC dataset. Our system could retrieve objects from one million images in around 0.5 seconds. We did not evaluate accuracy because there is no such large dataset with bound- ing box annotations. 10 <ref type="figure" target="#fig_5">Figure 6</ref> shows the retrieval results from one million images, which demon- strates that our system can accurately retrieve and localize objects from a very large-scale database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Query-Adaptive R-CNN is a simple yet strong framework for open-vocabulary object detection and retrieval. It achieves state-of-the-art perfor- mance on the Flickr30k phrase localization bench- mark and it can be used for large-scale object re- trieval by textual query. In addition, its retrieval accuracy can be further increased by using a novel training strategy called negative phrase augmenta- tion (NPA) that appropriately selects hard negative examples by using their linguistic relationship and confusion between categories. This simple and generic approach significantly improves the dis- criminative ability of the generated classifier. Acknowledgements: This work was supported by JST CREST JPMJCR1686 and JSPS KAK- ENHI 17J08378. <ref type="bibr">10</ref> adding distractors would also be difficult, because we cannot guarantee that relevant objects are not in the images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Training examples in open-vocabulary object detection. (a) positive example of skier classifier. (b) examples without positive annotation, which can be positive. (c) examples without positive annotation from an image that contains a positive example. (d) proposed approach to select hard and true negative examples by using linguistics knowledge.</figDesc><graphic url="image-4.png" coords="1,372.28,277.38,72.47,55.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Difference in network architecture between (a) Faster R-CNN and (b) Query-Adaptive R-CNN. While Faster R-CNN learns the classifier of a closed set of categories as learnable parameters of neural networks, Query-Adaptive R-CNN generates a classifier and regressor adaptively from a query text by learning a detector generator that transforms the text into a classifier and regressor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Difference in training between (a) closedvocabulary and (b) open-vocabulary object detection. The approach of NPA is illustrated in (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Qualitative results with and without NPA. Top-k retrieved results for two queries are shown (sorted by rank) and false alarms are depicted with a red border.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Retrievals from one million images. Top-k results for three queries are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>table . ..</head><label>.</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 1 : Phrase localization accuracy on Flickr30k Entities dataset.</head><label>1</label><figDesc></figDesc><table>IoU 
Architecture 
Params 0.5 
0.6 
0.7 
0.8 
0.9 

w/o regression 
-
65.21 53.19 35.70 14.32 1.88 

300-16(-4096) 
0.3M 
64.14 57.66 48.22 33.04 9.29 
300-64(-4096) 
1.1M 
63.87 57.43 49.05 33.84 10.55 
300-256(-4096) 4.3M 
63.84 57.70 48.71 33.87 10.05 
300-1024(-4096) 17M 
64.29 58.05 48.49 33.94 10.09 
300(-256-4096) 4.5M 
62.82 56.28 48.02 32.71 9.89 
300-4096 
1.2M 
63.23 56.92 48.17 32.66 9.20 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison of various bounding box regres-
sors on Flickr30k Entities for different IoU thresholds. 
The number of parameters in G r is also shown. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Open-vocabulary object detection perfor-
mance on Visual Genome and PASCAL VOC 2007 
datasets. WN and VG are the strategies to remove mu-
tually non-exclusive phrases. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Number of false alarms in top 100 results for 
five queries (w/o NPA → w/ NPA). The top 2 confusing 
categories are shown for each query. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 5 : Speed/memory in large-scale experiments.</head><label>5</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Although this assumption is not always true for datasets such as Flickr30k Entities, it nonetheless works well for them because exceptions are rare. 2 Whereas Faster R-CNN uses the softmax cross entropy over the C + 1 (background) classes, where C is the number of closed sets of a category, we use the sigmoid cross entropy because the Ci classes are not always mutually exclusive and a background class cannot be defined in the context of openvocabulary object detection.</note>

			<note place="foot" n="5"> Since the WordNet synset ID is assigned to each object, we add objects with labels of hyponyms as positives (e.g., man is positive for the person category). 6 We exclude the background (e.g., grass, sky, field), multiple objects (e.g., people, leaves), and ambiguous categories (e.g, top, line).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Andrew Zisserman, Relja Arandjelovic, and Andrew Zisserman. 2012. Multiple queries for large scale specific object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Immediate, scalable object category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting deep zero-shot convolutional neural networks using textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On-the-fly learning for visual search of large-scale image and video datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi´carandjelovi´c</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Multimedia Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="75" to="93" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Omkar Parkhi, and Andrew Zisserman</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Query-guided Regression Network with Context Policy for Phrase Grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Kovvuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Pedro F Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1672" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U C</forename><surname>Berkeley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Open-vocabulary object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics: Science and Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Region-based image retrieval revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Hinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin&amp;apos;ichi</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Largescale r-cnn with classifier adaptive quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Hinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shin&amp;apos;ichi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Natural language object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densecap: fully convolutional localization networks for dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Referitgame: referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fisher vectors derived from hybrid gaussianlaplacian mixture models for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalanditis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Visual genome: connecting language and vision using crowdsourced dense image annotations. IJCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">44</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image question answering using convolutional neural network with dynamic parameter prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Hongsuck</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Phrase localization and visual relationship Detection with Comprehensive ImageLanguage Cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C V</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aug</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Flickr30k entities: collecting region-to-phrase correspondences for richer imageto-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="74" to="93" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Grounding of textual phrases in images by reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Object retrieval and localization with spatially-constrained similarity measure and k-nn re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Locality in generic instance search from one example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold W M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Particular object retrieval with integral max-pooling of cnn activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Sicre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning deep structure-preserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Structured matching for phrase localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Azab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriyuki</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Discriminative Bimodal Networks for Visual Localization and Detection with Natural Language Queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
	<note>Zhiyuan He, IAn Huang, and Honglak Lee</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
