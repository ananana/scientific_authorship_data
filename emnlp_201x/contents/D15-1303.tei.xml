<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Convolutional Neural Network Textual Features and Multiple Kernel Learning for Utterance-Level Multimodal Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
							<email>sporia@ntu.edu.sg</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
							<email>cambria@ntu.edu.sg</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering</orgName>
								<orgName type="laboratory">Temasek Laboratory</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Centro de Investigación en Computación</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country>Singapore, Mexico</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Convolutional Neural Network Textual Features and Multiple Kernel Learning for Utterance-Level Multimodal Sentiment Analysis</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a novel way of extracting features from short texts, based on the activation values of an inner layer of a deep convolutional neural network. We use the extracted features in multimodal sentiment analysis of short video clips representing one sentence each. We use the combined feature vectors of textual, visual , and audio modalities to train a classi-fier based on multiple kernel learning, which is known to be good at heterogeneous data. We obtain 14% performance improvement over the state of the art and present a parallelizable decision-level data fusion method, which is much faster, though slightly less accurate.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The advent of the Social Web has enabled any- one with a smartphone or computer to easily cre- ate and share their ideas, opinions and content with millions of other people around the world. Much of the content being posted and consumed online is video. With billions of phones, tablets and PCs shipping today with built-in cameras and a host of new video-equipped wearables like Google Glass on the horizon, the amount of vid- eo on the Internet will only continue to increase.</p><p>It has become increasingly difficult for re- searchers to keep up with this deluge of video content, let alone organize or make sense of it. Mining useful knowledge from video is a critical need that will grow exponentially, in pace with the global growth of content. This is particularly important in sentiment analysis <ref type="bibr" target="#b3">(Cambria et al., 2013a;</ref>, as both service and product reviews are gradually shifting from unimodal to multimodal. We present a method for detecting sentiment polarity in short video clips of a person uttering a sentence.</p><p>We do it using all three modalities: visual, such as facial expression, audio, such as pitch, and textual, the contents of the uttered sentence. While the visual and the audio modalities pro- vide additional evidence that improves classifica- tion accuracy, we found the textual modality to have the greater impact on the result <ref type="bibr" target="#b2">(Cambria and Hussain, 2015;</ref><ref type="bibr" target="#b6">Cambria et al., 2013c;</ref><ref type="bibr" target="#b18">Poria et al., 2015a;</ref><ref type="bibr" target="#b19">2015b)</ref>.</p><p>In this paper, we propose a novel way for fea- ture extraction from text. Given a training corpus with hand-annotated sentiment polarity labels, following <ref type="bibr" target="#b10">Kim (2014)</ref>, we train a deep convolu- tional neural network (CNN) on it. However, instead of using it as a classifier, as Kim did, we use the values from its hidden layer as features for a much more advanced classifier, which gives superior accuracy. Similar ideas have been sug- gested in the context of computer vision for deal- ing with images, but have not been applied in the context of NLP to textual data, and, specifically, for sentiment polarity classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of the Method</head><p>In this paper, we present two different methods for dealing with multimodal data: feature-level fusion and decision-level fusion, each one having its advantages and disadvantages.</p><p>We extracted features from the data for each modality independently. In the case of feature- level fusion, we then concatenated the obtained feature vectors and fed the resulting long vector into a supervised classifier. In the case of deci- sion-level fusion, we fed the features of each modality into separate classifiers, and then com- bined their decisions. Our experimental results show that both of these methods outperform the state of the art by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Textual Features</head><p>We used a CNN as a trainable feature extractor to extract features from the textual data. Utter- ances in the original dataset are in Spanish. While usually it is better to work directly with the source language ( <ref type="bibr" target="#b23">Wang et al., 2013)</ref>, in this work we translated utterances into English using Google translator. Without the translation into English, 68.56% accuracy was obtained.</p><p>The choice of CNN for feature extraction is justified by the following considerations:</p><p>1. The convolution layers of CNN can be seen as a feature extractor, whose output is then fed into a rather simplistic classifier useful for training the network but not the best at actual classification. CNN forms local fea- tures for each word and combine them to produce a global feature vector for the whole text. However, the features that CNN builds internally can be extracted and used as input for another, more advanced classifier. This turns CNN, originally a supervised classifier, into a trainable feature extractor. 2. As a feature extractor, CNN is automatic and does not rely on handcrafted features. In par- ticular, it adapts well to the peculiarities of the specific dataset, in a supervised manner. 3. The features it gives are based on a hierarchy of local features, reflecting well the context.</p><p>A drawback of CNN as a classifier is that it finds only a local optimum, since it uses the same backpropagation technique as MLP. How- ever, inspired by ideas introduced in the context of computer vision <ref type="bibr" target="#b1">(Bluche et al., 2013)</ref>, we, for the first time in the context of NLP, extract the features that CNN builds internally and feed them into a much more advanced classifier. In our experiments, this was SVM, or roughly its multi-kernel version MKL, which is good at finding the global optimum. Thus, the properties of CNN and SVM complement each other in such a way that their advantages are combined.</p><p>To form the input for the CNN feature extrac- tor, for each word in the text we built a 306- dimensional vector by concatenating two parts:</p><p>1. Word embeddings. We used a publicly avail- able word2vec dictionary ( <ref type="bibr" target="#b11">Mikolov et al., 2013a;</ref>, trained on a 100 mil- lion word corpus from Google News using the continuous bag of words architecture. This dictionary provides a 300-dimensional vector for each word. For words not found in this dictionary, we used random vectors.</p><p>2. Part of speech. We used 6 basic parts of speech (noun, verb, adjective, adverb, prepo- sition, conjunction) encoded as a 6- dimensional binary vector. We used Stanford Tagger as a part of speech tagger.</p><p>For each input text, the input vectors for the CNN were a concatenation of three parts:</p><p>1. Left padding. Two dummy "words" with zero vectors were added at the beginning of each text, in order to provide space for con- volution, since at the convolution layers we used the kernel size of at most 3. 2. Text. All 306-dimensional vectors corre- sponding to each word were concatenated, preserving the word order. 3. Right padding. Again, at least 2 dummy "words" with zero vectors were added after each sentence to provide space for convolu- tion. To form vectors for all texts in the cor- pus of the same dimensionality, they were al- so padded at the right with the necessary amount of additional dummy "words."</p><p>In our experiments, all texts were very short, consisting of one sentence, the longest one being of 65 words. Thus all input vectors were of di- mension 306  (2 + 65 + 2) = 21,114.</p><p>The CNN we used consisted of 7 layers:</p><p>1. Input layer, of 21,114 neurons. 2. Convolution layer, with a kernel size of 3 and 50 feature maps. The output of this layer was computed with a non-linear function; we used the hyperbolic tangent. 3. Max-pool layer with max-pool size of 2. 4. Convolution layer: kernel size of 2, 100 fea- ture maps, also using the hyperbolic tangent. 5. Max-pool layer with max-pool size of 2. 6. Fully connected layer of 500 neutrons, whose values were later used as the extracted features. For regularization, we employed dropout on the penultimate layer with a con- straint on L2-norms of the weight vectors. 7. Output softmax layer of 2 neurons, by the number of training labels-the sentiment po- larity values: positive or negative. This layer was used only for training the CNN.</p><p>The CNN was trained using a standard back- propagation procedure. The training data for the output layer were the known sentiment polarity labels present in the training corpus for each text.</p><p>As features of the given text, we used the val- ues of the penultimate, fully connected, layer of the CNN. In this way, we used the last output layer of the CNN only for training, but for actual decision-making, we replaced it with much more sophisticated classifiers, namely, with SVM or MKL. Using only CNN as a classifier, 75.50% was obtained which is in fact lower than the re- sult (79.77%) obtained when CNN was used to extract trainable features for the SVM classifier.</p><p>We also tried other word vectors having dif- ferent dimensions, e.g., Glove word vectors and Collobart's word vectors. However, the best ac- curacy was obtained using Google word2vec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Visual Features</head><p>We split each clip into frames (still images). From each frame, we extracted 68 facial charac- teristic points (FCPs), such as the position of the left corner of the left eye, etc., using the facial recognition library CLM-Z ( <ref type="bibr" target="#b0">Baltrušaitis et al., 2012</ref>). For each pair of FCPs, we calculated the distance. Thus, we characterized each facial ex- pression by 68  67 / 2 = 2,278 distances. In ad- dition, for each frame we extracted 6 face posi- tion coordinates (3D-dimensional displacement and angular displacement of face and head) using the GAVAM software. This gave 2,278 + 6 = 2,284 values per frame.</p><p>For each of these values, we calculated its mean value and standard deviation over all frames of the clip; 4568 features in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Audio Features</head><p>We used the openSMILE software ( <ref type="bibr" target="#b7">Eyben et al., 2010</ref>) to extract audio features related to the pitch and voice intensity. This software extracts the so-called low-level descriptors, such as Mel frequency cepstral coefficients, spectral centroid, spectral flux, beat histogram, beat sum, strongest beat, pause duration, pitch, voice quality, percep- tual linear predictive coefficients, etc., and their statistical functions, such as amplitude mean, arithmetic mean, root quadratic mean, standard deviation, flatness, skewness, kurtosis, quartiles, inter-quartile ranges, linear regression slope, etc. This gave us 6373 audio features in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Feature-Level Fusion</head><p>Feature-level fusion consisted in concatenation of the feature vectors obtained for each of the three modalities. The resulted vectors and along with the sentiment polarity labels from the train- ing set, were used to train a classifier with a mul- tiple kernel learning (MKL) algorithm; we used the SPF-GMKL implementation (Jain et al., 2012) designed to deal with heterogeneous data. Clearly, feature vectors resulted from concatenat- ing so different data sources are heterogeneous.</p><p>The parameters of the classifier were found by cross validation. We chose a configuration with 8 kernels: 5 RBF with gamma from 0.01 to 0.05 and 3 polynomial with powers 2, 3, 4. We also tried Simple-MKL; it gave slightly lower results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Feature Selection</head><p>We significantly reduced the number of features using feature selection. We used two different feature selectors: one based on the cyclic correla- tion-based feature subset selection (CFS) and another based on principal component analysis (PCA) with top K features, where K was experi- mentally selected and varied for different exper- iment. For example, in case of audio, visual and textual fusion, K was set to 300.</p><p>The union of the features selected by the two methods was used. For each unimodal, each bi- modal, and the multimodal experiment, separate feature extraction was performed. selected features for each experiment is given in <ref type="table">Table 1</ref>. In all cases except for the unimodal ex- periment with audio modality, feature selection slightly improved the results, in addition to the improvement in processing time. In the only case where feature selection slightly deteriorated the result, the difference was rather small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Unimodal Classification and Decision-Level Fusion</head><p>For unimodal experiments and for decision-level fusion, we used one classifier per each modality; specifically, we used SVM. For each modality, in this way we obtained the probabilities of the la- bels. In unimodal experiments, we chose the la- bel with the greater probability. For decision-level fusion, we added these probabilities with weights, which were chosen experimentally, and, again, used the most proba- ble label. The weights we used for decision-level fusion were chosen using detailed search with an intuition that best performing unimodal classifier has higher importance in the fusion. We do not claim that these weights are optimal. They are indeed sub-optimal and hence encourage the scope of future research.</p><p>Knowing a specific decision for the text mo- dality allowed us to use evidence from a separate classifier; we used the one based on the Sentic Patterns (SP) ( <ref type="bibr" target="#b17">Poria et al., 2014a)</ref>. It structures natural language clauses into a sentiment hierar- chy used to infer the overall polarity label (posi- tive vs. negative) for the input sentence. E.g., a sentence "The car is very old but it is rather not expensive", is positive, expressing a favorable sentiment of the speaker, who recommends pur- chasing the product. However, "The car is very old though it is rather not expensive" is negative, expressing reluctance of the speaker to purchase the car. Despite the latter contains exactly the same concepts as the former, the polarity is op- posite because of the adversative dependency.</p><p>On benchmark datasets, SP perform better than state of the art sentiment classifiers, which outperforms the textual classifier described in Section 3. Since SP are a superior classifier, we used it as a bias to modify the weight of the tex- tual modality. However, SP do not report a prob- ability, but only a binary decision, so we only used them to tweak the weights in the probability mix: when the text-based unimodal classifier agreed with SP, we increased the weight of the text modality. Another benefit of the decision- level fusion is its speed, since fewer features are used for each classifier and since SVM, used as a unimodal classifier, is faster than MKL. In addi- tion, separate classifiers can be run in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Experimental Results</head><p>We report results for tenfold cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Dataset</head><p>We experimented on the dataset described by <ref type="bibr" target="#b14">Morency et al. (2011)</ref>. The dataset consists of 498 short video fragments where a person utters one sentence. The items are manually tagged for sentiment polarity, which can be positive, nega- tive, or neutral. We discarded the neutral items from the dataset, which gave us a dataset of 447 clips tagged as positive or negative.</p><p>The video in the dataset is present in MP4 format with the resolution of 360  480, to which the developers converted all videos originally collected in different formats with different reso- lution. The duration of the clips is about 5 sec- onds on average. About 80% of the clips present female speakers. The developers provided tran- scription of the text of the sentences, which we used in our textual modality processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Results for Each Modality Separately</head><p>As a baseline, we used classifiers trained on fea- tures extracted from each modality separately. The results are shown in <ref type="table">Table 1</ref>, unimodal sec- tion. The number of features after feature selec- tion is indicate for the modality used.</p><p>The table shows that the best results were ob- tained for textual modality; the visual modality performed worse, and the audio was least useful. However, even the worst of our results is much better than the state-of-the-art ( <ref type="bibr" target="#b15">Pérez-Rosas et al., 2013)</ref>. In each modality separately, our re- sults outperform the state of the art by about 9%, which is about 30% reduction in error rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Results with Feature-level Fusion</head><p>As a yet another baseline, we tried feature-level fusion of only two modalities.</p><p>The results are shown in <ref type="table">Table 1</ref>, bimodal sec- tion. Again, the number of features after feature selection is indicated for the two modalities used. As expected, missing the audio features was the least important, missing the video features was more significant, and missing the text features was most painful for the accuracy.</p><p>Even the worst result obtained with fusion of two modalities outperformed our best unimodal result, as well as the best result of the state of the art. Finally, the best result, shown in the multi- modal section of <ref type="table">Table 1</ref>, was obtained when all three modalities were fused. This result outper- forms the corresponding result of the state of the art by 14%, which gives 56% of error reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4">Results with Decision-level Fusion</head><p>The results for decision-level fusion are shown in <ref type="table">Table 2</ref>, last column. The shaded cells are shared with <ref type="table">Table 1</ref>. In the second section, three classi- fiers were fused at the decision level. In the third section, two modalities indicated with the plus sign were fused at the feature level (giving the accuracy indicated in the penultimate column) and then this classifier was fused at the decision level with the third modality. The weights corre- spond to the share of each modality. In the last section, the weight for the unimodal classifier is shown, and the weight for the bimodal classifier was its complement to 1.</p><p>For the experiments that involved tweaking of the weights with the SP oracle, pairs of weights are shown: the weight used when the text mo- dality results corresponded (left) with the SP prediction and the weight used when they did not (right). The accuracy with at least partial feature- level fusion was better than that for no feature- level fusion at all (3-way). As in the bimodal sec- tion of <ref type="table">Table 1</ref>, excluding audio from feature- level fusion was least problematic and excluding text was most problematic.</p><p>In all cases, decision-level fusion did not sig- nificantly improve the accuracy of the best sum- mand. However, separating text-based classifier permitted us to use the Sentic Patterns tweak, which cannot be used if the text-only results are not known. With this tweak, the best result was obtained. Even with this improvement, the accu- racy of decision-level fusion was slightly lower than that of feature-level fusion; in exchange for much about twice better processing speed.</p><p>A baseline decision level evaluation strategy was taken which allowed us to take majority vot- ing among the predicted class labels by unimodal classifiers. Based on this strategy the final class label was chosen by the maximum of the three unimodal models' votes. For the multimodal fu- sion using this baseline method only 72.83% ac- curacy was obtained. As expected the proposed feature and decision level fusion outperformed this baseline method by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusion</head><p>We have presented a novel method for determin- ing sentiment polarity in video clips of people speaking. We combine evidence from the words they utter, the facial expression, and the speech sound. The main novelty of this paper consists in using deep CNN to extract features from text and in using MKL to classify the multimodal hetero- geneous fused feature vectors.</p><p>We also presented a faster variant of our method, based on decision-level fusion. In case of the decision level fusion experiment, the cou- pling of Sentic Patterns to determine the weight of textual modality has enriched the performance of multimodal sentiment analysis framework considerably. However, the parameter selection for decision level fusion produced suboptimal results. A systematic mathematical approach for decision level fusion is an important future work.</p><p>Our future work will focus on extracting more relevant features from the visual modality. We will employ deep 3D convolutional neural net- works on this modality for feature extraction. We will use a feature selection method to obtain key features; this will ensure the scalability as well as stability of the framework. We will continue our study of reasoning over text ( <ref type="bibr" target="#b9">Jimenez et al., 2015;</ref><ref type="bibr" target="#b16">Pakray et al., 2011;</ref><ref type="bibr" target="#b22">Sidorov, 2014</ref>) and in particular of concept- based sentiment analysis ( <ref type="bibr" target="#b20">Poria et al., 2014b</ref>  <ref type="table">Table 2</ref>. Accuracy of our method with decision-level fusion and feature selection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>The number of</figDesc><table>Text Visual Audio 
Pérez-Rosas 
et al. (2013) 

Our method 
without feature 
selection 

with feature 
selection 
# features, without selection 500 4568 
6373 
 

Unimodal 

# features, 
with selection 

437 
-
-
70.94% 
79.14% 
79.77% 
-
398 
-
67.31% 
75.22% 
76.38% 
-
-
325 
64.85% 
74.49% 
74.22% 

Bimodal 

379 
109 
-
72.39% 
84.97% 
85.46% 
384 
-
81 
72.88% 
83.85% 
84.12% 
-
242 
209 
68.86% 
82.95% 
83.69% 
Multimodal 
305 
74 
58 
74.09% 
87.89% 
88.60% 

Table 1. Accuracy of state-of-the-art method compared with our method with feature-level fusion. 
The number of features is for our experiments, not for [16]. Shaded cells are shared with Table 2. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>).</figDesc><table>Sentic 
Patterns 

Weights 
Fusion Accuracy 
Text 
Visual 
Audio 
Feature Decision 
Unimodal accuracy 81.73% 79.77% 76.38% 74.22% 

Unimodal 
3-way 

3-way majority voting 
72.83% 
no 
0.45 
0.3 
0.25 
81.24% 
yes 
0.5 / 0.25 0.3 / 0.4 0.2 / 0.35 
82.06% 

Bimodal 
with unimodal 

+ 
+ 
0.3 
85.46% 85.53% 
+ 
0.23 
+ 
84.12% 84.86% 
no 
0.4 
+ 
+ 
83.69% 84.48% 
yes 
0.45 / 0.3 
+ 
+ 
same 
86.27% 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3D constrained local model for rigid and non-rigid facial tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louisphilippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2610" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Feature extraction with convolutional neural networks for handwritten word recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théodore</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kermorvant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="285" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Sentic Computing: A Common-Sense-Based Framework for Concept-Level Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
			<pubPlace>Cham, Switzerland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowledge-based approaches to concept-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="12" to="14" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Statistical approaches to concept-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="6" to="9" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Guest editorial: Big social data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bebo</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sentic blending: Scalable multimodal fusion for the continuous interpretation of semantics and sentics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Newton</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE SSCI</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="108" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">OpenSMILE: The Munich versatile and fast open-source audio feature extractor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Multimedia</title>
		<meeting>the International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1459" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SPF-GMKL: Generalized multiple kernel learning with a million kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manik</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="750" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Soft Cardinality in Semantic Text Processing: Experience of the SemEval International Competitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">A</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Polibits</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="63" to="72" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Sentence Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intern. Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards multimodal sentiment analysis: Harvesting opinions from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payal</forename><surname>Doshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th International Conference on Multimodal Interfaces</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Utterance-Level Multimodal Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verónica</forename><surname>Pérez-Rosas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="973" to="982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A textual entailment system using anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Pakray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snehasis</forename><surname>Neogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinaki</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">System Report. Text Analysis Conference, Recognizing Textual Entailment Track. Notebook</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sentic patterns: Dependency-based rules for concept-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregoire</forename><surname>Winterstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="45" to="63" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards an intelligent framework for multimodal affective data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="104" to="116" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fusing audio, visual and textual clues for sentiment analysis from multimodal content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Newton</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
		<idno type="doi">DOI:10.1016/j.neucom.2015.01.095</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">EmoSenticSpace: A novel framework for affective common-sense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KnowledgeBased Systems</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="108" to="123" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Soft Similarity and Soft Cosine Measure: Similarity of Features in Vector Space Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigori</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helena</forename><surname>Gómez-Adorno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pinto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computación y Sistemas</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="491" to="504" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Should Syntactic Ngrams Contain Names of Syntactic Relations? International</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigori</forename><surname>Sidorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Linguistics and Applications</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="23" to="46" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Common sense knowledge for handwritten Chinese recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiu-Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="234" to="242" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
