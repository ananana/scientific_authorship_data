<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Semantic Parsing with Type Constraints for Semi-Structured Tables</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Semantic Parsing with Type Constraints for Semi-Structured Tables</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1516" to="1526"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a new semantic parsing model for answering compositional questions on semi-structured Wikipedia tables. Our parser is an encoder-decoder neural network with two key technical innovations: (1) a grammar for the decoder that only generates well-typed logical forms; and (2) an entity embedding and linking module that identifies entity mentions while generalizing across tables. We also introduce a novel method for training our neural model with question-answer supervision. On the WIKITABLEQUESTIONS data set, our parser achieves a state-of-the-art accuracy of 43.3% for a single model and 45.9% for a 5-model ensemble, improving on the best prior score of 38.7% set by a 15-model ensemble. These results suggest that type constraints and entity linking are valuable components to incorporate in neural semantic parsers.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic parsing is the problem of translating hu- man language into computer language, and there- fore is at the heart of natural language understand- ing. A typical semantic parsing task is question answering against a database, which is accom- plished by translating questions into executable logical forms (i.e., programs) that output their an- swers. Recent work has shown that recurrent neu- ral networks can be used for semantic parsing by encoding the question then predicting each token of the logical form in sequence ( <ref type="bibr" target="#b11">Jia and Liang, 2016;</ref><ref type="bibr" target="#b5">Dong and Lapata, 2016)</ref>. These approaches, while effective, have two major limitations. First, they treat the logical form as an unstructured se- quence, thereby ignoring type constraints on well- formed programs. Second, they do not address en- tity linking, which is a critical subproblem of se- mantic parsing <ref type="bibr" target="#b28">(Yih et al., 2015)</ref>.</p><p>This paper introduces a novel neural semantic parsing model that addresses these limitations of prior work. Our parser uses an encoder-decoder architecture with two key innovations. First, the decoder generates from a grammar that guarantees that generated logical forms are well-typed. This grammar is automatically induced from typed log- ical forms, and does not require any manual engi- neering to produce. Second, the encoder incorpo- rates an entity linking and embedding module that enables it to learn to identify which question spans should be linked to entities. Finally, we also intro- duce a new approach for training neural semantic parsers from question-answer supervision.</p><p>We evaluate our parser on WIKITABLEQUES- TIONS, a challenging data set for question answer- ing against semi-structured Wikipedia tables <ref type="bibr" target="#b21">(Pasupat and Liang, 2015)</ref>. This data set has a broad variety of entities and relations across different ta- bles, along with complex questions that necessi- tate long logical forms. On this data set, our parser achieves a question answering accuracy of 43.3% and an ensemble of 5 parsers achieves 45.9%, both of which outperform the previous state-of-the-art of 38.7% set by an ensemble of 15 models <ref type="bibr" target="#b9">(Haug et al., 2017)</ref>. We further perform several ablation studies that demonstrate the importance of both type constraints and entity linking to achieving high accuracy on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Semantic parsers vary along a few important di- mensions:</p><p>Formalism Early work on semantic parsing used lexicalized grammar formalisms such as Combinatory Categorial <ref type="bibr">Grammar (Zettlemoyer and</ref><ref type="bibr">Collins, 2005, 2007;</ref><ref type="bibr" target="#b14">Kwiatkowski et al., 2011</ref><ref type="bibr" target="#b13">Kwiatkowski et al., , 2013</ref><ref type="bibr" target="#b12">Krishnamurthy and Mitchell, 2012;</ref>) and others <ref type="bibr" target="#b15">(Liang et al., 2011;</ref><ref type="bibr" target="#b2">Berant et al., 2013;</ref><ref type="bibr">Zhao and Huang, 2015;</ref><ref type="bibr">Mooney, 2006, 2007)</ref>. These for- malisms have the advantage of only generating well-typed logical forms, but the disadvantage of introducing latent syntactic variables that make learning difficult. Another approach is to treat se- mantic parsing as a machine translation problem, where the logical form is linearized then predicted as an unstructured sequence of tokens ( <ref type="bibr" target="#b0">Andreas et al., 2013)</ref>. This approach is taken by recent neu- ral semantic parsers <ref type="bibr" target="#b11">(Jia and Liang, 2016;</ref><ref type="bibr" target="#b5">Dong and Lapata, 2016;</ref><ref type="bibr" target="#b17">Locascio et al., 2016;</ref><ref type="bibr" target="#b16">Ling et al., 2016)</ref>. This approach has the advantage of predicting the logical form directly from the question without latent variables, which simpli- fies learning, but the disadvantage of ignoring type constraints on logical forms. Our type-constrained neural semantic parser inherits the advantages of both approaches: it only generates well-typed log- ical forms and has no syntactic latent variables as every logical form has a unique derivation. Recent work has explored similar ideas to ours in the con- text of Python code generation <ref type="bibr" target="#b30">(Yin and Neubig, 2017;</ref><ref type="bibr" target="#b24">Rabinovich et al., 2017)</ref>.</p><p>Entity Linking Identifying the entities men- tioned in a question is a critical subproblem of se- mantic parsing in broad domains and proper entity linking can lead to large accuracy improvements ( <ref type="bibr" target="#b28">Yih et al., 2015)</ref>. However, semantic parsers have typically ignored this problem by assuming that entity linking is done beforehand (as the neural parsers above do) or using a simple parameteri- zation for the entity linking portion (as the lexical- ized parsers do). Our parser explicitly includes an entity linking module that enables it to model the highly ambiguous and implicit entity mentions in WIKITABLEQUESTIONS.</p><p>Supervision Semantic parsers can be trained from labeled logical forms ( <ref type="bibr" target="#b31">Zelle and Mooney, 1996;</ref><ref type="bibr">Zettlemoyer and Collins, 2005)</ref> or question- answer pairs <ref type="bibr" target="#b15">(Liang et al., 2011;</ref><ref type="bibr" target="#b2">Berant et al., 2013)</ref>. Question-answer pairs were considered easier to obtain than labeled logical forms, though recent work has demonstrated that logical forms can be collected efficiently and are more effec- tive ( <ref type="bibr" target="#b29">Yih et al., 2016)</ref>. However, a key advantage of question-answer pairs is that they are agnos- tic to the domain representation and logical form language (e.g., lambda calculus or λ-DCS). This property is important for problems such as semi- structured tables where the proper domain repre- sentation is unclear.</p><p>Data Sets We use WIKITABLEQUESTIONS to evaluate our parser as this data set exhibits both a broad domain and complex questions. Early data sets, such as GEOQUERY ( <ref type="bibr" target="#b31">Zelle and Mooney, 1996)</ref> and ATIS ( <ref type="bibr" target="#b4">Dahl et al., 1994)</ref>, have small domains with only a handful of different predi- cates. More recent data sets for question answer- ing against Freebase have a much broader domain, but simple questions ( <ref type="bibr" target="#b2">Berant et al., 2013;</ref><ref type="bibr" target="#b3">Cai and Yates, 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>This section describes our semantic parsing model and training procedure. For clarity, we describe the model on WIKITABLEQUESTIONS, though it is also applicable to other problems. The input to our model is a natural language question and a context in which it is to be answered, which in our task is a table. The model predicts the answer to the question by semantically parsing it to a logical form then executing it against the table.</p><p>Our model follows an encoder-decoder archi- tecture using recurrent neural networks with Long Short Term Memory (LSTM) cells <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997</ref>). The input question and table entities are first encoded as vectors that are then decoded into a logical form <ref type="figure">(Figure 1</ref>). We make two significant additions to the encoder- decoder architecture. First, the encoder includes a special entity embedding and linking module that produces a link embedding for each question token that represents the table entities it links to (Section 3.2). Second, the action space of the decoder is de- fined by a type-constrained grammar which guar- antees that generated logical forms satisfy type constraints (Section 3.3).</p><p>We train the parser using question-answer pairs as supervision using an approximation of marginal loglikelihood based on enumerating logical forms via dynamic programming on denotations (Pasu- pat and Liang, 2016) (Section 3.4). This approx- imation makes it possible to train neural models with question-answer supervision, which is other- wise difficult for efficiency and gradient variance reasons.   <ref type="figure">Figure 1</ref>: Overview of our semantic parsing model. The encoder performs entity embedding and linking before encoding the question with a bidirectional LSTM. The decoder predicts a sequence of grammar rules that generate a well-typed logical form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>We follow <ref type="bibr" target="#b21">(Pasupat and Liang, 2015</ref>) in using the same table structure representation and λ-DCS language for expressing logical forms. In this representation, tables are expressed as knowledge graphs over 6 types of entities: cells, cell parts, rows, columns, numbers and dates. Each entity also has a name, which is typically a string value in the table. Our parser uses both the entity names and the knowledge graph structure to construct embeddings for each entity.</p><p>The logical form language consists of a collec- tion of named sets and entities, along with oper- ators on them. The named sets are used to se- lect table cells, e.g., united states is the set of cells that contain the text "united states". The operators include functions from sets to sets, e.g., the next operator maps a row to the next row. Columns are treated as functions from cells to their rows, e.g., (country united states) generates the rows whose country column con- tains "united states". Other operators include re- versing relations (e.g., in order to map rows to cells in a certain column), relations that interpret cells as numbers and dates, and set and arithmetic operations. The language also includes aggrega- tion and quantification operations such as count and argmax, along with λ abstractions that can be used to join binary relations.</p><p>Our parser also assigns a type to every λ-DCS expression, which is used to enforce type con- straints on generated logical forms. The base types are cells c, parts p, rows r, numbers i, and dates d. Columns such as country have the functional type c, r, representing functions from cells c to rows r. Other operations have more complex functional types, e.g., reverse has type c, r, r, c, which enables us to write (reverse country). <ref type="bibr">1</ref> The parser assigns ev- ery λ-DCS constant a type, then applies standard programming language type inference algorithms <ref type="bibr" target="#b23">(Pierce, 2002</ref>) to automatically assign types to larger expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoder</head><p>The encoder is a bidirectional LSTM augmented with an entity embedding and linking module.</p><p>Notation. Throughout this section, we denote entities as e, and their corresponding types as τ (e). The set of all entities is denoted as E, and the en- tities with type τ as E τ . E includes the cells, cell parts, and columns from the table in addition to numeric entities detected in the question by NER. The question is denoted as a sequence of tokens [q 1 , ..., q n ]. We use v w to denote a learned vec- tor representation (embedding) of word w, e.g., v q i denotes the vector representation of the ith ques- tion token.</p><p>Entity Embedding. The encoder first constructs an embedding for each entity in the knowledge graph given its type and position in the graph. Let W (e) denote the set of words in the name of en-tity e and N (e) the neighbors of entity e in the knowledge graph. Specifically, the neighbors of a column are the cells it contains, and the neighbors of a cell are the columns it belongs to. Each en- tity's embedding r e is a nonlinear projection of a type vector v τ (e) and a neighbor vector v N (e) :</p><formula xml:id="formula_0">v N (e) = 1 |N (e)| e ∈N (e) 1 |W (e )| w∈W (e ) v w r e = tanh P τ v τ (e) + P N v N (e)</formula><p>The type vector v τ (e) is a one-hot vector for τ (e), with dimension equal to the number of entity types in the grammar. The neighbor vector v N (e) is sim- ply an average of the word vectors in the names of e's neighbors. P τ and P N are learned parameter matrices for combining these two vectors.</p><p>Entity Linking. This module generates a link embedding l i for each question token representing the entities it links to. The first part of this module generates an entity linking score s(e, i) for each entity e and token index i:</p><formula xml:id="formula_1">s(e, i) = max w∈W (e) v w v q i + ψ φ(e, i)</formula><p>This score has two terms. The first represents similarity in word embedding space between the token and entity name, computed as function of the embeddings of words in e's name, W (e), and the word embedding of the ith token, v q i . The max-pooling architecture allows each question to- ken to pick its best match in the entity's name. The second term represents a linear classifier with pa- rameters ψ on features φ(e, i). The feature func- tion φ includes only a few features: indicators for exact token and lemma match, edit distance, an NER tag indicator, and a bias feature. It also in- cludes "related column" versions of the token and lemma features that are active when e is a col- umn and the original feature matches a cell entity in column e. We found that features were an ef- fective way to address sparsity in the entity name tokens, many of which appear too infrequently to learn embeddings for. We produce an independent score for each entity and token index even though we expect entities to link to multi-token spans in order to avoid the quadratic computational com- plexity of scoring each span.</p><p>Finally, the entity embeddings and linking scores are combined to produce a link embedding for each token. The scores s(e, i) are then fed into a softmax layer over all entities e of the same type, and the link embedding l i is an average of entity vectors r e weighted by the resulting distribution, then summed over all types. We include a null en- tity, ∅, in each softmax layer to permit the model to identify tokens that do not refer to an entity. The null entity's embedding is the all-zero vector and its score s(∅, ·) = 0. Note that the null entity may still be assigned high probability as the other entity scores of may be negative. The link embedding i for the ith question token is computed as:</p><formula xml:id="formula_2">p(e|i, τ ) = exp s(e, i) e ∈Eτ ∪{∅} exp s(e , i) l i = τ e∈Eτ r e p(e|i, τ )</formula><p>For WIKITABLEQUESTIONS, we ran the entity embedding and linking module over every entity. However, this approach may be prohibitively ex- pensive in applications with a very large number of entities. In these cases, our method can be applied by adding a preliminary filtering step to identify a subset of entities that may be mentioned in the question. This filter need not have high precision, and therefore could rely on simple text overlap or similarity heuristics. This reduced set of entities can then be fed into the entity embedding and link- ing module, which will learn to further prune this set of candidates.</p><p>Bidirectional LSTM. We concatenate the link embedding l i and the word embedding v q i of each token in the question, and feed them into a bidi- rectional LSTM:</p><formula xml:id="formula_3">x i = l i v q i (o f i , f i ) = LSTM(f i−1 , x i ) (o b i , b i ) = LSTM(b i+1 , x i ) o i = o f i o b i</formula><p>This process produces an encoded vector repre- sentation of each token o i . The final LSTM hid- den states f n+1 and b 0 are concatenated and used to initialize the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoder</head><p>The decoder is an LSTM with attention that selects parsing actions from a grammar over well-typed logical forms.</p><p>Type-Constrained Grammar. The parser maintains a state at each step of decoding that consists of a logical form with nonterminals standing for portions that are yet to be generated. Each nonterminal is a tuple [τ, Γ] of a type τ and a scope Γ that contains typed variable bindings, (x : α) ∈ Γ, where x is a variable name and α is a type. The scope is used to store and generate the arguments of lambda expressions. The grammar consists of a collection of four kinds of production rules on nonterminals:</p><formula xml:id="formula_4">1. Application [τ, Γ]→([β, τ , Γ] [β, Γ])</formula><p>rewrites a nonterminal of type τ by applying a function from β to τ to an argument of type β. We also permit applications with more than one argument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Constant [τ, Γ]→const where constant</head><p>const has type τ . This rule generates both table-independent operations such as argmax and table-specific entities such as united states.</p><formula xml:id="formula_5">3. Lambda [α, τ , Γ]→ λx. [τ, Γ ∪ {(x : α)}]</formula><p>generates a lambda expression where the ar- gument has type α. x represents a fresh vari- able name. The right hand side of this rule extends the scope Γ with a binding for x then generates an expression of type τ .</p><formula xml:id="formula_6">4. Variable [τ, Γ]→ x where (x : τ ) ∈ Γ.</formula><p>This rule generates a variable bound in a previously-generated lambda expression that is currently in scope.</p><p>We instantiate each of the four rules above by replacing the type variables τ, α, β with concrete types, producing, e.g.,</p><formula xml:id="formula_7">[c, Γ] → ([r, c, Γ] [r, Γ])</formula><p>from the application rule. The set of instanti- ated rules is automatically derived from a cor- pus of logical forms, which we in turn produce by running dynamic programming on denotations (see Section 3.4). Every logical form can be de- rived in exactly one way using the four kinds of rules above; this derivation is combined with the (automatically-assigned) type of each of the log- ical form's subexpressions to instantiate the type variables in each rule. Finally, as a postprocessing step, we filter out table-dependent rules, such as those that generate table cells, to produce a table- independent grammar. The table-dependent rules are handled specially by the decoder in order to guarantee that they are only generated when an- swering questions against the appropriate table. The table-independent grammar generates well- typed expressions that include functions such as next and quantifiers such as argmax; however, it cannot generate cells, columns, or other table entities.</p><p>The first action of the parser is to predict a root type for the logical form, and then decoding proceeds according to the production rules above. Each time step of decoding fills the leftmost non- terminal in the logical form, and decoding ter- minates when no nonterminals remain. <ref type="figure" target="#fig_0">Figure 2</ref> shows the sequence of decoder actions used to generate an example logical form.</p><p>Network Architecture. The decoder is an LSTM that outputs a distribution over grammar actions using an attention mechanism over the en- coded question tokens. The decoder also uses a copy-like mechanism on the entity linking scores to generate entities. Say that, during the jth time step, the current nonterminal has type τ . The de- coder generates a score for each grammar action whose left-hand side is τ using the following equa- tions:</p><formula xml:id="formula_8">(y j , h j ) = LSTM(h j−1 , g j−1 o j−1 )<label>(1)</label></formula><formula xml:id="formula_9">a j = softmax(OW a y j )<label>(2)</label></formula><formula xml:id="formula_10">o j = (a j ) T O<label>(3)</label></formula><formula xml:id="formula_11">s j = W 2 τ relu(W 1 y j o j + b 1 ) + b 2 τ (4) s j (e k ) = i s(e k , i)a ji (5) p j = softmax(     s j s j (e 1 ) s j (e 2 )</formula><p>...</p><formula xml:id="formula_12">    )<label>(6)</label></formula><p>The input to the LSTM g j−1 is a grammar ac- tion embedding for the action chosen in previ- ous time step. g 0 is a learned parameter vec- tor, and h 0 is the concatenated final hidden states of the encoder LSTMs. The matrix O contains the encoded token vectors o 1 , ..., , o n from the en- coder. The first three lines above perform a soft- max attention over O using a learned parameter matrix W a . The fourth line generates scores s j for the table-independent grammar rules applica- ble to type τ using a multilayer perceptron with </p><formula xml:id="formula_13">weights W 1 ,b 1 ,W 2 τ ,b 2 τ .</formula><p>The fifth line generates a score for each entity e with type τ by averaging the entity linking scores with the current attention a j . Finally, the table-independent and -dependent scores are concatenated and softmaxed to produce a probability distribution p j over grammar actions. If a table-independent action is chosen, g j is a learned parameter vector for that action. Other- wise g j = g τ , which is a learned parameter repre- senting the selection of an entity with type τ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">DPD Training</head><p>Our parser is trained from question-answer pairs, treating logical forms as a latent variable. We use an approximate marginal loglikelihood objective function that first automatically enumerates a set of correct logical forms for each example, then trains on these logical forms. This objective sim- plifies the search problem during training and is well-suited to training our neural model. The training data consists of a collection of n question-answer-table triples, {(q i , a i , T i )} n i=1 . We first run dynamic programming on denotations (Pasupat and Liang, 2016) on each table T i and answer a i to generate a set of logical forms ∈ L i that execute to the correct answer. Dynamic pro- gramming on denotations (DPD) is an automatic procedure for enumerating logical forms that exe- cute to produce a particular value; it leverages the observation that there are fewer denotations than logical forms to enumerate this set relatively effi- ciently. However, many of these logical forms are spurious, in the sense that they do not represent the question's meaning. Therefore, the objective must marginalize over the many logical forms generated in this fashion:</p><formula xml:id="formula_14">O(θ) = n i=1 log ∈L i P (|q i , T i ; θ)</formula><p>We optimize this objective function using stochastic gradient descent. If |L i | is small, e.g., 5-10, the gradient of the ith example can be com- puted exactly by simply replicating the parser's network architecture |L i | times, once per logical form. However, |L i | often contains many thou- sands of logical forms, which makes the above computation infeasible. We address this problem by truncating L i to the m = 100 shortest logical forms, then using a beam search with a beam of k = 5 to approximate the sum. Section 4.5 con- siders the effect of varying the number of logical forms m in this objective function.</p><p>We briefly contrast this approach with two other commonly-used approaches. The first is a sim- ilar marginal loglikelihood objective commonly used in prior semantic parsing work with loglin- ear models <ref type="bibr" target="#b15">(Liang et al., 2011;</ref><ref type="bibr" target="#b21">Pasupat and Liang, 2015)</ref>. However, this approach does not precom- pute correct logical forms. Therefore, computing its gradient requires running a wide beam search, generating, e.g., 300 logical forms, executing each one to identify which are correct, then backprop- agating through a term for each. The wide beam is required to find correct logical forms; however, such a wide beam is prohibitively expensive with a neural model due to the cost of each backpropa- gation pass. Another approach is to train the net- work with REINFORCE <ref type="bibr" target="#b25">(Williams, 1992)</ref>, which essentially samples a logical form instead of using beam search. This approach is known to be diffi- cult to apply when the space of outputs is large and the reward signal is sparse, and recent work has found that maximizing marginal loglikelihood is more effective in these circumstances ( <ref type="bibr" target="#b8">Guu et al., 2017)</ref>. Our approach makes it tractable to maxi- mize marginal loglikelihood with a neural model by using DPD to enumerate correct logical forms beforehand. This up-front enumeration, combined with the local normalization of the neural model, makes it possible to restrict the beam search to correct logical forms in the gradient computation, which enables training with a small beam size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We evaluate our parser on the WIKITABLEQUES- TIONS data set by comparing it to prior work and ablating several components to understand their contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>We used the standard train/test splits of WIK- ITABLEQUESTIONS. The training set consists of 14,152 examples and the test set consists of 4,344 examples. The training set comes divided into 5 cross-validation folds for development using an 80/20 split. All data sets are constructed so that the development and test tables are not present in the training set. We report question answer- ing accuracy measured using the official evalua- tion script, which performs some simple normal- ization of numbers, dates, and strings before com- paring predictions and answers. When generating answers from a model's predictions, we skip logi- cal forms that do not execute (which may occur for some baseline models) or answer with the empty string (which is never correct). All reported ac- curacy numbers are an average of 5 parsers, each trained on one training fold, using the respective development set to perform early stopping.</p><p>We trained our parser with 20 epochs of stochastic gradient descent.</p><p>We used 200- dimensional word embeddings for the question and entity tokens, mapping all tokens that oc- curred &lt; 3 times in the training questions to UNK. (We tried using a larger vocabulary that included frequent tokens in tables, but this caused the parser to seriously overfit.) The hidden and out- put dimensions of the forward/backward encoder LSTMs were set to 100, such that the concatenated representations were also 200-dimensional. The decoder LSTM uses 100-dimensional action em- beddings and has a 200-dimensional hidden state and output. The action selection MLP has a hidden layer dimension of 100. We used a dropout proba- bility of 0.5 on the output of both the encoder and decoder LSTMs, as well as on the hidden layer of the action selection MLP. All parameters are initialized using Glorot initialization <ref type="bibr" target="#b6">(Glorot and Bengio, 2010</ref>). The learning rate for SGD is ini- tialized to 0.1 with a decay of 0.01. At test time, we decode with a beam size of 10.</p><p>Our model is implemented as a probabilis- tic neural program <ref type="bibr" target="#b18">(Murray and Krishnamurthy, 2016</ref>). This Scala library combines ideas from dy- namic neural network frameworks (  and probabilistic programming <ref type="bibr" target="#b7">(Goodman and Stuhlmüller, 2014</ref>) to simplify the imple- mentation of complex neural structured prediction models. This library enables a user to specify the structure of the model in terms of discrete nonde- terministic choices -as in probabilistic program- ming -where a neural network is used to score each choice. We implement our parser by defining P (|q, T ; θ), from which the library automatically implements both inference and training. In partic- ular, the beam search and the corresponding back- propagation bookkeeping to implement the objec- tive in Section 3.4 are both automatically handled by the library. Code and supplementary material for this paper are available at:</p><p>http://allenai.org/paper-appendix/emnlp2017-wt/ <ref type="table">Table 1</ref> compares the accuracy of our semantic parser to prior work on WIKITABLEQUESTIONS. We distinguish between single models and ensem- bles, as we expect ensembling to improve accu- racy, but not all prior work has used it. Prior work on this data set includes a loglinear semantic parser <ref type="bibr" target="#b21">(Pasupat and Liang, 2015)</ref>, that same parser with a neural, paraphrase-based reranker <ref type="bibr" target="#b9">(Haug et al., 2017)</ref>, and a neural programmer that an- swers questions by predicting a sequence of table operations <ref type="bibr" target="#b19">(Neelakantan et al., 2017)</ref>. We find that our parser outperforms the best prior result on this data set by 4.6%, despite that prior result using a 15-model ensemble. An ensemble of 5 parsers improves accuracy by an additional 2.6% for a total improvement of 7.2%. This ensemble was constructed by averaging the logical form proba- bilities of parsers trained on each of the 5 cross- validation folds. Note that this ensemble is trained on the entire training set -the development data from one fold is training data for the others -so we therefore cannot report its development accu- racy. We investigate the sources of this accuracy improvement in the remainder of this section via ablation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Type Constraints</head><p>Our second experiment measures the importance of type constraints on the decoder by comparing it to sequence-to-sequence (seq2seq) and sequence- to-tree (seq2tree) models. The seq2seq model gen- erates the logical form a token at a time, e.g., <ref type="bibr">[(, (,reverse, ...]</ref>, and has been used in several recent neural semantic parsers ( <ref type="bibr" target="#b11">Jia and Liang, 2016;</ref><ref type="bibr" target="#b5">Dong and Lapata, 2016</ref>). The seq2tree model improves on the seq2seq model by includ- ing an action for generating matched parenthe- ses, then recursively generating the subtree within <ref type="bibr" target="#b5">(Dong and Lapata, 2016)</ref>. These baseline models use the same network architecture (including en- tity embedding and linking) and training regime as our parser, but assign every constant the same type and have a different grammar in the decoder. These models were implemented by preprocessing logical forms and applying a different type system. <ref type="table">Table 2</ref> compares the accuracy of our parser to both the seq2seq and seq2tree baselines. Both of these models perform considerably worse than our parser, demonstrating the importance of type con- straints during decoding. Interestingly, we found that both baselines typically generate well-formed logical forms: only 7.4% of seq2seq and 6.6% of seq2tree's predicted logical forms failed to exe- cute. Type constraints prevent these errors from occurring in our parser, though the relatively small number of such errors does not does not seem to fully explain the 9% accuracy improvement. We hypothesize that the additional improvement oc- curs because type constraints also increase the ef- fective capacity of the model, as both the seq2seq and seq2tree models must use some of their capac- ity to learn the type constraints on logical forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Entity Embedding and Linking</head><p>Our next experiment measures the contribution of the entity embedding and linking module. We trained several ablated versions of our parser, re- moving both the embedding similarity and featur- ized classifier from the entity linking module. Ta- ble 3 shows the accuracy of the resulting models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Dev. Accuracy </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="31.3">seq2tree</head><p>31.6 Our Parser 42.7 <ref type="table">Table 2</ref>: Development accuracy of our seman- tic parser compared to sequence-to-sequence and sequence-to-tree models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dev. Accuracy</head><p>Full model 42.7 token features, no similarity 28.1 all features, no similarity 37.8 similarity only, no features 27.5 <ref type="table">Table 3</ref>: Development accuracy of ablated parser variants trained without parts of the entity linking module.</p><p>The results demonstrate that the entity linking fea- tures are important, particularly the more complex features beyond simple token matching. In our ex- perience, the "related column" features are espe- cially important for this data set, as columns that appear in the logical form are often not mentioned in the text, but rather implied by a mention of a cell from the column. Embedding similarity alone is not very effective, but it does improve accuracy when combined with the featurized classifier. We found that word embeddings enabled the parser to overfit, which may be due to the relatively small size of the training set, or because we did not use pretrained embeddings. Incorporating pretrained embeddings is an area for future work.</p><p>We also examined the effect of the entity em- beddings computed using each entity's knowledge graph context by replacing them with one-hot vec- tors for the entity's type. The accuracy of this parser dropped from 42.7% to 41.8%, demonstrat- ing that the knowledge graph embeddings help.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">DPD Training</head><p>Our final experiment examines the impact on ac- curacy of varying the number of logical forms m used when training with dynamic programming on denotations. <ref type="table">Table 4</ref> shows the development ac- curacy of several parsers trained with varying m. These results demonstrate that using more logical forms generally leads to higher accuracy. # of logical forms 1 5 10 50 100 Dev. Accuracy 39.7 41.9 41.6 43.1 42.7 <ref type="table">Table 4</ref>: Development accuracy of our semantic parser when trained with varying numbers of log- ical forms produced by dynamic programming on denotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Error Analysis</head><p>To better understand the mistakes made by our system, we analyzed a randomly selected set of 100 questions that were answered incorrectly. We identified three major classes of error:</p><p>Parser errors (41%): These are examples where a correct logical form is available, but the parser does not select it. A large number of these errors (15%) occur on questions that require se- lecting an answer from a given list of options, as in Who had more silvers, Colombia or The Ba- hamas? In such cases, the type of the predicted answer is often wrong. Another common subclass is entity linking errors due to missing background knowledge (13%), e.g., understanding that largest implicitly refers to the Area column.</p><p>Representation failures (25%): The knowl- edge graph representation makes certain assump- tions about the table structure and cell values which are sometimes wrong. One common prob- lem is that the graph lacks some cell parts neces- sary to answer the question (15%). For example, answering a question asking for a state may re- quire splitting cell values in the Location column into city and state names. Another common prob- lem is unusual table structures (10%), such as a table listing the number of Olympic medals won by each country that has a final row for the to- tals. These structures often cause quantifiers such as argmax to select the wrong row. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present a new semantic parsing model for answering compositional questions against semi- structured Wikipedia tables. Our semantic parser extends recent neural semantic parsers by enforc- ing type constraints during logical form genera- tion, and by including an explicit entity embed- ding and linking module that enables it to iden- tify entity mentions while generalizing across ta- bles. An evaluation on WIKITABLEQUESTIONS demonstrates that our parser achieves state-of-the- art results, and furthermore that both type con- straints and entity linking make significant contri- butions to accuracy. Analyzing the errors made by our parser suggests that improving entity linking and using the table structure are two directions for future work.</p><p>Luke S. Zettlemoyer and Michael <ref type="bibr">Collins. 2005</ref>. Learning to map sentences to logical form: struc- tured classification with probabilistic categorial grammars. In UAI '05, Proceedings of the 21st Con- ference in Uncertainty in Artificial Intelligence.</p><p>Luke S Zettlemoyer and Michael Collins. 2007. On- line learning of relaxed CCG grammars for parsing to logical form. In EMNLP-CoNLL.</p><p>Kai Zhao and Liang Huang. 2015. Type-driven in- cremental semantic parsing with polymorphism. In HLT-NAACL.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The derivation of a logical form using the type-constrained grammar. The nonterminals in the left column have empty scope, and those in the right column have scope Γ = {(x : r)}</figDesc><graphic url="image-1.png" coords="6,73.14,62.81,216.00,197.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Unsupported operations (11%): These are ex- amples where the logical form language lacks a necessary function. Examples of missing func- tions are finding consecutive sets of values, com- puting percentages and performing string opera- tions on cell values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table Knowledge Graph</head><label>Knowledge</label><figDesc></figDesc><table>nation 

from encoder 

great 
britain 

south 
korea 

LSTM 

Entity 
Linking 

... 

Entity 
Linking 
LSTM 

Attention 

c 

g 1 

LSTM 

Attention 

c → &lt;r,c&gt; 

... 

Question 
Logical 
Form 

Decoder 

Knowledge graph 
Embedding 

LSTM 

athlete 

karl 
schafer 

kim 
yu-na 

year 

2002 

2014 

</table></figure>

			<note place="foot" n="1"> Technically, reverse has the parametric polymorphic type α, β, β, α, where α and β are type variables that can be any type. This type allows reverse to reverse any function. However, this is a detail that can largely be ignored. We only use parametric polymorphism when typing logical forms to generate the type-constrained grammar; the grammar itself does not have type variables, but rather a fixed number of concrete instances-such as c, r, r, c-of the above polymorphic type.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic parsing as machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of semantic parsers for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="62" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-scale semantic parsing via schema matching and lexicon extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqing</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Expanding the scope of the ATIS task: The ATIS-3 corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deborah</forename><forename type="middle">A</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madeleine</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Hunicke-Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pallett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Pao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rudnicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Human Language Technology</title>
		<meeting>the Workshop on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language to logical form with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The Design and Implementation of Probabilistic Programming Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stuhlmüller</surname></persName>
		</author>
		<ptr target="http://dippl.org.Ac-cessed" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2017" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">From language to programs: Bridging reinforcement learning and maximum marginal likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">Zheran</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Neural multi-step reasoning for question answering on semi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Till</forename><surname>Haug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Octavian-Eugen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulina</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grnarova</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1702.06589" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Data recombination for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weakly supervised training of semantic parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scaling semantic parsers with on-the-fly ontology matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lexical generalization in CCG grammar induction for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Latent predictor networks for code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk´kočisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural generation of regular expressions from natural language with minimal domain knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Locascio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">De</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><forename type="middle">W</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1612.00712" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning a natural language interface with neural programmer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1701.03980" />
	</analytic>
	<monogr>
		<title level="m">The dynamic neural network toolkit</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Inferring logical forms from denotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Types and Programming Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pierce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Abstract syntax networks for code generation and semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 55th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning for semantic parsing with statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL</title>
		<meeting>the Human Language Technology Conference of the NAACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning synchronous grammars for semantic parsing with lambda calculus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantic parsing via staged query graph generation: Question answering with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The value of semantic parse labeling for knowledge base question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jina</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A syntactic neural model for general-purpose code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 55th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to parse database queries using inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth National Conference on Artificial Intelligence</title>
		<meeting>the Thirteenth National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
