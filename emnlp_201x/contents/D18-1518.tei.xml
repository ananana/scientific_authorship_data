<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Word Representations with Cross-Sentence Dependency for End-to-End Co-reference Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyin</forename><surname>Luo</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>MIT</roleName><forename type="first">James</forename><surname>Glass</surname></persName>
							<email>glass@mit.edu</email>
						</author>
						<title level="a" type="main">Learning Word Representations with Cross-Sentence Dependency for End-to-End Co-reference Resolution</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4829" to="4833"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4829</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this work, we present a word embedding model that learns cross-sentence dependency for improving end-to-end co-reference resolution (E2E-CR). While the traditional E2E-CR model generates word representations by running long short-term memory (LSTM) recurrent neural networks on each sentence of an input article or conversation separately, we propose linear sentence linking and atten-tional sentence linking models to learn cross-sentence dependency. Both sentence linking strategies enable the LSTMs to make use of valuable information from context sentences while calculating the representation of the current input word. With this approach, the LSTMs learn word embeddings considering knowledge not only from the current sentence but also from the entire input document. Experiments show that learning cross-sentence dependency enriches information contained by the word representations, and improves the performance of the co-reference resolution model compared with our baseline.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Co-reference resolution requires models to cluster mentions that refer to the same physical entities. The models based on neural networks typically re- quire different levels of semantic representations of input sentences. The models usually need to calculate the representations of word spans, or mentions, given pre-trained character and word- level embeddings <ref type="bibr" target="#b18">(Turian et al., 2010;</ref><ref type="bibr" target="#b13">Pennington et al., 2014</ref>) before predicting antecedents. The mention-level embeddings are used to make co- reference decisions, typically by scoring mention pairs and making links ( <ref type="bibr" target="#b8">Lee et al., 2017;</ref><ref type="bibr" target="#b3">Clark and Manning, 2016a;</ref><ref type="bibr">Wiseman et al., 2016)</ref>. Long short-term memories (LSTMs) are often used to encode the syntactic and semantic information of input sentences.</p><p>Articles and conversations include more than one sentences. Considering the accuracy and ef- ficiency of co-reference resolution models, the en- coder LSTM usually processes input sentences separately as a batch ( <ref type="bibr" target="#b8">Lee et al., 2017)</ref>. The dis- advantage of this method is that the models do not consider the dependency among words from dif- ferent sentences, which plays a significant role in word representation learning and co-reference pre- dicting. For example, pronouns are often linked to entities mentioned in other sentences, while their initial word vectors lack dependency infor- mation. As a result, a word representation model cannot learn an informative embedding of a pro- noun without considering cross-sentence depen- dency in this case.</p><p>It is also problematic if we encode the input document considering cross-sentence dependency and treat the entire document as one sentence. An input article or conversation can be too long for a single LSTM cell to memorize. If the LSTM up- dates itself for too many steps, gradients will van- ish or explode ( <ref type="bibr" target="#b11">Pascanu et al., 2013)</ref>, and the co- reference resolution model will be very difficult to optimize. Regarding the entire input corpus as one sequence instead of a batch also significantly increases the time complexity of the model.</p><p>To solve the problem that traditional LSTM en- coders, which treat the input sentences as a batch, lack an ability to capture cross-sentence depen- dency, and to avoid the time complexity and dif- ficulties of training the model concatenating all input sentences, we propose a cross-sentence en- coder for end-to-end co-reference (E2E-CR). Bor- rowing the idea of an external memory module from <ref type="bibr" target="#b16">Sukhbaatar et al. (2015)</ref>, an external mem- ory block containing syntactic and semantic in- formation from context sentences is added to the standard LSTM model. With this context mem- ory block, the proposed model is able to encode input sentences as a batch, and also calculate the representations of input words by taking both tar- get sentences and context sentences into consider- ation. Experiments showed that this approach im- proved the performance of co-reference resolution models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Co-reference Resolution</head><p>A popular method of co-reference resolution is mention ranking <ref type="bibr" target="#b5">(Durrett and Klein, 2013)</ref>. Reading each mention, the model calculates co- reference scores for all antecedent mentions, and picks the mention with the highest positive score to be its co-reference. Many recent works are based on this approach. <ref type="bibr" target="#b5">Durrett and Klein (2013)</ref> designed a set of feature templates to improve the mention-ranking model. <ref type="bibr" target="#b12">Peng et al. (2015)</ref> pro- posed a mention-ranking model by jointly learn- ing mention heads and co-references. <ref type="bibr" target="#b3">Clark and Manning (2016a)</ref> proposed a reinforcement learn- ing framework for the mention ranking approach. Based on similar ideas but without using parsing features, the authors of <ref type="bibr" target="#b8">Lee et al. (2017)</ref> proposed the current state-of-the-art model which uses neu- ral networks to embed mentions and calculate mention and antecedent scores.  applied ELMo embeddings ( <ref type="bibr" target="#b14">Peters et al., 2018)</ref> to improve within-sentence dependency modeling and word representation learning. <ref type="bibr">Wiseman et al. (2016)</ref> and <ref type="bibr" target="#b4">Clark and Manning (2016b)</ref> proposed models using global entity-level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Language Representation Learning</head><p>Distributed word embeddings has been used as the basic unit of language representation for over a decade ( <ref type="bibr" target="#b0">Bengio et al., 2003)</ref>. Pre-trained word em- beddings, for example GloVe ( <ref type="bibr" target="#b13">Pennington et al., 2014</ref>) and Skip-Gram (  are widely used as the input of natural language pro- cessing models.</p><p>Long short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) are widely used for sentence modeling. A single-layer LSTM network was applied in the previous state-of-the- art co-reference model ( <ref type="bibr" target="#b8">Lee et al., 2017</ref>) to gen- erate word and mention representations. To cap- ture dependency of longer distances, <ref type="bibr" target="#b1">Campos et al. (2017)</ref> proposed a recurrent model that outputs hidden states by skipping input tokens.</p><p>Recently, memory networks ( <ref type="bibr" target="#b16">Sukhbaatar et al., 2015</ref>) have been applied in language modeling ( <ref type="bibr" target="#b2">Cheng et al., 2016;</ref><ref type="bibr" target="#b17">Tran et al., 2016)</ref>. Applying an attention mechanism on memory cells, memory networks allow the model to focus on significant words or segments for classification and genera- tion tasks. Previous works have shown that apply- ing memory blocks in LSTMs also improves long- distance dependency extraction ( <ref type="bibr">Yogatama et al., 2018</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Cross-Sentence dependency</head><p>To improve the word representation learning model for better co-reference resolution perfor- mance, we propose two word representation mod- els that learn cross-sentence dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Linear Sentence Linking</head><p>Instead of treating the entire input document as separate sentences and encode the sentences as a batch with an LSTM, the most direct way to consider cross-sentence dependency is to initial- ize LSTM states with the encodings of adjacent sentences. We name this method linear sentence linking (LSL). In LSL, we encode input sentences with a 2- layer bidirectional LSTM. Give input sentences [s 1 , s 2 . . . s n ], the outputs of the first layer are</p><formula xml:id="formula_0">[[ − → s 1 ; ← − s 1 ], [ − → s 2 ; ← − s 2 ], . . . [ − → s n ; ← − s n ]].</formula><p>In the sec- ond LSTM layer, the initial state of the forward LSTM of s i is initialized as</p><formula xml:id="formula_1">− → S i = [ − → c 2 0 ; [ − → s i−1 ; ← − s i−1 ]]</formula><p>while the backward state is initialized as</p><formula xml:id="formula_2">← − S i = [ ← − c 2 0 ; [ − → s i−1 ; ← − s i−1 ]]</formula><p>where c i 0 stands for the initial cell of the i- th layer, and x stands for the final output of the LSTMs in first layer. We then concatenate the out- puts of the forward and backward LSTMs in the second layer as the word representations for co- reference prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attentional Sentence Linking</head><p>It is difficult for LSTMs to embed enough in- formation about a long sentence into a low- dimensional distributed vector. To collect richer knowledge from neighbor sentences, we propose a long short-term recurrent memory module and an attention mechanism to improve sentence linking.</p><p>To describe the architecture of the proposed model, we focus on adjacent input sentences s i−1 and s i . We present the input embeddings of the j-th word in the i-th sentence with x i,j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Long Short-Term Memory RNNs</head><p>To solve the traditional recurrent neural networks, <ref type="bibr" target="#b6">Hochreiter and Schmidhuber (1997)</ref> proposed the LSTM architecture. The detail of recurrent state updating in LSTMs h t = f lstm (x t , h t−1 , c t−1 ) is shown in following equations.</p><formula xml:id="formula_3">it = σ(Wxixt + W hi ht−1 + bi) ft = σ(W xf xt + W hf ht−1 + b f ) ct = ft ct−1 + it tanh(Wxcxt + W hc ht−1 + bc) ot = σ(Wxoxt + W ho ht−1 + bo) ht = ot tanh(ct)</formula><p>where x t is the input embedding and h t is the output representation of the t-th word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">LSTMs with Cross-Sentence Attention</head><p>We design an LSTM module with cross-sentence attention for capturing cross-sentence dependency. We name this method attentional sentence link- ing (ASL). Considering input word x i,t in the i- th sentence and all words from the previous sen- tence </p><formula xml:id="formula_4">α j = e c j k e c k<label>(1)</label></formula><formula xml:id="formula_5">c k = f c ([x i,t ; h t−1 ; x i−1,k ] T )<label>(2)</label></formula><p>With the attention distribution α, we can get a vector summarizing related information from s i−1 ,</p><formula xml:id="formula_6">v i−1 = j α j · x i−1,j<label>(3)</label></formula><p>The model decides if it needs to pay more at- tention on the current input or cross-sentence in- formation with a context gate.</p><formula xml:id="formula_7">g t = σ(f g ([x i,t ; h t−1 ; v i−1 ] T ))<label>(4)</label></formula><formula xml:id="formula_8">ˆ x i,t = g t · x i,t + (1 − g t ) · v i−1<label>(5)</label></formula><p>σ(·) stands for the Sigmoid function. The word representation of the target word is calculated as</p><formula xml:id="formula_9">h i,t = f lstm (ˆ x i,t , h i,t−1 , c i,t−1 )<label>(6)</label></formula><p>where f lstm stands for standard LSTM update described in section 3.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Co-reference Prediction</head><p>In this work, we apply the mention-ranking end- to-end co-reference resolution (E2E-CR) model proposed by <ref type="bibr" target="#b8">Lee et al. (2017)</ref> for co-reference pre- diction. The word representations applied in E2E- CR model is formed by concatenating pre-trained word embeddings and the outputs of LSTMs. In our work, we represent words by concatenating pre-trained word embeddings and the outputs of LSL-and ASL-LSTMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We train and evaluate our model on the English corpus of the CoNLL-2012 shared task ( <ref type="bibr" target="#b15">Pradhan et al., 2012)</ref>. We implement our model based on the published implementation of the baseline E2E- CR model ( <ref type="bibr" target="#b8">Lee et al., 2017)</ref>  <ref type="bibr">1</ref> . Our implementa- tion is also available online for reproducing the re- sults reported in this paper 2 . In this section, we first describe our hyperparameter setup, and then show the experimental results of previous work and our proposed models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model and Hyperparameter Setup</head><p>In practice, the LSTM modules applied in our model have 200 output units. In ASL, we cal- culate cross-sentence dependency using a multi- layer perceptron with one hidden layer consisting of 150 hidden units. The initial learning rate is set as 0.001 and decays 0.001% every 100 steps. The model is optimized with the Adam algorithm ( <ref type="bibr" target="#b7">Kingma and Ba, 2014</ref>). We randomly select up to 40 continuous sentences for training if the input is too long. In co-reference prediction, we select 250 candidate antecedents as our baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Results and Discussion</head><p>We evaluate our model on the test set of the CoNLL-2012 shared task. The performance of previous work and our model are shown in <ref type="table">Table 1</ref>. We mainly focus on the average F1 score of MUC, B 3 , and CEAF metrics. Comparing with the base- line model that achieved 67.2% F1 score, the ASL model improved the performance by 0.6% and achieved 67.8% average F1. Experiments  <ref type="table">Table 1</ref>: Experimental results of previous models and cross-sentence dependency learning models on the CoNLL- 2012 shared task.</p><p>-I remember receiving an SMS like this one last year before it snowed since snowfall would affect road conditions in Beijing to a large extent. -Uh-huh . However, it did not give people such a special feeling as it did this time.</p><p>-Reporters are tired of the usual stand ups.</p><p>-They want to be riding on a train or walking in the rain or something to get attention .</p><p>-Planned terrorist bombing that ripped a 20 x 40 -foot hole in the Navy destroyer USS Cole in the Yemeni port of Aden. -The ship was there for refueling.</p><p>-Yemeni authorities claimed they have detained over 70 people for questioning.</p><p>-These include some Afghan -Arab volunteers. show that the models that consider cross-sentence dependency significantly outperform the baseline model, which encodes each sentence from the in- put document separately. Experiments also indicated that the ASL model has better performance than the LSL model, since it summarizes extracts context information with an attention mechanism instead of simply view- ing sentence-level embeddings. This gives the model a better ability to model cross-sentence de- pendency.</p><p>Examples for comparing the performance of the ASL model and the baseline are shown in <ref type="table" target="#tab_1">Table  2</ref>. Each example contains two continuous sen- tences with co-references distritubed in different sentences. Underlined spans in bold are target mentions and annotated co-references. Spans in green are ASL predictions, and spans in red are baseline predictions. A prediction on "-" means that no mention is predicted as a co-reference. <ref type="table" target="#tab_1">Table 2</ref> shows that the baseline model, which does not consider cross-sentence dependency, has difficulty in learning the semantics of pronouns whose co-references are not in the same sentence. The pretrained embeddings of pronouns are not in- formative enough. In the first example, "it" is not semantically similar with "SMS" in GloVe with- out any context, and in this case, "it" and "SMS" are in different sentences. As a result, if read- ing this two sentences separately, it is hard for the encoder to represent "it" with the semantics of "SMS". This difficulty makes the co-reference resolution model either prediction a wrong an- tecedent mention, or cannot find any co-reference.</p><p>However, with ASL, the model learns the se- mantics of pronouns with an attention to words in other sentences. With the proposed context gate, ASL takes knowledge from context sentences if local inputs are not informative enough. Based on word represents enhanced with cross-sentence dependency, the co-reference scoring model can make better predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>We proposed linear and attentional sentence link- ing models for learning word representations that captures cross-sentence dependency. Experiments showed that the embeddings learned by proposed models successfully improved the performance of the state-of-the-art co-reference resolution model, indicating that cross-sentence dependency plays an important role in semantic learning in articles and conversations consists of multiple sentences. It worth exploring if our model can improve the performance of other natural language processing</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>X i−1 = [x i−1,1 , x i−1,2 , . . . , x i−1,m ], we re- gard the matrix X i−1 as an external memory mod- ule and calculate an attention on its cells, where each cell contains a word embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Examples predictions of the ASL model and 
the baseline model. 

</table></figure>

			<note place="foot" n="1"> https://github.com/kentonl/e2e-coref 2 https://github.com/luohongyin/ coatt-coref</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Víctor</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Giró-I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06834</idno>
		<title level="m">Skip rnn: Learning to skip state updates in recurrent neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Long short-term memory-networks for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06733</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning for mention-ranking coreference models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08667</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improving coreference resolution by learning entitylevel distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01323</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Easy victories and uphill battles in coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1971" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07045</idno>
		<title level="m">End-to-end neural coreference resolution</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Higher-order coreference resolution with coarse-tofine inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05392</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A joint framework for coreference resolution and mention head detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoruo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Nineteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conll2012 shared task: Modeling multilingual unrestricted coreference in ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL-Shared Task</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Recurrent memory networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.01272</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
