<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modified Dirichlet Distribution: Allowing Negative Parameters to Induce Stronger Sparsity *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="1986">1986-1991. November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
							<email>tukw@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modified Dirichlet Distribution: Allowing Negative Parameters to Induce Stronger Sparsity *</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="1986">1986-1991. November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The Dirichlet distribution (Dir) is one of the most widely used prior distributions in statistical approaches to natural language processing. The parameters of Dir are required to be positive, which significantly limits its strength as a sparsity prior. In this paper, we propose a simple modification to the Dirichlet distribution that allows the parameters to be negative. Our modified Dirichlet distribution (mDir) not only induces much stronger sparsity, but also simultaneously performs smoothing. mDir is still conjugate to the multinomial distribution, which simplifies posterior inference. We introduce two simple and efficient algorithms for finding the mode of mDir. Our experiments on learning Gaussian mixtures and un-supervised dependency parsing demonstrate the advantage of mDir over Dir.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Dirichlet Distribution</head><p>The Dirichlet distribution (Dir) is defined over prob- ability vectors x = x 1 , . . . , x n with positive pa- rameter vector α = α 1 , . . . , α n :</p><formula xml:id="formula_0">Dir(x; α) = 1 B(α) n i=1 x α i −1 i</formula><p>where the normalization factor B(α) is the multi- variate beta function. When the elements in α are larger than one, Dir can be used as a smoothness prior that prefers more uniform probability vectors, with larger α values inducing more smoothness.</p><p>When the elements in α are less than one, Dir can be seen as a sparsity prior that prefers sparse probabil- ity vectors, with smaller α values inducing stronger sparsity. To better understand its sparsity preference, we take the logarithm of Dir:</p><p>log Dir(x; α) = n i=1 (α i − 1) log x i + constant</p><p>Since α i − 1 is negative, the closer x i is to zero, the higher the log probability becomes. The coef- ficient α i − 1 controls the strength of the sparsity preference. However, α i is required to be positive in Dir because otherwise the normalization factor becomes divergent. Consequently, the strength of the sparsity preference is upper bounded. This be- comes problematic when a strong prior is needed, for instance, when the training dataset is large rela- tive to the model size (e.g., in unsupervised learning of an unlexicalized probabilistic grammar) and thus the likelihood may dominate the posterior without a strong prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Modified Dirichlet Distribution</head><p>We make a simple modification to the Dirichlet dis- tribution that allows the parameters in α to become negative. To handle the divergent normalization fac- tor, we require that each x i must be lower bounded by a small positive constant . Our modified Dirich- let distribution (mDir) is defined as follows.</p><formula xml:id="formula_1">mDir(x; α, ) = 0 if ∃i, x i &lt; 1 Z(α,,) n i=1 x α i −1 i otherwise</formula><p>where we require 0 &lt; ≤ 1 n and do not require α i to be positive. With fixed values of α and , the unnormalized probability density is always bounded and hence the normalization factor Z(α, ) is finite.</p><p>It is easy to show that mDir is still conjugate to the multinomial distribution. Similar to Dir, mDir can be used as a smoothness/sparsity prior depending on the values of α. Because α is no longer required to be positive, we can achieve very strong sparsity preference by using a highly negative vector of α. Note that here we no longer achieve sparsity in its strict sense; instead, by sparsity we mean most ele- ments in x reach their lower bound . Parameter can thus be seen as a smoothing factor that prevents any element in x to become too small. Therefore, with proper parameters, mDir is able to simultane- ously achieve sparsity and smoothness. This can be useful in many applications where one wants to learn a sparse multinomial distribution in an iterative way without premature pruning of components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Finding the Mode</head><p>If ∀i, α i − 1 ≤ 0, then the mode of mDir can be shown to be:</p><formula xml:id="formula_2">x i = 1 − (n − 1) if i = arg max i α i otherwise</formula><p>Otherwise, we can find the mode with Algorithm 1. The algorithm first lets x i = if α i ≤ 1 and other- wise lets x i be proportional to α i − 1. It then looks for variables in x that are less than , increases them to , and renormalizes the rest of the variables. The renormalization may decrease some additional vari- ables below , so the procedure is repeated until all the variables are larger than or equal to .</p><p>Theorem 1. If ∃i, α i &gt; 1, then Algorithm 1 cor- rectly finds a mode of mDir(x; α, ).</p><p>Proof. First, we can show that for any i such that α i ≤ 1, we must have x i = at the mode. This is because if x i &gt; , then we can increase the probabil- ity density by first decreasing x i to (hence increas- ing</p><formula xml:id="formula_3">x α i −1 i )</formula><p>, and then increasing some other variable x j with α j &gt; 1 to satisfy the normalization condi- tion (hence also increasing x α j −1 j ). This is consis- tent with the output of the algorithm.</p><p>Once we fix the value to for any variable x i s.t. α i ≤ 1, the log probability density function becomes strictly concave on the simplex specified by the linear constraints i x i = 1 and x i ≥ .</p><p>Algorithm 1 Mode-finding of mDir(x; α, )</p><formula xml:id="formula_4">1: S ← {i|α i ≤ 1} 2: T ← ∅ 3: repeat 4:</formula><p>T ← T S</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>for i ∈ T do 6:</p><formula xml:id="formula_5">x i ← 7:</formula><p>end for 8:</p><formula xml:id="formula_6">z ← i / ∈T (α i − 1) 9:</formula><p>for i / ∈ T do 10:</p><formula xml:id="formula_7">x i ← αi−1 z × (1 − |T |) 11:</formula><p>end for 12:</p><formula xml:id="formula_8">S ← {i|x i &lt; } 13: until S = ∅ 14: return x 1 , . . . , x n</formula><p>The strict concavity can be proved by showing that the log probability density function is twice differ- entiable and the Hessian is negative definite at any point of the simplex.</p><p>With a concave function and linear constraints, the KKT conditions are sufficient for optimality. We need to show that the output of the algorithm satis- fies the following KKT conditions:</p><p>• Stationarity: ∀i, α i −1</p><formula xml:id="formula_9">x i = −µ i + λ • Primal feasibility: ∀i, x i ≥ and i x i = 1 • Dual feasibility: ∀i, µ i ≥ 0 • Complementary slackness: ∀i, µ i (x i − ) = 0 Let x (k) i</formula><p>and T (k) be the values of x i and T after k iterations of the algorithm. Suppose the algorithm terminates after K iterations. So the output of the algorithm is x</p><formula xml:id="formula_10">(K) 1 , . . . , x (K)</formula><p>n , which we will prove satisfies the KKT conditions. For any i s.t.</p><formula xml:id="formula_11">x (K) i &gt; , we set µ i = 0 and λ = α i −1 x (K) i</formula><p>to satisfy all the conditions involving x</p><formula xml:id="formula_12">(K) i . For any j s.t. x (K) j = , suppose x (k)</formula><p>j &lt; , i.e., x j falls below in iteration k and is set to afterwards. Pick some i s.t. i / ∈ T (K) . After iteration k and k +1 respectively, we have:</p><formula xml:id="formula_13">x (k) i α i − 1 = 1 − T (k) − j ∈T (k+1) \T (k) x (k) j j / ∈T (k+1) α j − 1 x (k+1) i α i − 1 = 1 − T (k+1) j / ∈T (k+1) α j − 1</formula><p>Algorithm 2 Fast mode-finding of mDir(x; α, )</p><p>1: α k1 , . . . , α kn ← α 1 , . . . , α n in ascending order 2: s n ← α kn − 1 3: for i = n − 1, . . . , 1 do 4:</p><formula xml:id="formula_14">s i = s i+1 + α ki − 1 So s i = j≥i (α kj − 1) 5: end for 6: t ← 0 7: for i = 1, . . . , n do 8: x ki ← α k i −1 si × (1 − t) 9:</formula><p>if x ki &lt; then 10:</p><p>x ki ← , t ← t + 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>end if 12: end for 13: return x 1 , . . . , x n Because for any j ∈ T (k+1) \T (k) we have x (k) j &lt; , from the two equations above we can deduce that</p><formula xml:id="formula_15">x (k) i &gt; x (k+1) i</formula><p>, i.e., x i monotonically decreases over iterations. Therefore,</p><formula xml:id="formula_16">(α j − 1) × x (K) i α i − 1 &lt; (α j − 1) × x (k) i α i − 1 = x (k) j &lt; So we get α j − 1 &lt; α i − 1 x (K) i = λ So we set µ j = λ − α j −1</formula><p>and all the conditions involving x (K) j are also satisfied. The proof is now complete.</p><p>The worst-case time complexity of Algorithm 1 is O(n 2 ), but in practice when is small, the algo- rithm almost always terminates after only one iter- ation, leading to linear running time. We also pro- vide a different mode-finding algorithm with better worst-case time complexity Θ(n log n) (Algorithm 2). It differs from Algorithm 1 in that the elements of α are first sorted, so we can finish computing x in one pass. It can be more efficient than Algorithm 1 when both and n are larger. Its correctness can be proved in a similar way to that of Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Related Distribution</head><p>The closest previous work to mDir is the pseudo- Dirichlet distribution ( <ref type="bibr" target="#b5">Larsson and Ugander, 2011)</ref>. It also allows negative parameters to achieve stronger sparsity. However, the pseudo-Dirichlet distribution is no longer conjugate to the multino- mial distribution. Consequently, its maximum a pos- teriori inference becomes complicated and has no time-complexity guarantee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Mixtures of Gaussians</head><p>We first evaluate mDir in learning mixtures of Gaus- sians from synthetic data. The ground-truth model contains two bivariate Gaussian components with equal mixing probabilities ( <ref type="figure">Figure 5(a)</ref>). From the ground-truth we sampled two training datasets of 20 and 200 data points. We then tried to fit a Gaussian mixture model with five components.</p><p>Three approaches were tested: maximum like- lihood estimation using expectation-maximization (denoted by EM), which has no sparsity preference; mean-field variational Bayesian inference with a Dir prior over the mixing probabilities (denoted by VB- Dir), which is the most frequently used inference approach for Dir with α &lt; 1; maximum a posteri- ori estimation using expectation-maximization with a mDir prior over the mixing probabilities (denoted by EM-mDir). The Dir and mDir priors that we used are both symmetric, i.e., all the elements in vector α have the same value, denoted by α. For mDir, we set = 10 −5 . We ran each approach under each parameter setting for 300 times with different ran- dom initialization and then reported the average re- sults. During learning, we pruned a Gaussian com- ponent whenever its covariance matrix becomes nu- merically singular (which means the component is estimated from only one or two data samples). <ref type="figure">Figure 1</ref>-4 show the average test set log likeli- hood and the effective numbers of mixture compo- nents of the models learned with different values of parameter α from 20 and 200 samples respectively. For VB-Dir, we show the results with the α value as low as 10 −5 . Further decreasing α did not improve the results. It can be seen that both VB-Dir and EM-mDir can achieve better test set likelihood and lower effective numbers of components than EM with proper α values. EM-mDir outperforms VB- Dir even with positive α values, and its performance is further boosted when α becomes negative. The improvement of EM-mDir when α becomes neg- ative is smaller in the 20-sample case than in the 200-sample case. This is because when the train-   ing dataset is small, a small positive α value may already be sufficient in inducing enough sparsity. <ref type="figure">Figure 5</ref>(b)-(e) show the typical models learned by VB-Dir and EM-mDir. When the training dataset is small, both Dir and mDir are effective sparsity priors that help prune unnecessary mixture compo- nents, though mDir can be more effective with a neg- ative α value. When the training dataset is large, however, the Dir prior is overwhelmed by the like- lihood in posterior inference and cannot effectively prune mixture components. On the other hand, with a highly negative α value, mDir is still effective as a sparsity prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Unsupervised Dependency Parsing</head><p>Unsupervised dependency parsing aims to learn a dependency grammar from unannotated text. Pre- vious work has shown that sparsity regularization improves the performance of unsupervised depen- dency parsing <ref type="bibr" target="#b3">(Johnson et al., 2007;</ref><ref type="bibr" target="#b2">Gillenwater et al., 2010)</ref>. In our experiments, we tried to learn a dependency model with valence (DMV) ( <ref type="bibr" target="#b4">Klein and Manning, 2004</ref>) from the Wall Street Journal cor- pus, with section 2-21 for training and section 23 for testing. Following previous work, we used sen- tences of length ≤ 10 with punctuation stripped off.</p><note type="other">-1.448 -1.446 -1.444 0 0.5 1 1.5 EM-mDir VB-Dir EM</note><p>Since DMV is an unlexicalized model, the number of dependency rules is small relative to the training corpus size. This suggests that a strong prior can be helpful in counterbalancing the influence of the training data. We tested six approaches. With a mDir prior, we tried EM, hard EM, and softmax-EM with σ = 0.5 (Tu and Honavar, 2012) (denoted by EM-mDir, HEM-mDir, SEM-mDir). With a Dir prior, we tried variational inference, hard variational inference, and softmax variational inference with σ = 0.5 (Tu and Honavar, 2012) (denoted by VB-Dir, HVB-Dir, SVB-Dir). Again, we used symmetric Dir and mDir priors. For mDir, we set = 10 −4 by default. <ref type="figure">Figure 6</ref> shows the directed accuracy of parsing the test corpus using the learned dependency mod- els. It can be seen that with positive α values, Dir and mDir have very similar accuracy under the stan- dard, hard and softmax versions of inference respec- tively. With negative α values, the accuracy of EM- mDir decreases; but for HEM-mDir and SEM-mDir, the accuracy is significantly improved with moder- ately negative α values. HEM-mDir consistently produces accuracy around 0.63 with a large range of α values (from -10 to -40), which is on a par with the best published results in learning the original DMV model <ref type="bibr" target="#b1">(Cohen and Smith, 2009;</ref><ref type="bibr" target="#b2">Gillenwater et al., 2010;</ref><ref type="bibr" target="#b0">Berg-Kirkpatrick et al., 2010)</ref>, even though these previous approaches employed more sophis- ticated features and advanced regularization tech- niques than ours. <ref type="figure" target="#fig_4">Figure 7</ref> shows the degree of sparsity of the learned dependency grammars. We computed the percentage of dependency rules with probabilities below 10 −3 to measure the degree of sparsity. It can be seen that even with positive α values, mDir leads to significantly more sparse grammars than Dir does. With negative values of α, mDir can induce even more sparsity. <ref type="figure" target="#fig_5">Figure 8</ref> plots the parsing accuracy with different values of parameter in mDir (α is set to -20). The best accuracy is achieved when is neither too large nor too small. This is because if is too large, the probabilities of dependency rules become too uni- form to be discriminative. On the other hand, if is too small, then the probabilities of many depen- dency rules may become too small in the early stages of learning and never be able to recover. Similar ob- servation was made by <ref type="bibr" target="#b3">Johnson et al. (2007)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We modify the Dirichlet distribution to allow nega- tive values of parameter α so that it induces stronger sparsity when used as a prior of a multinomial distribution. A second parameter is introduced which prevents divergence of the normalization fac- tor and also acts as a smoothing factor. Our modified Dirichlet distribution (mDir) is still conjugate to the multinomial distribution. We propose two efficient algorithms for finding the mode of mDir. Our ex- periments on learning Gaussian mixtures and unsu- pervised dependency parsing show the advantage of mDir over the Dirichlet distribution.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>-</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Test set log likelihood vs. the value of α (20 training samples)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Test set log likelihood vs. the value of α (200 training samples)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: The ground-truth model and four typical models learned by VB-Dir and EM-mDir. (b),(c): 20 training samples. (d),(e): 200 training samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Sparsity of the learned grammars vs. the value of α r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Parsing accuracy vs. the value of</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Painless unsupervised learning with features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="582" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="74" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sparsity in dependency grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL &apos;10: Proceedings of the ACL 2010 Conference Short Papers</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="194" to="199" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bayesian inference for pcfgs via markov chain monte carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="139" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Corpusbased induction of syntactic structure: Models of dependency and constituency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A concave regularization technique for sparse mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Martin O Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ugander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1890" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unambiguity regularization for unsupervised learning of probabilistic grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasant</forename><surname>Honavar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1324" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
