<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Neural Dependency Parsing *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjuan</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Neural Dependency Parsing *</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="763" to="771"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Unsupervised dependency parsing aims to learn a dependency grammar from text annotated with only POS tags. Various features and inductive biases are often used to incorporate prior knowledge into learning. One useful type of prior information is that there exist correlations between the parameters of grammar rules involving different POS tags. Previous work employed manually designed features or special prior distributions to encode such information. In this paper, we propose a novel approach to unsupervised dependency parsing that uses a neural model to predict grammar rule probabilities based on distributed representation of POS tags. The distributed representation is automatically learned from data and captures the correlations between POS tags. Our experiments show that our approach outperforms previous approaches utilizing POS correlations and is competitive with recent state-of-the-art approaches on nine different languages.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unsupervised structured prediction from data is an important problem in natural language processing, with applications in grammar induction, POS tag in- duction, word alignment and so on. Because the training data is unannotated in unsupervised struc- tured prediction, learning is very hard. In this pa- per, we focus on unsupervised dependency parsing, which aims to identify the dependency trees of sen- tences in an unsupervised manner.</p><p>Previous work on unsupervised dependency pars- ing is mainly based on the dependency model with valence (DMV) ( <ref type="bibr" target="#b13">Klein and Manning, 2004</ref>) and it- s extension <ref type="bibr" target="#b11">(Headden III et al., 2009;</ref><ref type="bibr" target="#b10">Gillenwater et al., 2010)</ref>. To effectively learn the DMV mod- el for better parsing accuracy, a variety of induc- tive biases and handcrafted features have been pro- posed to incorporate prior information into learning. One useful type of prior information is that there exist correlations between the parameters of gram- mar rules involving different POS tags. <ref type="bibr" target="#b3">Cohen and Smith (2009;</ref> employed special prior distribu- tions to encourage learning of correlations between POS tags. <ref type="bibr" target="#b0">Berg-Kirkpatrick et al. (2010)</ref> encoded the relations between POS tags using manually de- signed features.</p><p>In this work, we propose a neural based ap- proach to unsupervised dependency parsing. We incorporate a neural model into the DMV model to predict grammar rule probabilities based on dis- tributed representation of POS tags. We learn the neural network parameters as well as the distribut- ed representations from data using the expectation- maximization algorithm. The correlations between POS tags are automatically captured in the learned POS embeddings and contribute to the improvemen- t of parsing accuracy. In particular, probabilities of grammar rules involving correlated POS tags are au- tomatically smoothed in our approach without the need for manual features or additional smoothing procedures.</p><p>Our experiments show that on the Wall Street Journal corpus our approach outperforms the pre- vious approaches that also utilize POS tag correla-tions, and achieves a comparable result with recent state-of-the-art grammar induction systems. On the datasets of eight additional languages, our approach is able to achieve better performance than the base- line methods without any parameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work 2.1 Dependency Model with Valence</head><p>The dependency model with valence (DMV) ( <ref type="bibr" target="#b13">Klein and Manning, 2004</ref>) is the first model to outperform the left-branching baseline in unsupervised depen- dency parsing of English. The DMV model is a generative model of a sentence and its parse tree. It generates a dependency parse from the root in a re- cursive top-down manner. At each step, a decision is first made as to whether a new child POS tag shall be generated from the current head tag; if the decision is yes, then a new child POS tag is sampled; other- wise, the existing child tags are recursively visited. There are three types of grammar rules in the mod- el: CHILD, DECISION and ROOT, each with a set of multinomial parameters P CHILD (c|h, dir, val), P DECISION (dec|h, dir, val) and P ROOT (c|root), where dir is a binary variable indicating the genera- tion direction (left or right), val is a boolean variable indicating whether the current head POS tag already has a child in the current direction or not, c indicates the child POS tag, h indicates the head POS tag, and dec indicates the decision of either STOP or CON- TINUE. A CHILD rule indicates the probability of generating child c given head h on direction dir and valence val. A DECISION rule indicates the proba- bility of STOP or CONTINUE given the head, direc- tion and valence. A ROOT rule is the probability of a child c generated by the root. The probability of a dependency tree is the product of probabilities of all the grammar rules used in generating the dependen- cy tree. The probability of a sentence is the sum of probabilities of all the dependency trees consistent with the sentence.</p><p>The basic DMV model has the limitation of being oversimplified and unable to capture certain linguis- tic structures. <ref type="bibr" target="#b11">Headden et al. (2009)</ref> incorporated more types of valence and lexicalized information in the DMV model to increase its representation power and achieved better parsing accuracy than the basic DMV model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DMV-based Learning Algorithms for Unsupervised Dependency Parsing</head><p>To learn a DMV model from text, the Expectation Maximization (EM) algorithm ( <ref type="bibr" target="#b13">Klein and Manning, 2004</ref>) can be used. In the E step, the model calcu- lates the expected number of times each grammar rule is used in parsing the training text by using the inside-outside algorithm. In the M-step, these ex- pected counts are normalized to become the proba- bilities of the grammar rules. There have been many more advanced learning al- gorithms of the DMV model beyond the basic EM algorithm. In the work of <ref type="bibr" target="#b5">Cohen and Smith (2008)</ref>, a logistic normal prior was used in the DMV model to capture the similarity between POS tags. In the work of <ref type="bibr" target="#b0">Berg-Kirkpatrick et al. (2010)</ref>, features that group various morphological variants of nouns and verbs are used to predict the DECISION and CHILD parameters. These two approaches both utilize the correlations between POS tags to obtain better prob- ability estimation of grammar rules involving such correlated POS tags. In the work of <ref type="bibr" target="#b19">Tu and Honavar (2012)</ref>, unambiguity of parse trees is incorporated into the training objective function of DMV to ob- tain a better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Other Approaches to Unsupervised Dependency Parsing</head><p>There are many other approaches to unsupervised dependency parsing that are not based on DMV. Daumé III (2009) proposed a stochastic search based method to do unsupervised Shift-Reduce transition parsing. <ref type="bibr" target="#b15">Rasooli and Faili (2012)</ref> proposed a transi- tion based unsupervised dependency parser together with "baby-step" training ( <ref type="bibr" target="#b16">Spitkovsky et al., 2010</ref>) to improve parsing accuracy. <ref type="bibr" target="#b14">Le and Zuidema (2015)</ref> proposed a complicated reranking based unsuper- vised dependency parsing system and achieved the state-of-the-art performance on the Penn Treebank dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Neural based Supervised Dependency Parser</head><p>There exist several previous approaches on using neural networks for supervised dependency pars- ing. <ref type="bibr" target="#b8">Garg and Henderson (2011)</ref>    <ref type="formula">(2015)</ref> proposed a stack long short-term memory approach to supervised dependency parsing. To our knowl- edge, our work is the first attempt to incorporate neural networks into a generative grammar for un- supervised dependency parsing.</p><formula xml:id="formula_0">W dir p = Sof tmax(W f) W f = ReLU (W dir [v h ; v val ]) [v h ; v val ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural DMV</head><p>In this section, we introduce our neural based gram- mar induction approach. We describe the model in section 3.1 and the learning method in section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model</head><p>Our model is based on the DMV model (section 2.1), except that the CHILD and DECISION probabilities are calculated through two neural networks. We do not compute the ROOT probabilities using a neural network because doing that complicates the mod- el while leads to no significant improvement in the parsing accuracy. Parsing a sentence using our mod- el can be done in the same way as using DMV. Below we show how the CHILD rule probabilities are computed in our neural based DMV model. De- note the set of all possible POS tags by T . We build a neural network to compute the probabilities of pro- ducing child tag c ∈ T conditioned on the head, di- rection and valence (h, dir, val).</p><p>The full architecture of the neural network is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. First, we represent each head tag h as a d dimensional vector v h ∈ R d , represent each value of valence val as a d dimensional vector v val ∈ R d . We concatenate v h and v val as the in- put embedding vector. Then we map the input layer to a hidden layer with weight matrix W dir through a ReLU activation function. We have two versions of weight matrix W dir for the direction dir being left and right respectively.</p><formula xml:id="formula_1">f (h, dir, val) = ReLU(W dir [v h ; v val ])</formula><p>We then take the inner product of f and all the child POS tag vectors and apply a softmax function to ob- tain the rule probabilities:</p><formula xml:id="formula_2">[p c 1 , p c 2 , ..., p c T ] = Softmax(W T f ) where W = [v c 1 , v c 2 , ..., v c T ]</formula><p>is an embedding ma- trix composed of all the child POS tag vectors.</p><p>We use the same neural architecture to predict the probabilities of DECISION rules. The difference is that the neural network for DECISION has only t- wo outputs (STOP and CONTINUE). Note that the two networks share parameters such as head POS tag embeddings and direction weight matrices W lef t and W right . Valence embeddings are either shared or distinct between the two networks depending on the variant of DMV we use (i.e., whether the max- imal valences for CHILD and DECISION are the same).</p><p>The parameters of our neural based model in- clude the weights of the neural network and all the POS and valence embeddings, denoted by a set</p><formula xml:id="formula_3">Θ = {v h , v c , v val , v dec , W dir ; h, c ∈ T, val ∈ {0, 1, ...}, dir ∈ {lef t, right}, dec ∈ {STOP, CONTINUE}}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning</head><p>In this section, we describe an approach based on the EM algorithm to learn our neural DMV model. To learn the parameters, given a set of unannotated sen- tences x 1 , x 2 , ..., x N , our objective function is the log-likelihood function. The approach is visualized in the <ref type="figure" target="#fig_1">Figure 2</ref>. The E- step computes the expected number of times each grammar rule used in parsing each training sentence x i , denoted by e c (x i ) for CHILD rule c, e d (x i ) for DECISION rule d, and e r (x i ) for ROOT rule r. In the M-step of traditional DMV learning, these ex- pected counts are normalized to re-estimate the pa- rameters of DMV. This maximizes the expected log likelihood (ELL) with respect to the DMV model parameters.</p><formula xml:id="formula_4">L(Θ) = N α=1 log P(x α ; Θ)</formula><formula xml:id="formula_5">ELL(Θ) = N α=1 c e c (x i ) log p c + d e d (x i ) log p d + r e r (x i ) log p r</formula><p>In our model, however, we do not directly as- sign the optimal rule probabilities of CHILD and DECISION; instead, we train the neural networks to output rule probabilities that optimize ELL, which is equivalent to a weighted cross-entropy loss function for each neural network. Note that while the tradi- tional M-step produces the global optimum of ELL, our neural-based M-step does not. This is because a neural network tends to produce similar outputs for correlated inputs. In our case, the neural network is able to capture the correlations between different POS tags as well as different valence values and s- mooth the probabilities involving correlated tags and valences. In other words, our M-step can be seen as optimizing the ELL with a regularization term tak- ing into account the input correlations. We use mo- mentum based batch stochastic gradient descent al- gorithm to train the neural network and learn all the embeddings and weight matrices.</p><p>In addition to standard EM, we can also learn our neural based DMV model based on the Viterbi EM algorithm. The difference from standard EM is that in the E-step, we compute the number of times each grammar rule is used in the best parse of a training sentence instead of considering all possible parses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>We used the Wall Street Journal corpus (with section 2-21 for training, section 22 for validation and sec- tion 23 for testing) in section 4.2 and 4.3. Then we reported the results on eight additional languages in section 4.4. In each experiment, we trained our mod- el on gold POS tags with sentences of length less than 10 after punctuation has been stripped off. As the EM algorithm is very sensitive to initializations, we used the informed initialization method proposed in ( <ref type="bibr" target="#b13">Klein and Manning, 2004</ref>).</p><p>The length of embeddings is set to 10 for both POS tags and valence. We trained the neural net- works with batch size 10 and used the change of the validation set loss function as the stop criteria. We ran our model for five times and reported the av- eraged directed dependency accuracy (DDA) of the learned grammars on the test sentences with length less than 10 and all sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparisons of Approaches based on POS Correlations</head><p>We first evaluated our approach in learning the basic DMV model and compared the results against (Co- hen and Smith, 2009) and <ref type="bibr" target="#b0">(Berg-Kirkpatrick et al., 2010)</ref>, both of which have very similar motivation as ours in that they also utilize the correlation between POS tags to learn the basic DMV model.  <ref type="bibr" target="#b5">(Cohen et al., 2008)</ref> 59.4 40.5 Shared LN <ref type="bibr" target="#b3">(Cohen and Smith, 2009)</ref> 61.3 41.4 Feature DMV <ref type="bibr" target="#b0">(Berg-Kirkpatrick et al., 2010)</ref> 63.0 - Neural DMV <ref type="table">(Standard EM)</ref> 51.3 37.1 Neural DMV (Viterbi EM) 65.9 47.0 shows the results. It can be seen that our approach with Viterbi EM significantly outperforms the EM and viterbi EM baselines and also outperforms the two previous approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on the extended DMV model</head><p>We directly apply our neural approach to learning the extended DMV model <ref type="bibr" target="#b11">(Headden III et al., 2009;</ref><ref type="bibr" target="#b10">Gillenwater et al., 2010</ref>) (with the maximum va- lence value set to 2 for both CHILD and DECISION rules). As shown in <ref type="table" target="#tab_3">Table 2</ref>, we achieve comparable accuracy with recent state-of-the-art systems. If we initialize our model with the grammar learned by <ref type="bibr" target="#b19">Tu and Honavar (2012)</ref>, the accuracy of our approach can be further improved. Most of the recent state-of-the-art systems em- ploy more complicated models and learning algo- rithms. For example, <ref type="bibr" target="#b17">Spitkovsky et al. (2013)</ref> take several grammar induction techniques as modules and connect them in various ways; Le and Zuide- ma (2015) use a neural-based supervised parser and reranker that make use of high-order features and lexical information. We expect that the performance of our approach can be further improved when these more advanced techniques are incorporated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on other languages</head><p>We also applied our approach on datasets of eight additional languages from the PASCAL Challenge on Grammar Induction ( <ref type="bibr" target="#b9">Gelling et al., 2012</ref>). We ran our approach using the hyper-parameters from experiment 4.2 on the new datasets without any fur- ther tuning. We tested three versions of our ap- proach based on standard EM, softmax EM ( <ref type="bibr" target="#b19">Tu and Honavar, 2012)</ref> and Viterbi EM respectively. The results are shown in <ref type="table">Table 3</ref> for test sentence length no longer than ten and <ref type="table" target="#tab_7">Table 4</ref> for all test sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WSJ10 WSJ</head><p>Systems with Basic Setup EVG <ref type="bibr" target="#b11">(Headden III et al., 2009)</ref> 65.0 - TSG-DMV <ref type="bibr" target="#b1">(Blunsom and Cohn, 2010)</ref> 65.9 53.1 PR-S <ref type="bibr" target="#b10">(Gillenwater et al., 2010)</ref> 64.3 53.3 UR-A E-DMV ( <ref type="bibr" target="#b19">Tu and Honavar, 2012)</ref> 71.4 57.0 Neural E-DMV 69.7 52.5 Neural E-DMV <ref type="table">(Good Init)</ref> 72.5 57.6 Systems Using Extra Info LexTSG-DMV <ref type="bibr" target="#b1">(Blunsom and Cohn, 2010)</ref> 67.7 55.7 L-EVG <ref type="bibr" target="#b11">(Headden III et al., 2009)</ref> 68.8 - CS <ref type="bibr" target="#b17">(Spitkovsky et al., 2013)</ref> 72.0 64.4 MaxEnc ( <ref type="bibr" target="#b14">Le and Zuidema, 2015)</ref> 73.2 65.8 Our neural based methods achieve better results than their corresponding baselines in 75.0% of the cases for test sentences no longer than 10 and 77.5% for all test sentences. The good performance of our ap- proach without data-specific hyper-parameter tuning demonstrates the robustness of our approach. Care- fully tuned hyper-parameters on validation datasets, in our experience, can further improve the perfor- mance of our approach, in some cases by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effects of Hyper-parameters</head><p>We examine the influence of hyper-parameters on the performance of our approach with the same ex- perimental setup as in section 4.3.</p><p>Activation function We compare different linear and non-linear functions: ReLU, Leaky ReLU, Tan- h, Sigmoid. The results are shown in <ref type="table" target="#tab_5">Table 5</ref>. Non- linear activation functions can be seen to significant- ly outperform linear activation functions.</p><p>Length of the embedding vectors The dimen- sion of the embedding space is an important hyper- parameter in our system. As <ref type="figure" target="#fig_2">Figure 3</ref> illustrates, when the dimension is too low (such as dim = 5), the performance is bad probably because the embed- ding vectors cannot effectively discriminate between <ref type="table">Table 3</ref>: DDA results (on sentences no longer than 10) on eight additional languages. Our neural based approaches are compared with traditional approaches using standard EM, softmax EM (parameterized by σ) and Viterbi EM.   Shared parameters An alternative to our neural network architecture is to have two separate neural networks to compute CHILD and DECISION rule probabilities respectively. The embeddings of the head POS tag and the valence are not shared be- tween the two networks. As can be seen in <ref type="table" target="#tab_1">Table   WSJ10</ref> WSJ Separate Networks 68.6 52.1 Merged Network 69.7 52.5 <ref type="table">Table 6</ref>: Comparison between using two separate networks and using a merged network.</p><note type="other">Arabic Basque Czech Danish Dutch Portuguese Slovene Swedish Standard EM DMV 45.8 41.1 31.3 50.8 47.1 36.7 36.7 43.5 Neural DMV 43.4 46.5 33.1 55.6 49.0 30.4 42.2 44.3 Softmax EM σ = 0.25 DMV 49.3 45.6 30.4 43.6 46.1 33.5 29.8 50.3 Neural DMV 54.2 46.3 36.8 44.0 39.9 35.8 31.2 49.7 Softmax EM σ = 0.5 DMV 54.2 47.6 43.2 38.8 38.0 33.7 23.0 37.2 Neural DMV 44.6 48.9 33.4 50.3 37.5 35.3 32.2 43.3 Softmax EM σ = 0.75 DMV 42.2 48.6 22.7 41.0 33.8 33.5 23.2 41.6 Neural DMV 56.7 45.3 31.6 41.3 33.7 34.7 22.9 42.0 Viterbi EM DMV 32.5 47.1 27.1 39.1 37.1 32.3 23.7 42.6 Neural DMV 48.2 48.1 28.6 39.8 37.2 36.5 39.9 47.9</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Activation function WSJ10</head><p>6, sharing POS tags embeddings attribute to better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Model Analysis</head><p>In this section, we investigate what information our neural based DMV model captures and analyze how it contributes to better parsing performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Correlation of POS Tags Encoded in Embeddings</head><p>A main motivation of our approach is to encode cor- relation between POS tags in their embeddings so as to smooth the probabilities of grammar rules in- volving correlated POS tags. Here we want to ex- amine whether the POS embeddings learned by our approach successfully capture such correlation. We collected the POS embeddings learned in the experiment described in section 4.3 and visualized them on a 2D plane using the t-SNE algorithm <ref type="bibr" target="#b20">(Van der Maaten and Hinton, 2008</ref>  the data points in the high dimensional space. The "perplexity" hyper-parameter of the algorithm was set to 20.0 and the distance metric we used is the Euclidean distance. <ref type="figure" target="#fig_3">Figure 4</ref> shows the visualization result. It can be seen that in most cases, nearby POS tags in the figure are indeed similar. For example, VBP (Verb, non- 3rd person singular present), VBD (Verb, past tense) and VBZ (Verb, 3rd person singular present) can be seen to be close to each other, and they indeed have very similar syntactic behavior. Similar observation can be made to NN (Noun, singular or mass ), NNPS (Proper noun, plural) and NNS (Noun, plural).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Smoothing of Grammar Rule Probabilities</head><p>By using similar embeddings to represent correlat- ed POS tags, we hope to smooth the probabilities of rules involving correlated POS tags. Here we an- alyze whether our neural networks indeed predict more similar probabilities for rules with correlated POS tags.</p><p>We conducted a case study on all types of verb- s: VBP (Verb, non-3rd person singular present), VBZ (Verb, 3rd person singular present), VBD (Ver- b, past tense), VBN (Verb, past participle), VB (Verb, base form), VBG (Verb, gerund or present participle). We used the neural networks in our N-DMV model learned in the experiment described in section 4.2 to predict the probabilities of all the CHILD rules head- ed by a verb. For each pair of verb tags, we com- puted the total variation distance between the multi- nomial distributions of CHILD rules headed by the two verb tags. We also computed the total variation distances between CHILD rules of verb tags in the baseline DMV model learned by EM.</p><p>In <ref type="figure" target="#fig_4">Figure 5</ref>, We report the differences between the total variation distances computed from our model and from the baseline. A positive value means the distance is reduced in our model compared with that in the baseline. It can be seen that overall the dis- tances between CHILD rules of different verb tags become smaller in our model. This verifies that our approach smooths the probabilities of rules involv- ing correlated POS tags. From the figure one can see that the distance that reduces the most is be- tween VBG and VBN. These two verb tags indeed have very similar syntactic behaviors and thus have similar embeddings as shown in <ref type="figure" target="#fig_3">figure 4</ref>. One the other hand, the distances between VB and VBZ/VBP become larger. This is reasonable since VB is syn- tactically different from VBZ/VBP in that it is very likely to generate a child tag TO to the left while VBZ/VBP always generate a subject (e.g., a noun or a pronoun) to the left.</p><formula xml:id="formula_6">V BP V BZ V BD V BN V B V BG V BP V BZ V</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a neural based DMV model to do unsu- pervised dependency parsing. Our approach learn- s neural networks with continuous representations of POS tags to predict the probabilities of grammar rules, thus automatically taking into account the cor- relations between POS tags. Our experiments show that our approach outperforms previous approaches utilizing POS correlations and is competitive with recent state-of-the-art approaches on nine different languages.</p><p>For future work, we plan to extend our approach in learning lexicalized DMV models. In addition, we plan to apply our approach to other unsupervised tasks such as word alignment and sentence cluster- ing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Structure of the neural network. Both CHILD and DECISION use the same architecture for the calculation of distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Learning procedure of our neural based DMV model. Green dashed lines represent the EM algorithm for learning traditional DMV. Red solid lines represent the learning procedure of our model. P represents the rule probabilities of DMV, E represents the expected counts of rules, and W represents the parameters of the neural networks. In the traditional EM algorithm, the expected counts are directly used to re-estimate the rule probabilities. In our approach, parameter re-estimation is divided into two steps: training the neural networks from the expected counts and forward evaluation of the neural networks to produce the rule probabilities.</figDesc><graphic url="image-51.png" coords="4,139.68,156.02,53.16,53.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Parsing accuracy vs. length of POS embedding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A visualization of the distances between embeddings of different POS tags.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The change of the total variation distances between probabilities of CHILD rules headed by different verb tags in our model vs. the baseline. A positive value means the distance is reduced in our model compared with that in the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1</head><label>1</label><figDesc></figDesc><table>Methods 

WSJ10 WSJ 
Standard EM 
46.2 
34.9 
Viterbi EM 
58.3 
39.4 
LN </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Comparisons of Approaches based on POS Correla-

tions 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison of recent unsupervised dependency pars-

ing systems. Basic setup means learning from POS tags with 

sentences of length ≤ 10 and punctuation stripped off. Extra 

information may contain punctuations, longer sentences, lexi-

cal information, etc. For Neural E-DMV, "Good Init" means 

using the learned DMV model from Tu and Honavar (2012) as 

our initialization. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Comparison between activation functions.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>DDA results (on all the sentences) on eight additional languages. Our neural based approaches are compared with 

traditional approaches using standard EM, softmax EM (parameterized by σ) and viterbi EM. 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Painless unsupervised learning with features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="582" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised induction of tree substitution grammars for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1204" to="1213" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="74" to="82" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Covariance in unsupervised learning of probabilistic grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3017" to="3051" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Logistic normal priors for unsupervised probabilistic grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Shay B Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised search-based structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Transition-based dependency parsing with stack long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.08075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Temporal restricted boltzmann machines for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="11" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal challenge on grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Gelling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Graça</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure</title>
		<meeting>the NAACL-HLT Workshop on the Induction of Linguistic Structure</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="64" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparsity in dependency grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 Conference Short Papers</title>
		<meeting>the ACL 2010 Conference Short Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="194" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving unsupervised dependency parsing with richer contexts and smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>William P Headden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The</title>
		<meeting>Human Language Technologies: The</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="101" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Corpusbased induction of syntactic structure: Models of dependency and constituency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">478</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised dependency parsing: Let&apos;s use supervised parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="651" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast unsupervised dependency parsing with arcstandard transitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sadegh Rasooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heshaam</forename><surname>Faili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Workshop on Unsupervised and Semi-Supervised Learning in NLP</title>
		<meeting>the Joint Workshop on Unsupervised and Semi-Supervised Learning in NLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From baby steps to leapfrog: How less is more in unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Valentin I Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="751" to="759" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Breaking out of local optima with count transforms and model recombination: A study in grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Valentin I Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1983" to="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing using recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unambiguity regularization for unsupervised learning of probabilistic grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasant</forename><surname>Honavar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1324" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
