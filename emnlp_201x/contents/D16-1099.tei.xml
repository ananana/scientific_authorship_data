<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Effects of Data Size and Frequency Range on Distributional Semantic Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><forename type="middle">Sahlgren</forename><surname>Gavagai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pisa</orgName>
								<address>
									<addrLine>Slussplan 9, via Santa Maria 36</addrLine>
									<postBox>Box 1263</postBox>
									<postCode>111 30, 164 29, 56126</postCode>
									<settlement>Stockholm, Kista, Pisa</settlement>
									<country>Sweden, Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sics</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pisa</orgName>
								<address>
									<addrLine>Slussplan 9, via Santa Maria 36</addrLine>
									<postBox>Box 1263</postBox>
									<postCode>111 30, 164 29, 56126</postCode>
									<settlement>Stockholm, Kista, Pisa</settlement>
									<country>Sweden, Italy</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
							<email>alessandro.lenci@unipi.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Pisa</orgName>
								<address>
									<addrLine>Slussplan 9, via Santa Maria 36</addrLine>
									<postBox>Box 1263</postBox>
									<postCode>111 30, 164 29, 56126</postCode>
									<settlement>Stockholm, Kista, Pisa</settlement>
									<country>Sweden, Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Effects of Data Size and Frequency Range on Distributional Semantic Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="975" to="980"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper investigates the effects of data size and frequency range on distributional semantic models. We compare the performance of a number of representative models for several test settings over data of varying sizes, and over test items of various frequency. Our results show that neural network-based models underperform when the data is small, and that the most reliable model over data of varying sizes and frequency ranges is the inverted fac-torized model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributional Semantic Models (DSMs) have be- come a staple in natural language processing. The various parameters of DSMs -e.g. size of con- text windows, weighting schemes, dimensionality reduction techniques, and similarity measures - have been thoroughly studied ( <ref type="bibr" target="#b20">Weeds et al., 2004;</ref><ref type="bibr" target="#b18">Sahlgren, 2006;</ref><ref type="bibr" target="#b15">Riordan and Jones, 2011;</ref><ref type="bibr" target="#b3">Bullinaria and Levy, 2012;</ref><ref type="bibr" target="#b11">Levy et al., 2015)</ref>, and are now well understood. The impact of various pro- cessing models -matrix-based models, neural net- works, and hashing methods -have also enjoyed considerable attention lately, with at times conflict- ing conclusions ( <ref type="bibr" target="#b11">Levy et al., 2015;</ref><ref type="bibr" target="#b19">Schnabel et al., 2015;</ref><ref type="bibr" target="#b14">¨ Osterlund et al., 2015;</ref><ref type="bibr">Sahlgren et al., 2016</ref>). The consensus interpretation of such experiments seems to be that the choice of processing model is less important than the parame- terization of the models, since the various processing models all result in more or less equivalent DSMs (provided that the parameterization is comparable).</p><p>One of the least researched aspects of DSMs is the effect on the various models of data size and frequency range of the target items. The only pre- vious work in this direction that we are aware of is <ref type="bibr" target="#b0">Asr et al. (2016)</ref>, who report that on small data (the CHILDES corpus), simple matrix-based mod- els outperform neural network-based ones. Unfor- tunately, Asr et al. do not include any experiments using the same models applied to bigger data, mak- ing it difficult to compare their results with previous studies, since implementational details and parame- terization will be different.</p><p>There is thus still a need for a consistent and fair comparison of the performance of various DSMs when applied to data of varying sizes. In this pa- per, we seek an answer to the question: which DSM should we opt for if we only have access to lim- ited amounts of data? We are also interested in the related question: which DSM should we opt for if our target items are infrequent? The latter ques- tion is particularly crucial, since one of the major as- sets of DSMs is their applicability to create seman- tic representations for ever-expanding vocabularies from text feeds, in which new words may continu- ously appear in the low-frequency ranges.</p><p>In the next section, we introduce the contend- ing DSMs and the general experiment setup, before turning to the experiments and our interpretation of the results. We conclude with some general advice.</p><p>to gain an understanding of the effect of data size and frequency range on the various models, we fo- cus primarily on the differences in processing mod- els, hence the following typology of DSMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explicit matrix models</head><p>We here include what could be referred to as ex- plicit models, in which each vector dimension cor- responds to a specific context ( <ref type="bibr" target="#b10">Levy and Goldberg, 2014</ref>). The baseline model is a simple co-occurrence matrix F (in the following referred to as CO for Co- Occurrence). We also include the model that results from applying Positive Pointwise Mutual Informa- tion (PPMI) to the co-occurrence matrix. PPMI is de- fined as simply discarding any negative values of the PMI, computed as:</p><formula xml:id="formula_0">PMI(a, b) = log f ab × T f a f b (1)</formula><p>where f ab is the co-occurrence count of word a and word b, f a and f b are the individual frequencies of the words, and T is the number of tokens in the data. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Factorized matrix models</head><p>This type of model applies an additional factor- ization of the weighted co-occurrence counts. We here include two variants of applying Singular Value Decomposition (SVD) to the PPMI-weighting co- occurrence matrix; one version that discards all but the first couple of hundred latent dimensions (TSVD for truncated SVD), and one version that instead re- moves the first couple of hundred latent dimensions (ISVD for inverted SVD). SVD is defined in the stan- dard way:</p><formula xml:id="formula_1">F = U ΣV T (2)</formula><p>where U holds the eigenvectors of F , Σ holds the eigenvalues, and V ∈ U (w) is a unitary matrix map- ping the original basis of F into its eigenbasis. Since V is redundant due to invariance under unitary trans- formations, we can represent the factorization ofˆFofˆ ofˆF in its most compact formˆFformˆ formˆF ≡ U Σ.</p><p>1 We also experimented with smoothed PPMI, which raises the context counts to the power of α and normalizes them ( <ref type="bibr" target="#b11">Levy et al., 2015)</ref>, thereby countering the tendency of mutual infor- mation to favor infrequent events:</p><formula xml:id="formula_2">f (b) = #(b) α b #(b) α , but it did not lead to any consistent improvements compared to PPMI.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hashing models</head><p>A different approach to reduce the dimensionality of DSMs is to use a hashing method such as Random Indexing (RI) ( <ref type="bibr" target="#b8">Kanerva et al., 2000</ref>), which accumu- lates distributional vectors d(a) in an online fashion:</p><formula xml:id="formula_3">d(a) ← d(a i )+ c j=−c,j =0 w(x (i+j) )π j r(x (i+j) ) (3)</formula><p>where c is the extension of the context window, w(b) is a weight that quantifies the importance of context term b, 2 r d (b) is a sparse random index vector that acts as a fingerprint of context term b, and π j is a per- mutation that rotates the random index vectors one step to the left or right, depending on the position of the context items within the context windows, thus enabling the model to take word order into account ( <ref type="bibr" target="#b16">Sahlgren et al., 2008)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural network models</head><p>There are many variations of DSMs that use neural networks as processing model, ranging from simple recurrent networks <ref type="bibr" target="#b5">(Elman, 1990</ref>) to more complex deep architectures <ref type="bibr" target="#b4">(Collobert and Weston, 2008)</ref>. The incomparably most popular neural network model is the one implemented in the word2vec li- brary, which uses the softmax for predicting b given a (Mikolov et al., 2013):</p><formula xml:id="formula_4">p(b|a) = exp( b · a) b ∈C exp( b · a) (4)</formula><p>where C is the set of context words, and b and a are the vector representations for the context and target words, respectively. We include two versions of this general model; Continuous Bag of Words (CBOW) that predicts a word based on the context, and Skip- Gram Negative Sampling (SGNS) that predicts the context based on the current word.</p><p>varying sizes, we use one big corpus as starting point, and split the data into bins of varying sizes. We opt for the ukWaC corpus <ref type="bibr" target="#b6">(Ferraresi et al., 2008)</ref>, which comprises some 1.6 billion words after to- kenization and lemmatization. We produce sub- corpora by taking the first 1 million, 10 million, 100 million, and 1 billion words.</p><p>Since the co-occurrence matrix built from the 1 billion-word ukWaC sample is very big (more than 4,000,000 × 4,000,000), we prune the co- occurrence matrix to 50,000 dimensions before the factorization step by simply removing infrequent context items. <ref type="bibr">3</ref> As comparison, we use 200 di- mensions for TSVD, 2,800 (3,000-200) dimensions for ISVD, 2,000 dimensions for RI, and 200 dimen- sions for CBOW and SGNS. These dimensionalities have been reported to perform well for the respec- tive models <ref type="bibr" target="#b9">(Landauer and Dumais, 1997;</ref><ref type="bibr" target="#b16">Sahlgren et al., 2008;</ref><ref type="bibr" target="#b13">Mikolov et al., 2013;</ref><ref type="bibr" target="#b14">¨ Osterlund et al., 2015)</ref>. All DSMs use the same parameters as far as possible with a narrow context window of ±2 words, which has been shown to produce good results in se- mantic tasks <ref type="bibr" target="#b18">(Sahlgren, 2006;</ref><ref type="bibr" target="#b3">Bullinaria and Levy, 2012)</ref>.</p><p>We use five standard benchmark tests in these experiments; two multiple-choice vocabulary tests (the TOEFL synonyms and the ESL synonyms), and three similarity/relatedness rating benchmarks (SimLex-999 (SL) ( <ref type="bibr" target="#b7">Hill et al., 2015</ref>), MEN ( <ref type="bibr" target="#b2">Bruni et al., 2014)</ref>, and Stanford Rare Words (RW) ( <ref type="bibr" target="#b12">Luong et al., 2013)</ref>). The vocabulary tests measure the syn- onym relation, while the similarity rating tests mea- sure a broader notion of semantic similarity (SL and RW) or relatedness (MEN). <ref type="bibr">4</ref> The results for the vo- cabulary tests are given in accuracy (i.e., percentage of correct answers), while the results for the similar- ity tests are given in Spearman rank correlation.  <ref type="figure">Figure 1</ref>: Average results and standard deviation over all tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Comparison by data size</head><p>lion and 100 million-word data, and is competitive with SGNS on the 1 billion word data. <ref type="figure">Figure 1</ref> shows the average results and their standard devi- ations over all test settings. 5 It is obvious that there are no huge differences between the various models, with the exception of the baseline CO model, which consistently underperforms. The TSVD and RI mod- els have comparable performance across the differ- ent data sizes, which is systematically lower than the PPMI model. The ISVD model is the most consis- tently good model, with the neural network-based models steadily improving as data becomes bigger.</p><p>Looking at the different datasets, SL and RW are the hardest ones for all the models. In the case of SL, this confirms the results in ( <ref type="bibr" target="#b7">Hill et al., 2015)</ref>, and might be due to the general bias of DSMs to- wards semantic relatedness, rather than genuine se- mantic similarity, as represented in SL. The substan- dard performance on RW might instead be due to the low frequency of the target items. It is interesting to note that these are benchmark tests in which neural models perform the worst even when trained on the largest data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Comparison by frequency range</head><p>In order to investigate how each model handles dif- ferent frequency ranges, we split the test items into three different classes that contain about a third of the frequency mass of the test items each. This split was produced by collecting all test items into a common vocabulary, and then sorting this vo- cabulary by its frequency in the ukWaC 1 billion- word corpus. We split the vocabulary into 3 equally large parts; the HIGH range with frequencies rang- ing from 3,515,086 ("do") to 16,830 ("organism"), the MEDIUM range with frequencies ranging be- tween 16,795 ("desirable") and 729 ("prickly"), and the LOW range with frequencies ranging between 728 ("boardwalk") to hapax legomenon. We then split each individual test into these three ranges, de- pending on the frequencies of the test items. Test pairs were included in a given frequency class if and only if both the target and its relatum occur in the frequency range for that class. For the constituent words in the test item that belong to different fre- quency ranges, which is the most common case, we use a separate MIXED class. The resulting four classes contain 1,387 items for the HIGH range, 656 items for the MEDIUM range, 350 items for the LOW range, and 3,458 items for the MIXED range. <ref type="bibr">6</ref> Table 2 (next side) shows the average results over the different frequency ranges for the various DSMs trained on the 1 billion-word ukWaC data. We also include the highest and lowest individual test scores (signified by ↑ and ↓), in order to get an idea about the consistency of the results. As can be seen in the table, the most consistent model is ISVD, which produces the best results in both the MEDIUM and MIXED frequency ranges. The neural net- work models SGNS and CBOW produce the best re- sults in the HIGH and LOW range, respectively, with CBOW clearly outperforming SGNS in the lat- ter case. The major difference between these mod- els is that CBOW predicts a word based on a con- text, while SGNS predicts a context based on a word. Clearly, the former approach is more beneficial for low-frequent items.</p><p>The PPMI, TSVD and RI models perform simi- larly across the frequency ranges, with RI produc- ing somewhat lower results in the MEDIUM range, and TSVD producing somewhat lower results in the LOW range. The CO model underperforms in all frequency ranges. Worth noting is the fact that all models that are based on an explicit matrix (i.e. CO,  PPMI, TSVD and ISVD) produce better results in the MEDIUM range than in the HIGH range.</p><p>The arguably most interesting results are in the LOW range.</p><p>Unsurprisingly, there is a gen- eral and significant drop in performance for low frequency items, but with interesting differences among the various models. As already mentioned, the CBOW model produces the best results, closely followed by PPMI and RI. It is noteworthy that the low-dimensional embeddings of the CBOW model only gives a modest improvement over the high- dimensional explicit vectors of PPMI. The worst re- sults are produced by the ISVD model, which scores even lower than the baseline CO model. This might be explained by the fact that ISVD removes the la- tent dimensions with largest variance, which are ar- guably the most important dimensions for very low- frequent items. Increasing the number of latent di- mensions with high variance in the ISVD model im- proves the results in the LOW range (16.59 when removing only the top 100 dimensions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Our experiments confirm the results of <ref type="bibr" target="#b0">Asr et al. (2016)</ref>, who show that neural network-based models are suboptimal to use for smaller amounts of data. On the other hand, our results also show that none of the standard DSMs work well in situations with small data. It might be an interesting novel re- search direction to investigate how to design DSMs that are applicable to small-data scenarios.</p><p>Our results demonstrate that the inverted factor- ized model (ISVD) produces the most robust results over data of varying sizes, and across several dif- ferent test settings. We interpret this finding as fur- ther corroborating the results of <ref type="bibr" target="#b3">Bullinaria and</ref><ref type="bibr">Levy (2012), and¨Osterlundand¨ and¨Osterlund et al. (2015)</ref>, with the con- clusion that the inverted factorized model is a robust competitive alternative to the widely used SGNS and CBOW neural network-based models.</p><p>We have also investigated the performance of the various models on test items in different frequency ranges, and our results in these experiments demon- strate that all tested models perform optimally in the medium-to-high frequency ranges. Interestingly, all models based on explicit count matrices (CO, PPMI, TSVD and ISVD) produce somewhat better results for items of medium frequency than for items of high frequency. The neural network-based models and ISVD, on the other hand, produce the best results for high-frequent items.</p><p>None of the tested models perform optimally for low-frequent items. The best results for low- frequent test items in our experiments were pro- duced using the CBOW model, the PPMI model and the RI model, all of which uses weighted context items without any explicit factorization. By contrast, the ISVD model underperforms significantly for the low-frequent items, which we suggest is an effect of removing latent dimensions with high variance.</p><p>This interpretation suggests that it might be inter- esting to investigate hybrid models that use different processing models -or at least different parame- terizations -for different frequency ranges, and for different data sizes. We leave this as a suggestion for future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 summarizes the results over the different test settings. The most notable aspect of these results</head><label>1</label><figDesc></figDesc><table>DSM 
TOEFL 
ESL 
SL MEN 
RW 
1 million words 

CO 

17.50 20.00 −1.64 10.72 −3.96 

PPMI 

26.25 18.00 
8.28 21.49 −2.57 

TSVD 

27.50 20.00 
4.43 22.15 −1.56 

ISVD 

22.50 14.00 14.33 19.74 
5.31 

RI 

20.00 16.00 
5.65 17.94 
1.92 

SGNS 

15.00 
8.00 
3.64 12.34 
1.46 

CBOW 

15.00 10.00 −0.16 11.59 
1.39 
10 million words 

CO 

40.00 22.00 
4.77 15.20 
0.95 

PPMI 

52.50 38.00 26.44 39.83 
4.00 

TSVD 

38.75 30.00 19.27 34.33 
5.53 

ISVD 

45.00 44.00 30.19 44.21 
9.88 

RI 

47.50 24.00 20.44 34.56 
3.32 

SGNS 

43.75 42.00 28.30 26.59 
2.38 

CBOW 

40.00 30.00 22.22 28.33 
3.04 
100 million words 

CO 

45.00 30.00 10.00 19.36 
3.12 

PPMI 

66.25 54.00 33.75 46.74 15.05 

TSVD 

46.25 34.00 25.11 42.49 13.00 

ISVD 

66.25 66.00 40.98 54.55 21.27 

RI 

55.00 48.00 32.31 45.71 10.15 

SGNS 

65.00 58.00 40.75 52.83 11.73 

CBOW 

61.25 46.00 36.15 48.30 15.62 
1 billion words 

CO 

55.00 40.00 11.85 21.83 
6.82 

PPMI 

71.25 54.00 35.69 52.95 24.29 

TSVD 

56.25 46.00 31.36 52.05 13.35 

ISVD 

71.25 66.00 44.77 60.11 28.46 

RI 

61.25 50.00 35.35 50.51 18.58 

SGNS 

76.25 66.00 41.94 67.03 24.50 

CBOW 

75.00 56.00 38.31 59.84 22.80 

Table 1: Results for DSMs trained on data of varying sizes. 

is that the neural networks models do not produce 
competitive results for the smaller data, which cor-
roborates the results by Asr et al. (2016). The best 
results for the smallest data are produced by the fac-
torized models, with both TSVD and ISVD produc-
ing top scores in different test settings. It should 
be noted, however, that even the top scores for the 
smallest data set are substandard; only two models 
(PPMI and TSVD) manage to beat the random base-
line of 25% for the TOEFL tests, and none of the 
models manage to beat the random baseline for the 
ESL test. 

The ISVD model produces consistently good re-
sults; it yields the best overall results for the 10 mil-

977 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Average results for DSMs over four different frequency ranges for the items in the TOEFL, ESL, SL, MEN, and RW tests.</head><label>2</label><figDesc></figDesc><table>All DSMs are trained on the 1 billion words data. 

</table></figure>

			<note place="foot" n="2"> Distributional Semantic Models One could classify DSMs in many different ways, such as the type of context and the method to build distributional vectors. Since our main goal here is</note>

			<note place="foot" n="3"> Experiment setup Since our main focus in this paper is the performance of the above-mentioned DSMs on data of 2 We use w(b) = e −λ· f (b) V where f (b) is the frequency of context item b, V is the total number of unique context items seen thus far (i.e. the current size of the growing vocabulary), and λ is a constant that we set to 60 (Sahlgren et al., 2016).</note>

			<note place="foot" n="3"> Such drastic reduction has a negative effect on the performance of the factorized methods for the 1 billion word data, but unfortunately is necessary for computational reasons. 4 It is likely that the results on the similarity tests could be improved by using a wider context window, but such improvement would probably be consistent across all models, and is thus outside the scope of this paper.</note>

			<note place="foot" n="5"> Although rank correlation is not directly comparable with accuracy, they are both bounded between zero and one, which means we can take the average to get an idea about overall performance.</note>

			<note place="foot" n="6"> 233 test terms did not occur in the 1 billion-word corpus.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>This research was supported by the Swedish Re-search Council under contract 2014-28199.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Comparing predictive and co-occurrence based models of lexical semantics trained on child-directed speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatemeh</forename><surname>Asr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Willits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CogSci</title>
		<meeting>CogSci</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and svd. Behavior Research Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bullinaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">P</forename><surname>Levy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="890" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Introducing and evaluating ukwac, a very large web-derived corpus of english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><surname>Ferraresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eros</forename><surname>Zanchetta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bernardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WAC-4</title>
		<meeting>WAC-4</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="665" to="695" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Random indexing of text samples for latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pentti</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kristofersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Holst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CogSci</title>
		<meeting>CogSci</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page">1036</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A solution to platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="240" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Linguistic regularities in sparse and explicit word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Factorization of latent variables in distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvid¨osterlundarvid¨</forename><surname>Arvid¨osterlund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David¨odlingdavid¨</forename><surname>David¨odling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="227" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Redundancy in perceptual and linguistic experience: Comparing feature-based and distributional models of semantic representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Riordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">N</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="345" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Permutations as a means to encode order in word space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Holst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pentti</forename><surname>Kanerva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CogSci</title>
		<meeting>CogSci</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1300" to="1305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Jussi Karlgren, Fredrik Olsson, Per Persson, and Akshay Viswanathan. 2016. The Gavagai Living Lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaru</forename><surname>Cuba Gyllensten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Espinoza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ola</forename><surname>Hamfors</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Holst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The Word-Space Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>Stockholm University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Phd thesis</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evaluation methods for unsupervised word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="298" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Characterising measures of lexical distributional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1015" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
