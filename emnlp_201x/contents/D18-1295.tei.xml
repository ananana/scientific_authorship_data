<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sanskrit Word Segmentation Using Character-level Recurrent and Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Hellwig</surname></persName>
							<email>hellwig7@gmx.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">SFB 991</orgName>
								<orgName type="department" key="dep2">Center for Buddhist Studies</orgName>
								<orgName type="institution" key="instit1">University of Düsseldorf IVS</orgName>
								<orgName type="institution" key="instit2">University of Zurich</orgName>
								<orgName type="institution" key="instit3">University of Hamburg</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nehrdich</surname></persName>
							<email>nehrdbsd@googlemail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">SFB 991</orgName>
								<orgName type="department" key="dep2">Center for Buddhist Studies</orgName>
								<orgName type="institution" key="instit1">University of Düsseldorf IVS</orgName>
								<orgName type="institution" key="instit2">University of Zurich</orgName>
								<orgName type="institution" key="instit3">University of Hamburg</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sanskrit Word Segmentation Using Character-level Recurrent and Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2754" to="2763"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2754</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The paper introduces end-to-end neural network models that tokenize Sanskrit by jointly splitting compounds and resolving phonetic merges (Sandhi). Tokenization of Sanskrit depends on local phonetic and distant semantic features that are incorporated using convo-lutional and recurrent elements. Contrary to most previous systems, our models do not require feature engineering or extern linguistic resources, but operate solely on parallel versions of raw and segmented text. The models discussed in this paper clearly improve over previous approaches to Sanskrit word segmen-tation. As they are language agnostic, we will demonstrate that they also outperform the state of the art for the related task of German compound splitting.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sanskrit is an Indo-Aryan language that served as lingua franca for the religious, scientific and liter- ary communities of ancient India. Text production in Sanskrit started in the 2. millenium BCE and has continued until today. 1 A 19th century cata- loguing project recorded more than 40,000 San- skrit texts known at that time <ref type="bibr">(Aufrecht, 1891</ref><ref type="bibr">(Aufrecht, - 1903</ref>, which covers only a small part of the extant Sanskrit literature. Apart from the oldest Vedic texts, Sanskrit has little diachronic variation on the morphological level, because it was regularized by the grammarian P¯ an . ini in the 3rd c. BCE.</p><p>NLP of Sanskrit is challenging due to com- pounding (see Ex. 1) and the phonetic processes called Sandhi <ref type="bibr">('connection'; see Ex. 2-5)</ref>. Com- pounding is widely used in other languages, and NLP has developed methods for analyzing com- pounds <ref type="bibr" target="#b30">(Macherey et al., 2011;</ref>. In Sanskrit, however, syntactic co-and subordination tend to be diachronically replaced by compound- ing <ref type="bibr" target="#b26">(Lowe 2015</ref>; see also Sec. 3), so that many sentences in later literature consist only of a few long compounds that are loosely connected by a semantically light verb or an (optional) copula, as shown in this example:</p><p>(1) ¯ a´srayabh¯a´srayabh¯ utakh¯ adikathanena foundation-become-air-etc.-mentioning "(Something is described) by mentioning air etc. that have become <ref type="bibr">[its]</ref> foundations."</p><p>The term Sandhi denotes a set of phonetic pro- cesses by which the contact phonemes of neigh- boring word tokens are changed and merged, and which create unseparated strings spanning multi- ple tokens <ref type="bibr">(Whitney, 1879)</ref>. Sandhi occurs be- tween adjacent vowels <ref type="bibr">(vocalic Sandhi; Ex. 2)</ref>, between consonants and vowels ( <ref type="bibr">Ex. 4)</ref> and be- tween adjacent consonants (Ex. 5):</p><p>(2) r¯ aj¯ a+uv¯ aca 'the king said' In addition, Sandhi occurs between independent inflected words <ref type="bibr">(Ex. 2-5)</ref> as well as between members of compounds. <ref type="bibr">2</ref> Because different com- binations of unsandhied phonemes can result in the same surface phoneme, Sandhi resolution is non-deterministic and depends on the semantic context of the sentence (see <ref type="bibr">Ex.</ref> 3 for a morpho- logically and lexically valid, but semantically dis- preferred reading of the string r¯ ajov¯ aca).</p><p>Scriptorial and editorial conventions further complicate the analysis of compounds and Sandhi. While most Indian manuscripts don't insert spaces between strings, modern editors use spaces a gusto. Moreover, the (correct) application of Sandhi is not followed by all authors and editors to the same extent, so that the unsandhied tokens tat hi asti ('as this is . . . ') can occur as taddhyasti, taddhy asti, tad dhy asti, tad dhyasti or even tat hi asti (unchanged).</p><p>Our models aim at transforming a given sen- tence into a sequence of unsandhied tokens. We refer to this task as Sanskrit word splitting (SWS), and subsume Sandhi and compounding phenom- ena under the common term splits. We address SWS by using a combination of convolutional and recurrent elements. The recurrent elements inte- grate sentence level information that leads to qual- ified decisions about the semantic meaningfulness of possible compound and Sandhi splits (see Ex. 2 and 3), while the convolutional elements are meant to replace n-gram extraction, which is frequently used in word segmentation architectures. As our models operate on the character level, SWS can be formulated in a sequence labeling framework.</p><p>Consequently, this paper has three main contri- butions:</p><p>1. We introduce novel character-based models for SWS that beat state of the art models by large margins.</p><p>2. We compare against sequence-to-sequence models and demonstrate that our models work on par with them, but need significantly less time for training and inference.</p><p>3. We publish a new dataset for Sanskrit word splitting that consists of more than 560,000 sentences with manually validated splits. The dataset and the code are released at https: //github.com/OliverHellwig/ sanskrit/papers/2018emnlp.</p><p>In the rest of this paper, we use the following ter- minology. A token is an unsandhied word that is not itself a compound. A string is a sequence of characters that is delimited by a space or a dan . d . a.</p><p>Each string contains at least one token, at least one compound (that itself consists of at least two to- kens) or a Sandhied mixture of both. A sentence is a piece of Sanskrit text that is terminated by the punctuation mark called dan . d . a "stick" (|) and con- sists of at least one string. Any sentence can con- sist of multiple independent clauses, which are not demarcated by punctuation in Sanskrit, or consist of a part of a larger clause only. The paper proceeds as follows: Section 2 gives an overview of related work in NLP. Section 3 in- troduces our SWS dataset. Section 4 describes the sequence labeling models developed for this pa- per and three baseline systems, whose evalution is presented in Sec. 5. Section 6 summarizes the pa- per.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Research</head><p>Most NLP systems for SWS combine P¯ an . inis phonetic and morphological rules with a lexi- cal resource, either by using formal <ref type="bibr" target="#b17">(Huet, 2005;</ref><ref type="bibr" target="#b9">Goyal et al., 2009;</ref><ref type="bibr" target="#b24">Kulkarni and Shukla, 2009)</ref> or statistical methods, including Dirichlet processes ( <ref type="bibr" target="#b33">Natarajan and Charniak, 2011</ref>), finite state meth- ods <ref type="bibr" target="#b31">(Mittal, 2010)</ref>, graph queries ( <ref type="bibr" target="#b22">Krishna et al., 2016)</ref> and hybrid systems <ref type="bibr" target="#b15">(Hellwig, 2015a)</ref>.</p><p>A number of recent papers approaches SWS with deep learning models. Hellwig (2015b) splits isolated strings by applying a one-layer bidirec- tional LSTM to two parallel character based rep- resentations of a string. The restriction to isolated strings is problematic, because SWS relies on the grammatical and semantic context of the full sen- tence in many cases. Restricting a model to iso- lated strings ignores these linguistic clues. <ref type="bibr" target="#b34">Reddy et al. (2018)</ref> formulate SWS as a trans- lation task on the sentence level. They transform surface and unsandhied sentences using the sen- tencepiece model and "translate" the surface into the unsandhied sentence using a seq2seq model with attention.  use an encoder-decoder architecture with a global atten- tion mechanism and apply their model to iso- lated strings from a small dataset ( <ref type="bibr" target="#b1">Bhardwaj et al., 2018)</ref>. So far, no direct comparison of deep learn- ing models for SWS has been done, because the authors used different, partly unpublished datasets and reported performance on different linguistic levels (sentence, string) and with different evalu- ation methods. We will therefore try to make a fair and comprehensive comparison with the state of the art in Sec. 5.</p><p>SWS is closely related to word segmenta- tion for other Asian languages such as Thai ( <ref type="bibr" target="#b12">Haruechaiyasak et al., 2008)</ref>, Chinese or Japanese (Kanji), with most research being done for Chi- nese and Japanese. Contrary to Sanskrit, Chinese and Japanese don't exhibit Sandhi phenomena and their logographic scripts condense information, making it possible to use "word-level" CRFs on the output, for example. <ref type="bibr" target="#b5">Chen et al. (2015)</ref> in- terpret Chinese word segmentation (CWS) as a sequence labeling task and evaluate a range of (stacked) bidirectional recurrent architectures that are combined with a final sentence level likelihood layer <ref type="bibr" target="#b6">(Collobert et al., 2011</ref>) maximizing the tran- sition score of the BMES encoded target sequence. Their best model uses a single layer bidirectional LSTM with bigrams of pre-trained character em- beddings as inputs. <ref type="bibr" target="#b4">Cai and Zhao (2016)</ref> deal with CWS by first forming word hypotheses from characters using a gated unit and then process- ing the word hypotheses with an LSTM-based lan- guage model. They minimize the combined word and sentence level scores using a structured mar- gin loss and achieve better performance than <ref type="bibr" target="#b5">Chen et al. (2015)</ref> on standard CWS datasets. <ref type="bibr" target="#b20">Kitagawa and Komachi (2017)</ref> adapt the model proposed by <ref type="bibr" target="#b5">Chen et al. (2015)</ref> for Japanese word splitting, but use characters, character n-grams and lexicon- based word boundary features as inputs. The au- thors report state of the art performance, but ob- serve a clear drop in the F score of their model, when texts contain a high proportion of Hiragana characters and thus come closer to syllabic or al- phabetic scripts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>Several datasets for SWS have been published in the last years. While the dataset of <ref type="bibr" target="#b1">Bhardwaj et al. (2018)</ref> may be too small and unvaried for train- ing deep learning models, <ref type="bibr" target="#b23">Krishna et al. (2017)</ref> re- analyze 560,000 sentences from the Digital Cor- pus of Sanskrit (DCS) 3 using the Sanskrit Her- itage Reader ( <ref type="bibr" target="#b10">Goyal and Huet, 2016)</ref>. Re-analysis is necessary, because the DCS stores the morpho- lexical analysis of strings, but does not record split points and Sandhi rules applied. Due to dif- ferent linguistic choices (P¯ an . inian vs. corpus- oriented) and to different ideas about the (non- )compositional meanings of compounds their final dataset contains only 115,000 sentences (see the discussion in <ref type="bibr" target="#b23">Krishna et al. 2017</ref> and the analysis in Sec. 3.1). As the size of the dataset is crucial for Surface r ¯ a j o v ¯ a c a Unsandhied r ¯ a j ¯ a-u v ¯ a c a <ref type="table">Table 1</ref>: Data extracted from the string r¯ ajov¯ aca, which is split into the two tokens r¯ aj¯ a ("king") and uv¯ aca ("(he, she) said"; see Ex. 2).</p><p>most deep learning methods, we decided to release a new dataset along with this paper. Each sentence contained in the DCS is re-analyzed using the San- skritTagger software <ref type="bibr" target="#b14">(Hellwig, 2009)</ref>. Our dataset contains the surface forms of sentences in the DCS and the split points and Sandhi rules that the tag- ger proposes for their morpho-lexical gold analy- ses stored in the DCS. We didn't differentiate be- tween compound and inter-word splits, as this dis- tinction introduces morphological categories into the dataset. <ref type="table">Table 1</ref> shows an example of the an- notation format. <ref type="table" target="#tab_1">Table 2</ref> shows the statistics of our dataset, split by text genres (first column). The dataset contains 2,978,509 strings and 4,171,682 tokens in 561,596 sentences. Most sentences come from the Epic and scientific (medicine, alchemy, astronomy) do- main. While Epic texts are mostly written in easy, plain Sanskrit, the scientific works use many un- common terms (likely to reoccur in the lexico- graphic domain) and long compounds. Sentence length is higher in the prose subcorpora (Buddhist, Vedic prose, ritualistic texts).</p><p>The fourth column shows that split phenomena are frequent in Sanskrit, occurring for more than 8% of all characters. Columns 5 and 6 report the proportions of complicated splits in relation to all splits. While 15% of all splits are resolved into a vocalic Sandhi, compound breaks are the dom- inant split type, which is also responsible for the majority of errors and ambiguities (see Sections 3.1 and 5.2). The last column also reflects the di- achronic development from earlier texts with lim- ited compounding (Vedic, ritualistic and Dharma texts) towards classical Sanskrit, which shows a strong preference for compounding. We use a fixed split of 90% of the sentences for training, a development set of 5% for parameter optimization and 5% for testing.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Quality of the training data</head><p>The dataset released by <ref type="bibr" target="#b23">Krishna et al. (2017)</ref> and the one released with this paper both build on the DCS as gold standard. As this corpus was curated by a single user and the project never released a proper annotation guideline, one may suspect that it contains a certain level of inconsistencies and errors that influence the quality of the models and impose an upper limit for the model accuracy. In order to estimate the size of these effects, the authors of this paper independently corrected the analyses of 50 sentences randomly drawn from the training set (250 words, 2,354 characters includ- ing spaces). The corrections made by the authors differed at 23 character positions, corresponding to 20 strings in 15 sentences. 16 of these dif- ferences concerned compound splits, where the authors disagreed about the (non-)compositional meaning of compounds. A good example for such a disagreement is the string r¯ ajayoga, which was split as r¯ aja-yoga "king-Yoga" = "Yoga of a king" by one author (compositional reading), but left un- changed as the name of a school of Yoga by the other one (non-compositional reading). After ad- judicating these disagreements, there remain 5 of 250 strings with annotation errors in the training data, which corresponds to an error level of 2% of all strings and 0.2% of all characters for this sam- ple.</p><p>We further explored the effect of composition- ality by independently splitting 56 sentences of the Buddhist treatise Trim . ´ sik¯ avijñaptibh¯ as . ya, which is not part of the DCS. As the text uses highly tech- nical terminology, the degree of disagreement can be expected to be higher than for plain narrative texts. We adjudicated our Sandhi annotations, but kept conflicts in compound splitting unresolved. 94.5% of all strings (394 of 417) and 69.7% of all sentences obtained the same compound analy- sis by both authors. Again, the majority of differ- ences (11 of 23) showed up when a compound can have a non-compositional meaning that is closely connected with its compositional reading. Evalua- tion will show that these cases are responsible for a large parts of the model errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Input Features</head><p>The character based models are trained with em- beddings of the indidual surface characters, which are initialized with uniform random values from [−1, +1] and updated during training. Follow- ing <ref type="bibr" target="#b20">Kitagawa and Komachi (2017)</ref>, the input can be enriched with multinomial split probabilities that are built from the training data. When the training data contain a split rule for surface character t i at position i, we extract left (g L i,n ) and right (g R i,n ) character n-grams with lengths n ∈ <ref type="bibr">[2,</ref><ref type="bibr">7]</ref> 4 that end/start at position i, so that g L i,n = {t i−n+1 , t i−n+2 , . . . t i } and g R i,n = {t i , t i+1 , . . . t i+n−1 }. Counts #(.) for individual n-grams are accumulated over the whole training set. At training and test time, a vector v p ∈ R 2·(7−2+1)=12 is assigned to each character posi- tion. Its element corresponding to the left n-gram of length 2, for example, is calculated as</p><formula xml:id="formula_0">v p (L, 2) = #(g L i,2 ) max #(g L * ,2 )<label>(6)</label></formula><p>We evaluate the influence of split probabilities in the ablation study (Sec. 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Extern Models for Comparison</head><p>We compare our models against the following baselines:</p><p>Bidirectional RNN We re-implement the model described in Hellwig (2015b), but apply it to full sentences instead of isolated strings. Character embeddings are fed into a bidirectional recurrent layer with LSTM units. The output of the re- current layer is additionally regularized by us- ing dropout ( <ref type="bibr" target="#b38">Srivastava et al., 2014)</ref>, and classi- fication is performed using softmax with cross- entropy loss. We decode the output of the softmax in a greedy fashion without considering interac- tions between adjacent output classes.</p><p>seq2seq We retrain the model described in <ref type="bibr" target="#b34">Reddy et al. (2018)</ref> with our data after pre- processing them with the unsupervised text to- kenizer sentencepiece (Schuster and Nakajima, 2012). 5</p><p>Transformer As an alternative to recurrency based seq2seq, we apply the model described in <ref type="bibr" target="#b40">Vaswani et al. (2017)</ref> to the input pre-processed with sentencepiece. This model relies entirely on an attention mechanism to draw global dependen- cies between input and output. To our best knowl- edge, this is the first time that this model has been used for SWS. We use the publicy available imple- mentation tensor2tensor. 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Models Combining RNN and CNN</head><p>Convolutional Element Combinations of recur- rent and convolutional ( <ref type="bibr" target="#b25">LeCun et al., 1998</ref>) ele- ments are effective for tasks where complex local features are extracted by the convolutional element and then considered in larger contexts by the re- current element (and vice versa; see <ref type="bibr" target="#b3">Bjerva et al. 2016</ref> or Ma and Hovy 2016). We use convolu- tional features c i as proposed by <ref type="bibr" target="#b18">Kim (2014)</ref>. Let w denote the width of the input matrix X of the convolution (= number of time steps), h its height, n the width of the convolutional filter f n ∈ R n×h , σ(.) a non-linearity (Rectified Linear Units <ref type="bibr" target="#b32">(Nair and Hinton, 2010</ref>) in this paper) and b a bias. A convolutional feature at character position i and for filter j is defined as:</p><formula xml:id="formula_1">c n ij = σ(f n j · X [i:i+n−1, * ] + b)<label>(7)</label></formula><p>The feature map c n i for m different filters is formed by concatenating the convolutional fea- tures (c n i = [c n i1 , c n i2 , . . . c n im ]) and the output c of the convolutional element is formed by concate- nating the feature maps (c i = c 1 i ⊕ c 3 i ⊕ . . .). We use use odd filter widths only to avoid problems with patch alignment. We tested convolution with small quadratic filters as used in image convolu- tion as well as other methods for combining the learned filter such as averaging, addition or max- pooling of the stacked filters, but did not observe improved performance on the dev set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model 1: Convolution → Recurrency (crNN)</head><p>As an alternative to n-gram extraction <ref type="bibr" target="#b5">(Chen et al., 2015;</ref><ref type="bibr" target="#b20">Kitagawa and Komachi, 2017)</ref>, a convolu- tional element is applied to the character embed- dings (see <ref type="figure">Fig. 1a</ref>). Its outputs (Eq. 7) are fed into a bidirectional recurrent layer <ref type="bibr" target="#b36">(Schuster and Paliwal, 1997)</ref>. As in the baseline RNN (Sec. 4.2), dropout is inserted after the recurrent layer, and classification is performed using softmax with cross-entropy loss and greedy decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model 2: Recurrency → Convolution (rcNN)</head><p>The order of convolutional and recurrent elements is switched (see <ref type="figure">Fig. 1b</ref>), so that the convolutional operation replaces additive n-gram formation be- fore the classification layer. The remaining archi- tecture is identical to that of crNN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model 3: rcNN with Shortcuts (rcNN short )</head><p>This model extends rcNN by adding shortcut con- nections <ref type="bibr" target="#b2">(Bishop, 2000</ref>) that concatenate the char- acter embeddings and the RNN outputs with the concatenated feature maps c (see <ref type="figure">Fig. 1c)</ref>. When e i denotes the embedding of character i and r i the output of the recurrent layer at position i, the input to the classification layer is defined as e i ⊕ r i ⊕ c i . Shortcuts are evaluated because we hypothesized that the access to unconvolved information about the input sequence and the output of the recur- rent layer would facilitate the exact prediction of split locations. For a better control of informa- tion flow, we also experimented with residual <ref type="bibr" target="#b13">(He et al., 2016</ref>) and highway ( <ref type="bibr" target="#b39">Srivastava et al., 2015</ref>) instead of shortcut layers, but could not observe improvements on the dev set, most probably be- cause our models are not deep enough for these layer types to show effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Settings</head><p>We use the following settings found on the dev set for the character based models: embedding size: 128; 200 hidden recurrent units; 100 convolutional feature maps with filter widths of 3,5 and 7. We use regularized ( <ref type="bibr">Zaremba et al., 2014</ref>  <ref type="figure">Figure 1</ref>: Character based models, unfolded for the sequence labeling task j+o+v → j+a-u+v.</p><p>vanilla LSTM units. All models are trained with the Adam optimizer ( <ref type="bibr" target="#b19">Kingma and Ba, 2015)</ref>, an initial learning rate of 0.005 and batch size of 100. Gradients with a magnitude higher than 5.0 are cut. The models used for model selection (Sec. 5.2) are trained for 5, the other character-based models for 10 iterations. We train the Transformer in its default configuration as described in <ref type="bibr" target="#b40">Vaswani et al. (2017)</ref> with a vocabulary size 5k <ref type="bibr">7</ref> and report performance on the test set based on evaluations on the dev set. The model of <ref type="bibr" target="#b34">Reddy et al. (2018)</ref> is trained for 80 epochs with our training data and the same parameters as described in the original paper. All calculations are run on a Maxwell Ti- tan X GPU. We compare the models using sen- tence accuracy ( #sens. with errors #all sens.</p><p>) and string based P(recision), R(ecall) and F score ( , where P and R are equivalent to the measures used in the CWS bakeoffs <ref type="bibr" target="#b37">(Sproat and Emerson, 2003)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Model Selection</head><p>The upper half of Tab. 3 compares the evalua- tion metrics for the three character based models introduced in this paper trained with and with- out split probabilities (Sec. 4.1). We test dif- ferences in string accuracy using the McNemar test. <ref type="bibr">8</ref> In general, all models that use recurrency before convolution (rcNN*) have string accuracy rates that are significantly higher at the 0.001 level than for models that use convolution before recur- rency (crNN). <ref type="table" target="#tab_4">Table 3</ref> shows that the differences in the per- formance of crNN and rcNN* are almost as large as between the RNN baseline and the best model from this paper (lower half of Tab. 3), although crNN and rcNN* differ only by the switched or- der of recurrent and convolutional elements. We found this result surprising, because applying con- volution to the character embeddings appeared like a good parametrized alternative to n-gram ex- traction, which is often the first step in architec- tures for Chinese and Japanese word segmenta- tion.</p><p>To further investigate this phenomenon, we evaluated 60 randomly chosen strings from the test set in which either crNN split or (XOR) rcNN split short made an error. 45 of the errors relate to compound splitting, partly combined with vocalic Sandhi, ei- ther by missing a split (rcNN split short : 11, crNN split : 15) or by oversegmenting compounds (rcNN split short : 13, crNN split : 6). Most notably, rcNN split short tends to insert more splits than crNN split . This behav- ior can be observed for missing splits and espe- cially for oversegmentations. A more detailed in- spection shows that 11 of 13 oversegmentations actually induce a compositional reading of a com- pound. saral¯ a˙ nga "name of a pine resin", for ex- ample, is oversegmented into sarala-a˙ nga "pine- limb", which is the etymological derivation of this compound. In contrast, crNN creates oversegmen- tations such as´sras´ as´sr . ˙ ngavanti-ah . , wheré sr . ˙ ngavanti "having horns" (nom. pl. neutre) is a valid form, while ah . is not an independent word form in San- skrit. <ref type="figure">Interestingly, rcNN</ref>   gold analysis, this segmentation gives the correct derivational analysis of the adjective (noun´srnoun´ noun´sr . ˙ nga "horn" + inflected form of the adjectivizing pos- sessive affix -vat). The results of rcNN split short thus re- flect the inherent inconsistencies of the dataset on the level of compound splitting (see Sec. 3.1), and their erroneous splits are frequently semantically meaningful while glossing over minute semantic distinctions. Errors of crNN, in contrast, tend to be real mis-segmentations, indicating that its abil- ity to reflect the semantic level is underdeveloped.</p><p>Split probabilities (Sec. 4.1) have a small, but positive effect on string accuracy of the rcNN* models. When the same model with and without split probabilities is compared using the McNemar test, split probabilities significantly increase string accuracy at the 0.1 level for rcNN and at the 0.001 level for rcNN short , while they don't result in sig- nificantly better performance for crNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with Baseline Models</head><p>The lower half of Tab. 3 compares the best model introduced in this paper (rcNN split short ) with baselines proposed for SWS in previous research. rcNN split short outperforms the character based RNN described in Hellwig (2015b) by a wide margin. While Tab. 3 shows differences of almost 8% in sen- tence and 3% in string accuracy, Tab. 4 presents the improvements for the single surface character <ref type="bibr" target="#b16">Hellwig (2015b)</ref>   <ref type="table">Table 4</ref>: P, R and F for rules that produce the sur- face phoneme ¯ a. Data in the left half are from the original publication. As all metrics are consis- tently better for this paper, we refrain from high- lighting the best results in the right half of the ta- ble.</p><p>¯ a, which can correspond to a compound split (¯ a-) or to various vocalic Sandhis (a-a etc.). For this complicated character, rcNN split short achieves consis- tent improvements of up to 15% on all metrics. We found it especially relevant to observe that rcNN split short made large progress for rare rule types such as ¯ a-¯ a or ¯ a-a, indicating its increased ability for semantic generalization.</p><p>The seq2seq model ( <ref type="bibr" target="#b34">Reddy et al., 2018)</ref> per- forms on a similar level of accuracy as the one proposed in <ref type="bibr" target="#b16">Hellwig (2015b)</ref>. Similar to crNN (Sec. 5.2) it tends to miss splits and to insert faulty ones (e.g., d¯ an¯ ad¯ anaratih . , should: d¯ ana-¯ ad¯ ana- ratih . "pleasure in giving and taking", is: d¯ an¯ at- ¯ anaratih . "from giving . . . UNK").  evaluate their model us- ing location and split 9 prediction accuracy. The authors report 95.0 location and 79.5 split accu- racy, but don't specify how they calculated these values. For this reason and because they evalu- ate on isolated strings only, we cannot compare directly against their work, but would like to re- port the following measures for rcNN split short : • P, R and F for location prediction <ref type="bibr">10</ref> :</p><p>97.64/98.19/97.91</p><p>• Micro-averaged P, R and F for individual rule types such as vocalic Sandhi or compound <ref type="bibr">9</ref> This seems to mean prediction of the correct Sandhi rule; see <ref type="bibr">Gantayat et al. (2018, 4.2)</ref>.</p><p>10 P = T P T P +F P , R = T P T P +F N , TP: number of characters for which gold and model both record a split (though not nec- essarily of the same type), FP: number of characters at which the model over-segmentates, FN: number of character where the model fails to detect a valid split.  The Transformer performs almost on par with rcNN split short , and the differences in string accuracy are not statistically significant, although rcNN split short takes less time for training (2 h vs. 55 h) and in- ference (less than 1 min vs. 30 min when analyz- ing the test set). To better understand if the sys- tems make orthogonal errors and could therefore be used in a mixture of experts, we performed a domain-specific evaluation with 73 sentences from the Buddhist treatise Trim . ´ sik¯ avijñaptibh¯ as . ya and 104 sentences from the philosophical text Ny¯ aya- mañjar¯ ı. We preserved the non-standard orthog- raphy of both texts in order to simulate the ap- plication of the models to real-world data. This includes the presence of typos, unsolved textual problems and erratic (non)-application of Sandhi.</p><p>Both models show a significant drop in overall performance when applied to these data (see Tab. 5). This is not surprising, because the input con- ventions of these files do not match the conven- tions of the training-data. Most errors arise again from disagreement about the (non-)compositional reading of technical compounds such as sarva- jña-tva "all-knowing-ness" (see Sec. 3.1). It has to be noted that both models agree well in their cor- rect decisions and in the type of errors they pro- duce on these data. This indicates that the dis- crepancy in the orthographical conventions is in- deed responsible for a large part of the drop in performance. Given the fact that both texts exhibit a lot of special vocabulary that is not present or used in a very different way in the training set, both models perform surprisingly well. Typical errors common to both models are for example svalpam instead of su-alpam "very small". Both Model P R A  0.955 0.941 0.943 rcNN short , no sp 0.958 0.958 0.955 <ref type="table">Table 6</ref>: Results for splitting German compounds; evaluation metrics according to <ref type="bibr" target="#b21">Koehn and Knight (2003)</ref> models have difficulties to seperate Sandhi in pas- sages that do not adhere to the common practice for typesetting of Indian texts in Latin translit- eration. ayam . parin . ¯ amah . , for example, was not separated into the usual form ayam . parin . ¯ amah . . There are certain cases of disagreement between both models that are noteworthy. While Trans- former has changed the misspelled word abhu- pagamyate to the correct form abhyupagamyate in one case (overlooked by rcNN split short ), rcNN split short cor- rectly identified the verbal form upacaryante iti, where Transformer inserted the semantically dis- preferred, but grammatically possible present par- ticiple upacaryantah . iti. Overall, none of the mod- els shows a generally better or worse performance in these cases of disagreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Application to German Compounds</head><p>In order to test if the character based models gen- eralize well to other languages with limited train- ing resources, we applied rcNN short with split prob- abilities and the same settings as for SWS to the task of splitting German compounds. The cur- rent state of the art is set with a CRF operating on n-grams of characters ( . Ta- ble 6 shows that our model achieves an improve- ment of about 1% for recall and accuracy when trained with the training set of  only. We sampled 20 examples for the three er- ror classes "wrong split", "wrong faulty split" and "wrong non-split" ( <ref type="bibr">Ma et al., 2016, 78</ref>). While our model failed to detect splits for all 20 examples of the type "wrong non-split", the type "wrong split" contained 10 cases, where the split(s) proposed by the model make(s) good sense for us, but are not recorded in the test set (e.g. "Viermaster" 'four- master', "Viermaster" in test; already remarked by ). We observed a similar level of inconsistencies for the "wrong faulty split" type (8 instances), where, for example, our model an- alyzed "Bundes-tags-vize-präsident" 'vice presi-dent of the Federal Parliament', while the test set had "Bundes-tags-vizepräsident".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>While the models discussed in this paper have produced clear performance gains when compared with previous research on SWS, we expect that fu- ture research will improve over our results, but it will be difficult to approach error-free per- formance. The reservation is due to the errors in the training data and especially the question of (non-)compositional readings of compounds, which seems to produce related levels of confu- sion for human annotators and ML models. While following this track of research, we would like to expand its scope to joint learning of splits, lexical and morphological annotations. Here, we expect that especially lexical and morphological analy- sis will benefit from a joint model. We hypothe- size that CTC (Graves, 2012) trained as a co-task or segmental NNs ( <ref type="bibr" target="#b27">Lu et al., 2016</ref>) with a mod- ified objective (including split probabilities) may be suitable for this task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>¯ a+u=o −→ r¯ ajov¯ aca (3) *r¯ aja+uv¯ aca 'O king, he said' a+u=o −→ r¯ ajov¯ aca (4) pr¯ ac+eva 'before indeed' c+e=ge −→ pr¯ ageva (5) tad+hi 'because this ...' d+h=ddh −→ taddhi</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Statistics of the full dataset; ¯ 
|S|: average 
sentence length in characters; columns 4-6 give 
the average proportion of splits/string 
¯ 

spl 

S , of vo-
calic Sandhis/split ¯ 

voc 

spl and of compound splits/split 
¯ 

cp 
spl 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Upper half: Results for model selection 
(Sec. 5.2); lower half: Comparison with baseline 
models (Sec. 4.2 and 5.3) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Results for evaluation on the Trim . ´ 
sik¯ a-
vijñaptibh¯ as . ya and the Ny¯ ayamañjar¯ ı; S/A: sen-
tence accuracy 

split: 95.12/95.12/95.12 

</table></figure>

			<note place="foot" n="1"> Text production was oral until the first centuries BCE (Falk, 1993). The texts were transmitted by memorization in this period, making them less (!) prone to transmission errors than in written form.</note>

			<note place="foot" n="2"> The compound in Ex. 1 is split as ¯ a´srayaa´sraya-bh¯ uta-kha-¯ adikathanena, and kha +¯ adi = kh¯ adi is a Sandhi phenomenon.</note>

			<note place="foot" n="3"> http://kjc-sv013.kjc.uni-heidelberg. de/dcs/</note>

			<note place="foot" n="4"> Longer n-grams did not produce improvements on the dev set.</note>

			<note place="foot" n="5"> Code for the model: https://github.com/ cvikasreddy/skt; for the tokenizer: https:// github.com/google/sentencepiece 6 https://github.com/tensorflow/ tensor2tensor</note>

			<note place="foot" n="7"> Larger vocabulary sizes did not improve on the dev set, but performance gains by further decreasing the vocabulary size appear to be possible. 8 Testing the sentence accuracy produced highly correlated test statistics. Results are therefore not discussed.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">1891-1903. Catalogus Catalogorum. An Alphabetical Register of Sanskrit Works and Authors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodor</forename><surname>Aufrecht</surname></persName>
		</author>
		<imprint>
			<publisher>Otto Harrassowitz</publisher>
			<pubPlace>Leipzig</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SandhiKosh: A benchmark corpus for evaluating Sanskrit Sandhi tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neelamadhav</forename><surname>Gantayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumeet</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC</title>
		<meeting>the LREC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Neural Networks for Pattern Recognition</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Bjerva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07053</idno>
		<title level="m">Semantic tagging with deep residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural word segmentation learning for Chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the ACL</title>
		<meeting>the 54th Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="409" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory neural networks for Chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on EMNLP</title>
		<meeting>the 2015 Conference on EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1197" to="1206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Schrift im alten Indien: Ein Forschungsbericht mit Anmerkungen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harry</forename><surname>Falk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Gunter Narr Verlag</publisher>
			<pubPlace>Tübingen</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Sanskrit sandhi splitting using seq2(seq) 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gantayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aralikatte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Panwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Analysis of Sanskrit text: Parsing and semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawan</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipul</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laxmidhar</forename><surname>Behera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sanskrit Computational Linguistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="200" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Design and analysis of a lean interface for Sanskrit corpus annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawan</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gérard</forename><surname>Huet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Language Modelling</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="145" to="182" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Supervised Sequence Labelling with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Verlag</publisher>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A comparative study on Thai word segmentation approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Choochart</forename><surname>Haruechaiyasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarawoot</forename><surname>Kongyoung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Dailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECTI-CON</title>
		<meeting>ECTI-CON</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="125" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SanskritTagger, a stochastic lexical and POS tagger for Sanskrit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Hellwig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sanskrit Computational Linguistics. First and Second International Symposia</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">5402</biblScope>
			<biblScope unit="page" from="266" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Morphological disambiguation of Classical Sanskrit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Hellwig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Systems and Frameworks for Computational Morphology</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="41" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using Recurrent Neural Networks for joint compound splitting and Sandhi resolution in Sanskrit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Hellwig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th LTC</title>
		<meeting>the 7th LTC</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="289" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A functional toolkit for morphological and phonological processing, application to a Sanskrit tagger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gérard</forename><surname>Huet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Functional Programming</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="573" to="614" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on EMNLP</title>
		<meeting>the 2014 Conference on EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICLR</title>
		<meeting>the ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Long Short-Term Memory for Japanese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiaki</forename><surname>Kitagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Empirical methods for compound splitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Conference of EACL</title>
		<meeting>the Tenth Conference of EACL</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="187" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Word segmentation in Sanskrit using path constrained random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrith</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishal</forename><surname>Santra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavankumar</forename><surname>Satuluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasanth</forename><surname>Sasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhumi</forename><surname>Bandaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuvendra</forename><surname>Faldu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawan</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING</title>
		<meeting>the COLING</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="494" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A dataset for Sanskrit word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrith</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><surname>Kumar Satuluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawan</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</title>
		<meeting>the Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sanskrit morphological analyser: Some issues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amba</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devanand</forename><surname>Shukla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Indian Linguistics</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="169" to="177" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">J</forename><surname>Lowe</surname></persName>
		</author>
		<title level="m">The syntax of Sanskrit compounds. Language</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="71" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Segmental recurrent neural networks for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00223</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Letter sequence labeling for compound splitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Henrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhard</forename><surname>Hinrichs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th SIGMORPHON Workshop</title>
		<meeting>the 14th SIGMORPHON Workshop</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="76" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional LSTM-CNNsCRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the ACL</title>
		<meeting>the 54th Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Languageindependent compound splitting with morphological operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashok</forename><forename type="middle">C</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the ACL</title>
		<meeting>the 49th Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1395" to="1404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automatic Sanskrit segmentizer using finite state transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipul</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 Student Research Workshop</title>
		<meeting>the ACL 2010 Student Research Workshop<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="85" to="90" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rectified linear units improve Restricted Boltzmann Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ICML</title>
		<meeting>the 27th ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sˆ3statistical Sandhi splitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhiram</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="301" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Building a word segmenter for Sanskrit overnight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrith</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vishnu Dutt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawan</forename><surname>Vineeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC</title>
		<meeting>the LREC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Japanese and Korean voice search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisuke</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICASSP</title>
		<meeting>the ICASSP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldip</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The first international Chinese word segmentation bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Emerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the second SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="133" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Deep Learning Workshop at ICML</title>
		<meeting>the Deep Learning Workshop at ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Whitney</forename><surname>William Dwight</surname></persName>
		</author>
		<title level="m">1879. A Sanskrit Grammar. Breitkopf and Härtel</title>
		<meeting><address><addrLine>Leipzig</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
