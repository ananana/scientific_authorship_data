<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Model of Zero-Shot Learning of Spoken Language Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
							<email>majid.yazdani@unige.ch</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Xerox Research Center Europe</orgName>
								<orgName type="institution">University of Geneva</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
							<email>james.henderson@xrce.xerox.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Xerox Research Center Europe</orgName>
								<orgName type="institution">University of Geneva</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Model of Zero-Shot Learning of Spoken Language Understanding</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>When building spoken dialogue systems for a new domain, a major bottleneck is developing a spoken language understanding (SLU) module that handles the new domain&apos;s terminology and semantic concepts. We propose a statistical SLU model that generalises to both previously unseen input words and previously unseen output classes by leveraging unlabelled data. After mapping the utterance into a vector space, the model exploits the structure of the output labels by mapping each label to a hyperplane that separates utterances with and without that label. Both these mappings are initialised with unsupervised word embeddings, so they can be computed even for words or concepts which were not in the SLU training data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Spoken Language Understanding (SLU) in dia- logue systems is the task of taking the utterance output by a speech recognizer and assigning it a semantic label that represents the dialogue actions of that utterance accompanied with their associ- ated attributes and values. For example, the utter- ance "I would like Chinese food" is labelled with inform(food=Chinese), in which inform is the dia- logue action that provides the value of the attribute food that is Chinese.</p><p>Dialogue systems often use hand-crafted gram- mars for SLU, such as Phoenix ( <ref type="bibr" target="#b19">Ward, 1994)</ref>, which are expensive to develop, and expensive to extend or adapt to new attributes and values. Statistical SLU models are usually trained on the data obtained from a specific domain and loca- tion, using a structured output classifier that can be discriminative ( <ref type="bibr" target="#b15">Pradhan et al., 2004;</ref><ref type="bibr" target="#b9">Kate and Mooney, 2006;</ref><ref type="bibr" target="#b6">Henderson et al., 2012</ref>) or genera- tive ( <ref type="bibr" target="#b16">Schwartz et al., 1996;</ref><ref type="bibr" target="#b5">He and Young, 2005</ref>).</p><p>Gathering and annotating SLU data is costly and time consuming and therefore SLU datasets are small compare to the number of possible labels.</p><p>Because training sets for a new domain are small, or non-existent, learning is often an in- stance of Zero-shot or One-shot learning prob- lems ( <ref type="bibr" target="#b14">Palatucci et al., 2009;</ref><ref type="bibr" target="#b10">L. Fei-Fei;</ref><ref type="bibr" target="#b11">Fergus, 2006</ref>), in which zero or few examples of some output classes are available during the training. For example, in the restaurant reservation domain, not all possible combinations of foods and dia- logue actions may be included in the training set. The general idea to solve this type of problems is to map the input and class labels to a semantic space of usually lower dimension in which simi- lar classes are represented by closer points in the space ( <ref type="bibr" target="#b14">Palatucci et al., 2009;</ref><ref type="bibr" target="#b20">Weston et al., 2010)</ref>. Usually unsupervised knowl- edge sources are used to form semantic codes of the labels that helps us to generalize to unseen la- bels.</p><p>On the other hand, there are also different ways to express the same meaning, and similarly, most of them can not be included in the training set. For instance, the system may have seen "Please give me the telephone number" in training, but the user might ask "Please give me the phone" at test time. This problem, feature sparsity, is a common issue in many NLP tasks. Decomposition of in- put feature parameters using vector-matrix mul- tiplication ( <ref type="bibr" target="#b1">Bengio et al., 2003;</ref><ref type="bibr" target="#b3">Collobert et al., 2011;</ref><ref type="bibr" target="#b2">Collobert and Weston, 2008)</ref> has addressed this sparsity issue successfully in previous work. In this way, by sharing the word representations and composition matrices, we can overcome fea- ture sparsity by producing similar representations for similar utterances.</p><p>In order to represent words and concepts we use word embeddings, which are a form of vec- tor space model. Word embeddings have proven to be effective models of semantic representation of words in various NLP tasks ( <ref type="bibr" target="#b0">Baroni et al., 2014;</ref><ref type="bibr" target="#b22">Yazdani and Popescu-Belis, 2013;</ref><ref type="bibr" target="#b3">Collobert et al., 2011;</ref><ref type="bibr" target="#b2">Collobert and Weston, 2008;</ref><ref type="bibr" target="#b8">Huang et al., 2012;</ref><ref type="bibr" target="#b13">Mikolov et al., 2013b</ref>). In addition to pa- rameter sharing, these representations enable us to leverage large scale unlabelled data. Because word embeddings trained on unlabeled data reflect the similarity between words, they help the model generalize from the words in the original training corpora to the words in the new extended domain, and help generalize from small amounts of data in the extended domain.</p><p>The contribution of this paper is to build a rep- resentation learning classifier for the SLU task that can generalize to unseen words and labels. For ev- ery utterance we learn how to compose the word vectors to form the semantics of that utterance for this task of language understanding. Furthermore, we learn how to compose the semantics of each la- bel from the semantics of the words used to name that label. This enables us to generalize to unseen labels.</p><p>In this work we use the word2vec software of <ref type="bibr" target="#b12">Mikolov et al. (2013a)</ref>  <ref type="bibr">1</ref> to induce unsupervised word embeddings that are used to initialize word embedding parameters. For this, we use an En- glish Wikipedia dump as our unlabelled training corpus, which is a diverse broad-coverage corpus. It has been shown ( <ref type="bibr" target="#b0">Baroni et al., 2014;</ref><ref type="bibr" target="#b13">Mikolov et al., 2013b</ref>) that these embeddings capture lex- ical similarities even when they are trained on a diverse corpus like Wikipedia. We test our models on a restaurant booking domain. We investigate domain adaptation by adding new attribute types (e.g. goodformeal) and new attribute values (e.g. Hayes Valley as a restaurant location). Our exper- iments indicate that our model has better perfor- mance compared to a hand-crafted system as well as a SVM baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SLU Datasets</head><p>The dialogue utterances used to build the SLU dataset were collected during a trial of online di- alogue policy adaptation for a restaurant reserva- tion system based in San Francisco. The trial be- gan with (area, pricerange and food), and adapted the Interaction Manager online to handle the ad- ditional attribute types near, allowedforkids, and goodformeal ( <ref type="bibr" target="#b4">Ga≈°ic et al., 2014</ref>). User utterances from these trials were transcribed and annotated with dialogue acts by an expert, and afterwards edited by another expert 2 . Each user utterance was annotated with a set of labels, where each label consists of an act type (e.g. inform, request), an attribute type (e.g. foodtype, pricerange), and an attribute value (e.g. Chinese, Cheap).</p><p>The dataset is separated into four subsets, SFCore, SF1Ext, SF2Ext and SF3Ext, each with an increasing set of attribute types, as speci- fied in <ref type="table">Table 1</ref>. This table also gives the total num- ber of utterances in each data set. For our first ex- periment, we split each dataset into about 15% for the testing set and 85% for the training set. For our second experiment we use each extended subset for testing and its preceding subsets for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ontology Attribute types ( # of values ) # of utterances SFCore</head><p>food <ref type="formula">(59)</ref>, area(155), pricerange(3) 1103 SF1Ext</p><p>SFCore + near(39) 1810 SF2Ext</p><p>SF1Ext + allowedforkids(2) 1571 SF3Ext</p><p>SF2Ext +goodformeal(4) 1518 <ref type="table">Table 1</ref>: Domains for San Francisco (SF) restau- rants expanding in complexity</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Dialogue Act Representation Learning Classifier</head><p>The SLU model is run on each hypothesis output by the ASR component, and tries to predict the correct set of dialogue act labels for each hypoth- esis. This problem is in general an instance of multi-label classification, because a single utter- ance can have multiple dialogue act labels. Also, these labels are structured, since each label consist of an act type, an attribute type, and an attribute value. Each label component also has canonical text associated with it, which is the text used to name the label component (e.g. "Chinese" as a value). The number of possible dialogue acts grows rapidly as the domain is extended with new at- tribute types and values, making this task one of multi-label classification with a very large number of labels. One natural approach to this task is to train one binary classifier for each possible label, to decide whether or not to include it in the output. In our case, this requires training a large number of classifiers, and it is impossible to generalize to <ref type="bibr">2</ref> This data is publically available from https://sites.google.com/site/ parlanceprojectofficial/home/ datarepository dialogue acts that include attributes or values that were not in the training set since there won't be any parameter sharing among label classifiers.</p><p>In our alternative approach, we build the rep- resentation of the utterance and the representation of the label from their constituent words, then we check if these representations match or not. In the following we explain in details this representation learning model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Utterance Representation Learning</head><p>In this section we explain how to build the utter- ance representation from its constituent words. In addition to words, we use bigrams, since they have been shown previously to be effective features for this task <ref type="figure" target="#fig_0">(Henderson et al., 2012)</ref>. Following the success in transfer learning from parsing to under- standing tasks <ref type="bibr" target="#b7">(Henderson et al., 2013;</ref><ref type="bibr" target="#b18">Socher et al., 2013)</ref>, we use dependency parse bigrams in our features as well. We learn to build a local rep- resentation at each word position in the utterance by using the word representation, adjacent word representations, and the head word representation. Let œÜ(w) be a d dimensional vector representing the word w, and œÜ(U i ) be a h dimensional vector which is the local representation at word position i. We compute the local representation as follows:</p><formula xml:id="formula_0">œÜ(U i ) = œÉ(œÜ(w i )W word + œÜ(w h )W parse R k + œÜ(w j )W previous + œÜ(w k )W next ) (1)</formula><p>in which w h is the head word with the depen- dency relation R k to w i , and w j and w k are the previous and next words. W word is a d √ó h ma- trix that transforms the word embedding to hidden representation inputs. W parse R k is a d √ó h ma- trix for the relation R k that similarly transforms the head word embedding (so W parse is a tensor), and W previous and W next similarly transform the previous and next words' embeddings. <ref type="figure" target="#fig_0">Figure 1</ref> depicts this representation building at each word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Label Representation Learning</head><p>One standard way to address the problem of multi- label classification is building binary classifiers for each possible label. Large margin classifiers have been shown to be an effective tool for this task ( <ref type="bibr" target="#b15">Pradhan et al., 2004;</ref><ref type="bibr" target="#b9">Kate and Mooney, 2006</ref>). We use the same idea of binary classifiers to learn one hyperplane per label, which separates the utterances with this label from all other utter- ances, with a large margin. In the standard way of We exploit the structure of labels by assuming that each hyperplane representation is a compo- sition of representations of the label's constituent components, namely dialogue action, attribute and attribute value. We learn the composition function and the constituent representations while training the classifiers, using the labelled SLU data. The constituent representations are initialised as the word embeddings for the label constituent's name string, such as "inform", "food" and "Chinese", where these embeddings are trained on the unla- belled data. <ref type="figure" target="#fig_0">Figure 1</ref> depicts the classifier model.</p><p>We define the hyperplane of the label a j (att k = val l ) with its normal vector W a j ,att k ,val l as:</p><formula xml:id="formula_1">W a j ,att k ,val l = œÉ([œÜ(a j ), œÜ(att k ), œÜ(val l )]W ih )W ho</formula><p>where œÜ(¬∑) is the same mapping to d dimensional word vectors that is used above in the utterance representation, W ih is a 3d √ó h matrix and W ho is a h √ó h matrix. The score of each local represen- tation vector œÜ(U i ) is its distance from this label hyperplane, which is computed as the dot product of the local vector œÜ(U i ) with the normal vector W a j ,att k ,val l .</p><p>We sum these local scores for each po- sition i to build the whole utterance score:</p><formula xml:id="formula_2">i œÜ(U i )W T a j ,att k ,val l</formula><p>. Alternatively we can think of this computation as summing the local vectors to get a whole-utterance representation œÜ(U ) = i œÜ(U i ) and then doing the dot product. The pooling method (sum) used in the model is (inten- tionally) over-simplistic. We did not want to dis- tract from the main contribution of the paper, and our dataset did not justify any more complex solu- tion since utterances are short. It can be replaced by more powerful approaches if it is needed.</p><p>To train a large margin classifier, we train all the parameters such that the score of an utterance is bigger than a margin for its labels and less than the negative margin for all other labels. Thus, the loss function is as follows:</p><formula xml:id="formula_3">min Œ∏ Œª 2 Œ∏ 2 + U max(0, 1‚àíy i œÜ(U i )W T a j ,att k ,val l )<label>(2)</label></formula><p>where Œ∏ is all the parameters of the model, namely œÜ(w i ) (word embeddings), W word , W P arse , W previous , W next , W ih , and W ho . y is either 1 or ‚àí1 depending whether the input U has that label or not.</p><p>To optimize this large margin classifier we per- form stochastic gradient descent by using the ada- grad algorithm on this primal loss function, sim- ilarly to Pegasos SVM ( <ref type="bibr" target="#b17">Shalev-Shwartz et al., 2007)</ref>, but here we backpropagate the errors to the representations to train the word embeddings and composition functions. In each iteration of the stochastic training algorithm, we randomly select an utterance and its labels as positive examples and choose randomly another utterance with a dif- ferent label as a negative example. When choos- ing the negative sample randomly, we sample ut- terances with the same dialogue act but different attribute or value with 4 times higher probability than utterances with a different dialogue act. This biased negative sampling speeds up the training process since it provides more difficult training ex- amples to the learner.</p><p>The model is able to address the adaptivity is- sues because the utterance and the dialogue act representations are in the same space using the same shared parameters œÜ(w), which are ini- tialised with unsupervised word embeddings. It has been shown that such word embeddings cap- ture word similarities and hence the classifier is no longer ignorant about any new attribute type or attribute value. Also, there is parameter sharing between dialogue acts because these word/label embeddings are shared, and the matrices for the composition of these representations are the same across all dialogue acts. This can help overcome sparsity in the SLU training set by transferring learning between similar situations and similar dialogue act triples. For example, if the train- ing set does not contain any examples of the act "request(postcode)", but many examples of "re- quest(phone)", sharing the parameters can help with the recognition of "request(postcode)" in ut- terances similar to "request(phone)". Moreover, the SLU model is to some extent robust against paraphrasing in the input utterance because it maps the utterance to a semantic space, and uses parse bigrams. More sophisticated vector-space semantic representations of the utterance are an area for future work, but should be largely orthog- onal to the contribution of this paper.</p><p>To find the set of compatible dialogue acts for a given utterance, we should check all possible dia- logue acts. This can severely slow down SLU. To avoid testing all possible dialogue combinations, we build three different classifiers: The first one recognises the act types in the utterance, the sec- ond one recognises the attribute types for each of the chosen act types, and the third classifier recog- nises the full dialogue acts as we described above, but only for the chosen pairs of act types and at- tribute types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SLU Experiments</head><p>In the first experiment, we measure SLU perfor- mance trained on all available data, by building a dataset that is the union of all the above datasets. This measures the performance of SLU when there is a small amount of data for an extended do- main. This dataset, similarly to SF3Ext, has 6 main attribute types. <ref type="table" target="#tab_1">Table 2</ref> shows the perfor- mance of this model. We report as baselines the performance of the Phoenix system (hand crafted for this domain) and a binary linear SVM trained on the same data. The hidden layers have size h=d=50. For this experiment, we split each dataset into about 15% for the testing set and 85% for the training set.  Our SLU model can adapt well to the extended domain with more attribute types. We observe Test set model, train set SF1Ext SF2Ext SF3Ext P-R-F P-R-F P-R-F Our SFcore 73. <ref type="bibr">36-66.11-69.54 74.61-59.73-66.34 72.54-53.86-61.81 SVM SFcore 50.66-38.7-43.87 49.64-34.70-40.84 48.99-30.91-37.90 Our SF1Ext 83.18-66.08-73.65 78.32-59.98-67.93 SVM SF1Ext 58.72-41.71-48.77 53.25-34.88-42.15 Our SF2Ext 84.12-67.78-75</ref>.07 SVM SF2Ext</p><p>59.27-42.80-49.70 particularly that the recall is almost twice as high as the hand-crafted baseline. This shows that our SLU can recognise most of the dialogue acts in an utterance, where the rule-based Phoenix sys- tem and a classifier without composed output can- not. Overall there are 1042 dialogue acts in the test set. SLU recall is very important in the over- all dialogue system performance, as the effect of a missed dialogue act is hard to handle for the Inter- action Manager. Both hand-crafted and our system show relatively high precision.</p><p>In the next experiment, we measure how well the new SLU model performs in an extended do- main without any training examples from that ex- tended domain. We train a SLU model on each subset, and test it on each of the more inclusive subsets. <ref type="table" target="#tab_2">Table 3</ref> shows the results.</p><p>Not surprisingly, the performance is better if SLU is trained on a similar domain to the test do- main, and adding more attribute types and values decreases the performance more. But our SLU can generalise very well to the extended domain, achieving much better generalisation that the SVM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Conclusion</head><p>In this paper, we describe a new SLU model that is designed for improved domain adaptation. The multi-label classification problem of dialogue act recognition is addressed with a classifier that learns to build an utterance representation and a dialogue act representation, and decides whether or not they are compatible. The dialogue act repre- sentation is a vector composition of its constituent labels' embeddings, and is trained as the hyper- plane of a large margin binary classifier for that di- alogue act. The utterance representation is trained as a composition of word embeddings. Since the utterance and the dialogue act representations are both built using unsupervised word embeddings and share these embedding parameters, the model can address the issues of domain adaptation. Word embeddings capture word similarities, and hence the classifier is able to generalise from known at- tribute types or values to similar novel attribute types or values. We tested this SLU model on datasets where the number of attribute types and values is increased, and show much better re- sults than the baselines, especially in recall. The model succeeds in both adapting to an extended domain using relatively few training examples and in recognising novel attribute types and values.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The multi-label classifier</figDesc><graphic url="image-1.png" coords="3,317.20,62.80,198.42,170.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance on union of data (SF-
Core+SF1Ext+SF2Ext+SF3Ext) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>SLU performance: trained on a smaller domain and tested on more inclusive domains. 

</table></figure>

			<note place="foot" n="1"> https://code.google.com/p/word2vec/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research leading to this work was funded by the EC FP7 programme FP7/2011-14 under grant agreement no. 287615 (PARLANCE), and Hasler foundation project no. 15019, Deep Neural Net-work Dependency Parser for Context-aware Rep-resentation Learning. The authors also would like to thank Dr.Helen Hastie for her help in annotating the dataset.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germ√°n</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R√©jean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning, ICML &apos;08</title>
		<meeting>the 25th International Conference on Machine Learning, ICML &apos;08</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L√©on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Incremental on-line adaptation of pomdp-based dialogue managers to extended domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ga≈°ic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tsiakoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Breslin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic processing using the hidden vector state model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="85" to="106" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discriminative Spoken Language Understanding Using Word Confusion Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Ga≈°i¬¥ga≈°i¬¥c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pirros</forename><surname>Tsiakoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multilingual joint parsing of syntactic and semantic dependencies with a latent variable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Merlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Musillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="949" to="998" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving Word Representations via Global Context and Multiple Word Prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using string-kernels for learning semantic parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Kate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL-44</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL-44</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="913" to="920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="594" to="611" />
			<date type="published" when="2006-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-2013)</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-2013)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Zero-shot learning with semantic output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Palatucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1410" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Shallow semantic parsing using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kadri</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">H</forename><surname>Hacioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Language understanding using hidden understanding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Stallard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language, 1996. ICSLP 96. Proceedings., Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="997" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pegasos: Primal estimated sub-gradient solver for svm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning</title>
		<meeting>the 24th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Extracting information in spontaneous speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSLP</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large scale image annotation: Learning to rank with joint word-image embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="page">81</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wsabie: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence-Volume Volume Three</title>
		<meeting>the Twenty-Second International Joint Conference on Artificial Intelligence-Volume Volume Three</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2764" to="2770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Computing text semantic relatedness using the contents and links of a hypertext encyclopedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Popescu-Belis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="176" to="202" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
