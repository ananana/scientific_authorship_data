<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adapting Topic Models using Lexical Associations with Tree Priors</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Yang</surname></persName>
							<email>wwyang@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science (CS)</orgName>
								<orgName type="institution" key="instit1">University of Maryland College Park</orgName>
								<orgName type="institution" key="instit2">University of Maryland College Park</orgName>
								<orgName type="institution" key="instit3">University of Maryland College Park</orgName>
								<address>
									<region>MD, MD, MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science (CS)</orgName>
								<orgName type="institution" key="instit1">University of Maryland College Park</orgName>
								<orgName type="institution" key="instit2">University of Maryland College Park</orgName>
								<orgName type="institution" key="instit3">University of Maryland College Park</orgName>
								<address>
									<region>MD, MD, MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>iSchool, LSC</roleName><forename type="first">Umiacs</forename><surname>Cs</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science (CS)</orgName>
								<orgName type="institution" key="instit1">University of Maryland College Park</orgName>
								<orgName type="institution" key="instit2">University of Maryland College Park</orgName>
								<orgName type="institution" key="instit3">University of Maryland College Park</orgName>
								<address>
									<region>MD, MD, MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">Resnik</forename><surname>Linguistics</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science (CS)</orgName>
								<orgName type="institution" key="instit1">University of Maryland College Park</orgName>
								<orgName type="institution" key="instit2">University of Maryland College Park</orgName>
								<orgName type="institution" key="instit3">University of Maryland College Park</orgName>
								<address>
									<region>MD, MD, MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umiacs</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science (CS)</orgName>
								<orgName type="institution" key="instit1">University of Maryland College Park</orgName>
								<orgName type="institution" key="instit2">University of Maryland College Park</orgName>
								<orgName type="institution" key="instit3">University of Maryland College Park</orgName>
								<address>
									<region>MD, MD, MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adapting Topic Models using Lexical Associations with Tree Priors</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1901" to="1906"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Models work best when they are optimized taking into account the evaluation criteria that people care about. For topic models, people often care about inter-pretability, which can be approximated using measures of lexical association. We integrate lexical association into topic optimization using tree priors, which provide a flexible framework that can take advantage of both first order word associations and the higher-order associations captured by word embeddings. Tree priors improve topic interpretability without hurting ex-trinsic performance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Goodman (1996) introduces a key insight for ma- chine learning models in natural language process- ing: if you know how performance on a problem is evaluated, it makes more sense to optimize using that evaluation metric, rather than others. Good- man applies his insight to parsing algorithms, but this insight has had an even larger impact in ma- chine translation, where the introduction of the fully automatic BLEU metric makes it possible to tune systems using a score correlated with hu- man rankings of MT system performance ( <ref type="bibr" target="#b21">Papineni et al., 2002</ref>). <ref type="bibr" target="#b4">Chang et al. (2009)</ref> provide a similar insight for topic models ( <ref type="bibr">Blei et al., 2003, LDA)</ref>: if what you care about is the interpretability of topics, the standard objective function for parameter in- ference (likelihood) is not only poorly correlated with a human-centered measurement of topic co- herence, but inversely correlated. Nonetheless, most topic models are still trained using meth- ods that optimize likelihood <ref type="bibr" target="#b17">(McAuliffe and Blei, 2008;</ref><ref type="bibr" target="#b20">Nguyen et al., 2013</ref>).</p><p>We take the logical next step suggested when you bring together the insights of <ref type="bibr" target="#b6">Goodman (1996)</ref> and <ref type="bibr" target="#b4">Chang et al. (2009)</ref>, namely incorporating an approximation of human topic interpretabil- ity into the topic model optimization process in a way that is effective and more straightforward than previous methods <ref type="bibr" target="#b19">(Newman et al., 2011</ref>). We take advantage of the human-centered evaluation of <ref type="bibr" target="#b4">Chang et al. (2009)</ref>, which can be reasonably approximated using an automatic metric based on word associations derived from a large, more gen- eral corpus ( <ref type="bibr" target="#b15">Lau et al., 2014</ref>). We exploit LDA and its Bayesian formulation by bringing word associ- ations into the picture using a prior-specifically, we use external lexical association to create a tree structure and then use tree LDA <ref type="bibr">(Boyd-Graber et al., 2007, tLDA)</ref>, which derives topics using a given tree prior.</p><p>We construct tree priors with combinations of two types of word association scores (skip-gram probability ( <ref type="bibr" target="#b18">Mikolov et al., 2013</ref>) and G2 likeli- hood ratio <ref type="bibr" target="#b5">(Dunning, 1993)</ref>) and three construc- tion algorithms (two-level, hierarchical clustering with and without leaf duplication). Then tLDA identifies topics with these tree priors in Amazon reviews and the 20NewsGroups datasets. tLDA topics are more coherent compared with "vanilla" LDA topics, while retaining and often slightly im- proving topics' extrinsic performance as features for supervised classification. Our approach can be viewed as a form of adaptation, and the flexibility of the tree prior approach-amenable to any kind of association score-suggests that there are many directions to pursue beyond the two flavors of as- sociation explored here.  <ref type="figure">Figure 1</ref>: An example of a tree prior (the tree structure) and gold posterior edge and word prob- abilities learned by tLDA. Numbers beside the edges denote the probability of moving from the parent node to the child node. A word's probabil- ity, i.e., the number below the word, is the product of probabilities moving from the root to the leaf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tree LDA: LDA with Tree Priors</head><p>Tree priors organize the vocabulary of a dataset in a tree structure, contrasting with introducing topic correlations ( <ref type="bibr" target="#b1">Blei and Lafferty, 2007;</ref><ref type="bibr" target="#b8">He et al., 2017)</ref>. Words are located at the leaf level and share ancestor internal nodes. In our use of tree pri- ors, if two words have a lower association score, their common ancestor node will be closer to the root node, e.g., contrast (orbit, satellite) with (or- bit, launch) in <ref type="figure">Figure 1</ref>. Tree LDA (Boyd-Graber et al., 2007, tLDA) is an LDA extension that creates topics from a tree prior. A topic in tLDA is a multinomial distribu- tion over the paths from the root to leaves. An in- ternal node, i.e., the circles in <ref type="figure">Figure 1</ref>, is a multi- nomial distribution over its child nodes. The prob- ability of a path is the product of probabilities of picking the nodes in the path, e.g., Pr(satellite) = 0.614 × 0.962 × 0.427 ≈ 0.252. Thus two paths with shared nodes have correlated weights in a topic. The generative process of tLDA is:</p><p>1. For topics k ∈ {1, . . . , K} and internal nodes ni</p><formula xml:id="formula_0">(a) Draw child distribution 1 π k,i ∼ Dir(β) 2. For each document d ∈ {1, . . . , D} (a) Draw topic distribution θ d ∼ Dir(α) (b) For each token t d,n in document d i. Draw topic assignment z d,n ∼ Mult(θ d ) ii.</formula><p>Draw path y d,n to word w d,n with probability</p><formula xml:id="formula_1">(i,j)∈y d,n πz d,n ,i,j</formula><p>tLDA can perform different tasks using differ- ent tree priors. If we encode synonyms in the tree prior, tLDA disambiguates word senses <ref type="bibr">(BoydGraber et al., 2007</ref>). With word translation priors, it is a multilingual topic model ( <ref type="bibr" target="#b11">Hu et al., 2014</ref>). The words in the internal nodes denote concepts and have no effect in tLDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tree Prior Construction from Word Association Scores</head><p>A two-level tree is the most straightforward con- struction. <ref type="bibr">2</ref> Each internal node, n i , is a concept as- sociated with a word v i in the vocabulary. Then we sort all other words in descending order of their as- sociation scores with v i and select the top N words (we use N = 10) as n i 's child leaf nodes. n i has an additional child node which represents v i , to ensure that every word appears at the leaf level at least once ( <ref type="figure" target="#fig_0">Figure 2</ref>). 3 Thus, if the vocabulary size is V , there will be a total of (N + 1)V leaf nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hierarchical Clustering (HAC)</head><p>While a two-level tree is bushy (high branching factor) and flat, hierarchical agglomerative cluster- ing <ref type="bibr">(Lukasová, 1979, HAC)</ref> reduces the number of leaf nodes and encodes levels of word association information in its hierarchy ( <ref type="figure">Figure 1</ref>). The HAC process starts from V clusters repre- senting the V words in the vocabulary. It then repeatedly merges the two clusters with the high- est association score until there is only one cluster left. If at least one of the two clusters, c i and c j , has multiple words, their association score is the average association score of the pairwise words from the two clusters:</p><formula xml:id="formula_2">S(c i , c j ) = 1 |c i ||c j | w i ∈c i w j ∈c j S(w i , w j ). (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">HAC with Leaf Duplication (HAC-LD)</head><p>HAC might merge words with multiple senses. For example, the word "spring" could mean either a season (similar to "summer") or a place with water (similar to "lake"). Assigning "spring" to either side will cause information loss on the other side.</p><p>To alleviate this problem, we first pair every word with its most similar word and create a clus- ter with the pair. Thus "spring" is paired with "summer" and "lake" simultaneously ( <ref type="figure">Figure 3</ref>). spring lake lake spring summer summer river winter <ref type="figure">Figure 3</ref>: An example of HAC-LD for the words "spring", "summer", and "lake", whose paired words are shaded in gray. HAC-LD alleviates the problem in HAC that a word with multiple senses can only be assigned to a single cluster close to one of its senses. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We compute two versions of word association scores from Gigaword, using word2vec ( <ref type="bibr" target="#b18">Mikolov et al., 2013</ref>) and G2 likelihood ratio <ref type="bibr" target="#b5">(Dunning, 1993)</ref>. <ref type="bibr">4</ref> Given the word vectors v i and v j , which represent words w i and w j , their word2vec asso- ciation score is</p><formula xml:id="formula_3">S(w i , w j ) = exp (v i · v j ) k exp (v i · v k ) .<label>(2)</label></formula><p>Then we apply the three tree construction algo- rithms to construct six tree priors. In the two-level trees, the value of N , i.e., the number of child nodes per internal node, is ten.</p><p>We use Amazon reviews (Jindal and Liu, 2008) and 20NewsGroups <ref type="bibr">(Lang, 1995, 20NG)</ref>. We ap- ply the same tokenization and stopword removal methods. We then sort the words by their docu- ment frequencies and return the top words, while also removing words that appear in more than 30% of the documents <ref type="table">(Table 1)</ref>.</p><p>Both corpora are split into five folds. For classi- fication tasks, each fold is further equally split into a development set and a test set. All the results are averaged across five-fold cross-validation using 20 topics with hyper-parameters α = β = 0.01. For 20NewsGroups classification, a post's newsgroup is its label. For Amazon reviews, 4-5 star reviews have positive labels, 1-2 stars negative, and re- views with 3 stars are discarded.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Perplexity</head><p>Before evaluating topic quality, we conduct a san- ity check of the models' average perplexity on the test sets <ref type="table" target="#tab_3">(Table 2)</ref>.</p><p>LDA achieves the lowest perplexity among all models on both corpora while tLDA models yield suboptimal perplexity results owing to the con- straints given by tree priors. As shown in the fol- lowing sections, the sacrifice in perplexity brings improvement in topic coherence, while not hurting or slightly improving extrinsic performance using topics as features in supervised classification.</p><p>Tree priors built from word2vec generally out- perform the ones built using the G2 likelihood ra- tio. Among the three tree prior construction algo- rithms, the two-level is the best on the 20News- Groups corpus. However, there is no such consis- tent pattern on Amazon reviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Topic Coherence</head><p>Instead of manually evaluating topic quality us- ing word intrusion ( <ref type="bibr" target="#b4">Chang et al., 2009)</ref>, we use an automatic alternative to compute topic coher- ence ( <ref type="bibr" target="#b15">Lau et al., 2014</ref>). For every topic, we extract its top ten words and compute average pairwise PMI on a reference corpus (Wikipedia as of Octo- ber 8, 2014).</p><p>We include LDA and the latent concept topic model ( <ref type="bibr">Hu and Tsujii, 2016, LCTM)</ref> as baselines. LCTM also incorporates prior knowledge from word embeddings. It assumes that latent concepts exist in the embedding space and are Gaussian dis- tributions over word embeddings, and a topic is a multinomial distribution over these concepts. We marginalize over concepts and obtain the probabil- ity mass of every word in every topic and compare against LDA and tLDA topics.   Most tLDA models yield more coherent topics <ref type="figure" target="#fig_3">(Figure 4)</ref>. Among all tLDA models, the two-level tree built on word2vec improves the most. LCTM performs poorly: after marginalizing out the con- cepts on 20NewsGroups, all its topics consist of words like "don", "dodgers", "au", "alot", "peo- ple", "alicea", "uw", "arabia", "sps", and "entry" with slight differences in ordering.</p><p>To show how subjective topic quality improves over LDA, we extract the topics given by LDA and tLDA (with two-level tree built on word2vec scores) on 20NewsGroups, pair them, and sort the pairs based on KL divergence (KLD). In <ref type="table" target="#tab_5">Table 3</ref>, we select and present three topics from each of the top, middle, and bottom third of the sorted topics.</p><p>Topics with low KLD (Christian, Security, and Middle East) do not differ significantly. Although the topics of Sports have medium KLD and quite different words, they are generally coherent. As the KLD increases, tLDA topics have more coher- ent words. In University Research topics, tLDA includes more research-related words, e.g., "cen- ter", "science", and "institute". In Health top- ics, the tLDA topic has more coherent words like "patients", "insurance", "aids", and "treat- ment", while LDA includes less relevant words, e.g., "food", "sex", and "cramer".</p><p>In the topics with large KLD, tLDA topics are also more coherent. For instance, in the Images topics, the LDA topic contains less relevant words like "mail" and "data", while the tLDA topic mostly consists of words related to images, and even includes words like "jpeg", "color", and "bit" that are not among the top words in the LDA topic. In the topics for Hardware, there are more words closer to the hardware level for tLDA, e.g., "drives", "dos", "controller", and "ide", in con- trast to LDA, e.g., "mac", "pc", and "apple". tLDA also ranks hardware-related words higher. For in- stance, "scsi" and "disk" come before "mb". The words in the topics for People are generally coher- ent, except "didn" and "time" in the LDA topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Extrinsic Classification</head><p>To extrinsically evaluate topic quality, we use bi- nary and multi-class classification on Amazon re- views and 20NewsGroups corpora using SVM- light <ref type="bibr" target="#b13">(Joachims, 1998)</ref> and SVM-multiclass. <ref type="bibr">5</ref> We tune the parameter C, the trade-off between train- ing error and margin, on the development set and apply the trained model with the best performance on the development set to the test set. The classi- fication accuracies are given in <ref type="table" target="#tab_7">Table 4</ref>.</p><p>We compare the accuracies of features of bag- of-words (BOW) and LDA/LCTM/tLDA topics. For the tLDA models with two-level and HAC-LD tree priors, the path assignment is an additional fea- ture. <ref type="bibr">6</ref> We also include the features of BOW and the average word vector for the document (BOW+VEC).  <ref type="table" target="#tab_3">-W2V-HAC   TLDA-W2V-2LV   TLDA-G2-HAC-LD   TLDA-G2-HAC   TLDA-G2-</ref> Features based on most tLDA topics perform at least as well as LDA-based topic features; with no statistically significant differences, our tree pri- ors do not sacrifice extrinsic performance for im- proving topic coherence. In addition, the path assignment feature improves topical classification but not sentiment classification. Although the word2vec feature (BOW+VEC) performs the best on Amazon reviews, it lacks the interpretability of topic models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Learned Trees</head><p>Tree-based topics distinguish polysemous words. In <ref type="figure">Figure 5</ref>, the upper sub-tree comes from the Politics topic ("president", "people", "clinton", "myers", "money", etc.) where "pounds" is more likely to be reached in the sense of British cur- rency. In the Health topic <ref type="table" target="#tab_5">(Table 3)</ref>, "pounds" is more associated with weights (lower tree).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>Combining topic models and vector space models is an emerging area. We introduce a method that is simpler and more flexible than previous work <ref type="bibr" target="#b9">(Hu and Tsujii, 2016)</ref>, and although we extract prior knowledge from word vectors, our model is not restricted to this and can use any word association  Figure 5: Sub-trees for "pounds" in two topics, from 20NewsGroups corpus using two-level tree prior from word2vec. "Pounds" is more associated with British currency in Politics (upper), while closer to weight in Health (lower). High probabil- ity paths are shaded; high probability edges have thicker lines. scores. Our model yields more coherent topics and maintains extrinsic performance, and in addition it is less computationally costly. <ref type="bibr">7</ref> We plan to merge tree prior construction and the topic modeling into a unified framework ( <ref type="bibr" target="#b22">Teh et al., 2007;</ref><ref type="bibr" target="#b7">Görür and Teh, 2009;</ref><ref type="bibr" target="#b10">Hu et al., 2013</ref>). This will allow tree priors to change along with the topics they produce instead of using a static one constructed a priori.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A two-level tree example with N = 2. The words in the internal nodes denote concepts and have no effect in tLDA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Topic</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Average PMI of top 10 words in topics given by models on 20NewsGroups (upper) and Amazon (lower). Most tLDA topics are more coherent than LDA topics. The PMI of LCTM are too low to be included: 8.862±0.657 on 20NewsGroups and 6.340±1.208 on Amazon reviews.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The average perplexity results on the test 
sets by various models. LDA gives the lowest per-
plexity, because tLDA models have constraint from 
the tree priors and sacrifice the perplexity. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>jesus, church, christ, christian, bible, man, christians, lord, sin tLDA god, jesus, bible, christian, christ, church, christians, faith, people, lord Security 0.720 LDA key, encryption, chip, clipper, keys, government, public, security, system, law tLDA key, encryption, chip, clipper, government, keys, privacy, security, system, public Middle East 0.765 LDA israel, jews, war, israeli, jewish, arab, people, world, peace, muslims tLDA israel, jews, israeli, war, jewish, arab, muslims, people, peace, world</head><label></label><figDesc></figDesc><table>KLD 

Model Words 

Christian 
0.709 

LDA 

god, Sports 
1.212 

LDA 

hockey, team, game, play, la, nhl, ca, period, pit, cup 
tLDA 
game, team, year, games, play, players, hockey, season, win, baseball 
University 
Research 
1.647 

LDA 

university, information, national, april, states, year, research, number, united, american 
tLDA 
university, research, information, april, national, center, science, year, number, institute 

Health 
1.914 

LDA 

medical, people, disease, health, cancer, food, sex, cramer, men, drug 
tLDA 
health, medical, disease, drug, cancer, patients, insurance, drugs, aids, treatment 

Images 
1.995 

LDA 

image, ftp, software, graphics, mail, data, version, file, pub, images 
tLDA 
file, image, jpeg, graphics, images, files, format, bit, color, program 

Hardware 
2.127 

LDA 

drive, card, mb, scsi, disk, mac, system, pc, apple, bit 
tLDA 
drive, scsi, disk, mb, hard, drives, dos, controller, ide, system 

People 
2.512 

LDA 

armenian, people, turkish, armenians, armenia, turkey, turks, didn, soviet, time 
tLDA 
armenian, turkish, armenians, armenia, turkey, turks, soviet, people, russian, genocide 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>We sort topics into thirds by Kullback-Leibler divergence (KLD): low, medium, and high diver-
gence between vanilla LDA and tLDA. Unique coherent words are in black and bold. Unique incoherent 
words are in red and italic. tLDA brings in more topic-relevant words. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Accuracies of topical classification on 
20NewsGroups and sentiment analysis on Ama-
zon reviews. Although not significantly improving 
the performance, tLDA topics at least do not hurt. 

dollar pounds 
revenue lbs 

ton 
worth 
million 

pounds 
pounds pounds 

pounds 

1.32E-3 2.06E-7 1.89E-5 1.87E-7 1.87E-8 1.87E-8 1.87E-8 

dollar pounds 
revenue lbs 

ton 
worth 
million 

pounds 
pounds pounds 

pounds 

2.00E-8 2.00E-8 2.19E-7 2.19E-7 2.20E-5 1.74E-4 2.00E-8 

</table></figure>

			<note place="foot" n="1"> Unlike other tree-based topic models such as Andrzejewski et al. (2009), all Dirichlet hyperparameters are the same for all internal nodes. Regardless of cardinality, all Dirichlet parameters are the same scalar β.</note>

			<note place="foot" n="2"> The root node is not considered a level. 3 All tree prior examples are real sub-trees of the priors built on Gigaword.</note>

			<note place="foot" n="4"> https://catalog.ldc.upenn.edu/ ldc2011t07.</note>

			<note place="foot" n="5"> SVM-light: http://svmlight.joachims.org/. SVM-multiclass: https://www.cs.cornell.edu/ people/tj/svm_light/svm_multiclass.html. 6 tLDA models with HAC prior do not have this feature, because the paths have a 1-to-1 mapping with the vocabulary.</note>

			<note place="foot" n="7"> tLDA Java implementation converges in twelve hours; LCTM needs sixty hours (2.8GHz Intel Xeon and 110G RAM).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank the anonymous reviewers for their in-sightful and constructive comments. This research has been supported in part, under subcontract to Raytheon BBN Technologies, by DARPA award HR0011-15-C-0113. Boyd-Graber is also sup-ported by NSF grants <ref type="bibr">IIS-1320538, IIS-1409287, IIS-1564275, IIS-1652666, and NCSE-1422492.</ref> Any opinions, findings, conclusions, or recom-mendations expressed here are those of the authors and do not necessarily reflect the view of the spon-sors.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Incorporating domain knowledge into topic modeling via Dirichlet forest priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Andrzejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A correlated topic model of science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A topic model for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Accurate methods for the statistics of surprise and coincidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Dunning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Parsing algorithms and metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An efficient sequential Monte Carlo algorithm for coalescent clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilan</forename><surname>Görür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient correlated topic modeling with topic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A latent concept topic model for robust topic inference using word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Binary to bushy: Bayesian hierarchical clustering with the Beta coalescent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Irene</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Polylingual tree-based topic models for translation domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Opinion spam and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Web Search and Data Mining</title>
		<meeting>ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Komplexitätsreduktion in Multivariaten Datenstrukturen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SFB</title>
		<imprint>
			<biblScope unit="volume">475</biblScope>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>Universität Dortmund</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
	<note>Making large-scale SVM learning practical</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Newsweeder: Learning to filter netnews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical agglomerative clustering procedure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alena</forename><surname>Lukasová</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Supervised topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving topic coherence with regularized topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin</forename><forename type="middle">V</forename><surname>Bonilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wray</forename><surname>Buntine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lexical and hierarchical topic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Viet-An Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">BLEU: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bayesian agglomerative clustering with coalescents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
