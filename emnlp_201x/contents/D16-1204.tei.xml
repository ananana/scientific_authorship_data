<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
							<email>mooney@cs.utexas.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
							<email>saenko@bu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
								<address>
									<settlement>Austin, Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1961" to="1966"/>
							<date type="published">November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper investigates how linguistic knowledge mined from large text corpora can aid the generation of natural language descriptions of videos. Specifically, we integrate both a neu-ral language model and distributional semantics trained on large text corpora into a recent LSTM-based architecture for video description. We evaluate our approach on a collection of Youtube videos as well as two large movie description datasets showing significant improvements in grammaticality while modestly improving descriptive quality.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ability to automatically describe videos in nat- ural language (NL) enables many important appli- cations including content-based video retrieval and video description for the visually impaired. The most effective recent methods <ref type="bibr" target="#b4">(Venugopalan et al., 2015a;</ref>) use recurrent neural net- works (RNN) and treat the problem as machine translation (MT) from video to natural language. Deep learning methods such as RNNs need large training corpora; however, there is a lack of high- quality paired video-sentence data. In contrast, raw text corpora are widely available and exhibit rich linguistic structure that can aid video description. Most work in statistical MT utilizes both a language model trained on a large corpus of monolingual tar- get language data as well as a translation model trained on more limited parallel bilingual data. This paper explores methods to incorporate knowledge from language corpora to capture general linguistic regularities to aid video description. This paper integrates linguistic information into a video-captioning model based on Long Short Term Memory (LSTM) <ref type="bibr">(Hochreiter and Schmidhuber, 1997</ref>) RNNs which have shown state-of-the-art performance on the task. Further, LSTMs are also effective as language models (LMs) <ref type="bibr">(Sundermeyer et al., 2010)</ref>. Our first approach (early fusion) is to pre-train the network on plain text before train- ing on parallel video-text corpora. Our next two ap- proaches, inspired by recent MT work ( <ref type="bibr" target="#b1">Gulcehre et al., 2015)</ref>, integrate an LSTM LM with the existing video-to-text model. Furthermore, we also explore replacing the standard one-hot word encoding with distributional vectors trained on external corpora.</p><p>We present detailed comparisons between the ap- proaches, evaluating them on a standard Youtube corpus and two recent large movie description datasets. The results demonstrate significant im- provements in grammaticality of the descriptions (as determined by crowdsourced human evaluations) and more modest improvements in descriptive qual- ity (as determined by both crowdsourced human judgements and standard automated comparison to human-generated descriptions). Our main contribu- tions are 1) multiple ways to incorporate knowledge from external text into an existing captioning model, 2) extensive experiments comparing the methods on three large video-caption datasets, and 3) human judgements to show that external linguistic knowl- edge has a significant impact on grammar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">LSTM-based Video Description</head><p>We use the successful S2VT video description framework from <ref type="bibr" target="#b4">Venugopalan et al. (2015a)</ref> as our and decodes them to a sentence. We propose to add knowledge from text corpora to enhance the quality of video description.</p><p>underlying model and describe it briefly here. S2VT uses a sequence to sequence approach ( <ref type="bibr" target="#b3">Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Cho et al., 2014</ref>) that maps an input x = (x 1 , ... , x T ) video frame feature sequence to a fixed dimensional vector and then decodes this into a sequence of output words y = (y 1 , ... , y N ).</p><p>As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, it employs a stack of two LSTM layers. The input x to the first LSTM layer is a sequence of frame features obtained from the penultimate layer (fc 7 ) of a Convolutional Neural Network (CNN) after the ReLu operation. This LSTM layer encodes the video sequence. At each time step, the hidden control state h t is provided as input to a second LSTM layer. After viewing all the frames, the second LSTM layer learns to decode this state into a sequence of words. This can be viewed as using one LSTM layer to model the visual fea- tures, and a second LSTM layer to model language conditioned on the visual representation. We modify this architecture to incorporate linguistic knowledge at different stages of the training and generation pro- cess. Although our methods use S2VT, they are sufficiently general and could be incorporated into other CNN-RNN based captioning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Existing visual captioning models ( <ref type="bibr" target="#b5">Vinyals et al., 2015;</ref><ref type="bibr" target="#b4">Donahue et al., 2015</ref>) are trained solely on text from the caption datasets and tend to exhibit some linguistic irregularities associated with a restricted language model and a small vocabulary. Here, we investigate several techniques to integrate prior lin- guistic knowledge into a CNN/LSTM-based net- work for video to text (S2VT) and evaluate their ef- fectiveness at improving the overall description.</p><p>Early Fusion. Our first approach (early fusion), is to pre-train portions of the network modeling lan- guage on large corpora of raw NL text and then continue "fine-tuning" the parameters on the paired video-text corpus. An LSTM model learns to esti- mate the probability of an output sequence given an input sequence. To learn a language model, we train the LSTM layer to predict the next word given the previous words. Following the S2VT architecture, we embed one-hot encoded words in lower dimen- sional vectors. The network is trained on web-scale text corpora and the parameters are learned through backpropagation using stochastic gradient descent. <ref type="bibr">1</ref> The weights from this network are then used to ini- tialize the embedding and weights of the LSTM lay- ers of S2VT, which is then trained on video-text data. This trained LM is also used as the LSTM LM in the late and deep fusion models.</p><p>Late Fusion. Our late fusion approach is similar to how neural machine translation models incorpo- rate a trained language model during decoding. At each step of sentence generation, the video caption model proposes a distribution over the vocabulary. We then use the language model to re-score the fi- nal output by considering the weighted average of the sum of scores proposed by the LM as well as the S2VT video-description model (VM). More specif- ically, if y t denotes the output at time step t, and if p V M and p LM denote the proposal distributions of the video captioning model, and the language mod- els respectively, then for all words y ∈ V in the vocabulary we can recompute the score of each new word, p(y t = y ) as:</p><formula xml:id="formula_0">α · p V M (y t = y ) + (1 − α) · p LM (y t = y ) (1)</formula><p>Hyper-parameter α is tuned on the validation set.</p><p>Deep Fusion. In the deep fusion approach <ref type="figure" target="#fig_1">(Fig. 2)</ref>, we integrate the LM a step deeper in the genera- tion process by concatenating the hidden state of the language model LSTM (h LM t ) with the hidden state of the S2VT video description model (h V M t ) and use the combined latent vector to predict the out- put word. This is similar to the technique proposed by <ref type="bibr" target="#b1">Gulcehre et al. (2015)</ref> for incorporating language models trained on monolingual corpora for machine translation. However, our approach differs in two  <ref type="formula">(2)</ref> we fix the weights of the LSTM language model but train the full video captioning network. In this case, the probability of the predicted word at time step t is:</p><formula xml:id="formula_1">p(y t | y &lt;t , x) ∝ exp(Wf(h V M t , h LM t ) + b) (2)</formula><p>where x is the visual feature input, W is the weight matrix, and b the biases. We avoid tuning the LSTM LM to prevent overwriting already learned weights of a strong language model. But we train the full video caption model to incorporate the LM outputs while training on the caption domain.</p><p>Distributional Word Representations. The S2VT network, like most image and video cap- tioning models, represents words using a 1-of-N (one hot) encoding. During training, the model learns to embed "one-hot" words into a lower 500d space by applying a linear transformation. However, the embedding is learned only from the limited and possibly noisy text in the caption data. There are many approaches ( <ref type="bibr">Mikolov et al., 2013;</ref><ref type="bibr" target="#b2">Pennington et al., 2014</ref>) that use large text corpora to learn vector-space representations of words that capture fine-grained semantic and syntactic regularities. We propose to take advantage of these to aid video description. Specifically, we replace the embedding matrix from one-hot vectors and instead use 300-dimensional GloVe vectors ( <ref type="bibr" target="#b2">Pennington et al., 2014</ref>) pre-trained on 6B tokens from Gigaword and Wikipedia 2014. In addition to using the distributional vectors for the input, we also explore variations where the model predicts both the one-hot word (trained on the softmax loss), as well as predicting the distributional vector from the LSTM hidden state using Euclidean loss as the objective. Here the output vector (y t ) is computed as y t = (W g h t + b g ), and the loss is given by:</p><formula xml:id="formula_2">L(y t , w glove ) = (W g h t + b g ) − w glove 2 (3)</formula><p>where h t is the LSTM output, w glove is the word's GloVe embedding and W , b are weights and biases. The network then essentially becomes a multi-task model with two loss functions. However, we use this loss only to influence the weights learned by the network, the predicted word embedding is not used.</p><p>Ensembling. The overall loss function of the video-caption network is non-convex, and difficult to optimize. In practice, using an ensemble of net- works trained slightly differently can improve per- formance <ref type="bibr">(Hansen and Salamon, 1990</ref>). In our work we also present results of an ensemble by averaging the predictions of the best performing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets. Our language model was trained on sentences from Gigaword, BNC, UkWaC, and Wikipedia. The vocabulary consisted of 72,700 most frequent tokens also containing GloVe embed- dings. Following the evaluation in <ref type="bibr" target="#b4">Venugopalan et al. (2015a)</ref>, we compare our models on the Youtube dataset (Chen and Dolan, 2011), as well as two large movie description corpora: MPII-MD ( <ref type="bibr" target="#b4">Rohrbach et al., 2015)</ref> and M-VAD ( ).</p><p>Evaluation Metrics. We evaluate performance using machine translation (MT) metrics ME- TEOR <ref type="bibr">(Denkowski and Lavie, 2014</ref>) and BLEU ( <ref type="bibr">Papineni et al., 2002</ref>) to compare the machine- generated descriptions to human ones. For the movie corpora which have just a single description we use only METEOR which is more robust.</p><p>Human Evaluation. We also obtain human judge- ments using Amazon Turk on a random subset of 200 video clips for each dataset. Each sentence was rated by 3 workers on a Likert scale of 1 to 5 (higher is better) for relevance and grammar. No video was provided during grammar evaluation. For movies, due to copyright, we only evaluate on grammar. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Youtube Video Dataset Results</head><p>Comparison of the proposed techniques in <ref type="table">Table 1</ref> shows that Deep Fusion performs well on both ME- TEOR and BLEU; incorporating Glove embeddings substantially increases METEOR, and combining them both does best. Our final model is an ensem- ble (weighted average) of the Glove, and the two Glove+Deep Fusion models trained on the external and in-domain COCO ( ) sentences.</p><p>We note here that the state-of-the-art on this dataset is achieved by HRNE (Pan et al., 2015) (METEOR 33.1) which proposes a superior visual processing pipeline using attention to encode the video. Human ratings also correlate well with the ME- TEOR scores, confirming that our methods give a modest improvement in descriptive quality. How- ever, incorporating linguistic knowledge signifi- cantly 2 improves the grammaticality of the results, making them more comprehensible to human users.</p><p>Embedding Influence. We experimented multiple ways to incorporate word embeddings: (1) GloVe in- put: Replacing one-hot vectors with GloVe on the LSTM input performed best. (2) Fine-tuning: Ini- tializing with GloVe and subsequently fine-tuning the embedding matrix reduced validation results by 0.4 METEOR. (3) Input and Predict. Training the LSTM to accept and predict GloVe vectors, as de- scribed in Section 3, performed similar to (1). All scores reported in <ref type="table" target="#tab_2">Tables 1 and 2</ref> correspond to the setting in (1) with GloVe embeddings only as input.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Movie Description Results</head><p>Results on the movie corpora are presented in Ta- ble 2. Both MPII-MD and M-VAD have only a sin- gle ground truth description for each video, which makes both learning and evaluation very challeng- ing (E.g. <ref type="figure" target="#fig_3">Fig.3</ref>). METEOR scores are fairly low on both datasets since generated sentences are com- pared to a single reference translation. S2VT † is a re-implementation of the base S2VT model with the new vocabulary and architecture (embedding dimen- sion). We observe that the ability of external lin- guistic knowledge to improve METEOR scores on these challenging datasets is small but consistent. Again, human evaluations show significant (with p &lt; 0.0001) improvement in grammatical quality. propose CNN-RNN based models that generate a vector rep- resentation for the video and "decode" it using an LSTM sequence model to generate a description. <ref type="bibr" target="#b4">Venugopalan et al. (2015b)</ref> also incorporate exter- nal data such as images with captions to improve video description, however in this work, our focus is on integrating external linguistic knowledge for video captioning. We specifically investigate the use of distributional semantic embeddings and LSTM- based language models trained on external text cor- pora to aid existing CNN-RNN based video descrip- tion models. LSTMs have proven to be very effective language models ( <ref type="bibr">Sundermeyer et al., 2010)</ref>. <ref type="bibr" target="#b1">Gulcehre et al. (2015)</ref> developed an LSTM model for machine translation that incorporates a monolingual language model for the target language showing improved re- sults. We utilize similar approaches (late fusion, deep fusion) to train an LSTM for translating video to text that exploits large monolingual-English cor- pora (Wikipedia, BNC, UkWac) to improve RNN based video description networks. However, unlike <ref type="bibr" target="#b1">Gulcehre et al. (2015)</ref> where the monolingual LM is used only to tune specific parameters of the transla- tion network, the key advantage of our approach is that the output of the monolingual language model is used (as an input) when training the full underlying video description network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Contemporaneous to us, <ref type="bibr">Yu et al. (2015)</ref>, <ref type="bibr">Pan et al. (2015)</ref> and <ref type="bibr" target="#b0">Ballas et al. (2016)</ref> propose video de- scription models focusing primarily on improving the video representation itself using a hierarchical visual pipeline, and attention. Without the attention mechanism their models achieve METEOR scores of 31.1, 32.1 and 31.6 respectively on the Youtube dataset. The interesting aspect, as demonstrated in our experiments ( <ref type="table">Table 1</ref>), is that the contribution of language alone is considerable and only slightly less than the visual contribution on this dataset. Hence, it is important to focus on both aspects to generate better descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper investigates multiple techniques to in- corporate linguistic knowledge from text corpora to aid video captioning. We empirically evaluate our approaches on Youtube clips as well as two movie description corpora. Our results show significant improvements on human evaluations of grammar while modestly improving the overall descriptive quality of sentences on all datasets. While the pro- posed techniques are evaluated on a specific video- caption network, they are generic and can be ap- </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The S2VT architecture encodes a sequence of frames</figDesc><graphic url="image-1.png" coords="2,72.00,57.83,228.07,112.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of our late and deep fusion approaches to integrate an independently trained LM to aid video captioning. The deep fusion model learns jointly from the hidden representations of the LM and S2VT video-to-text model (Vid-LSTM), whereas the late fusion re-scores the softmax output of the video-to-text model. key ways: (1) we only concatenate the hidden states of the S2VT LSTM and language LSTM and do not use any additional context information, (2) we fix the weights of the LSTM language model but train the full video captioning network. In this case, the probability of the predicted word at time step t is:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Two frames from a clip. Models generate visually relevant sentences but differ from groundtruth (GT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Following the success of LSTM-based models on Machine Translation (Sutskever et al., 2014; Bah- danau et al., 2015), and image captioning (Vinyals et al., 2015; Donahue et al., 2015), recent video de- scription works (Venugopalan et al., 2015b; Venu- gopalan et al., 2015a; Yao et al., 2015)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Representative frames from clips in the movie description corpora. S2VT is the baseline model, Glove indicates the model trained with input Glove vectors, and Glove+Deep uses input Glove vectors with the Deep Fusion approach. GT indicates groundtruth sentence. plied to many captioning models. The code and models are shared on http://vsubhashini. github.io/language_fusion.html.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Movie Corpora: METEOR (%) and human 
grammar ratings (1-5, higher is better). Best results in 
bold, * indicates significant over S2VT. 

</table></figure>

			<note place="foot" n="1"> The LM was trained to achieve a perplexity of 120</note>

			<note place="foot" n="2"> Using the Wilcoxon Signed-Rank test, results were significant with p &lt; 0.02 on relevance and p &lt; 0.001 on grammar.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by NSF awards IIS-1427425 and IIS-1212798, and ONR ATL Grant N00014-11-1-010, and DARPA under AFRL grant FA8750-13-2-0026. Raymond Mooney and Kate Saenko also acknowledge support from a Google grant. Lisa Anne Hendricks is supported by the Na-tional Defense Science and Engineering Graduate (NDSEG) Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Delving deeper into convolutional networks for learning video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Ballas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<editor>ICLR. [Chen and Dolan2011] David Chen and William Dolan</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Collecting highly parallel data for paraphrase evaluation</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03535</idno>
	</analytic>
	<monogr>
		<title level="m">On the properties of neural machine translation: Encoderdecoder approaches. Syntax, Semantics and Structure in Statistical Translation</title>
		<editor>Hansen and Salamon1990] L. K. Hansen and P. Salamon</editor>
		<imprint>
			<date type="published" when="1990-10" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>IEEE TPAMI</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural encoder for video representation with application to captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<editor>CVPR. [Papineni et al.2002] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu</editor>
		<meeting>the Empiricial Methods in Natural Language Processing<address><addrLine>Anna Rohrbach, Marcus Rohrbach</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>CVPR. Sundermeyer et al.2010] M. Sundermeyer, R. Schluter, and H. Ney. 2010. Lstm neural networks for language modeling. In INTERSPEECH</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ilya Sutskever, Oriol Vinyals</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using descriptive video services to create a large data source for video annotation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Torabi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01070v1</idno>
	</analytic>
	<monogr>
		<title level="m">Translating videos to natural language using deep recurrent neural networks</title>
		<editor>ICCV. [Venugopalan et al.2015b] Subhashini Venugopalan, Huijuan Xu</editor>
		<meeting><address><addrLine>Jeff Donahue, Marcus Rohrbach, Raymond Mooney, and Kate Saenko</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>NAACL</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Video paragraph captioning using hierarchical recurrent neural networks. CVPR</title>
		<editor>ICCV. [Yu et al.2015] Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, and Wei Xu</editor>
		<meeting><address><addrLine>Nicolas Ballas, Christopher Pal, Hugo Larochelle, and Aaron Courville</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Describing videos by exploiting temporal structure</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
