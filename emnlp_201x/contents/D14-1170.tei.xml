<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Generation of Related Work Sections in Scientific Papers: An Optimization Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">The MOE Key Laboratory of Computational Linguistics Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">The MOE Key Laboratory of Computational Linguistics Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Generation of Related Work Sections in Scientific Papers: An Optimization Approach</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1624" to="1633"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we investigate a challenging task of automatic related work generation. Given multiple reference papers as input, the task aims to generate a related work section for a target paper. The generated related work section can be used as a draft for the author to complete his or her final related work section. We propose our Automatic Related Work Generation system called ARWG to address this task. It first exploits a PLSA model to split the sentence set of the given papers into different topic-biased parts, and then applies regression models to learn the importance of the sentences. At last it employs an optimization framework to generate the related work section. Our evaluation results on a test set of 150 target papers along with their reference papers show that our proposed ARWG system can generate related work sections with better quality. A user study is also performed to show ARWG can achieve an improvement over generic multi-document summarization baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The related work section is an important part of a paper. An author often needs to help readers to understand the context of his or her research problem and compare his or her current work with previous works. A related work section is often used for this purpose to show the differ- ences and advantages of his or her work, com- pared with related research works. In this study, we attempt to automatically generate a related work section for a target academic paper with its reference papers. This kind of related work sec- tions can be used as a basis to reduce the author's time and effort when he or she wants to complete his or her final related work section.</p><p>Automatic related work section generation is a very challenging task. It can be considered a top- ic-biased, multiple-document summarization problem. The input is a target academic paper, which has no related work section, along with its reference papers. The goal is to create a related work section that describes the related works and addresses the relationship between the target pa- per and the reference papers. Here we assume that the set of reference papers has been given as part of the input. Existing works in the NLP and recommendation systems communities have al- ready focused on the task of finding reference papers. For example, citation prediction <ref type="bibr" target="#b24">(Nallapati et al., 2008)</ref> aims at finding individual pa- per citation patterns.</p><p>Generally speaking, automatic related work section generation is a strikingly different prob- lem and it is much more difficult in comparison with general multi-document summarization tasks. For example, multi-document summariza- tion of news articles aims at synthesizing con- tents of similar news and removing the redundant information contained by the different news arti- cles. However, each scientific paper has much specific content to state its own work and contri- bution. Even for the papers that investigate the same research topic, their contributions and con- tents can be totally different. The related work section generation task needs to find the specific contributions of individual papers and arrange them into one or several paragraphs.</p><p>In this study, we focus on the problem of au- tomatic related work section generation and pro- pose a novel system called ARWG to address the problem. For the target paper, we assume that the abstract and introduction sections have already been written by the author and they can be used to help generate the related work section. For the reference papers, we only consider and extract the abstract, introduction, related work and con- clusion sections, because other sections like the method and evaluation sections always describe the extreme details of the specific work and they are not suitable for this task. Then we generate the related work section using both sentence sets which are extracted from the target paper and reference papers, respectively.</p><p>Firstly, we use a PLSA model to group both sentence sets of the target paper and its reference papers into different topic-biased clusters. Sec- ondly, the importance of each sentence in the target paper and the reference papers is learned by using two different Support Vector Regres- sion (SVR) models. At last, a global optimization framework is proposed to generate the related work section by selecting sentences from both the target paper and the reference papers. Mean- while, the framework selects sentences from dif- ferent topic-biased clusters globally.</p><p>Experimental results on a test set of 150 target papers show our method can generate related work sections with better quality than those of several baseline methods. With the ROUGE toolkit, the results indicate the related work sec- tions generated by our system can get higher ROUGE scores. Moreover, our related work sec- tions can get higher rating scores based on a user study. Therefore, our related work sections can be much more suitable for the authors to prepare their final related work sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There are few studies to directly address auto- matic related work generation. <ref type="bibr" target="#b11">Hoang and Kan (2010)</ref> proposed a related work summarization system given the set of keywords arranged in a hierarchical fashion that describes the paper's topic. They used two different rule-based strate- gies to extract sentences for general topics as well as detailed ones.</p><p>A few studies focus on multi-document scien- tific article summarization. <ref type="bibr" target="#b0">Agarwal et al., (2011)</ref> introduced an unsupervised approach to the prob- lem of multi-document summarization. The input is a list of papers cited together within the same source article. The key point of this approach is a topic based clustering of fragments extracted from each co-cited article. They rank all the clus- ters using a query generated from the context surrounding the co-cited list of papers. <ref type="bibr" target="#b34">Yeloglu et al., (2011)</ref> compared four different approaches for multi-document scientific articles summariza- tion: MEAD, MEAD with corpus specific vo- cabulary, LexRank and W3SS.</p><p>Other studies investigate mainly on the single- document scientific article summarization. Early works including <ref type="bibr" target="#b16">(Luhn 1958;</ref><ref type="bibr" target="#b1">Baxendale 1958;</ref><ref type="bibr">Edumundson 1969</ref>) tried to use various features specific to scientific text (e.g., sentence position, or rhetorical clues features). They have proved that these features are effective for the scientific article summarization. Citation information has been already shown effective in summarize the scientific articles. Works including <ref type="bibr" target="#b19">(Mei and Zhai 2008;</ref><ref type="bibr" target="#b25">Qazvinian and Radev 2008;</ref><ref type="bibr" target="#b28">Schwartz and Hearst 2006;</ref><ref type="bibr" target="#b22">Mohammad et al., 2009</ref>) em- ployed citation information for the single scien- tific article summarization. Earlier work ( <ref type="bibr" target="#b23">Nakov et al., 2004</ref>) indicated that citation sentences may contain important concepts that can give useful descriptions of a paper.</p><p>Various methods have been proposed for news document summarization, including rule-based methods ( <ref type="bibr">Barzilay and Elhadad 1997;</ref><ref type="bibr">Marcu and Daniel 1997)</ref>, graph-based methods <ref type="bibr">(Mani and Bloedorn 2000;</ref><ref type="bibr">Michalcea and Tarau 2005)</ref>, learning-based methods ( <ref type="bibr" target="#b4">Conroy et al., 2001;</ref><ref type="bibr" target="#b29">Shen et al., 2007;</ref><ref type="bibr" target="#b26">Ouyang et al., 2007;</ref><ref type="bibr">Galanis et al., 2008)</ref>, optimization- based methods <ref type="bibr" target="#b18">(McDonald 2007;</ref><ref type="bibr" target="#b10">Gillick et al., 2009;</ref><ref type="bibr" target="#b32">Xie et al., 2009;</ref><ref type="bibr" target="#b2">Berg-Kirkpatrick et al., 2011;</ref><ref type="bibr">Lei Huang et al., 2011;</ref><ref type="bibr" target="#b31">Woodsend et al., 2012</ref>; Galanis 2012), etc.</p><p>The most relevant work is <ref type="bibr" target="#b11">(Hoang and Kan, 2010)</ref> as mentioned above. They also assumed the set of reference papers was given as part of the input. They also adopt the hierarchical topic tree that describes the topic structure in the target paper as an essential input for their system. However, it is non-trivial to build the hierar- chical topic tree. Moreover, they do not consider the content of the target paper to construct the related work section, which is actually crucial in the related work section. To the best of our knowledge, no previous works have used super- vised learning and optimization framework to deal with the multiple scientific article summari- zation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Analysis and Corpus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Analysis</head><p>We firstly analyze the structure of related work sections briefly. By using examples for illustra- tion, we can gain insight on how to generate re- lated work sections. A specific related work ex- ample is shown in <ref type="figure">Figure 1</ref>.</p><p>This related work section introduces previous related works for a paper on Automatic Taxono- my Induction. From <ref type="figure">Figure 1</ref>, we can have a glance at the structure of related work sections. Related work sections usually discuss several different topics, such as " pattern-based" and "cluster-based" approaches shown in the <ref type="figure">Figure  1</ref>. Besides the knowledge of previous works, the author often compares his own work with the previous works. The differences and advantages are generally mentioned. The example in <ref type="figure">Figure  1</ref> also indicates this phenomenon.</p><p>Therefore, we design our system to generate related work sections according to the related work section structure mentioned above. Our system takes the target paper for which a related work section needs to be drafted besides its ref- erence papers as input. The goal of our system is to generate a related work section with the above structure. The generated related work section should have several topic-biased parts. The au- thor's own work is also needed to be described and its difference with other works is needed to be emphasized on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Corpus and Preprocessing</head><p>We build a corpus that contains academic papers and their corresponding reference papers. The academic papers are selected from the ACL An- thology <ref type="bibr">1</ref> . The ACL Anthology currently hosts 1 http://aclweb.org/anthology/ over 24,500 papers from major conferences such as ACL, EMNLP, COLING in the fields of com- putational linguistics and natural language pro- cessing. We remove the papers that contain relat- ed work sections with very short length, and ran- domly select 1050 target papers to construct our whole corpus.</p><p>The papers are all in PDF format. We extract their texts by using PDFlib 2 and detect their physical structures of paragraphs, subsections and sections by using ParsCit <ref type="bibr">3</ref> . For the target papers, the related work sections are directly ex- tracted as the gold summaries. The references are also extracted. For the references that can be found in the ACL Anthology, we download them from the ACL Anthology. The other reference papers are searched and downloaded by using Google Scholar. References to books and PhD theses are discarded, for their verbosity may change the problem drastically <ref type="bibr" target="#b21">(Mihalcea and Ceylan, 2007)</ref>.</p><p>The input of our system includes the abstract and introduction sections of the target paper, and the abstract, introduction, related work and con- clusion sections of the reference papers. As men- tioned above, the method and evaluation sections in the reference papers are not used as input be- cause these sections usually describe extreme details of the methods and evaluation results and they are not suitable for related work generation. Note that it is reasonable to make use of the ab- stract and introduction sections of the target pa- per to help generate the related work section, because an author usually has already written the abstract and introduction sections before he or she wants to write the related work section for the target paper. Otherwise, we cannot get any information about the author's own work. All other sections in the target paper are not used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Proposed System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>In this paper, we propose a system called ARWG to automatically generate a related work section for a given target paper. The architecture of our system is shown in <ref type="figure">Figure 2</ref>. We take both the target paper and its reference papers as input and they are represented by several sections men- tioned in Section 3.2. After preprocessing, we extract the feature vectors for sentences in the target paper and the reference papers, respective- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two different topics</head><p>Comparison with the author's work ly. The importance scores for sentences in the target paper and the reference papers are as- signed by using two SVR based sentence scoring models. The two SVR models are trained for sentences in the target paper and the reference papers, respectively. Meanwhile, a topic model is applied to the whole set of sentences in both the target paper and reference papers. The sentences are grouped into several different topic-biased clusters. The sentences with importance scores and topic cluster information are taken as the input for the global optimization framework. The optimization framework extracts sentences to describe both the author's own work and back- ground knowledge. More details of each part will be discussed in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Topic Model Learning</head><p>As mentioned in the previous section, the related work section usually addresses several different topics. The topics may be different research themes or different aspects of a broad research theme. The related work section should describe the specific details for each topic, respectively.</p><p>Therefore, we aim to discover the hidden top- ics of the input papers, and we use the Probabil- istic latent semantic analysis (PLSA) <ref type="bibr" target="#b14">(Hofmann, 1999)</ref> to solve this problem.</p><p>The PLSA approach models each word in a document as a sample from a mixture model. The mixture components are multinomial ran- dom variables that can be viewed as representa- tions of "topics". Different words in a document may be generated from different topics. Each document is represented a list of mixing propor- tions for these mixture components and can be reduced to a probability distribution on a fixed set of topics.</p><p>Considering that the sentences in one paper may relate to different topics, we treat each sen- tence as a "document" d. We treat the noun phases in the sentences as the "words" w. In or- der to extract the noun phrases, chunking imple- mented by the OpenNLP toolkit 4 is applied to the sentences. Noun phrases that contain words such as "paper" and "data" are discarded.</p><p>Then the sentences with their corresponding noun phrases are taken as input into the PLSA model. Here both the sentences in the target pa- per and the sentences in the reference papers are treated the same in the model. Finally, we can get the sentence set with topic information and use it in the subsequent steps. Each sentence has a topic weight t in each topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sentence Important Assessment</head><p>In our proposed system, sentence importance assessment aims to assign an importance score to each sentence in the target paper and reference papers. The score of each sentence will be used in the subsequent optimization framework. We propose to use the support vector regression model to achieve this goal. In the above topic model learning process, we do not distinguish the sentences in the target paper and reference pa- pers. In contrast, we train two different support vector regression models separately for the sen- tences in the target paper and the sentences in the reference papers. In the related work section, the sentences that describe the author's own work usually address the differences from the related works, while the sentences that describe the re- lated works often focus on the specific details. We think the two kinds of sentences should be treated differently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scoring Method</head><p>To construct training data based on the papers collected, we apply a similarity scoring method to assign the importance scores to the sentences in the papers. The main hypothesis is that the sentences in the gold related work sections should summarize the target paper and reference papers as well. Thus the sentences in the papers which are more similar to the sentences in the gold related work sections should be considered more important and suitable to be selected. Our scoring method should assign higher scores to them. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Postprocessing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Section</head><p>We define the importance score of a sentence in the papers as below:</p><p>í µí± í µí±í µí±í µí±í µí±(í µí± ) = í µí±í µí±í µí±¥</p><formula xml:id="formula_0">í µí± í µí± * ∈í µí± * (í µí± í µí±í µí±(í µí± , í µí± í µí± * )) (1)</formula><p>where s is a sentence in the papers, í µí± * is the set of the sentences in the corresponding gold relat- ed work section. The standard cosine measure is employed as the similarity function.</p><p>Considering the difference between the sen- tences that describe the author's work and the sentences that describe the related works, we split the set of sentences in the gold related work section into two parts: one discusses the author's own work and the other introduces the related works. We observe that sentences related to the author's own work often feature specific words or phrases (such as "we", "our work", "in this paper" etc.) in the related work section. So we check the sentences about whether they contain clue words or phrases (i.e., "in this paper", "our work" and 18 other phrases). If the clue phrase check fails, the sentence belongs to the related work part. If not, it belongs the own work part.</p><p>Thus for the sentences in the target paper, í µí± * is the set of sentences in the own work part of the gold related work section, while for the sentences in the reference papers, í µí± * is the set of sentences in the related work part of the gold related work section. Then we can use the scoring method to compute the target scores of the sentences in the training set. It is noteworthy that two SVR mod- els can be trained on the two parts of the training data, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature</head><p>Each sentence is represented by a set of features. The common features used for the sentences of the target paper and reference papers are shown in <ref type="table" target="#tab_0">Table 1</ref>. The additional features applied to the sentences of the target paper are introduced in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Here, s is a sentence that needs to extract fea- tures. th is paper title, section headings and sub- section headings set of the reference papers or target paper for the two SVR models, respective- ly. Each feature with "*" represent a feature set that contains similar features.</p><p>All the features are scaled into <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. Thus we can learn SVR models based on the features and importance scores of the sentences, and then use the models to predict an importance score for each sentence in the test set. The SVR models are trained and applied for the target paper and reference papers, respectively.  <ref type="bibr">*</ref> The position of s in its section or subsection í µí±í µí±í µí°¼(í µí± ) <ref type="bibr">*</ref> The parse tree information of s, including the number of noun phrase and verb phrases, the depth of the parse tree, etc. í µí°¼í µí± í µí°»í µí±í µí±í µí±(í µí± ) <ref type="bibr">*</ref> Indicates whether s is the first sen- tence of the section or subsection í µí°¼í µí± í µí°¸í µí±í µí±(í µí± ) <ref type="bibr">*</ref> Indicates whether s is the last sen- tence of the section or subsection SWP(s)</p><p>The percentage of the stop words Length(s)</p><p>The length of sentence s Length_rw(s)</p><p>The length of s after removing stop words SI(s)</p><p>The section index of s that indi- cates which section s is from. í µí° ¶í µí±í µí±¢í µí±í µí±ℎí µí±í µí±í µí± í µí±(í µí± ) * Indicates whether a clue phrase appears in s. the clue phrases in- clude "our work", "propose" and other 20 words. Each clue phrase corresponds to one feature. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HasCitation(s)</head><p>Indicates whether s contains a citation í µí±hí µí±í µí±í µí± í µí±í µí°¹í µí±í µí±í µí° ¶í µí±í µí±(í µí± ) * Indicates whether s contains words or phrases used for com- parison such as "in contrast", "instead" and other 26 words. Each word or phrase corre- sponds to one feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">A Global Optimization Framework</head><p>In the above steps, we can get the predicted im- portance score and topic information for each sentence in the target paper and reference papers.</p><p>Here, we introduce a global optimization frame- work to generate the related work section. According to the structure of the related work section mentioned above, the related work sec- tion usually discusses several topics. In each top- ic, the related works and their details are intro- duced. Besides, the author often compares his own work with these previous works.</p><p>Therefore, we propose to formulate the genera- tion as an optimization problem. Basically, we will be searching for a set of sentences to opti- mize the objective function. To design the objective function, three aspects should be considered: 1) First, the related work section we generate should introduce the previous works well. In our assumption, sentences with higher im- portance scores are better to be selected. In addition, very short sentences should be pe- nalized. So we introduce the first part of our objective function below: ∑ (í µí±í µí± í µí± í µí±¤í µí± í µí± ∑ í µí±¡ í µí±í µí± í µí±¥í µí± í µí±í µí± ) í µí± í µí±=1 í µí±í µí± í µí±=1</p><p>(2) We add the sentence length as a multipli- cation factor in order to penalize the very short sentences, or the objective function tends to select more and shorter sentences. At the same time, the objective function does not tend to select the very long sentences. The total length of the sentences selected is fixed. So if the objective function tends to select the longer sentences, the fewer sen- tences can be selected. A tradeoff needs to be made between the number and the average length of the sentences selected.</p><p>The constraints introduced below ensure that the sentence can only be selected into one topic and the topic weight is used to measure the degree that the sentence is rele- vant to the specific topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Second, similar to the first part, we should</head><p>consider the own work part of the related work section. Thus the second part of our ob- jective function is shown as follows: ∑ (í µí±í µí±¡ í µí± í µí±¤í µí±¡ í µí± ∑ í µí±¡ í µí±í µí± í µí±¥í µí±¡ í µí±í µí± ) í µí± í µí±=1 í µí±í µí±¡ í µí±=1 + ∑ ∑ í µí±¥í µí±¡ í µí±í µí± í µí± í µí±=1 í µí± í µí±¡ í µí± ∈í µí±í µí±¡ í µí± í µí± í µí± í µí± ∈í µí±í µí± í µí± ≥ í µí±¦ í µí± , í µí± = 1, … |í µí°µ| (13) í µí±¥í µí± í µí±í µí± , í µí±¥í µí±¡ í µí±í µí± , í µí±¦ í µí± ∈ {0,1}</p><p>All the three parts in the objective function are normalized to <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> by using the maximum length í µí°¿ í µí±í µí±í µí±¥ and the total number of bigrams |í µí°µ * |.</p><p>í µí¼ 1 , í µí¼ 2 and í µí¼ 3 are parameters for tuning the three parts and we set í µí¼ 1 +í µí¼ 2 +í µí¼ 3 = 1. We explain the constraints as follows: Constraint (6): It ensures that the total word count of the part of topic j does not exceed í µí°¿ í µí± .</p><p>Constraints <ref type="formula">(7)</ref>, <ref type="formula">(8)</ref>: The two constraints try to balance the lengths of the previous works part and the own work part, respectively. í µí»¼ is set to 2/3.</p><p>Constraints (9), (10): These two constraints guarantee that the sentence can only be included into one topic.</p><p>Constraints (11), (12): When these two con- straints hold, all bigrams that í µí± í µí± has are selected if í µí± í µí± is selected.</p><p>Constraint <ref type="formula">(13)</ref>: This constraint makes sure that at least one sentence in í µí±í µí± í µí± or í µí±í µí±¡ í µí± is select- ed if bigram í µí± í µí± is selected.</p><p>Therefore, we transform our optimization problem into a linear programing problem. We solve this linear programming problem by using the IBM CPLEX optimizer <ref type="bibr">5</ref> . It generally takes tens of seconds to solve the problem and it is very efficient.</p><p>Finally, ARWG post-processes sentences to improve readability, including replacing agentive forms with a citation to the specific article (e.g., "our work" → "(Hoang and Kan, 2010)") for the sentences extracted from reference papers. The sentences belonging to different topics are placed separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Setup</head><p>To set up our experiments, we divide our dataset which contains 1050 target papers and their ref- erence papers into two parts: 700 target papers for training, 150 papers for test and the other 200 papers for validation. The PLSA topic model is applied to the whole dataset. We train two SVR regression models based on the own work part and the previous work part of the training data and apply the models to the test data. The global optimization framework is used to generate the related work sections. We set the maximum word count of the generated related work section to be equal to that of the gold related work section. The parameter values of í µí¼ 1 , í µí¼ 2 and í µí¼ 3 are set to 0.3, 0.1 and 0.6, respectively. The parameter val- ues are tuned on the validation data.</p><p>We compare our system with five baseline sys- tems: MEAD-WT, LexRank-WT, ARWG-WT, MEAD and LexRank. MEAD 6 ( ) is an open-source extractive multi- document summarizer. LexRank 7 ( <ref type="bibr">Eran and Radev, 2004</ref>) is a multi-document summarization system which is based on a random walk on the similarity graph of sentences. We also implement the MEAD, LexRank baselines and our method with only the reference papers (i.e. the target pa- per's content is not considered). Those methods are signed by "-WT".</p><p>To evaluate the effectiveness of the SVR mod- els we employ, we implement a baseline system RWGOF that uses the random walk scores as the important scores of the sentences and take the scores as inputs for the same global optimization framework as our system to generate the related work section. The random walk scores are com- puted for the sentences in the reference papers and the target paper, respectively.</p><p>We use the ROUGE toolkit to evaluate the content quality of the generated related work sec- tions. ROUGE <ref type="bibr" target="#b15">(Lin, 2004</ref>) is a widely used au- tomatic summarization evaluation method based on n-gram comparison. Here, we use the F- Measure scores of ROUGE-1, ROUGE-2 and ROUGE-SU4. The model texts are set as the gold related work sections extracted from the target papers, and word stemming is utilized. ROUGE-N is an n-gram based measure between a candidate text and a reference text. The recall oriented score, the precision oriented score and the F-measure score for ROUGE-N are comput- ed as follows: where n stands for the length of the n-gram í µí±í µí±í µí±í µí± í µí± , and í µí° ¶í µí±í µí±¢í µí±í µí±¡ í µí±í µí±í µí±¡í µí±ℎ (í µí±í µí±í µí±í µí± í µí± ) is the maxi- mum number of n-grams co-occurring in a can- didate text and a reference text. In addition, we conducted a user study to sub- jectively evaluate the related work sections to get more evidences. We selected the related work sections generated by different methods for 15 random target papers in the test set. We asked three human judges to follow an evaluation guideline we design and evaluate these related work sections. The human judges are graduate students in the computer science field and they did not know the identities of the evaluated relat- ed work sections. They were asked to give a rat- ing on a scale of 1 (very poor) to 5 (very good) for the correctness, readability and usefulness of the related work sections, respectively: 1) Correctness: Is the related work section ac- tually related to the target paper? 2) Readability: Is the related work section easy for the readers to read and grasp the key content? 3) Usefulness: Is the related work section useful for the author to prepare their final related work section? Paired T-Tests are applied to both the ROUGE scores and rating scores for comparing ARWG and baselines and comparing the systems with WT and without WT.   The evaluation results over ROUGE metrics are presented in <ref type="table" target="#tab_3">Table 4</ref>. It shows that our proposed system can get higher ROUGE scores, i.e., better content quality. In our system, we split the sen- tence set into different topic-biased parts, and the importance scores of sentences in the target pa- per and reference papers are learned differently. So the obtained importance scores of the sen- tences are more reliable. The global optimization framework considers the extraction of both the previous work part and the own work part. We can see the importance of the own work part by comparing the results of the methods with or without considering the own work part. MEAD, LexRank and our method all get a significant improvement after considering the own work part by extracting sentences from the target paper. The results also prove our as- sumption about the related work section structure. <ref type="figure" target="#fig_4">Figure 3</ref> presents the fluctuation of ROUGE scores when tuning the parameters λ 1 , λ 2 and λ 3 . We can see our method generally performs better than the baselines. All the three parts in the ob- jective function are useful to generate related work sections with good quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and Discussion</head><p>The average scores rated by human judges for each method are showed in <ref type="table" target="#tab_4">Table 5</ref>. We can see that the related work sections generated by our system are more related to the target papers. Moreover, because of the good structure of our generated related work sections, our generated related work sections are considered more reada- ble and more useful for the author to prepare the final related work sections.</p><p>T-test results show that the performance im- provements of our method over baselines are statistically significant on both automatic and manual evaluations. Most of p-values for t-test are far smaller than 0.01.</p><p>Overall, the results indicate that our method can generate much better related work sections than the baselines on both automatic and human evaluations. <ref type="table" target="#tab_5">Table 6</ref> shows the comparison results between ARWG and RWGOF. We can see ARWG per- forms better than RWGOF. It proves that the SVR models can better estimate the importance scores of the sentences. For the SVR models are trained from the large dataset, the sentence scores predicted by the SVR models can be more reliable to be used in the global optimization framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>This paper proposes a novel system called ARWG to generate related work sections for ac- ademic papers. It first exploits a PLSA model to split the sentence set of the given papers into dif- ferent topic-biased parts, and then applies regres- sion models to learn the importance scores of the sentences. At last an optimization framework is proposed to generate the related work section. Evaluation results show that our system can gen- erate much better related work sections than the baseline methods.</p><p>In future work, we will make use of citation sentences to improve our system. Citation sen- tences are the sentences that contains an explicit reference to another paper and they usually high- light the most important aspects of the cited pa- pers. So citation sentences are likely to contain important and rich information for generating related work sections.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2There has been a substantial amount of research on automatic taxonomy induction. As we mentioned earlier, two main approaches are pattern-based and clustering-based. Pattern-based approaches are the main trend for automatic taxonomy induction. … Pattern-based approaches started from and still pay a great deal of attention to the most common is-a relations. … Clustering-based approaches usually represent word contexts as vectors and cluster words based on similarities of the vectors (Brown et al., 1992 ; Lin, 1998 ). … Many clustering-based approaches face the challenge of appropriately labeling non-leaf clusters. … In this paper, we take an incremental clustering approach,... The advantage of the incremental approach is that it eliminates the trouble of inventing cluster labels and concentrates on placing terms in the correct positions in a taxonomy hierarchy. The work by Snow et al. ( 2006 ) is the most similar to ours … Moreover, our approach employs heterogeneous features from a wide range; while their approach only used syntactic dependency.</head><label>199219982006</label><figDesc>Figure 1: A sample related work section (Yang and Callan 2009)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: System Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>At last, redundancy reduction should be con- sidered in the objective function. The last part of the objective function is shown below: ∑ í µí± í µí± í µí± í µí±¦ í µí± |í µí°µ| í µí±=1 (4) The intuition is that the more unique bi- grams the related work section contains, the less redundancy the related work section has. We add í µí± í µí± í µí± as the weight of the bigram in or- der to include more important bigrams. By combing all the parts defined above, we have the following full objective function: max í µí±¥í µí±,í µí±¥í µí±¡ í µí¼ 1 ∑ ( í µí±í µí± í µí± í µí»¼í µí°¿ í µí±í µí±í µí±¥ í µí±¤í µí± í µí± ∑ í µí±¡ í µí±í µí± í µí±¥í µí± í µí±í µí± ) í µí± í µí±=1 + í µí±í µí± í µí±=1 í µí¼ 2 ∑ ( í µí±í µí±¡ í µí± (1−í µí»¼)í µí°¿ í µí±í µí±í µí±¥ í µí±¤í µí±¡ í µí± ∑ í µí±¡ í µí±í µí± í µí±¥í µí±¡ í µí±í µí± )∑ í µí±í µí± í µí± í µí±¥í µí± í µí±í µí± í µí±í µí± í µí±=1 + ∑ í µí±í µí±¡ í µí± í µí±¥í µí±¡ í µí±í µí± í µí±í µí±¡ í µí±=1 &lt; í µí°¿ í µí± , í µí±í µí±í µí± í µí± =∑ í µí±¦ í µí± í µí± í µí± ∈í µí°µ í µí± ≥ |í µí°µ í µí± | ∑ í µí±¥í µí± í µí±í µí± í µí± í µí±=1 , í µí±í µí±í µí± í µí± = 1, … , í µí±í µí± (11) ∑ í µí±¦ í µí± í µí± í µí± ∈í µí°µ í µí± ≥ |í µí°µ í µí± | ∑ í µí±¥í µí±¡ í µí±í µí± í µí± í µí±=1 , í µí±í µí±í µí± í µí± = 1, … , í µí±í µí±¡ (12) ∑ ∑ í µí±¥í µí± í µí±í µí± í µí± í µí±=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>í µí± í µí±í µí±í µí°ºí µí°¸−µí°¸− í µí± í µí± í µí±í µí±í µí±í µí±í µí± = ∑ ∑ í µí° ¶í µí±í µí±¢í µí±í µí±¡ í µí±í µí±í µí±¡í µí±ℎ (í µí±í µí±í µí±í µí± í µí± ) í µí±í µí±í µí±í µí± í µí± í µí±∈{í µí± í µí±í µí±í µí±í µí±í µí±í µí±í µí±í µí± í µí±í µí±í µí±¥í µí±¡} / ∑ ∑ í µí° ¶í µí±í µí±¢í µí±í µí±¡(í µí±í µí±í µí±í µí± í µí± ) í µí±í µí±í µí±í µí± í µí± í µí±∈{í µí± í µí±í µí±í µí±í µí±í µí±í µí±í µí±í µí± í µí±í µí±í µí±¥í µí±¡} (15) í µí± í µí±í µí±í µí°ºí µí°¸−µí°¸− í µí± í µí±í µí±í µí±í µí±í µí±í µí± í µí±í µí±í µí± = ∑ ∑ í µí° ¶í µí±í µí±¢í µí±í µí±¡ í µí±í µí±í µí±¡í µí±ℎ (í µí±í µí±í µí±í µí± í µí± ) í µí±í µí±í µí±í µí± í µí± í µí±∈{í µí± í µí±í µí±í µí±í µí±í µí±í µí±í µí±í µí± í µí±í µí±í µí±¥í µí±¡} / ∑ ∑ í µí° ¶í µí±í µí±¢í µí±í µí±¡(í µí±í µí±í µí±í µí± í µí± ) í µí±í µí±í µí±í µí± í µí± í µí±∈{í µí° ¶í µí±í µí±í µí±í µí±í µí±í µí±í µí±¡í µí± í µí±í µí±í µí±¥í µí±¡} (16) í µí± í µí±í µí±í µí°ºí µí°¸−µí°¸− í µí± í µí°¹−í µí±í µí±í µí±í µí± í µí±¢í µí±í µí± = 2 * í µí± í µí±í µí±í µí°ºí µí°¸−µí°¸− í µí± í µí± í µí±í µí±í µí±í µí±í µí± * í µí± í µí±í µí±í µí°ºí µí°¸−µí°¸− í µí± í µí±í µí±í µí±í µí±í µí±í µí± í µí±í µí±í µí± / í µí± í µí±í µí±í µí°ºí µí°¸−µí°¸− í µí± í µí± í µí±í µí±í µí±í µí±í µí± + í µí± í µí±í µí±í µí°ºí µí°¸−µí°¸− í µí± í µí±í µí±í µí±í µí±í µí±í µí± í µí±í µí±í µí± (17)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Parameter influences (horizontal, vertical axis are í µí¼ 1 , í µí¼ 2 , respectively, í µí¼ 3 = 1 − í µí¼ 1 − í µí¼ 2 )</figDesc><graphic url="image-2.png" coords="8,248.38,105.28,77.28,70.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Common features employed in the SVR models Feature Description í µí±í µí±í µí±(í µí± , í µí±¡ℎ) * The similarity between s and each title in th; Stop words are</head><label>1</label><figDesc></figDesc><table>removed 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Additional features for sentences in the 
target paper 

Feature 
Description 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : Notations used in this section Symbol Description í µí± í µí± í µí± /í µí± í µí±¡ í µí± the sentence in the reference/target paper í µí±í µí± í µí± /í µí±í µí±¡ í µí± the length of sentence í µí± í µí± í µí± / í µí± í µí±¡ í µí± í µí±¤í µí± í µí± /í µí±¤í µí±¡ í µí± the importance score of í µí± í µí± í µí± /í µí± í µí±¡ í µí± í µí±¥í µí± í µí±í µí± /í µí±¥í µí±¡ í µí±í µí± indicates whether í µí± í µí± í µí± /í µí± í µí±¡ í µí± is selected into the part of topic j in the</head><label>3</label><figDesc></figDesc><table>generated related 
work section 
nr/nt 
the number of sentences in the refer-
ence/target papers 
m 
the topic count 
í µí±¡ í µí±í µí± 
the topic weight of í µí± í µí± í µí± /í µí± í µí±¡ í µí± in topic j from 
the PLSA model 
B 
the set of unique bigrams 
í µí±¦ í µí± 
indicates whether bigram í µí± í µí± is included 
in the result 
í µí± í µí± í µí± 
the count of the occurrences of bigram í µí± í µí± 
in the both target paper and reference 
papers 
í µí°¿ í µí±í µí±í µí±¥ 
the maximum word count of the related 
work section 
í µí°¿ í µí± 
the maximum word count of the part of 
topic j which depends on the percentage 
of sentences belong to topic j 
í µí°µ  *  
the total set of bigrams in the whole pa-
per set 
í µí°µ í µí± 
the set of bigrams that sentence í µí± í µí± í µí± /í µí± í µí±¡ í µí± 
contains 
í µí±í µí± í µí± /í µí±í µí±¡ í µí± 
the set of sentences that include bigram 
í µí± í µí± in the reference/target papers 
í µí¼ 1 , í µí¼ 2 , í µí¼ 3 parameters for tuning 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 : ROUGE F-measure comparison results</head><label>4</label><figDesc></figDesc><table>Method 
ROUGE-1 
ROUGE-2 
ROUGE-
SU4 

Mead-
WT 

0.39720 
0.08785 
0.14694 

LexRank-
WT 

0.43267 
0.09228 
0.16312 

ARWG-
WT 

0.45077  * {1,2} 
0.09987  * {1,2} 
0.16731  * {1}#{2} 

Mead 

0.41012  * {1} 
0.09642  * {1} 
0.15441  * {1} 

LexRank 

0.44235  * {2} 
0.10090  * {2} 
0.17067  * {2} 

ARWG 

í µí¿. í µí¿í µí¿í µí¿í µí¿í µí¿  * {í µí¿−í µí¿} 
í µí¿. í µí¿í µí¿í µí¿í µí¿í µí¿  * {í µí¿−í µí¿} í µí¿. í µí¿í µí¿í µí¿í µí¿í µí¿  * {í µí¿−í µí¿} 

( * represents pairwise t-test value p &lt; 0.01; # rep-
resents p &lt; 0.05; the numbers in the brackets rep-
resent the indices of the methods compared, e.g. 
1 for MEAD-WT, 2 for LexRank-WT, etc.) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 5 : Average rating scores of judges</head><label>5</label><figDesc></figDesc><table>Method 
Correctness 
Readability Usefulness 
Mead 

2.971 
2.664 
2.716 

LexRank 

2.958 
2.847 
2.784 

ARWG 

3.433  * # 
3.420  * # 
3.382  * # 

( *# represents pairwise t-test value p &lt; 0.01, 
compared with Mead and LexRank, respectively.) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>ROUGE F-measure comparison of dif-
ferent sentence importance scores 

Method 
ROUGE-1 
ROUGE-2 
ROUGE-SU4 
RWGOF 

0.46932 
0.11791 
0.18426 

ARWG 

0.47940 
0.12176 
0.18618 

</table></figure>

			<note place="foot" n="5"> www-01.ibm.com/software/integration/optimization/cplexoptimizer/ 6 http://www.summarization.com/mead/ 7 In our experiments, LexRank performs much better than the more complex variant-C-LexRank (Qazvinian and Radev, 2008), and thus we choose LexRank, rather than CLexRank, to represent graph-based summarization methods for comparison in this paper.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work was supported by National Natural Science Foundation of China (61170166, 61331011), Beijing Nova Program (2008B03) and National Hi-Tech Research and Develop-ment Program (863 Program) of China (2012AA011101). We also thank the anonymous reviewers for very helpful comments. The corre-sponding author of this paper, according to the meaning given to this role by Peking University, is Xiaojun Wan.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards multidocument summarization of scientific articles: making interesting comparisons with SciSumm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Gvr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Shankar Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><forename type="middle">Penstein</forename><surname>Rosé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages</title>
		<meeting>the Workshop on Automatic Summarization for Different Genres, Media, and Languages</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="8" to="15" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Machine-made index for technical literature: an experiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phyllis</forename><forename type="middle">B</forename><surname>Baxendale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="354" to="361" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Jointly learning to extract and compress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Chung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011" />
			<publisher>TIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Text summarization via hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianne</forename><forename type="middle">P</forename><surname>O&amp;apos;leary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">New methods in automatic extracting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><forename type="middle">P</forename><surname>Edmundson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="264" to="285" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">LexPageRank: Prestige in Multi-Document Text Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In EMNLP</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="365" to="371" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">LexRank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res.(JAIR)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extractive MultiDocument Summarization with Integer Linear Programming and Support Vector Regression</title>
	</analytic>
	<monogr>
		<title level="m">Dimitrios Galanis, Gerasimos Lampouras, and Ion Androutsopoulos</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="911" to="926" />
		</imprint>
	</monogr>
	<note>COLING</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Aueb at tac 2008. InProceedings of the TAC</title>
	</analytic>
	<monogr>
		<title level="m">Dimitrios Galanis, and Prodromos Malakasiotis</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A scalable global model for summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Favre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">InProceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards automated related work summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Duy Vu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="427" to="435" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling document summarization as multiobjective optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Information Technology and Security Informatics (IITSI)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<title level="m">Third International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="382" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries. InText Summarization Branches Out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-04 Workshop</title>
		<meeting>the ACL-04 Workshop</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The automatic creation of literature abstracts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Peter Luhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of research and development</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="165" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Summarizing similarities and differences among related documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjeet</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Bloedorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="35" to="67" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A study of global inference algorithms in multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generating Impact-Based Summaries for Scientific Literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACL</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="816" to="824" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A language independent algorithm for single and multiple document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Explorations in Automatic Book Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Ceylan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="380" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Using citations to generate surveys of scientific paradigms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Egan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Muthukrishan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahed</forename><surname>Qazvinian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zajic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="584" to="592" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Citation sentences for semantic analysis of bioscience text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGIR&apos;04 workshop on Search and Discovery in Bioinformatics</title>
		<meeting>the SIGIR&apos;04 workshop on Search and Discovery in Bioinformatics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint latent topic models for text and citations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><forename type="middle">M</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="542" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scientific paper summarization using citation summary networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Vahed Qazvinian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Developing learning strategies for topic-based summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixteenth ACM conference on Conference on information and knowledge management</title>
		<meeting>the sixteenth ACM conference on Conference on information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MEAD-a platform for multidocument multilingual text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Allison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasha</forename><surname>Blairgoldensohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arda</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanko</forename><surname>Dimitrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliott</forename><surname>Drabek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC</title>
		<meeting>the 4th International Conference on Language Resources and Evaluation (LREC</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Summarizing key concepts using citation sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><forename type="middle">S</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marti</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Linking Natural Language Processing and Biology: Towards Deeper Biological Literature Analysis</title>
		<meeting>the Workshop on Linking Natural Language Processing and Biology: Towards Deeper Biological Literature Analysis</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="134" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Document Summarization Using Conditional Random Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dou</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Tao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2862" to="2867" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Dialogue act modeling for automatic tagging and recognition of conversational speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Ries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Coccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><surname>Van Essdykema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Meteer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="339" to="373" />
		</imprint>
	</monogr>
	<note>Computational linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multiple aspect summarization using integer linear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Woodsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="233" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Leveraging sentence weights in a concept-based optimization framework for extractive meeting summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shasha</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Favre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1503" to="1506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A metric-based framework for automatic taxonomy induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-document summarization of scientific corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozge</forename><surname>Yeloglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Milios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nur</forename><surname>Zincirheywood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 ACM Symposium on Applied Computing</title>
		<meeting>the 2011 ACM Symposium on Applied Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="252" to="258" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
