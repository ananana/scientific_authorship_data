<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country>USA ?</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep Reinforcement Learning</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2231" to="2240"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Building a dialogue agent to fulfill complex tasks, such as travel planning, is challenging because the agent has to learn to collectively complete multiple subtasks. For example, the agent needs to reserve a hotel and book a flight so that there leaves enough time for commute between arrival and hotel check-in. This paper addresses this challenge by formulating the task in the mathematical framework of options over Markov Decision Processes (MDPs), and proposing a hierarchical deep reinforcement learning approach to learning a dialogue manager that operates at different temporal scales. The dialogue manager consists of: (1) a top-level dialogue policy that selects among subtasks or options , (2) a low-level dialogue policy that selects primitive actions to complete the subtask given by the top-level policy, and (3) a global state tracker that helps ensure all cross-subtask constraints be satisfied. Experiments on a travel planning task with simulated and real users show that our approach leads to significant improvements over three baselines, two based on hand-crafted rules and the other based on flat deep reinforcement learning.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There is a growing demand for intelligent personal assistants, mainly in the form of dialogue agents, that can help users accomplish tasks ranging from meeting scheduling to vacation planning. How- ever, most of the popular agents in today's market, such as Amazon Echo, Apple Siri, Google Home and Microsoft Cortana, can only handle very sim- ple tasks, such as reporting weather and requesting songs. Building a dialogue agent to fulfill complex tasks remains one of the most fundamental chal- lenges for the NLP community and AI in general.</p><p>In this paper, we consider an important type of complex tasks, termed composite task, which con- sists of a set of subtasks that need to be fulfilled collectively. For example, in order to make a travel plan, we need to book air tickets, reserve a hotel, rent a car, etc. in a collective way so as to satisfy a set of cross-subtask constraints, which we call slot constraints. Examples of slot constraints for travel planning are: hotel check-in time should be later than the flight's arrival time, hotel check-out time may be earlier than the return flight depart time, the number of flight tickets equals to that of hotel check-in people, and so on.</p><p>It is common to learn a task-completion dia- logue agent using reinforcement learning (RL); see <ref type="bibr" target="#b24">Su et al. (2016)</ref>; <ref type="bibr">Cuayáhuitl (2017)</ref>; <ref type="bibr" target="#b28">Williams et al. (2017)</ref>; <ref type="bibr" target="#b5">Dhingra et al. (2017)</ref> and <ref type="bibr" target="#b13">Li et al. (2017a)</ref> for a few recent examples. Compared to these dialogue agents developed for individ- ual domains, the composite task presents addi- tional challenges to commonly used, flat RL ap- proaches such as DQN ( <ref type="bibr" target="#b17">Mnih et al., 2015)</ref>. The first challenge is reward sparsity. Dialogue pol- icy learning for composite tasks requires explo- ration in a much larger state-action space, and it often takes many more conversation turns between user and agent to fulfill a task, leading to a much longer trajectory. Thus, the reward signals (usu- ally provided by users at the end of a conversa- tion) are delayed and sparse. As we will show in this paper, typical flat RL methods such as DQN with naive ✏-greedy exploration is rather ineffi- cient. The second challenge is to satisfy slot con- straints across subtasks. This requirement makes most of the existing methods of learning multi- domain dialogue agents <ref type="bibr" target="#b2">(Cuayáhuitl, 2009;</ref><ref type="bibr" target="#b9">Gasic et al., 2015b</ref>) inapplicable: these methods train a collection of policies, one for each domain, and there is no cross-domain constraints required to successfully complete a dialogue. The third chal- lenge is improved user experience: we find in our experiments that a flat RL agent tends to switch between different subtasks frequently when con- versing with users. Such incoherent conversations lead to poor user experience, and are one of the main reasons that cause a dialogue session to fail.</p><p>In this paper, we address the above mentioned challenges by formulating the task using the math- ematical framework of options over <ref type="bibr">MDPs (Sutton et al., 1999</ref>), and proposing a method that com- bines deep reinforcement learning and hierarchi- cal task decomposition to train a composite task- completion dialogue agent. At the heart of the agent is a dialogue manager, which consists of (1) a top-level dialogue policy that selects subtasks (options), (2) a low-level dialogue policy that se- lects primitive actions to complete a given subtask, and (3) a global state tracker that helps ensure all cross-subtask constraints be satisfied.</p><p>Conceptually, our approach exploits the struc- tural information of composite tasks for efficient exploration. Specifically, in order to mitigate the reward sparsity issue, we equip our agent with an evaluation module (internal critic) that gives in- trinsic reward signals, indicating how likely a par- ticular subtask is completed based on its current state generated by the global state tracker. Such intrinsic rewards can be viewed as heuristics that encourage the agent to focus on solving a sub- task before moving on to another subtask. Our ex- periments show that such intrinsic rewards can be used inside a hierarchical RL agent to make ex- ploration more efficient, yielding a significantly reduced state-action space for decision making. Furthermore, it leads to a better user experience, as the resulting conversations switch between sub- tasks less frequently.</p><p>To the best of our knowledge, this is the first work that strives to develop a composite task- completion dialogue agent. Our main contribu- tions are three-fold:</p><p>• We formulate the problem in the mathemati- cal framework of options over MDPs.</p><p>• We propose a hierarchical deep reinforce- ment learning approach to efficiently learning the dialogue manager that operates at differ- ent temporal scales.</p><p>• We validate the effectiveness of the proposed approach in a travel planning task on simu- lated as well as real users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Task-completion dialogue systems have attracted numerous research efforts. Reinforcement learn- ing algorithms hold the promise for dialogue pol- icy optimization over time with experience <ref type="bibr" target="#b21">(Scheffler and Young, 2000;</ref><ref type="bibr" target="#b12">Levin et al., 2000;</ref><ref type="bibr" target="#b30">Young et al., 2013;</ref><ref type="bibr" target="#b28">Williams et al., 2017)</ref>. Recent ad- vances in deep learning have inspired many deep reinforcement learning based dialogue systems that eliminate the need for feature engineering ( <ref type="bibr" target="#b24">Su et al., 2016;</ref><ref type="bibr">Cuayáhuitl, 2017;</ref><ref type="bibr" target="#b28">Williams et al., 2017;</ref><ref type="bibr" target="#b5">Dhingra et al., 2017;</ref><ref type="bibr" target="#b13">Li et al., 2017a</ref>). All the work above focuses on single-domain problems. Extensions to composite-domain dia- logue problems are non-trivial due to several rea- sons: the state and action spaces are much larger, the trajectories are much longer, and in turn re- ward signals are much more sparse. All these challenges can be addressed by hierarchical re- inforcement learning <ref type="bibr" target="#b26">(Sutton et al., 1999</ref><ref type="bibr" target="#b25">(Sutton et al., , 1998</ref><ref type="bibr" target="#b23">Singh, 1992;</ref><ref type="bibr" target="#b6">Dietterich, 2000;</ref><ref type="bibr" target="#b1">Barto and Mahadevan, 2003)</ref>, which decomposes a complicated task into simpler subtasks, possibly in a recursive way. Different frameworks have been proposed, such as Hierarchies of Machines <ref type="bibr" target="#b18">(Parr and Russell, 1997)</ref> and MAXQ decomposition <ref type="bibr" target="#b6">(Dietterich, 2000)</ref>. In this paper, we choose the options framework for its conceptual simplicity and generality <ref type="bibr" target="#b25">(Sutton et al., 1998</ref>); more details are found in the next sec- tion. Our work is also motivated by hierarchical- DQN ( <ref type="bibr" target="#b11">Kulkarni et al., 2016</ref>) which integrates hi- erarchical value functions to operate at different temporal scales. The model achieved superior per- formance on a complicated ATARI game "Mon- tezuma's Revenge" with a hierarchical structure.</p><p>A related but different extension to single- domain dialogues is multi-domain dialogues, where each domain is handled by a sepa- rate agent <ref type="bibr" target="#b16">(Lison, 2011;</ref><ref type="bibr">Gasic et al., 2015a,b;</ref><ref type="bibr" target="#b4">Cuayáhuitl et al., 2016)</ref>. In contrast to composite- domain dialogues studied in this paper, a conver- sation in a multi-domain dialogue normally in- volves one domain, so completion of a task does not require solving sub-tasks in different domains. Consequently, work on multi-domain dialogues focuses on different technical challenges such as transfer learning across different domains ( <ref type="bibr" target="#b8">Gasic et al., 2015a</ref>) and domain selection <ref type="bibr" target="#b4">(Cuayáhuitl et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dialogue Policy Learning</head><p>Our composite task-completion dialogue agent consists of four components: (1) an LSTM- based language understanding module <ref type="bibr">(HakkaniTür et al., 2016;</ref><ref type="bibr" target="#b29">Yao et al., 2014</ref>) for identifying user intents and extracting associated slots; (2) a state tracker for tracking the dialogue state; (3) a dialogue policy which selects the next action based on the current state; and (4) a model-based natural language generator ) for converting agent actions to natural language re- sponses. Typically, a dialogue manager contains a state tracker and a dialogue policy. In our imple- mentation, we use a global state tracker to main- tain the dialogue state by accumulating informa- tion across all subtasks, thus helping ensure all inter-subtask constraints be satisfied. In the rest of this section, we will describe the dialogue policy in details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Options over MDPs</head><p>Consider the following process of completing a composite task (e.g., travel planning). An agent first selects a subtask (e.g., book-flight-ticket), then takes a sequence of actions to gather related information (e.g., departure time, number of tick- ets, destination, etc.) until all users' requirements are met and the subtask is completed, and finally chooses the next subtask (e.g., reserve-hotel) to complete. The composite task is fulfilled after all its subtasks are completed collectively. The above process has a natural hierarchy: a top-level process selects which subtasks to complete, and a low- level process chooses primitive actions to com- plete the selected subtask. Such hierarchical de- cision making processes can be formulated in the options framework ( <ref type="bibr" target="#b26">Sutton et al., 1999</ref>), where op- tions generalize primitive actions to higher-level actions. Different from the traditional MDP set- ting where an agent can only choose a primitive action at each time step, with options the agent can choose a "multi-step" action which for exam- ple could be a sequence of primitive actions for completing a subtask. As pointed out by <ref type="bibr" target="#b26">Sutton et al. (1999)</ref>, options are closely related to actions in a family of decision problems known as semi- Markov decision processes. Following <ref type="bibr" target="#b26">Sutton et al. (1999)</ref>, an option con- sists of three components: a set of states where the option can be initiated, an intra-option policy that selects primitive actions while the option is in control, and a termination condition that specifies when the option is completed. For a composite task such as travel planning, subtasks like book- flight-ticket and reserve-hotel can be modeled as options. Consider, for example, the option book- flight-ticket: its initiation state set contains states in which the tickets have not been issued or the destination of the trip is long away enough that a flight is needed; it has an intra-option policy for re- questing or confirming information regarding de- parture date and the number of seats, etc.; it also has a termination condition for confirming that all information is gathered and correct so that it is ready to issue the tickets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hierarchical Policy Learning</head><p>The intra-option is a conventional policy over primitive actions, we can consider an inter-option policy over sequences of options in much the same way as we consider the intra-option policy over sequences of actions. We propose a method that combines deep reinforcement learning and hierar- chical value functions to learn a composite task- completion dialogue agent as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. It is a two-level hierarchical reinforcement learning agent that consists of a top-level dialogue policy ⇡ g and a low-level dialogue policy ⇡ a,g , as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The top-level policy ⇡ g perceives state s from the environment and selects a subtask g 2 G, where G is the set of all possible subtasks. The low-level policy ⇡ a,g is shared by all options. It takes as input a state s and a subtask g, and out- puts a primitive action a 2 A, where A is the set of primitive actions of all subtasks. The subtask g remains a constant input to ⇡ a,g , until a terminal state is reached to terminate g. The internal critic in the dialogue manager provides intrinsic reward r i t (g t ), indicating whether the subtask g t at hand has been solved; this signal is used to optimize ⇡ a,g . Note that the state s contains global infor- mation, in that it keeps track of information for all subtasks.</p><p>Naturally, we aim to optimize the low-level pol- icy ⇡ a,g so that it maximizes the following cumu- lative intrinsic reward at every step t:</p><formula xml:id="formula_0">max ⇡a,g E h X k0 k r i t+k s t = s, g t = g, a t+k = ⇡ a,g (s t+k ) i ,</formula><p>where r i t+k denotes the reward provided by the in- ternal critic at step t + k. Similarly, we want the top-level policy ⇡ g to optimize the cumulative ex- trinsic reward at every step t:</p><formula xml:id="formula_1">max ⇡g E h X k0 k r e t+k s t = s, a t+k = ⇡ g (s t+k ) i ,</formula><p>where r e t+k is the reward received from the envi- ronment at step t + k when a new subtask starts.</p><p>Both the top-level and low-level policies can be learned with deep Q-learning methods, like DQN. Specifically, the top-level dialogue policy estimates the optimal Q-function that satisfies the following:</p><formula xml:id="formula_2">Q ⇤ 1 (s, g) = E h N 1 X k=0 k r e t+k + N · max g 0 Q ⇤ 1 (s t+N , g 0 )|s t = s, g t = g i ,<label>(1)</label></formula><p>where N is the number of steps that the low-level dialogue policy (intra-option policy) needs to ac- complish the subtask. g 0 is the agent's next subtask in state s t+N . Similarly, the low-level dialogue policy estimates the Q-function that satisfies the following:</p><formula xml:id="formula_3">Q ⇤ 2 (s, a, g) = E h r i t + · max a t+1 Q ⇤ 2 (s t+1 , a t+1 , g)|s t = s, g t = g</formula><p>i .</p><p>Both Q ⇤ 1 (s, g) and Q ⇤ 2 (s, a, g) are represented by neural networks, Q 1 (s, g; ✓ 1 ) and Q 2 (s, a, g; ✓ 2 ), parameterized by ✓ 1 and ✓ 2 , respectively.</p><p>The top-level dialogue policy tries to minimize the following loss function at each iteration i:</p><formula xml:id="formula_4">L 1 (✓ 1,i ) = E (s,g,r e ,s 0 )⇠D 1 [(y i Q 1 (s, g; ✓ 1,i )) 2 ] y i = r e + N max g 0 Q 1 (s 0 , g 0 , ✓ 1,i1 ) ,</formula><p>where, as in Equation <ref type="formula" target="#formula_2">(1)</ref>, r e = P N 1 k=0 k r e t+k is the discounted sum of reward collected when sub- goal g is being completed, and N is the number of steps g is completed.</p><p>The low-level dialogue policy minimizes the following loss at each iteration i using:</p><formula xml:id="formula_5">L 2 (✓ 2,i ) = E (s,g,a,r i ,s 0 )⇠D 2 [(y i Q 2 (s, g, a; ✓ 2,i )) 2 ] y i = r i + max a 0 Q 2 (s 0 , g, a 0 , ✓ 2,i1 ) .</formula><p>We use SGD to minimize the above loss func- tions. The gradient for the top-level dialogue pol- icy yields:</p><formula xml:id="formula_6">r ✓ 1,i L 1 (✓ 1,i ) = E (s,g,r e ,s 0 )⇠D 1 [(r e + N max g 0 Q 2 (s 0 , g 0 , ✓ 1,i1 ) Q 1 (s, g, ✓ 1,i )) r ✓ 1,i Q 1 (s, g, ✓ 1,i )]<label>(2)</label></formula><p>The gradient for the low-level dialogue policy yields:</p><formula xml:id="formula_7">r ✓ 2,i L 2 (✓ 2,i ) = E (s,g,a,r i ,s 0 )⇠D 2 [(r i + max a 0 Q 2 (s 0 , g, a 0 , ✓ 2,i1 ) Q 2 (s, g, a, ✓ 2,i )) r ✓ 2,i Q 2 (s, g, a, ✓ 2,i )]<label>(3)</label></formula><p>Following previous studies, we apply two most commonly used performance boosting methods: target networks and experience replay. Experi- ence replay tuples (s, g, r e , s 0 ) and (s, g, a, r i , s 0 ), are sampled from the experience replay buffers D 1 and D 2 respectively. A detailed summary of the learning algorithm for the hierarchical dialogue policy is provided in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>To evaluate the proposed method, we conduct ex- periments on the composite task-completion dia- logue task of travel planning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>In the study, we made use of a human-human conversation data derived from a publicly avail- able multi-domain dialogue corpus 1 <ref type="bibr" target="#b7">(El Asri et al., 2017)</ref>, which was collected using the Wizard-of- Oz approach. We made a few changes to the schema of the data set for the composite task- completion dialogue setting. Specifically, we added inter-subtask constraints as well as user preferences (soft constraints). The data was mainly used to create simulated users, as will be explained below shortly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Agents</head><p>We benchmark the proposed HRL agent against three baseline agents:</p><p>• A Rule Agent uses a sophisticated hand- crafted dialogue policy, which requests and informs a hand-picked subset of necessary slots, and then confirms with the user about the reserved tickets.</p><p>• A Rule+ Agent requests and informs all the slots in a pre-defined order exhaustedly, and then confirms with the user about the re- served tickets. The average turn of this agent is longer than that of the Rule agent.</p><p>• A flat RL Agent is trained with a standard flat deep reinforcement learning method (DQN) which learns a flat dialogue policy using ex- trinsic rewards only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">User Simulator</head><p>Training reinforcement learners is challenging be- cause they need an environment to interact with. In the dialogue research community, it is common to use simulated users as shown in <ref type="figure">Figure 3</ref> for this purpose ( <ref type="bibr" target="#b19">Schatzmann et al., 2007;</ref><ref type="bibr" target="#b0">Asri et al., 2016)</ref>. In this work, we adapted the publicly- available user simulator, developed by , to the composite task-completion dialogue setting using the human-human conversation data described in Section 4. simulator provides the agent with an (extrinsic) re- ward signal at the end of the dialogue. A dialogue is considered to be successful only when a travel plan is made successfully, and the information provided by the agent satisfies user's constraints. At the end of each dialogue, the agent receives a positive reward of 2⇤max turn (max turn = 60 in our experiments) for success, or a negative re- ward of max turn for failure. Furthermore, at each turn, the agent receives a reward of 1 so that shorter dialogue sessions are encouraged.</p><p>User Goal A user goal is represented by a set of slots, indicating the user's request, requirement and preference. For example, an inform slot, such as dst city="Honolulu", indicates a user require- ment, and a request slot, such as price="?", indi- cates a user asking the agent for the information.</p><p>In our experiment, we compiled a list of user goals using the slots collected from the human- human conversation data set described in Sec- tion 4.1, as follows. We first extracted all the slots that appear in dialogue sessions. If a slot has multiple values, like "or city=[San Francisco, San Jose]", we consider it as a user preference (soft constraint) which the user may later revise its value to explore different options in the course of the dialogue. If a slot has only one value, we treat it as a user requirement (hard constraint), which is unlikely negotiable. If a slot is with value "?", we treat it as a user request. We removed those slots from user goals if their values do not exist in our database. The compiled set of user goals contains 759 entries, each containing slots from at least two subtasks: book-flight-ticket and reserve-hotel.</p><p>User Type To compare different agents' ability to adapt to user preferences, we also constructed three additional user goal sets, representing three different types of (simulated) users, respectively:</p><p>• Type A: All the informed slots in a user goal have a single value. These users have hard constraints for both the flight and hotel, and have no preference on which subtask to ac- complish first.</p><p>• Type B: At least one of informed slots in the book-flight-ticket subtask can have multiple values, and the user (simulator) prefers to start with the book-flight-ticket subtask. If the user receives "no ticket available" from the agent during the conversation, she is willing to explore alternative slot values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User Dialogue Action</head><p>Inform <ref type="formula">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Natural Language Understanding(NLU)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3: Illustration of the Composite Task-Completion dialogue System</head><p>• Type C: Similar to Type B, at least one of in- formed slots of the reserve-hotel subtask in a user goal can have multiple values. The user prefers to start with the reserve-hotel subtask.</p><p>If the user receives a "no room available" re- sponse from the agent, she is willing to ex- plore alternative slot values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation</head><p>For the RL agent, we set the size of hidden layer to 80. For the HRL agent, both top-level and low- level dialogue policies had a hidden layer size of 80. RMSprop was applied to optimize the pa- rameters. We set batch size to 16. During train- ing, we used the ✏-greedy strategy for exploration. For each simulation epoch, we simulated 100 dia- logues and stored these state transition tuples in an experience replay buffer. At the end of each sim- ulation epoch, the model was updated with all the transition tuples in the buffer in a batch manner. The experience replay strategy is critical to the success of deep reinforcement learning. In our ex- periments, at the beginning, we used a rule-based agent to run N (N = 100) dialogues to popu- late the experience replay buffer, which was an implicit way of imitation learning to initialize the RL agent. Then, the RL agent accumulated all the state transition tuples and flushes the replay buffer only when the current RL agent reached a success rate threshold no worse than that of the Rule agent. This strategy was motivated by the following observation. The initial performance of an RL agent was often not strong enough to result in dialogue sessions with a reasonable success rate. With such data, it was easy for the agent to learn the locally optimal policy that "failed fast"; that is, the policy would finish the dialogue immediately, so that the agent could suffer the least amount of per-turn penalty. Therefore, we provided some rule-based examples that succeeded reasonably of- ten, and did not flush the buffer until the per- formance of the RL agent reached an acceptable level. Generally, one can set the threshold to be the success rate of the Rule agent. To make a fair comparison, for the same type of users, we used the same Rule agent to initialize both the RL agent and the HRL agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Simulated User Evaluation</head><p>On the composite task-completion dialogue task, we compared the HRL agent with the baseline agents in terms of three metrics: success rate 3 , av- erage rewards, and the average number of turns per dialogue session. <ref type="figure" target="#fig_2">Figure 4</ref> shows the learning curves of all four agents trained on different types of users. Each learning curve was averaged over 10 runs. <ref type="table">Table 1</ref> shows the performance on test data. For all types of users, the HRL-based agent yielded more robust dialogue policies outperforming the hand-crafted rule-based agents and flat RL-based agent mea- sured on success rate. It also needed fewer turns per dialogue session to accomplish a task than the rule-based agents and flat RL agent. The results  across all three types of simulated users suggest the following conclusions.</p><p>First, he HRL agent significantly outperformed the RL agent. This, to a large degree, was at- tributed to the use of the hierarchical structure of the proposed agent. Specifically, the top-level di- alogue policy selected a subtask for the agent to focus on, one at a time, thus dividing a complex task into a sequence of simpler subtasks. The se- lected subtasks, combined with the use of intrin- sic rewards, alleviated the sparse reward and long- horizon issues, and helped the agent explore more efficiently in the state-action space. As a result, as shown in <ref type="figure" target="#fig_2">Figure 4</ref> and <ref type="table">Table 1</ref>, the performance of the HRL agent on types B and C users (who may need to go back to revise some slots during the dialogue) does not drop much compared to type A users, despite the increased search space in the for- mer. Additionally, we observed a large drop in the performance of the RL Agent due to the increased complexity of the task, which required more di- alogue turns and posed a challenge for temporal credit assignment.</p><p>Second, the HRL agent learned much faster than the RL agent. The HRL agent could reach the same level of performance with a smaller num- ber of simulation examples than the RL agent, <ref type="table">Table 2</ref>: Sample dialogue by RL and HRL agents with real user: Left column shows the dialogue with the RL agent; Right column shows the dialogue with the HRL agent; bolded slots are the joint constraints between two subtasks. demonstrating that the hierarchical dialogue poli- cies were more sample-efficient than flat RL pol- icy and could significantly reduce the sample com- plexity on complex tasks.</p><p>Finally, we also found that the Rule+ and flat RL agents had comparable success rates, as shown in <ref type="figure" target="#fig_2">Figure 4</ref>. However, a closer look at the correlation between success rate and the average number of turns in <ref type="table">Table 1</ref> suggests that the Rule+ agent re- quired more turns which adversely affects its suc- cess, whilst the flat RL agent achieves similar suc- cess with much less number of turns in all the user types. It suffices to say that our hierarchical RL agent outperforms all in terms of success rate as depicted in <ref type="figure" target="#fig_2">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Human Evaluation</head><p>We further evaluated the agents, which were trained on simulated users, against real human users, recruited from the authors' affiliation. We conducted the study using the HRL and RL agents, each tested against two types of users: Type A users who had no preference for subtask, and Type B users who preferred to complete the book-flight- ticket subtask first. Note that Type C users were symmetric to Type B ones, so were not included in the study. We compared two (agent, user type) pairs: {RL A, HRL A} and {RL B, HRL B}; in other words, four agents were trained against their specific user types. In each dialogue session, one of the agents was randomly picked to converse with a user. The user was presented with a user goal sampled from our corpus, and was instructed to converse with the agent to complete the task. If one of the slots in the goal had multiple values, the user had multiple choices for this slot and might revise the slot value when the agent replied with a message like "No ticket is available" during the conversation. At the end of each session, the user was asked to give a rating on a scale from 1 to 5 based on the naturalness and coherence of the di- alogue. (1 is the worst rating, and 5 the best). We collected a total of 225 dialogue sessions from 12 human users. <ref type="figure">Figure 5</ref> presents the performance of these agents against real users in terms of success rate. <ref type="figure" target="#fig_3">Figure 6</ref> shows the comparison in user rating. For all the cases, the HRL agent was consistently bet- ter than the RL agent in terms of success rate and user rating. <ref type="table">Table 2</ref> shows a sample dialogue ses- sion. We see that the HRL agent produced a more coherent conversation, as it switched among sub- tasks much less frequently than the flat RL agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusions</head><p>This paper considers composite task-completion dialogues, where a set of subtasks need to be ful- filled collectively for the entire dialogue to be suc- cessful. We formulate the policy learning prob- lem using the options framework, and take a hier- archical deep RL approach to optimizing the pol- icy. Our experiments, both on simulated and real users, show that the hierarchical RL agent signifi- cantly outperforms a flat RL agent and rule-based agents. The hierarchical structure of the agent also improves the coherence of the dialogue flow.</p><p>The promising results suggest several directions for future research. First, the hierarchical RL approach demonstrates strong adaptation ability to tailor the dialogue policy to different types of users. This motivates us to systematically inves- tigate its use for dialogue personalization. Sec- ond, our hierarchical RL agent is implemented us- ing a two-level dialogue policy. But more com- plex tasks might require multiple levels of hierar- chy. Thus, it is valuable to extend our approach to handle such deep hierarchies, where a subtask can invoke another subtask and so on, taking full ad- vantage of the options framework. Finally, design- ing task hierarchies requires substantial domain knowledge and is time-consuming. This challenge calls for future work on automatic learning of hi- erarchies for complex dialogue tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of a composite taskcompletion dialogue agent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of a two-level hierarchical dialogue policy learner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Learning curves of dialogue policies for different User Types under simulation</figDesc><graphic url="image-2.png" coords="7,72.00,353.92,218.27,150.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Distribution of user ratings for HRL agent versus RL agent, and total.</figDesc><graphic url="image-3.png" coords="7,307.28,353.92,218.27,150.06" type="bitmap" /></figure>

			<note place="foot" n="3"> Success rate is the fraction of dialogues where the tasks are successfully accomplished within the maximum turns.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Baolin Peng was in part supported by General Re-search Fund of Hong Kong (14232816). We would like thank anonymous reviewers for their insight-ful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A sequence-to-sequence model for user simulation in spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Layla</forename><forename type="middle">El</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1151" to="1155" />
		</imprint>
	</monogr>
	<note>ISCA</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Recent advances in hierarchical reinforcement learning. Discrete Event Dynamic Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridhar</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahadevan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="41" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Hierarchical reinforcement learning for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heriberto</forename><surname>Cuayáhuitl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SimpleDS: A simple deep reinforcement learning dialogue system</title>
	</analytic>
	<monogr>
		<title level="m">Dialogues with Social Robots</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="109" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning for multi-domain dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heriberto</forename><surname>Cuayáhuitl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghak</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashley</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Carse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08675</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end reinforcement learning of dialogue agents for information access</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical reinforcement learning with the MAXQ value function decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="227" to="303" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Frames: A corpus for adding memory to goal-oriented dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Layla</forename><forename type="middle">El</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremie</forename><surname>Zumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emery</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00057</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distributed dialogue policies for multi-domain statistical dialogue management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pirros</forename><surname>Tsiakoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>South Brisbane, Queensland, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-04-19" />
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page" from="5371" to="5375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Policy committee for adaptation in multidomain spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<meeting><address><addrLine>Scottsdale, AZ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-12-13" />
			<biblScope unit="page" from="806" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-domain joint semantic frame parsing using bi-directional RNN-LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeyi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 17th Annual Meeting of the International Speech Communication Association</title>
		<meeting>The 17th Annual Meeting of the International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tejas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ardavan</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="3675" to="3683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A stochastic model of human-machine interaction for learning dialog strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esther</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Pieraccini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Eckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="23" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">End-to-end task-completion neural dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01008</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Investigation of language understanding impact for reinforcement learning based dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07055</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A user simulator for task-completion dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Zachary C Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.05688</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-policy dialogue management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Lison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="294" to="300" />
		</imprint>
	</monogr>
	<note>Proceedings of the SIGDIAL 2011</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stig</forename><surname>Petersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Charles Beattie</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
			<publisher>Shane Legg, and Demis Hassabis</publisher>
		</imprint>
	</monogr>
	<note>Nature</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reinforcement learning with hierarchies of machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Parr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 10</title>
		<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="1043" to="1049" />
		</imprint>
	</monogr>
	<note>NIPS Conference</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Agenda-based user simulation for bootstrapping a POMDP dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Schatzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Weilhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings</title>
		<meeting><address><addrLine>Rochester, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2007-04-22" />
			<biblScope unit="page" from="149" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The hidden agenda user simulation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Schatzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="733" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Probabilistic simulation of human-machine dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Scheffler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="5" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Istanbul</title>
		<imprint>
			<biblScope unit="page" from="1217" to="1220" />
			<date type="published" when="2000-06" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reinforcement learning with a hierarchy of abstract models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th National Conference on Artificial Intelligence</title>
		<meeting>the 10th National Conference on Artificial Intelligence<address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press / The MIT Press</publisher>
			<date type="published" when="1992-07-12" />
			<biblScope unit="page" from="202" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Rojasbarahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsunghsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02689</idno>
		<title level="m">Continuously learning neural dialogue management</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Intra-option learning about temporally abstract actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth International Conference on Machine Learning (ICML 1998</title>
		<meeting>the Fifteenth International Conference on Machine Learning (ICML 1998<address><addrLine>Madison, Wisconsin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-07-24" />
			<biblScope unit="page" from="556" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Between MDPs and Semi-MDPs: A framework for temporal abstraction in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="181" to="211" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantically conditioned LSTM-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="1711" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hybrid code networks: Practical and efficient end-to-end dialog control with supervised and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavosh</forename><surname>Jason D Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spoken language understanding using long short-term memory neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Spoken Language Technology Workshop</title>
		<imprint>
			<biblScope unit="page" from="189" to="194" />
			<date type="published" when="2014-12-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">POMDP-based statistical spoken dialog systems: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1160" to="1179" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
