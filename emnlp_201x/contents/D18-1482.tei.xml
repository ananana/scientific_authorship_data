<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Word Mover&apos;s Embedding: From Word2Vec to Document Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of William and Mary</orgName>
								<orgName type="institution" key="instit1">IBM Research</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit3">IBM Research</orgName>
								<orgName type="institution" key="instit4">IBM Research</orgName>
								<orgName type="institution" key="instit5">IBM Research</orgName>
								<orgName type="institution" key="instit6">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit7">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>En-Hsu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of William and Mary</orgName>
								<orgName type="institution" key="instit1">IBM Research</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit3">IBM Research</orgName>
								<orgName type="institution" key="instit4">IBM Research</orgName>
								<orgName type="institution" key="instit5">IBM Research</orgName>
								<orgName type="institution" key="instit6">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit7">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">College of William and Mary</orgName>
								<orgName type="institution" key="instit1">IBM Research</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit3">IBM Research</orgName>
								<orgName type="institution" key="instit4">IBM Research</orgName>
								<orgName type="institution" key="instit5">IBM Research</orgName>
								<orgName type="institution" key="instit6">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit7">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
							<email>kun.xu1@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department">College of William and Mary</orgName>
								<orgName type="institution" key="instit1">IBM Research</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit3">IBM Research</orgName>
								<orgName type="institution" key="instit4">IBM Research</orgName>
								<orgName type="institution" key="instit5">IBM Research</orgName>
								<orgName type="institution" key="instit6">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit7">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangli</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of William and Mary</orgName>
								<orgName type="institution" key="instit1">IBM Research</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit3">IBM Research</orgName>
								<orgName type="institution" key="instit4">IBM Research</orgName>
								<orgName type="institution" key="instit5">IBM Research</orgName>
								<orgName type="institution" key="instit6">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit7">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Balakrishnan</surname></persName>
							<email>avinash.bala@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department">College of William and Mary</orgName>
								<orgName type="institution" key="instit1">IBM Research</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit3">IBM Research</orgName>
								<orgName type="institution" key="instit4">IBM Research</orgName>
								<orgName type="institution" key="instit5">IBM Research</orgName>
								<orgName type="institution" key="instit6">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit7">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
							<email>Pin-yu.chen@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department">College of William and Mary</orgName>
								<orgName type="institution" key="instit1">IBM Research</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit3">IBM Research</orgName>
								<orgName type="institution" key="instit4">IBM Research</orgName>
								<orgName type="institution" key="instit5">IBM Research</orgName>
								<orgName type="institution" key="instit6">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit7">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
							<email>pradeepr@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of William and Mary</orgName>
								<orgName type="institution" key="instit1">IBM Research</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit3">IBM Research</orgName>
								<orgName type="institution" key="instit4">IBM Research</orgName>
								<orgName type="institution" key="instit5">IBM Research</orgName>
								<orgName type="institution" key="instit6">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit7">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Witbrock</surname></persName>
							<email>witbrock@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department">College of William and Mary</orgName>
								<orgName type="institution" key="instit1">IBM Research</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit3">IBM Research</orgName>
								<orgName type="institution" key="instit4">IBM Research</orgName>
								<orgName type="institution" key="instit5">IBM Research</orgName>
								<orgName type="institution" key="instit6">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit7">IBM Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Word Mover&apos;s Embedding: From Word2Vec to Document Embedding</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4524" to="4534"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4524</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>While the celebrated Word2Vec technique yields semantically rich representations for individual words, there has been relatively less success in extending to generate unsupervised sentences or documents embeddings. Recent work has demonstrated that a distance measure between documents called Word Mover&apos;s Distance (WMD) that aligns semantically similar words, yields unprecedented KNN classification accuracy. However, WMD is expensive to compute, and it is hard to extend its use beyond a KNN classifier. In this paper , we propose the Word Mover&apos;s Embedding (WME), a novel approach to building an unsupervised document (sentence) embedding from pre-trained word embeddings. In our experiments on 9 benchmark text classification datasets and 22 textual similarity tasks, the proposed technique consistently matches or outperforms state-of-the-art techniques, with significantly higher accuracy on problems of short length.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text representation plays an important role in many NLP-based tasks such as document classification and clustering ( <ref type="bibr" target="#b61">Zhang et al., 2018;</ref><ref type="bibr" target="#b21">Gui et al., 2016</ref><ref type="bibr" target="#b22">Gui et al., , 2014</ref>), sense disambiguation ( <ref type="bibr" target="#b18">Gong et al., 2017</ref><ref type="bibr" target="#b17">Gong et al., , 2018a</ref>), machine translation ( <ref type="bibr" target="#b37">Mikolov et al., 2013b</ref>), document matching ( <ref type="bibr" target="#b42">Pham et al., 2015)</ref>, and sequential alignment ( <ref type="bibr" target="#b39">Peng et al., 2016</ref><ref type="bibr" target="#b40">Peng et al., , 2015</ref>. Since there are no explicit features in text, much work has aimed to develop effective text represen- tations. Among them, the simplest bag of words (BOW) approach <ref type="bibr" target="#b47">(Salton and Buckley, 1988)</ref> and its term frequency variants (e.g. TF-IDF) <ref type="bibr" target="#b44">(Robertson and Walker, 1994)</ref> are most widely used due to simplicity, efficiency and often surprisingly high ac- curacy ( <ref type="bibr" target="#b52">Wang and Manning, 2012)</ref>. However, sim- ply treating words and phrases as discrete symbols fails to take into account word order and the seman- tics of the words, and suffers from frequent near- orthogonality due to its high dimensional sparse representation. To overcome these limitations, La- tent Semantic Indexing ( <ref type="bibr" target="#b13">Deerwester et al., 1990)</ref> and Latent Dirichlet Allocation ( <ref type="bibr" target="#b8">Blei et al., 2003)</ref> were developed to extract more meaningful repre- sentations through singular value decomposition ( <ref type="bibr" target="#b56">Wu and Stathopoulos, 2015)</ref> and learning a proba- bilistic BOW representation.</p><p>A recent empirically successful body of research makes use of distributional or contextual informa- tion together with simple neural-network models to obtain vector-space representations of words and phrases ( <ref type="bibr" target="#b7">Bengio et al., 2003;</ref><ref type="bibr">Mikolov et al., 2013a,c;</ref><ref type="bibr" target="#b41">Pennington et al., 2014)</ref>. A number of researchers have proposed extensions of these to- wards learning semantic vector-space representa- tions of sentences or documents. A simple but often effective approach is to use a weighted average over some or all of the embeddings of words in the doc- ument. While this is simple, important information could easily be lost in such a document representa- tion, in part since it does not consider word order. A more sophisticated approach ( <ref type="bibr" target="#b32">Le and Mikolov, 2014;</ref><ref type="bibr" target="#b11">Chen, 2017)</ref> has focused on jointly learning embeddings for both words and paragraphs using models similar to Word2Vec. However, these only use word order within a small context window; moreover, the quality of word embeddings learned in such a model may be limited by the size of the training corpus, which cannot scale to the large sizes used in the simpler word embedding models, and which may consequently weaken the quality of the document embeddings.</p><p>Recently, <ref type="bibr" target="#b31">Kusner et al. (Kusner et al., 2015)</ref> presented a novel document distance metric, Word Mover's Distance (WMD), that measures the dis- similarity between two text documents in the Word2Vec embedding space. Despite its state- of-the-art KNN-based classification accuracy over other methods, combining KNN and WMD incurs very high computational cost. More importantly, WMD is simply a distance that can be only com- bined with KNN or K-means, whereas many ma- chine learning algorithms require a fixed-length feature representation as input.</p><p>A recent work in building kernels from distance measures, D2KE (distances to kernels and em- beddings) ( <ref type="bibr" target="#b57">Wu et al., 2018a</ref>) proposes a general methodology of the derivation of a positive-definite kernel from a given distance function, which enjoys better theoretical guarantees than other distance- based methods, such as k-nearest neighbor and dis- tance substitution kernel ( <ref type="bibr" target="#b23">Haasdonk and Bahlmann, 2004)</ref>, and has also been demonstrated to have strong empirical performance in the time-series domain ( <ref type="bibr" target="#b58">Wu et al., 2018b)</ref>.</p><p>In this paper, we build on this recent innova- tion D2KE ( <ref type="bibr" target="#b57">Wu et al., 2018a)</ref>, and present the Word Mover's Embedding (WME), an unsupervised generic framework that learns continuous vector representations for text of variable lengths such as a sentence, paragraph, or document. In par- ticular, we propose a new approach to first con- struct a positive-definite Word Mover's Kernel via an infinite-dimensional feature map given by the Word Mover's distance (WMD) to random docu- ments from a given distribution. Due to its use of the WMD, the feature map takes into account align- ments of individual words between the documents in the semantic space given by the pre-trained word embeddings. Based on this kernel, we can then de- rive a document embedding via a Random Features approximation of the kernel, whose inner products approximate exact kernel computations. Our tech- nique extends the theory of Random Features to show convergence of the inner product between WMEs to a positive-definite kernel that can be in- terpreted as a soft version of (inverse) WMD.</p><p>The proposed embedding is more efficient and flexible than WMD in many situations. As an example, WME with a simple linear classifier reduces the computational cost of WMD-based KNN from cubic to linear in document length and from quadratic to linear in number of samples, while simultaneously improving accuracy. WME is extremely easy to implement, fully paralleliz- able, and highly extensible, since its two build- ing blocks, Word2Vec and WMD, can be replaced by other techniques such as <ref type="bibr">GloVe (Pennington et al., 2014;</ref><ref type="bibr" target="#b54">Wieting et al., 2015b</ref>) or S-WMD ( <ref type="bibr" target="#b25">Huang et al., 2016)</ref>. We evaluate WME on 9 real-world text classification tasks and 22 textual similarity tasks, and demonstrate that it consis- tently matches or outperforms other state-of-the- art techniques. Moreover, WME often achieves orders of magnitude speed-up compared to KNN- WMD while obtaining the same testing accuracy. Our code and data is available at https://github. com/IBM/WordMoversEmbeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Word2Vec and Word Mover's Distance</head><p>We briefly introduce Word2Vec and WMD, which are the key building blocks of our proposed method. Here are some notations we will use throughout the paper. Given a total number of documents N with a vocabulary W of size |W| = n, the Word2vec embedding gives us a d-dimensional vector space V ✓ R d such that any word in the vocabulary set w 2 W is associated with a semantically rich vec- tor representation v w 2 R d . Then in this work, we consider each document as a collection of word vectors x := (v j ) L j=1 and denote X := S Lmax L=1 V L as the space of documents.</p><p>Word2Vec. In the celebrated Word2Vec approach ( <ref type="bibr" target="#b36">Mikolov et al., 2013a</ref>,c), two shallow yet effective models are used to learn vector-space representa- tions of words (and phrases), by mapping those that co-occur frequently, and consequently with plausibly similar meaning, to nearby vectors in the embedding vector space. Due to the model's sim- plicity and scalability, high-quality word embed- dings can be generated to capture a large number of precise syntactic and semantic word relationships by training over hundreds of billions of words and millions of named entities. The advantage of docu- ment representations building on top of word-level embeddings is that one can make full use of high- quality pre-trained word embeddings. Throughout this paper we use Word2Vec as our first building block but other (unsupervised or supervised) word (a) WMD (b) WME <ref type="figure" target="#fig_1">Figure 1</ref>: An illustration of the WMD and WME. All non-stop words are marked as bold face. WMD measures the distance between two documents. WME approximates a kernel derived from WMD with a set of random documents.</p><p>embeddings <ref type="bibr" target="#b41">(Pennington et al., 2014;</ref><ref type="bibr" target="#b54">Wieting et al., 2015b</ref>) could also be utilized.</p><p>Word Mover's Distance. Word Mover's Distance was introduced by <ref type="bibr" target="#b31">(Kusner et al., 2015</ref>) as a special case of the Earth Mover's Distance ( <ref type="bibr" target="#b46">Rubner et al., 2000</ref>), which can be computed as a solution of the well-known transportation problem <ref type="bibr" target="#b24">(Hitchcock, 1941;</ref><ref type="bibr" target="#b4">Altschuler et al., 2017)</ref>. WMD is a distance between two text documents x, y 2 X that takes into account the alignments between words. Let |x|, |y| be the number of distinct words in x and y. Let f x 2 R |x| , f y 2 R |y| denote the normalized frequency vectors of each word in the documents x and y respectively (so that f T x 1 = f T y 1 = 1). Then the WMD distance between documents x and y is defined as:</p><formula xml:id="formula_0">WMD(x, y) := min F 2R |x|⇥|y| + hC, F i, s.t., F 1 = f x , F T 1 = f y . (1)</formula><p>where F is the transportation flow matrix with F ij denoting the amount of flow traveling from i-th word x i in x to j-th word y j in y, and C is the transportation cost with C ij := dist(v x i , v y j ) being the distance between two words measured in the Word2Vec embedding space. A popular choice is the Euclidean distance dist(</p><formula xml:id="formula_1">v x i , v y j ) = kv x i v y j k 2 . When dist(v x i , v y j )</formula><p>is a metric, the WMD distance in Eq. (1) also qualifies as a metric, and in particular, satisfies the triangle in- equality ( <ref type="bibr" target="#b46">Rubner et al., 2000</ref>). Building on top of Word2Vec, WMD is a particularly useful and accurate for measure of the distance between doc- uments with semantically close but syntactically different words as illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>(a).</p><p>The WMD distance when coupled with KNN has been observed to have strong performance in classification tasks ( <ref type="bibr" target="#b31">Kusner et al., 2015)</ref>. However, WMD is expensive to compute with computational complexity of O(L 3 log(L)), especially for long documents where L is large. Additionally, since WMD is just a document distance, rather than a doc- ument representation, using it within KNN incurs even higher computational costs O(N 2 L 3 log(L)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Document Embedding via Word Mover's Kernel</head><p>In this section, we extend the framework in ( <ref type="bibr" target="#b57">Wu et al., 2018a</ref>), to derive a positive-definite kernel from an alignment-aware document distance met- ric, which then gives us an unsupervised semantic embeddings of texts of variable length as a by- product through the theory of Random Feature Ap- proximation (Rahimi and Recht, 2007).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Mover's Kernel</head><p>We start by defining the Word Mover's Kernel:</p><formula xml:id="formula_2">k(x, y) := Z p(!) ! (x) ! (y)d!,</formula><p>where</p><formula xml:id="formula_3">! (x) := exp(WMD(x, !)).<label>(2)</label></formula><p>where ! can be interpreted as a random docu- ment {v j } D j=1 that contains a collection of ran- dom word vectors in V, and p(!) is a distribution over the space of all possible random documents</p><formula xml:id="formula_4">⌦ := S Dmax D=1 V D . ! (x)</formula><p>is an possibly infinite- dimensional feature map derived from the WMD between x and all possible documents ! 2 ⌦.</p><p>An insightful interpretation of this kernel <ref type="formula" target="#formula_3">(2)</ref>:</p><formula xml:id="formula_5">k(x, y) := exp(softmin p(!) f (!))</formula><p>where</p><formula xml:id="formula_6">softmin p(!) f (!) := 1 log Z p(!)e f (!) d!,</formula><p>and f (!) = {WMD(x, !) + WMD(!, y)}, is a version of soft minimum function parameter- ized by p(!) and . Comparing this with the usual definition of soft minimum softmin i f i := softmax (f i ) = log P i e f i , it can be seen that the soft-min-variant in the above Equations uses a weighting of the objects ! via the probabil- ity density p(!), and moreover has the additional parameter to control the degree of smoothness. When is large and f (!) is Lipschitz-continuous, the value of the soft-min-variant is mostly deter- mined by the minimum of f (!).</p><p>Note that since WMD is a metric, by the triangu- lar inequality we have</p><formula xml:id="formula_7">WMD(x, y)  min !2⌦ (WMD(x, !) + WMD(!, y))</formula><p>and the equality holds if we allow the length of random document D max to be not smaller than L. Therefore, the kernel (2) serves as a good approx- imation to the WMD between any pair of docu- ments x, y as illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>(b), while it is positive-definite by the definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Word Mover's Embedding</head><p>Given the Word-Mover's Kernel in Eq. (2), we can then use the Monte-Carlo approximation:</p><formula xml:id="formula_8">k(x, y) ⇡ hZ(x), Z(y)i = 1 R R X i=1 ! i (x) ! i (y) (3) where {! i } R i=1 are i.i.d. random documents drawn from p(!) and Z(x) := ( 1 p R ! i (x)) R i=1</formula><p>gives a vector representation of document x. We call this random approximation Word Mover's Embedding. Later, we show that this Random Features approxi- mation in Eq. (3) converges to the exact kernel <ref type="formula" target="#formula_3">(2)</ref> uniformly over all pairs of documents (x, y) .</p><p>Distribution p(!). A key ingredient in the Word Mover's Kernel and Embedding is the distribution p(!) over random documents. Note that ! 2 X consists of sets of words, each of which lies in the Word2Vec embedding space; the characteris- tics of which need to be captured by p(!) in order to generate (sets of) "meaningful" random words. Several studies have found that the word vectors v are roughly uniformly dispersed in the word em- bedding space ( <ref type="bibr" target="#b5">Arora et al., 2016</ref><ref type="bibr" target="#b6">Arora et al., , 2017</ref>. This is also consistent with our empirical findings, that the uniform distribution centered by the mean of all word vectors in the documents is generally appli- cable for various text corpora. Thus, if d is the dimensionality of the pre-trained word embedding space, we can draw a random word u 2 R d as u j ⇠ Uniform[v min , v max ], for j = 1, . . . , d, and where v min and v max are some constants.</p><p>Given a distribution over random words, the re- maining ingredient is the length D of random doc- uments. It is desirable to set these to a small num- ber, in part because this length is indicative of the number of hidden global topics, and we expect the number of such global topics to be small. In par- ticular, these global topics will allow short random documents to align with the documents to obtain "topic-based" discriminatory features. Since there is no prior information for global topics, we choose to uniformly sample the length of random docu- ments as D ⇠ Uniform <ref type="bibr">[1, D max ]</ref>, for some con- stant D max . Stitching the distributions over words, and over the number of words, we then get a distri- bution over random documents. We note that our WME embedding allows potentially other random distributions, and other types of word embeddings, making it a flexible and powerful feature learning framework to utilize state-of-the-art techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Word Mover's Embedding: An Unsu- pervised Feature Representation for Documents</head><p>Input: </p><formula xml:id="formula_9">Texts {x i } N i=1 , D max , R. Output: Matrix Z N ⇥R ,</formula><formula xml:id="formula_10">Draw D j ⇠ Uniform[1, D max ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Generate a random document ! j consist- ing of D j number of random words drawn as</p><formula xml:id="formula_11">! j` ⇠ Uniform[v min , v max ] d , ` = 1, . . . , D j . 5:</formula><p>Compute f x i and f ! j using a popular weighting scheme (e.g. NBOW or TF-IDF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Compute the WME feature vector</p><formula xml:id="formula_12">Z j = ! j ({x i } N i=1 ) using WMD in Equation (2). 7: end for 8: Return Z({x i } N i=1 ) = 1 p R [Z 1 Z 2 . . . Z R ]</formula><p>Algorithm 1 summarizes the overall procedure to generate feature vectors for text of any length such as sentences, paragraphs, and documents. KNN-WMD, which uses the WMD distance together with KNN based classification, requires O(N 2 ) evaluations of the WMD distance, which in turn has O(L 3 log(L)) complexity, assuming that documents have lengths bounded by L, leading to an overall complexity of O(N 2</p><note type="other">L 3 log(L). In contrast, our WME approximation only requires super-linear complexity of O(NRLlog(L)) when D is constant. This is because in our case each evaluation of WMD only requires O(D 2 L log(L)) (Bourgeois and Lassalle, 1971), due to the short length D of our random documents. This dramatic reduction in computation significantly accelerates training and testing when combined with empiri- cal risk minimization classifiers such as SVMs.</note><p>A simple yet useful trick is to pre-compute the word distances to avoid redundant computations since a pair of words may appear multiple times in differ- ent pairs of documents. Note that the computation of the ground distance between each pair of word vectors in documents has a O(L 2 d) complexity, which could be close to one WMD evaluation if document length L is short and word vector di- mension d is large. This simple scheme leads to additional improvement in runtime performance of our WME method that we show in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Convergence of WME</head><p>In this section, we study the convergence of our embedding (3) to the exact kernel (2) under the framework of Random Features (RF) approxima- tion ( <ref type="bibr" target="#b43">Rahimi and Recht, 2007)</ref>. Note that the standard RF convergence theory applies only to the shift-invariant kernel operated on two vectors, while our kernel (2) operates on two documents x, y 2 X that are sets of word vectors. In (Wu et al., 2018a), a general RF convergence theory is provided for any distance-based kernel as long as a finite covering number is given w.r.t. the given distance. In the following lemma, we provide the covering number for all documents of bounded length under the Word Mover's Distance. Without loss of generality, we will assume that the word embeddings {v} are normalized s.t. kvk  1. Lemma 1. There exists an ✏-covering E of X under the WMD metric with Euclidean ground distance, so that:</p><formula xml:id="formula_13">8x 2 X , 9x i 2 E, WMD(x, x i )  ✏, that has size bounded as |E|  ( 2 ✏ ) d L ,</formula><p>where L is a bound on the length of document x 2 X .</p><p>Equipped with Lemma 1, we can derive the fol- lowing convergence result as a simple corollary of the theoretical results in ( <ref type="bibr" target="#b57">Wu et al., 2018a</ref>). We defer the proof to the appendix A. Theorem 1. Let R (x, y) be the difference be- tween the exact kernel (2) and the random approxi- mation (3) with R samples, we have uniform con- vergence</p><formula xml:id="formula_14">P ⇢ max x,y2X | R (x, y)| &gt; 2t  2 ✓ 12 t ◆ 2dL e Rt 2 /2 .</formula><p>where d is the dimension of word embedding and L is a bound on the document length. In other words, to guarantee | R (x, y)|  ✏ with probability at least 1 , it suffices to have</p><formula xml:id="formula_15">R = ⌦ ✓ dL ✏ 2 log( ✏ ) + 1 ✏ 2 log( 1 ) ◆ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We   <ref type="table" target="#tab_2">Application  BBCSPORT  5  517  220  13243  117  BBC sports article labeled by sport  TWITTER  3  2176  932  6344  9.9  tweets categorized by sentiment  RECIPE  15  3059  1311  5708  48.5  recipe procedures labeled by origin  OHSUMED  10  3999  5153  31789  59.2  medical abstracts (class subsampled)</ref>  (b) CLASSIC Effects of R. We investigate how the performance changes when varying the number of Random Fea- tures R from 4 to 4096 with fixed D. <ref type="figure">Fig. 2</ref> shows that both training and testing accuracies gener- ally converge very fast when increasing R from a small number (R = 4) to a relatively large num- ber (R = 1024), and then gradually reach to the optimal performance. This confirms our analysis in Theory 1 that the proposed WME can guarantee the fast convergence to the exact kernel.</p><p>Effects of D. We further evaluate the training and testing accuracies when varying the length of ran- dom document D with fixed R. As shown in <ref type="figure" target="#fig_0">Fig. 3</ref>, we can see that near-peak performance can usually be achieved when D is small (typically D  6). This behavior illustrates two important aspects: (1) using very few random words (e.g. D = 1) is not enough to generate useful Random Features when R becomes large; (2) using too many random words (e.g. D 10) tends to generate similar and redundant Random Features when increasing R.</p><p>Conceptually, the number of random words in a random document can be thought of as the number of the global topics in documents, which is gen- erally small. This is an important desired feature that confers both a performance boost as well as computational efficiency to the WME method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with KNN-WMD</head><p>Baselines. We now compare two WMD-based methods in terms of testing accuracy and total train- ing and testing runtime. We consider two variants of WME with different sizes of R. WME(LR) stands for WME with large rank that achieves the best accuracy (using R up to 4096) with more com- putational time, while WME(SR) stands for WME with small rank that obtains comparable accuracy in less time. We also consider two variants of both methods where +P denotes that we precompute the ground distance between each pair of words to avoid redundant computations.</p><p>Setup. Following ( <ref type="bibr" target="#b31">Kusner et al., 2015;</ref><ref type="bibr" target="#b25">Huang et al., 2016)</ref>, for datasets that do not have a predefined train/test split, we report average and standard devi- ation of the testing accuracy and average run-time of the methods over five 70/30 train/test splits. For WMD, we provide the results (with respect to ac- curacy) from (Kusner et al., 2015); we also reran the experiments of KNN-WMD and found them to be consistent with the reported results. For all methods, we perform 10-fold cross validation to search for the best parameters on the training docu- ments. We employ a linear SVM implemented us- ing LIBLINEAR <ref type="bibr" target="#b14">(Fan et al., 2008</ref>) on WME since it can isolate the effectiveness of the feature repre- sentation from the power of the nonlinear learning solvers. For additional results on all KNN-based methods, please refer to Appendix B.3.</p><p>Results. <ref type="table" target="#tab_4">Table 2</ref> corroborates the significant advan- Test accuracy, and total training and testing time (in seconds) of WME against KNN-WMD. Speedups are computed between the best numbers of KNN-WMD+P and these of WME(SR)+P when achieving similar testing accuracy. Bold face highlights the best number for each dataset.</p><p>Classifier KNN-WMD KNN-WMD+P WME(SR) WME(SR)+P WME(LR) WME <ref type="formula">(</ref>  tages of WME compared to KNN-WMD in terms of both accuracy and runtime. First, WME(SR) can consistently achieve better or similar accuracy compared to KNN-WMD while requiring order-of- magnitude less computational time on all datasets. Second, both methods can benefit from precom- putation of the ground distance between a pair of words but WME gains much more from prefetch (typically 3-5x speedup). This is because the typ- ical length D of random documents is very short where computing ground distance between word vectors may be even more expensive than the corre- sponding WMD distance. Finally, WME(LR) can achieve much higher accuracy compared to KNN- WMD while still often requiring less computational time, especially on large datasets like 20NEWS and relatively long documents like OHSUMED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons with Word2Vec &amp; Doc2Vec</head><p>Baselines. We compare against 6 document repre- Setup. Word2Vec+nbow, Word2Vec+tf-idf and WME use pre-trained Word2Vec embeddings while SIF uses its default pre-trained GloVe embeddings. Following <ref type="bibr" target="#b11">(Chen, 2017)</ref>, to enhance the perfor- mance of PV-DBOW, PV-DM, and Doc2VecC these methods are trained transductively on both train and test, which is indeed beneficial for generating a better document representation (see <ref type="bibr">Appendix B.4)</ref>. In contrast, the hyperparameters of WME are obtained through a 10-fold cross validation only on training set. For a fair comparison, we run a linear SVM using LIBLINEAR on all methods.</p><p>Results. <ref type="table" target="#tab_6">Table 3</ref> shows that WME consistently out- performs or matches existing state-of-the-art doc- ument representation methods in terms of testing accuracy on all datasets except one (OHSUMED). The first highlight is that simple average of word embeddings often achieves better performance than SIF(Glove), indicating that removing the first prin- ciple component could hurt the expressive power of the resulting representation for some of clas- sification tasks. Surprisingly, these two methods often achieve similar or better performance than PV-DBOW and PV-DM, which may be because of the high-quality pre-trained word embeddings. On the other hand, Doc2VecC achieves much better testing accuracy than these previous methods on two datasets (20NEWS, and RECIPE_L). This is mainly because that it benefits significantly from transductive training (See Appendix B.4). Finally, the better performance of WME over these strong baselines stems from fact that WME is empow- ered by two important building blocks, WMD and Word2Vec, to yield a more informative representa- tion of the documents by considering both the word alignments and the semantics of words. We refer the readers to additional results on the Imdb dataset in Appendix B.4, which also demon- strate the clear advantage of WME even compared to the supervised RNN method as well as the afore- mentioned baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparisons on textual similarity tasks</head><p>Baselines. We compare WME against 10 super- vised, simi-sepervised, and unsupervised methods for performing textual similarity tasks. Six su- pervised methods are initialized with Paragram- SL999(PSL) word vectors ( <ref type="bibr" target="#b54">Wieting et al., 2015b)</ref> and then trained on the PPDB dataset, includ- ing: 1) PARAGRAM-PHRASE (PP) ( <ref type="bibr" target="#b53">Wieting et al., 2015a)</ref>: simple average of refined PSL word vec- tors; 2) Deep Averaging Network (DAN) (Iyyer et al., 2015); 3) RNN: classical recurrent neural network; 4) iRNN: a variant of RNN with the acti- vation being the identify; 5) LSTM(no) <ref type="bibr" target="#b15">(Gers et al., 2002</ref>): LSTM with no output gates; 6) LSTM(o.g.) <ref type="bibr" target="#b15">(Gers et al., 2002</ref>): LSTM with output gates. Four unsupervised methods are: 1) Skip-Thought Vectors (ST) ( <ref type="bibr" target="#b30">Kiros et al., 2015)</ref>: an encoder-decoder RNN model for generalizing Skip-gram to the sentence level; 2) nbow: simple averaging of pre-trained GloVe word vectors; 3) tf-idf : a weighted average of GloVe word vecors using TF-IDF weights; 4) SIF (Arora et al., 2017): a simple yet strong method on textual similarity tasks using GloVe word vecors. Two semi-supervised methods use PSL word vec- tors, which are trained using labeled data ( <ref type="bibr" target="#b54">Wieting et al., 2015b</ref>).</p><p>Setup. There are total 22 textual similarity datasets from STS tasks <ref type="bibr" target="#b3">( ) (Agirre et al., 2012</ref><ref type="bibr" target="#b50">( , 2013</ref><ref type="bibr" target="#b28">( , 2014</ref>, SemEval 2014 Semantic Relat- edness task ( <ref type="bibr" target="#b60">Xu et al., 2015)</ref>, and <ref type="bibr">SemEval 2015</ref><ref type="bibr">Twitter task (Marelli et al., 2014</ref>). The goal of these tasks is to predict the similarity between two input sentences. Each year STS usually has 4 to 6 differ- ent tasks and we only report the averaged Pearson's scores for clarity. Detailed results on each dataset are listed in Appendix B.5.</p><p>Results. <ref type="table" target="#tab_7">Table 4</ref> shows that WME consistently matches or outperforms other unsupervised and su- pervised methods except the SIF method. Indeed, compared with ST and nbow, WME improves Pear- son's scores substantially by 10% to 33% as a re- sult of the consideration of word alignments and the use of TF-IDF weighting scheme. tf-idf also improves over these two methods but is slightly worse than our method, indicating the importance of taking into account the alignments between the words. SIF method is a strong baseline for tex- tual similarity tasks but WME still can beat it on STS'12 and achieve close performance in other cases. Interestingly, WME is on a par with three su- pervised methods RNN, LSTM(no), and LSTM(o.g.) in most cases. The final remarks stem from the fact that, WME can gain significantly benefit from the supervised word embeddings similar to SIF, both showing strong performance on PSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Two broad classes of unsupervised and supervised methods have been proposed to generate sentence and document representations. The former primar- ily generate general purpose and domain indepen- dent embeddings of word sequences <ref type="bibr" target="#b48">(Socher et al., 2011;</ref><ref type="bibr" target="#b30">Kiros et al., 2015;</ref><ref type="bibr" target="#b6">Arora et al., 2017</ref>); many unsupervised training research efforts have focused on either training an auto-encoder to learn the la- tent structure of a sentence <ref type="bibr" target="#b50">(Socher et al., 2013</ref>), a paragraph, or document ( <ref type="bibr" target="#b33">Li et al., 2015)</ref>; or gen- eralizing Word2Vec models to predict words in a paragraph ( <ref type="bibr" target="#b32">Le and Mikolov, 2014;</ref><ref type="bibr" target="#b11">Chen, 2017)</ref> or in neighboring sentences ( <ref type="bibr" target="#b30">Kiros et al., 2015)</ref>. How- ever, some important information could be lost in the resulting document representation without con- sidering the word order. Our proposed WME over- comes this difficulty by considering the alignments between each pair of words.</p><p>The other line of work has focused on developing compositional supervised models to create a vector representation of sentences ( <ref type="bibr" target="#b29">Kim et al., 2016;</ref><ref type="bibr" target="#b19">Gong et al., 2018b</ref>). Most of this work proposed com- position using recursive neural networks based on parse structure <ref type="bibr" target="#b49">(Socher et al., 2012</ref><ref type="bibr" target="#b50">(Socher et al., , 2013</ref>, deep av- eraging networks over bag-of-words models <ref type="bibr" target="#b26">(Iyyer et al., 2015;</ref><ref type="bibr" target="#b53">Wieting et al., 2015a</ref>), convolutional neural networks <ref type="bibr" target="#b28">(Kim, 2014;</ref><ref type="bibr" target="#b27">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b59">Xu et al., 2018)</ref>, and recurrent neural net- works using long short-term memory <ref type="bibr" target="#b51">(Tai et al., 2015;</ref><ref type="bibr" target="#b34">Liu et al., 2015</ref>). However, these methods are less well suited for domain adaptation settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have proposed an alignment-aware text kernel using WMD for texts of variable lengths, which takes into account both word alignments and pre-trained high quality word embeddings in learning an effective semantics-preserving feature representation. The proposed WME is simple, ef- ficient, flexible, and unsupervised. Extensive ex- periments show that WME consistently matches or outperforms state-of-the-art models on various text classification and textual similarity tasks. WME embeddings can be easily used for a wide range of downstream supervised and unsupervised tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Train (Blue) and Test (Red) accuracy when varying D with fixed R.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>sentations methods: 1 )</head><label>1</label><figDesc>Smooth Inverse Frequency (SIF) (Arora et al., 2017): a recently proposed sim- ple but tough to beat baseline for sentence embed- dings, combining a new weighted scheme of word embeddings with dominant component removal; 2) Word2Vec+nbow: a weighted average of word vectors using NBOW weights; 3) Word2Vec+tf- idf : a weighted average of word vectors using TF-IDF weights; 4) PV-DBOW (Le and Mikolov, 2014): distributed bag of words model of Para- graph Vectors; 5) PV-DM (Le and Mikolov, 2014): distributed memory model of Paragraph Vectors; 6) Doc2VecC (Chen, 2017): a recently proposed document-embedding via corruptions, achieving state-of-the-art performance in text classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 : Properties of the datasets</head><label>1</label><figDesc></figDesc><table>Dataset 
C:Classes 
N :Train 
M :Test 
BOW Dim 
L:Length 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 3 : Testing accuracy of WME against Word2Vec and Doc2Vec-based methods.</head><label>3</label><figDesc></figDesc><table>Dataset 
SIF(GloVe) 
Word2Vec+nbow 
Word2Vec+tf-idf 
PV-DBOW 
PV-DM 
Doc2VecC 
WME 
BBCSPORT 
97.3 ± 1.2 
97.3 ± 0.9 
96.9 ± 1.1 
97.2 ± 0.7 
97.9 ± 1.3 
90.5 ± 1.7 
98.2 ± 0.6 
TWITTER 
57.8 ± 2.5 
72.0 ± 1.5 
71.9 ± 0.7 
67.8 ± 0.4 
67.3 ± 0.3 
71.0 ± 0.4 
74.5 ± 0.5 
OHSUMED 
67.1 
63.0 
60.6 
55.9 
59.8 
63.4 
64.5 
CLASSIC 
92.7 ± 0.9 
95.2 ± 0.4 
93.9± 0.4 
97.0 ± 0.3 
96.5 ± 0.7 
96.6 ± 0.4 
97.1 ± 0.4 
REUTERS 
87.6 
96.9 
95.9 
96.3 
94.9 
96.5 
97.2 
AMAZON 
94.1 ± 0.2 
94.0 ± 0.5 
92.2 ± 0.4 
89.2 ± 0.3 
88.6 ± 0.4 
91.2 ± 0.5 
94.3 ± 0.4 
20NEWS 
72.3 
71.7 
70.2 
71.0 
74.0 
78.2 
78.3 
RECIPE_L 
71.1 ± 0.5 
74.9 ± 0.5 
73.1 ± 0.6 
73.1 ± 0.5 
71.1 ± 0.4 
76.1 ± 0.4 
79.2 ± 0.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 4 : Pearson's scores of WME against other unsupervised, semi-supervised, and supervised methods on 22 textual similarity tasks. Results are collected from (Arora et al., 2017) except our approach.</head><label>4</label><figDesc></figDesc><table>Approaches 
Supervised 
Unsupervised 
Semi-supervised 
WordEmbeddings 
PSL 
GloVe 
PSL 
Tasks 
PP 
Dan 
RNN 
iRNN 
LSTM(no) 
LSTM(o.g.) 
ST 
nbow 
tf-idf 
SIF 
WME 
SIF 
WME 
STS'12 
58.7 
56.0 
48.1 
58.4 
51.0 
46.4 
30.8 
52.5 
58.7 
56.2 
60.6 
59.5 
62.8 
STS'13 
55.8 
54.2 
44.7 
56.7 
45.2 
41.5 
24.8 
42.3 
52.1 
56.6 
54.5 
61.8 
56.3 
STS'14 
70.9 
69.5 
57.7 
70.9 
59.8 
51.5 
31.4 
54.2 
63.8 
68.5 
65.5 
73.5 
68.0 
STS'15 
75.8 
72.7 
57.2 
75.6 
63.9 
56.0 
31.0 
52.7 
60.6 
71.7 
61.8 
76.3 
64.2 
SICK'14 
71.6 
70.7 
61.2 
71.2 
63.9 
59.0 
49.8 
65.9 
69.4 
72.2 
68.0 
72.9 
68.1 
Twitter'15 
52.9 
53.7 
45.1 
52.9 
47.6 
36.1 
24.7 
30.3 
33.8 
48.0 
41.6 
49.0 
47.4 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on interpretability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inigo</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Montse</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Maritxalar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval@ NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 10: Multilingual semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval@ COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="81" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2013. sem 2013 shared task: Semantic textual similarity, including a pilot on typed-similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalezagirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">* SEM 2013: The Second Joint Conference on Lexical and Computational Semantics</title>
		<imprint>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semeval-2012 task 6: A pilot on semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the First Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="385" to="393" />
		</imprint>
	</monogr>
	<note>Proceedings of the Sixth International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Near-linear time approximation algorithms for optimal transport via sinkhorn iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Altschuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Weed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Rigollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1964" to="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A latent variable model approach to pmi-based word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="385" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An extension of the munkres algorithm for the assignment problem to rectangular matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Claude</forename><surname>Lassalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="802" to="804" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Automatic query expansion using smart: Trec 3. NIST special publication sp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Singhal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="69" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient vector representation for documents through corruption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Marginalized denoising autoencoders for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th international conference on Machine learning</title>
		<meeting>the 29th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Indexing by latent semantic analysis. Journal of the American society for information science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harshman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">391</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning precise timing with lstm recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felix A Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nicol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="115" to="143" />
			<date type="published" when="2002-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Embedding syntax and semantics of prepositions via tensor decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suma</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramod</forename><surname>Viswanath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="896" to="906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suma</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramod</forename><surname>Viswanath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01466</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">Prepositions in context. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Document similarity for texts of varying lengths via hidden topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarek</forename><surname>Sakakini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suma</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2341" to="2351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Probabilistic topic models. Latent Semantic Analysis: A Road to Meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Representative vector machines: a unified framework for classical classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1877" to="1888" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">How to estimate the regularization parameter for spectral regression discriminant analysis and its kernel version? IEEE Transactions on Circuits and Systems for Video Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xindong</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="211" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning with distance substitution kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Haasdonk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claus</forename><surname>Bahlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="220" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The distribution of a product from several sources to numerous localities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hitchcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studies in Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="224" to="230" />
			<date type="published" when="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Supervised word mover&apos;s distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4862" to="4870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1681" to="1691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.2188</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">From word embeddings to document distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="957" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01057</idno>
		<title level="m">A hierarchical neural autoencoder for paragraphs and documents</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-timescale long short-term memory neural network for modelling sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 conference on empirical methods in natural language processing</title>
		<meeting>the 2015 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2326" to="2335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval@ COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A recurrent encoder-decoder network for sequential face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rogerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="38" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Piefa: Personalized incremental and ensemble face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3880" to="3888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning distributed representations for multilingual text sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="88" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Stephen E Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR conference on Research and development in information retrieval</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Okapi at trec-3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Stephen E Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micheline</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nist Special Publication Sp</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page">109</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The earth mover&apos;s distance as a metric for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="121" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Termweighting approaches in automatic text retrieval. Information processing &amp; management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Buckley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="513" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Towards universal paraphrastic sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08198</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">From paraphrase database to compositional paraphrase model and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the ACL (TACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Primme_svds: A high-performance preconditioned svd solver for accurate large-scale computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eloy</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stathopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="248" to="271" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A preconditioned hybrid svd method for accurately computing singular triplets of large matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stathopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="365" to="388" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">D2ke: From distance to kernel and embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>En-Hsu Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fnagli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Witbrock</forename><surname>Michael</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1802.04956" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Random warping series: A random features method for timeseries embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>En-Hsu Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Witbrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="793" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Graph2seq: Graph to sequence learning with attention-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Sheinin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00823</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<title level="m">Semeval-2015 task 1: Paraphrase and semantic similarity in twitter (pit). In SemEval@ NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02474</idno>
		<title level="m">Sentencestate lstm for text representation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
