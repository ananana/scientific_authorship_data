<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Parsing low-resource languages using Gibbs sampling for PCFGs with latent annotations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Mielens</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution" key="instit1">The University of Texas at Austin</orgName>
								<orgName type="institution" key="instit2">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution" key="instit1">The University of Texas at Austin</orgName>
								<orgName type="institution" key="instit2">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Parsing low-resource languages using Gibbs sampling for PCFGs with latent annotations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="290" to="300"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>PCFGs with latent annotations have been shown to be a very effective model for phrase structure parsing. We present a Bayesian model and algorithms based on a Gibbs sam-pler for parsing with a grammar with latent annotations. For PCFG-LA, we present an additional Gibbs sampler algorithm to learn annotations from training data, which are parse trees with coarse (unannotated) symbols. We show that a Gibbs sampling technique is capable of parsing sentences in a wide variety of languages and producing results that are on-par with or surpass previous approaches. Our results for Kinyarwanda and Malagasy in particular demonstrate that low-resource language parsing can benefit substantially from a Bayesian approach.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Despite great progress over the past two decades on parsing, relatively little work has considered the prob- lem of creating accurate parsers for low-resource lan- guages. Existing work in this area focuses primarily on approaches that use some form of cross-lingual boot- strapping to improve performance. For instance, <ref type="bibr">Hwa et al. (2005)</ref> use a parallel Chinese/English corpus and an English dependency grammar to induce an anno- tated Chinese corpus in order to train a Chinese de- pendency grammar. <ref type="bibr" target="#b27">Kuhn (2004b)</ref> also considers the benefits of using multiple languages to induce a mono- lingual grammar, making use of a measure for data re- liability in order to weight training data based on confi- dence of annotation. Bootstrapping approaches such as these achieve markedly improved results, but they are dependent on the existence of a parallel bilingual cor- pus. Very few such corpora are readily available, par- ticularly for low-resource languages, and creating such corpora obviously presents a challenge for many practi- cal applications. <ref type="bibr" target="#b26">Kuhn (2004a)</ref> shows some of the diffi- culty in handling low-resource languages by examining various tasks using Q'anjob'al as an example. Another approach is that of <ref type="bibr" target="#b1">Bender et al. (2002)</ref>, who take a more linguistically-motivated approach by making use of linguistic universals to seed newly developed gram- mars. This substantially reduces the effort by making it unnecessary to learn the basic parameters of a lan- guage, but it lacks the robustness of grammars learned from data.</p><p>Recent work on Probabilistic Context-Free Gram- mars with latent annotations (PCFG-LA) ( <ref type="bibr" target="#b32">Matsuzaki et al., 2005;</ref><ref type="bibr" target="#b36">Petrov et al., 2006</ref>) have shown them to be effective models for syntactic parsing, especially when less training material is available ( <ref type="bibr" target="#b29">Liang et al., 2009;</ref><ref type="bibr" target="#b39">Shindo et al., 2012</ref>). The coarse nonterminal symbols found in vanilla PCFGs are refined by latent variables; these latent annotations can model subtypes of gram- mar symbols that result in better grammars and enable better estimates of grammar productions. In this pa- per, we provide a Gibbs sampler for learning PCFG- LA models and show its effectiveness for parsing low- resource languages such as Malagasy and Kinyawanda.</p><p>Previous PCFG-LA work focuses on the prob- lem of parameter estimation, including expectation- maximization (EM) ( <ref type="bibr" target="#b32">Matsuzaki et al., 2005;</ref><ref type="bibr" target="#b36">Petrov et al., 2006</ref>), spectral learning ( <ref type="bibr" target="#b11">Cohen et al., 2012;</ref><ref type="bibr" target="#b12">Cohen et al., 2013)</ref>, and variational inference ( <ref type="bibr" target="#b29">Liang et al., 2009</ref>; <ref type="bibr" target="#b41">Wang and Blunsom, 2013)</ref>. Regardless of inference method, previous work has used the same method to parse new sentences: a Viterbi parse un- der a new sentence-specific PCFG obtained from an approximation of the original grammar ( <ref type="bibr" target="#b32">Matsuzaki et al., 2005</ref>). Here, we provide an alternative approach to parsing new sentences: an extension of the Gibbs sam- pling algorithm of <ref type="bibr" target="#b23">Johnson et al. (2007)</ref>, which learns rule probabilities in an unsupervised PCFG.</p><p>We use a Gibbs sampler to collect sampled trees theoretically distributed from the true posterior distri- bution in order to parse. Priors in a Bayesian model can control the sparsity of grammars (which the inside- outside algorithm fails to do), while naturally incorpo- rating smoothing into the model <ref type="bibr" target="#b23">(Johnson et al., 2007;</ref><ref type="bibr" target="#b29">Liang et al., 2009</ref>). We also build a Bayesian model for parsing with a treebank, and incorporate informa- tion from training data as a prior. Moreover, we ex- tend the Gibbs sampler to learn and parse PCFGs with latent annotations. Learning the latent annotations is a compute-intensive process. We show how a small amount of training data can be used to bootstrap: af- ter running a large number of sampling iterations on a small set, the resulting parameters are used to seed a smaller number of iterations on the full training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>290</head><p>This allows us to employ more latent annotations while maintaining reasonable training times and still making full use of the available training data.</p><p>To determine the cross-linguistic applicability of these methods, we evaluate on a wide variety of lan- guages with varying amounts of available training data. We use English and Chinese as examples of languages with high data availability, while Italian, Malagasy, and Kinyarwanda provide examples of languages with little available data.</p><p>We find that our technique comes near state of the art results on large datasets, such as those for Chinese and English, and it provides excellent results on limited datasets -both artificially limited in the case of En- glish, and naturally limited in the case of Italian, Mala- gasy, and Kinyarwanda. This, combined with its abil- ity to run off-the-shelf on new languages without any supporting materials such as parallel corpora, make it a valuable technique for the parsing of low-resource lan- guages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Gibbs sampling for PCFGs</head><p>Our starting point is a Gibbs Sampling algorithm for vanilla PCFGs introduced by <ref type="bibr" target="#b23">Johnson et al. (2007)</ref> for estimating rule probabilities in an unsupervised PCFG. We focus instead on using this algorithm for parsing new sentences and then extending it to learn PCFGs with latent annotations. We begin by summarizing the Bayesian PCFG and Gibbs sampler defined by <ref type="bibr" target="#b23">Johnson et al. (2007)</ref>.</p><p>Bayesian PCFG For a grammar G, each rule r in the set of rules R has an associated probability θ r . The probabilities for all the rules that expand the same non- terminal A must sum to one:</p><p>A→β∈R θ A→β = 1. Given an input corpus w=(w (1) , · · · , w (n) ), we in- troduce a latent variable t=(t (1) , · · · , t (n) ) for trees generated by G for each sentence. The joint posterior distribution of t and θ conditioned on w is:</p><formula xml:id="formula_0">p(t, θ | w) ∝ p(θ)p(w | t)p(t | θ) = p(θ)( n i=1 p(w (i) | t (i) )p(t (i) | θ)) = p(θ)( n i=1 p(w (i) | t (i) ) r∈R θ fr(t (i) r )) (1)</formula><p>Here f r (t) is the number of occurrences of rule r in the derivation of t; p(w (i) | t (i) ) = 1 if the yield of t (i) is the sequence w (i) , and 0 otherwise. We use a Dirichlet distribution parametrized by α A : Dir(α A ) as the prior of the probability distribution for all rules expanding non-terminal A (p(θ A )). The prior for all θ, p(θ), is the product of all Dirichlet distri- butions over all non-terminals A ∈ N :</p><formula xml:id="formula_1">p(θ | α) = A∈N p(θ A | α A ).</formula><p>Since the Dirichlet distribution is conjugate to the Multinomial distribution, which we use to model the likelihood of trees, the conditional posterior of θ A can be updated as follows:</p><formula xml:id="formula_2">p G (θ | t, α) ∝ p G (t | θ)p(θ | α) ∝ ( r∈R θ fr(t) r )( r∈R θ αr−1 r ) = r∈R θ fr(t)+αr−1 r (2)</formula><p>which is still a Dirichlet distribution with updated pa- rameter f r (t) + α r for each rule r ∈ R.</p><p>Gibbs sampler The parameters of the PCFG model can be learned from an annotated corpus by simply counting rules. However, parsing cannot be done di- rectly with standard CKY as with standard PCFGs, so we use the Gibbs sampling algorithm presented in <ref type="bibr" target="#b23">Johnson et al. (2007)</ref>. An additional motivation for us- ing this algorithm is that Johnson et al. use it for learn- ing without annotated structures, and in future work we seek to learn from fewer, and at times partial, annota- tions.</p><p>An advantage of using Gibbs sampling for Bayesian inference, as opposed to other approximation algo- rithms such as Variational Bayesian inference (VB) and Collapsed Variational Bayesian inference (CVB), is that Markov Chain Monte Carlo (MCMC) algorithms are guaranteed to converge to a sample from the true posterior under appropriate conditions <ref type="bibr" target="#b40">(Taddy, 2011)</ref>. Both VB and CVB converge to inaccurate and locally optimal solutions, like EM. In some models, CVB can achieve more accurate results due to weaker assump- tions ( <ref type="bibr" target="#b41">Wang and Blunsom, 2013)</ref>. Another advantage of Gibbs sampling is that the sampler allows for parallel computation by allowing each sentence to be sampled entirely independently of the others. After each paral- lel sampling stage, all model parameters are updated in a single step, and the process then repeats (see §2).</p><p>To sample the joint posterior p(t, θ | w), we sample production probabilities θ and then trees t from these conditional distributions:</p><formula xml:id="formula_3">p(t | θ, w, α) = n i=1 p(t i | w i , θ) (3) p(θ | t, w, α) = A∈N Dir(θ A | f A (t) + α A ) (4)</formula><p>Step 1: Sample Rule Probabilities. Given trees t and prior α, the production probabilities θ A for each non- terminal A∈N are sampled from a Dirichlet distribu- tion with parameters f A (t) + α A . f A (t) is a vector, and each component of f A (t), is the number of occur- rences of one rule expanding nonterminal A.</p><p>Step 2: Sample Tree Structures. To sample trees from p(t i | w i , θ), we use the efficient sampling scheme used in previous work <ref type="bibr" target="#b20">(Goodman, 1998;</ref><ref type="bibr" target="#b16">Finkel et al., 2006;</ref><ref type="bibr" target="#b23">Johnson et al., 2007)</ref>. There are two parts to this algorithm. The first constructs an inside table as in the Inside-Outside algorithm for PCFGs <ref type="bibr" target="#b28">(Lary and Young, 1990</ref>). The second selects the tree by recursively sam- pling productions from top to bottom.</p><p>Require: A is parent node of binary rule; w i,k is a span of words: i + 1 &lt; k function TREESAMPLER(A, i, k) for i &lt; j &lt; k and pair of child nodes of A:B, C do P (j, B, C) = θ A→BC ·p B,i,j ·p C,j,k · p A,i,k end for Sample j * , B * , C * from multinomial distribution for (j, B, C) with probabilities calculated above return j * , B * , C * end function Algorithm 1: Sampling split position and rule to ex- pand parent node Consider a sentence w, with sub-spans w i,k = (w i+1 , · · · , w k ). Given θ, we construct the inside ta- ble with entries p A,i,k for each nonterminal and each word span w i,k : 0 ≤ i &lt; k ≤ l, where p A,i,k = P G A (w i,k |θ) is the probability that words i through k were produced by the non-terminal A. The table is computed recursively by</p><formula xml:id="formula_4">p A,k−1,k = θ A→w k (5) p A,i,k = A→BC∈R i&lt;j&lt;k θ A→BC · p B,i,j · p C,j,k (6) for all A, B, C ∈ N and 0 ≤ i &lt; j &lt; k ≤ l.</formula><p>The resulting inside probabilities are then used to generate trees from the distribution of all valid trees of the sentence. The tree is generated from top to bottom recursively with the function T reeSampler defined in Algorithm 1.</p><p>In unsupervised PCFG learning, the rule probabil- ities can be resampled using the sampled trees, then used to reparse the corpus, and so on. We use this property to refine latent annotations for the PCFG-LA model described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PCFG with latent annotations</head><p>When labeled trees are available, rule frequencies can be directly extracted and used as priors for a PCFG. However, when learning PCFG-LAs, we must learn the fine-grained rules from the coarse trees, so we extend the Gibbs sampler to assign latent annotations to unan- notated trees. The resulting learned PCFG-LA parser outputs samples of annotated trees so that we can ob- tain unannotated trees after marginalizing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model</head><p>With the PCFG-LA model ( <ref type="bibr" target="#b32">Matsuzaki et al., 2005;</ref><ref type="bibr" target="#b36">Petrov et al., 2006</ref>) fine-grained CFG rules are auto- matically induced from training, effectively providing a form of feature engineering without human interven- tion. Given H = {1, · · · , K}, a set of latent annotation symbols, and x ∈ H:</p><formula xml:id="formula_5">• θ A[x]→U is the probability of rule A[x] → U ,</formula><p>where U ∈ N × N ∪ T . The probabilities for all rules that expand the same annotated non-terminal must sum to one.</p><p>• β A <ref type="bibr">[x]</ref>,B,C→y,z is the probability of assigning la- tent annotation y, z to child nodes B, C of A <ref type="bibr">[x]</ref>. y,z∈H×H β A <ref type="bibr">[x]</ref>,B,C→y,z = 1. The inputs to the PCFG-LA are a CFG G with finite number of latent annotations for each non-terminal, an initial guess of probabilities of grammar rule θ 0 , and a prior α θ is learned from training.</p><p>The joint posterior distribution of t and θ, β condi- tioned on w is:</p><formula xml:id="formula_6">p(t, θ, β | w) ∝ p(θ, β)p(w | t)p(t | θ, β) = p(θ)p(β)( n i=1 p(w i | t i )p(t i | θ, β))<label>(7)</label></formula><p>We assume that θ and β are independent to get P (θ, β) = P (θ)P (β).</p><p>To learn parameters θ, β, we use a Dirichlet distribu- tion as a prior for both θ and β. The distribution for all rules expanding A <ref type="bibr">[x]</ref> is:</p><formula xml:id="formula_7">P (θ | α θ ) = A∈N,x∈H P (θ A[x] | α θ A[x] )<label>(8)</label></formula><p>The distribution for latent annotations associated with child nodes of A[x] → BC is:</p><formula xml:id="formula_8">P (β | α β ) = y,z∈H×H P (β A[x],B,C | α β A[x],B,C ).<label>(9)</label></formula><p>With this setting, the conditional posterior of θ A <ref type="bibr">[x]</ref> and β A <ref type="bibr">[x]</ref>,B,C can be updated, as in §2. For all unary and binary rules r expanding A <ref type="bibr">[x]</ref>:</p><formula xml:id="formula_9">θ A[x] | t, α θ ∼ Dir(f r (t) + α θ r )<label>(10)</label></formula><p>Here, f r (t) is the number of occurrence of annotated rule r in t. Also, for combination of latent annotations</p><formula xml:id="formula_10">y, z ∈ H × H assigned to B, C in rule A[x] → B, C: β A[x],B,C | t, α β ∼ Dir(f d (t) + α β d )<label>(11)</label></formula><p>Here, f d (t) is the number of occurrences of combina- tion d in t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning PCFG-LAs from raw text</head><p>To learn from raw text, we extend the sampler in §2 to PCFG-LA. Given priors α θ , α β and raw text, the al- gorithm alternates between two steps. The first sam- ples trees for the entire corpus; the second samples θ and β from Dirichlet distributions with updated param- eters, combining priors and counts from sampled trees. The algorithm then alternates between these steps un- til convergence. The outputs are samples of θ, β and annotated trees. The parsing process is specified in Algorithm 2. The first step assigns a tree to a sentence, say w 0,l . We first Require: w 1 , · · · , w n are raw sentences; θ 0 , β 0 are initial values; α θ , α β are priors; M is the number of iterations function PARSE(w 1 , .., w n , θ 0 , β 0 , α θ , α β , M ) for iteration i = 1 to M do for sentence s = 1 to n do Calculate Inside <ref type="table">Table  Sample</ref> tree nodes and associated latent annotations, get tree structure t </p><formula xml:id="formula_11">s , · · · , T (M ) s Find the mode of T (1) s , · · · , T (M ) s : T s end for return T 1 , · · · , T n end function</formula><p>Algorithm 2: Parsing new sentences construct an inside table (see §2). Each entry in the ta- ble stores the probability that a word span is produced by a given annotated nonterminal. For root node S, with θ, β and inside table p A <ref type="bibr">[x]</ref>,i,k , we sample one an- notation based on all p S[x],0,l , x ∈ H. Assume that we sampled x for S, we further sample a rule to ex- pand S <ref type="bibr">[x]</ref> and possible splits of the span w 0,l jointly. Assume that we sampled nonterminals B, C to expand S <ref type="bibr">[x]</ref>, where B is responsible for w 0,j and C is respon- sible for w j,l . We further sample annotations for B, C together, say y, z. Then we sample rules and split po- sitions to expand B[y] and C <ref type="bibr">[z]</ref>, and continue until reaching the terminals.</p><p>This algorithm alone could be used for unsupervised learning of PCFG-LA if we input a non-informed or weakly-informed prior α θ and α β . With access to unannotated trees for training, we only need to assign latent annotations to them and then use the frequen- cies of these annotated rules as the prior when parsing. The details of training when trees are available are il- lustrated in §3.3.</p><p>Once we have trees (with latent annotations), the step of sampling θ and β from a Dirichlet distribution is direct. We need to count the number of occurrences f r (t) for each rule r like A[x] → U, U ∈ N × N ∪ T in updated annotated trees t, and draw θ A <ref type="bibr">[x]</ref> from the updated Dirichlet distribution Dir(f A <ref type="bibr">[x]</ref> (t) + α θ A <ref type="bibr">[x]</ref> ). We also need to count the number of occurrences of f d (t) for each combination of yz ∈ H × H assigned to B, C given A[x] → B, C in t, and draw β A <ref type="bibr">[x]</ref>,B,C from the updated Dirichlet distribution</p><formula xml:id="formula_12">Dir(f A[x],B,C (t) + α β A[x],B,C ) similarly.</formula><p>To parse a sentence, we first calculate the inside table and then sample the tree.</p><p>Calculate the inside table. Given θ, β and a string <ref type="bibr">x]</ref> (w i,k |θ, β) is the probability that words i through k were produced by the annotated non- terminal A <ref type="bibr">[x]</ref>. The table can be computed recursively, for all A ∈ N , x ∈ H, by</p><note type="other">w=w 0,l , we construct a table with entries p A[x],i,k for each A∈N , x ∈ H and 0</note><formula xml:id="formula_13">≤ i &lt; k ≤ l, where p A[x],i,k = P G A[</formula><formula xml:id="formula_14">p A[x],k−1,k = θ A[x]→w k (12) p A[x],i,k = A[x]→BC:BC∈N ×N j:i&lt;j&lt;k yz∈H×H θ A[x]→BC β A[x]BC→yz p B[y],i,j p C[z],j,k<label>(13)</label></formula><p>Sample the tree, top to bottom. First, from start sym- bol S, sample latent annotation from multinomial with probability π S[x] p S[x],0,l for each x ∈ H. Next, given annotated non-terminal A <ref type="bibr">[x]</ref> and i, k, sample possible child nodes and split positions from multinomial with probability:</p><formula xml:id="formula_15">p(B, C, j) = 1 p A[x],i,k · y,z∈H θ A[x]→BC β A[x]BC→yz p B[y],i,j p C[z],j,k<label>(14)</label></formula><p>Here the probability is calculated by marginaliz- ing all possible latent annotations for B, C, and </p><formula xml:id="formula_16">θ A[x]→BC β A[x]</formula><formula xml:id="formula_17">p(y, z) = β A[x]BC→yz p B[y],i,j p C[z],j,k y,z β A[x]BC→yz p B[y],i,j p C[z],j,k<label>(15)</label></formula><p>A crucial aspect of this procedure is that all trees can be sampled independently. This parallel process pro- duces a substantial speed gain that is important partic- ularly when using more latent annotations. After all trees have been sampled (independently), the counts from each individual tree are combined prior to the next sampling iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning from coarse training trees</head><p>In training, we need to learn the probabilities of fine- grained rules given coarsely-labeled trees. We perform Gibbs sampling on the training data by first iteratively sampling probabilities and then assigning annotations to tree nodes. We use the average counts of anno- tated production rules from sampled trees to produce the prior α θ and α β incorporated into parsing raw sen- tences.</p><p>We first index the non-terminal nodes of each tree T by 1, 2, · · · from top to bottom, and left to right. Then the sampler iterates between two steps. The first sam- ples θ, β given annotated trees (as in §3.2). The sec- ond samples latent annotations for nonterminal nodes</p><note type="other">Require: T 1 , · · · , T n are fully parsed trees; θ 0 , β 0</note><p>are initial values; α θ0 , α β0 are priors; M is the number of iterations function ANNO(T 1 , · · · , T n , θ 0 , β 0 , α θ0 , α β0 , M ) for iteration i = 1 to M do for sentence s = 1 to n do Calculate inside probability Sample latent annotations for each node in the tree, get tree with latent annotations t</p><note type="other">(i) s end for Sample θ (i) , β (i) end for return Mean of number of occurrences of production rules and associated latent annotations from all sampled annotated trees end function Algorithm 3: Learning prior from training</note><p>in parsed trees, which also takes two steps. The first step is to, for each node in the tree, calculate and store the probability that the node is annotated by x. The second step is to jointly sample latent annotations for child nodes of root nodes, and then continue this pro- cess from top to bottom until reaching the pre-terminal nodes.</p><p>Step one: inside probabilities. Given tree T , com- pute b i T [x] for each non-terminal i recursively: 1. If node N i is a pre-terminal node above terminal symbol w, then for x∈H</p><formula xml:id="formula_18">b i T [x] = θ Ni[x]→w<label>(16)</label></formula><p>2. Otherwise, let j, k be two child nodes of i, then for</p><formula xml:id="formula_19">x ∈ H b i T [x] = y,z∈H θ Ni[x]→Nj N k β Ni[x]Nj N k →y,z b j T [y]b k T [z] (17)</formula><p>Step two: outside sampling. Given inside probabil- ity b i T [x] for every non-terminal i and all latent annota- tions x∈H, we sample the latent annotations from top to bottom:</p><p>1. If node i is the root node (i = 1), then sample x ∈ H from a multinomial distribution with</p><formula xml:id="formula_20">f i T [x] = π(N i [x]).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">For a parent node with sampled latent annotation</head><formula xml:id="formula_21">N i [x]</formula><p>with children N j , N k , sample latent annota- tions for these two nodes from a multinomial dis- tribution with</p><formula xml:id="formula_22">f i T [y, z] = 1 b i T [x] · θ Ni[x]→Nj N k β Ni[x]Nj N k →y,z b j T [y]b k T [z]<label>(18)</label></formula><p>After training, we take the average counts of sampled annotated rules and combinations of latent annotations as priors to parse raw sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments 1</head><p>Our goal is to understand parsing efficacy using sam- pling and latent annotations for low-resource lan- guages, so we perform experiments on five languages with varying amount of training data. We compare our results to a number of previously established base- lines. First, for all languages, we use both a stan- dard unsmoothed PCFG and the Bikel parser, trained on the training corpus. Additionally, we compare to state-of-the-art results for both English and Chinese, which have an existing body of work in PCFGs using a Bayesian framework. For Chinese, we compare to <ref type="bibr" target="#b21">Huang &amp; Harper (2009)</ref>, using their results that only use the Chinese Treebank (CTB). For English, we com- pare to <ref type="bibr" target="#b29">Liang et al. (2009)</ref>. Prior results for parsing the constituency version of the Italian data are avail- able from <ref type="bibr" target="#b0">Alicante et al. (2012)</ref>, but as they make use of a different version of the treebank including extra sentences, and additionally use the extensive functional tags present in the corpus, we do not directly compare our results to theirs. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>English (ENG) and Chinese (CHI) are the two main languages used for this work; they are commonly used in parser evaluation and have previous examples of sta- tistical parsers using a Bayesian framework. And since we primarily are interested in parsing low-resource lan- guages, we include results for Kinyarwanda (KIN) and Malagasy (MLG) as examples of languages without substantial existing treebanks. Finally, as a middle- ground language, we use Italian (ITL).</p><p>For English, we use the Wall-Street Journal section of the Penn Treebank (WSJ) ( <ref type="bibr" target="#b31">Marcus et al., 1993</ref>). The data split is sections 02-21 for training, section 22 for development, and section 23 for testing. For Chinese, the Chinese Treebank (CTB5) ( <ref type="bibr" target="#b42">Xue et al., 2005</ref>) was used. The data split is files 81-899 for training, files 41- 80 for development, and files 1-40/900-931 for testing.</p><p>The ITL data is from the Turin University Treebank (TUT) ( <ref type="bibr" target="#b6">Bosco et al., 2000</ref>) and consists of 2,860 Italian sentences from a variety of domains. It was split into training, development, and test sets with a 70/15/15 percentage split.</p><p>The KIN texts are transcripts of testimonies by sur- vivors of the Rwandan genocide provided by the Ki- gali Genocide Memorial Center, along with a few BBC news articles. The MLG texts are articles from the websites Lakroa and La Gazette and Malagasy Global Voices. Both datasets are described in . The KIN and MLG data is very small compared to ENG and CHI: the KIN dataset con-tains 677 sentences, while the MLG dataset has only 113. Also, we simulated a small training set for ENG data by using only section 02 of the WSJ for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>As a preprocessing step, all trees are converted into Chomsky Normal-Form such that all non-terminal pro- ductions are binary and all unary chains are removed.</p><p>Additional standard normalization is performed. Functional tags (e.g. the SBJ part of NP-SBJ), empty nodes (traces), and indices are removed. Our binariza- tion is simple: given a parent, select the rightmost child as the head and add a stand-in node that contains the remainder of the original children; the process then re- curses. This simple technique uses no explicit head- finding rules, which eases cross-linguistic applicability.</p><p>From this normalized data, we train latent PCFGs with K=1,2,4,8,16,32 (where K=1 is equivalent to the plain PCFG described in section 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Practical refinements</head><p>Unknown word handling. We use a similar unknown word handling procedure to <ref type="bibr" target="#b29">Liang et al. (2009)</ref>. From our raw corpus we extract features associated with ev- ery word, these features include surrounding context words as well as substring suffix/prefix features. Using these features we produce fifty clusters using k-means. Then, as a pre-parsing step, we replace all words oc- curring less than five times with their cluster label - this simulates unknown words for training. Finally, during evaluation, any word not seen in training was also replaced with its corresponding cluster label. This final step is simple because there are no 'unknown un- knowns' in our corpus, as the clustering has been per- formed over the entire corpus prior to training. This approach is similar to methods for unsupervised POS- tag induction that also utilize clusters in this manner <ref type="bibr">(Dasgupta &amp; Ng, 2007)</ref>.</p><p>We compare this unknown word handling method to one in which the clustering and a classifier is trained not on the corpus under consideration, but rather on a separate corpus of unrelated data. This comparison was made to understand the effects of including the eval- uation set in the training data (without labels) versus training on out-of-domain texts. This is a more real- istic measurement of out-of-the-box performance of a trained model. Jump-starting sampling. In the basic setup, train- ing high K-value models takes a prohibitively long time, so we also consider a jump-start technique that allows larger annotation values (such as K=16) to be run in less time. We train these high-K value models first on a highly reduced training set (5% of the full training set) for a large number of iterations, and then use the found θ values as the starting point for training on the full training set for a small number of iterations. Although many of the estimated parameters on the re- duced set will be zero, the prior allows us to eventually System K=1 K=2 K=4 K=8 <ref type="table" target="#tab_0">K=16  Unsmoothed PCFG 40.2  - - - - Bikel Parser  57.9  - - - - Liang et al. 07</ref> 60.5 71.1 77.2 79.2 78.2 Berkeley Parser 60.8 74.4 78.4 79.1 78.7 Gibbs PCFG 61.0 71.3 76.6 78.7 78.0 <ref type="table">Table 1</ref>: F1 scores for small English training data ex- periments. 'K' is the number of latent annotations - K=1 represents a vanilla, unannotated PCFG.</p><p>recover this information in the full set. This allows us to train on the full training set, which is desirable rela- tive to training on a reduced set, while still allowing the model sufficient iterations to burn in. The fact that we are likely starting in a fairly good position within the search space (due to estimating θ from the corpus) also likely helps enable these lower iteration counts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We start with <ref type="table" target="#tab_0">Tables 1 and 2</ref>, which show performance when training on section 02 of the WSJ (pretending En- glish is a "low-resource" language). The results show that the basic Gibbs PCFG (where K=1), with an F- score of 61.0, substantially outperforms not only an unsmoothed PCFG (the simplest baseline), but also the Bikel parser (Bikel, 2004b) trained on the same amount of data. <ref type="table">Table 1</ref> also shows further large gains are obtained from using latent annotations-from 60.5 for K=1 to 78.7 for K=8. The Gibbs PCFG also compares quite favorably to the PCFG-LA of <ref type="bibr" target="#b29">Liang et al. (2009)</ref>-slightly better for K=1 and K=2 and slightly worse for K=4 and K=8. <ref type="table" target="#tab_0">Table 2</ref> shows that the Gibbs PCFG is able to produce results with a smaller amount of variance relative to the Berkeley Parser, even at low training sizes. This trend is repeated in <ref type="table">Table 3</ref>, which shows that the Gibbs PCFG also produces less variance when training on dif- ferent single sections of the WSJ relative to the Berke- ley Parser, although it again produces slightly lower F1 scores.</p><p>We also use the small English corpus to determine the effects of weighting the prior when sampling anno- tations, varying α between 0.1 and 10.0. Though per- formance is not sensitive to varying α for larger cor- pora, <ref type="figure" target="#fig_2">Figure 1</ref> shows it can make a substantial differ- ence for smaller corpora (with an optimal value was obtained with an α value of 5 for this small training set). This seems to indicate that the lower counts asso- ciated with the smaller training sets should be compen- sated for by weighting those counts more heavily when processing the evaluation set, as we had anticipated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>WSJ Sec. 02 KIN MLG Berkeley Parser 78.3 ± 0.93 60.6 ± 1.1 52.2 ± 2.0 Gibbs PCFG 76.7 ± 0.63 67.2 ± 0.92 57.5 ± 1.1 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>F1 / StDev Berkeley Parser 77.5 ± 2.1 Gibbs PCFG 77.0 ± 1.4 <ref type="table">Table 3</ref>: F1 scores with standard deviations over twenty runs, training on individual WSJ sections (02-21). To evaluate the effectiveness of the jump-start tech- nique, we ran the full ENG data set with K=4 to com- pare the results from the full training setup to jump- starting. For this, we performed 100 training iterations on the reduced training set (WSJ section 02) and then used the resulting θ values to seed training on the full training set. Those training runs varied between three and nine iterations, and the results are shown in <ref type="figure" target="#fig_3">Figure  2</ref>. The full ENG K=4 F-score is 86.2, so these results represent a slight step back. Nonetheless, the technique is still valuable in that it allows for inferring latent an- notations for higher K-values than would typically be available to us in a reasonable timeframe. <ref type="table">Table 4</ref> shows the results for the main experiments. Sampling a vanilla PCFG (K=1) produces results that are not state-of-the-art, but still good overall and al- ways better than an unsmoothed PCFG.  <ref type="table">Table 4</ref>: F1 scores for experiments on sampled PCFGs. Note that <ref type="bibr" target="#b41">Wang and Blunsom (2013)</ref> obtain an ENG F- score of 77.9% using collapsed VB for K=2. Though they do not give exact numbers, their <ref type="figure">Fig. 7</ref> indicates an F-score of about 87% for K=16. of F1 score in all languages, as compared to the vanilla PCFG. Experiments were run up to K=32 primarily due to time constraint. Although previous literature results report increases up to the equivalent of K=64, it may be the case that higher K values with no merge step more easily lead to overfitting in our model -reduc- ing the effectiveness of those high values, as shown by the overall poorer performance on several languages at K=32 when compared to K=16 as well as the general levelling-off seen at the high K values. For English and Chinese, the previous Bayesian framework parsers outperform our own, but only by around two points. Additionally, our parsing of Chi- nese improves on the Bikel parser (trained on our train- ing data) despite the fact that the Bikel parser makes use of language specific optimizations. Our parser needs no changes to switch languages.</p><p>The Gibbs PCFG with K=16 is superior to the strong Bikel and Berkeley Parser benchmarks for both KIN and MLG, a promising result for future work on pars- ing low-resource languages in general. Note also that our parser exhibits less variance than Berkeley Parser especially for KIN and MLG, which supports the fact that the variance of Berkeley Parser is higher for mod- els with few subcategories ( <ref type="bibr" target="#b36">Petrov et al., 2006</ref>).</p><p>Examples of the improvement across latent annota- tions for a given tree are shown in <ref type="figure" target="#fig_4">Figure 3</ref>. The details of the noun phrase 'no major bond offerings' were the same for each tree, and are thus abstracted here. The low K-value tree (K=2) is shown in 3a, and primarily suffers from issues related to the prepositional phrase, 'in Europe friday'. In particular, the low K-value tree incorrectly groups 'Europe friday' as a noun phrase ob- ject of 'in'.</p><p>The higher K-value tree (K=8) is shown in 3b. This tree manages to correctly analyze the preposi- tional phrase, accurately separately the temporal loca- tive 'Friday' from the actual prepositional phrase of 'in Europe'. However, the high K-value tree makes a different mistake that the low K-value tree did not; it groups 'no major bond offerings in Europe Friday' as a noun phrase, when it should be three separate phrases (two noun phrases and a prepositional phrase). This er- ror may be related to the additional latent annotations. With more available noun phrase subtypes, it may be the case that a more unusual noun phrase could be per- mitted that would have been too low probability with only a few subtypes.</p><p>To determine whether the substantial range in F1 scores across languages are primarily the result of the much larger training corpora available for certain lan- guages, two extreme training set reduction experiments were conducted. The training sets for all languages were reduced to a total of either 100 or 500 sen- tences. This process was repeated 10 times in a cross- validation setup, where 10 separate sets of sentences were selected for each language. The results of these experiments are shown in <ref type="table">Table 5</ref>.</p><p>We conclude that while data availability is a major factor in the higher performance of English and Chi- nese in our original experiments, it is not the only is- sue. Clearly, either the linguistic facts of particular languages or perhaps choices of formalism and annota- tion conventions in the corpora make some of the lan- guages more difficult to parse than others. The primary questions is why Gibbs-PCFG is able to achieve higher relative performance on the KIN/MLG datasets when compared to the other parsers, and why this advantage does not necessarily transfer to the extreme small-scale versions of the ENG/CHI/ITL data. Preliminary inves- tigation into the properties of the corpora have revealed a number of potential answers. For instance, the POS tagsets for KIN/MLG are substantially reduced com- pared to the other corpora, and there are differences in the branching factor of the native versions of the corpora as well: a typical maximum branching fac- tor for a tree in ENG/CHI/ITL is around 4-5, while for KIN/MLG it is almost always 2 (natively binary). Branching factors above 5 essentially never occur in KIN/MLG, while they are not rare in ENG/CHI/ITL. The question of exactly why the Gibbs-PCFG seems to perform well on these corpora remains an open ques- tion, but these differences could provide a starting point</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Condition</head><p>In  for future analysis.</p><p>In addition to the actual F1 scores, the relative uni- formity of the standard deviation results indicates that the individual parsers are not that much different in terms of their ability to provide consistent results at these small data extremes, as opposed to the slightly higher training levels where the Gibbs-PCFG generated smaller variances.</p><p>Considering the effects of unknown word handling, <ref type="table" target="#tab_3">Table 6</ref> shows that using the evaluation set when creat- ing the unknown word classifier does improve overall parsing accuracy when compared to an unknown word handler that is trained on out-of-domain texts. This shows that results reported in previous work somewhat overstate the accuracy of these parsers when used in the wild-which matters greatly in the low-resource set- ting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Our experiments demonstrate that sampling vanilla PCFGs, as well as PCFGs with latent annotations, is feasible with the use of a Gibbs sampler technique and produces results that are in line with previous parsers on controlled test sets. Our results also show that our methods are effective on a wide variety of languages-including two low-resource languages- with no language-specific model modifications needed.</p><p>Additionally, although not a uniform winner, the Gibbs-PCFG shows a propensity for performing well on naturally small corpora (here, KIN/MLG). The ex- act reason for this remains slightly unclear, but the fact that a similar advantage is not found for extremely small versions of large corpora indicates that our ap- proach may be particularly well-suited for application in real low-resource environments as opposed to a sim- <ref type="table">Parser   Size  ENG  CHI  ITL  KIN  MLG  Bikel</ref> 100 54.7 ± 2.2 51.4 ± 3.0 51 ± 2.4 47.1 ± 2.3 44.4 ± 2.0 Berkeley 100 55.2 ± 2.6 53.9 ± 2.9 50 ± 2.8 47.8 ± 2.1 44.5 ± 2.3 Gibbs-PCFG 100 54.5 ± 2.0 51.7 ± 2.4 49.5 ± 3.6 50.3 ± 2.3 45.8 ± 1.8 Bikel 500 56.2 ± 2.0 54.1 ± 2.7 54.2 ± 2.4 - - Berkeley 500 58.9 ± 2.2 56.4 ± 2.7 52.5 ± 2.7 - - Gibbs-PCFG 500 58.1 ± 2.0 55.7 ± 2.3 51.1 ± 3.2 - - <ref type="table">Table 5</ref>: 100/500 sentence training set results, including st.dev over 10 runs. KIN/MLG did not have enough data to run the 500 sentence version.</p><p>ulated environment.</p><p>Having established this procedure and its relative tol- erance for low amounts of data, we would like to extend the model to make use of partial bracketing information instead of complete trees, perhaps in the form of Frag- mentary Unlabeled Dependency Grammar annotations ( <ref type="bibr" target="#b38">Schneider et al., 2013)</ref>. This would allow the sam- pling procedure to potentially operate using corpora with lighter annotations than full trees, making initial annotation effort not quite as heavy and potentially in- creasing the amount of available data for low-resource languages. Additionally, using the expert partial anno- tations to help restrict the sample space could provide good gains in terms of training time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Sample θ (i) , β (i) end for for sentence s = 1 to n do Marginalize the latent annotations to get unannotated trees T (1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>BC→yz is the probability of choosing B[y], C[z] to expand A[x], and p B[y],i,j p C[z],j,k are the probabilities for B[y] and C[z] to be responsible for word span w i,j and w j,k respectively. And p A[x],i,k is the normalizing term. Third, given A[x], B, C, i, j, k, sample annotations for B, C from multinomial with probability:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Accuracy by varying α levels for small English data.</figDesc><graphic url="image-1.png" coords="7,56.14,137.62,246.61,178.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: F-Score for K=4, varying full-set training iterations (with and without 100x jump start).</figDesc><graphic url="image-2.png" coords="7,307.28,62.81,226.78,178.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of tree progression in the Gibbs PCFG with a) K=2, b) K=8, and c) gold tree.</figDesc><graphic url="image-3.png" coords="8,65.01,54.07,462.27,138.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>F1 scores with standard deviation over ten runs 
of small training data, K=4. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>The benefits of the latent annotations are further shown in the increase</figDesc><table>Condition 
ENG CHI ITA KIN MLG 
Unsmoothed PCFG 
69.9 66.8 62.1 45.9 
49.2 
Liang et al. 07 
87.1 
-
-
-
-
Huang &amp; Harper09 
-
84.1 
-
-
-
Bikel Parser 
86.9 81.1 74.5 55.7 
49.5 
Berkeley Parser 
90.1 83.4 71.6 61.4 
51.8 
Gibbs PCFG,K=1 
79.3 75.4 66.3 58.5 
55.1 
Gibbs PCFG,K=2 
82.6 79.8 69.3 65.0 
57.0 
Gibbs PCFG,K=4 
86.0 82.3 71.9 67.2 
57.8 
Gibbs PCFG,K=16 
87.2 83.2 72.4 68.1 
58.2 
Gibbs PCFG,K=32 
87.4 83.4 71.0 66.8 
55.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Effect of differing regimes for handling un-
known words. 

</table></figure>

			<note place="foot" n="1"> Code available at github.com/jmielens/gibbs-pcfg-2014, along with instructions for replicating experiments when possible 2 As part of a standardized pre-processing step, we strip functional tags, which makes a direct comparison to their results inappropriate.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Supported by the U.S. Army Research Office un-der grant number W911NF-10-1-0533. Any opin-ions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of the U.S. Army Research Office.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A treebank-based study on the influence of Italian word order on parsing performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anita</forename><surname>Alicante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Bosco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Corazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Lavelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC&apos;12</title>
		<editor>Khalid Choukri, Thierry Declerck, Mehmet Uur Doan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis</editor>
		<meeting>LREC&apos;12<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Nicoletta Calzolari. European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Grammar Matrix: An Open-Source Starter-Kit for the Rapid Development of CrossLinguistically Consistent Broad-Coverage Precision Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Flickinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Oepen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Grammar Engineering and Evaluation at the 19th International Conference on Computational Linguistics</title>
		<editor>John Carroll, Nelleke Oostdijk, and Richard Sutcliffe</editor>
		<meeting>the Workshop on Grammar Engineering and Evaluation at the 19th International Conference on Computational Linguistics<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">On The Parameter Space of Generative Lexicalized Statistical Parsing Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Bikel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Intricacies of Collins&apos; parsing model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bikel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="479" to="511" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards history-based grammars: Using richer models for probabilistic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ezra</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>David M Magerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Speech and Natural Language</title>
		<meeting>the workshop on Speech and Natural Language</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="134" to="139" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Applying probability measures to abstract languages. Computers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard A</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="442" to="450" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Building a Treebank for Italian: a Data-driven Annotation Schema</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Bosco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincenzo</forename><surname>Lombardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Vassallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Lesmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Language Resources and Evaluation LREC-2000</title>
		<meeting>the Second International Conference on Language Resources and Evaluation LREC-2000</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="99" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Two experiments on learning probabilistic dependency grammars from corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Univ</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tree-bank grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence</title>
		<meeting>the National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1031" to="1036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A maximum-entropyinspired parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference</title>
		<meeting>the 1st North American chapter of the Association for Computational Linguistics conference</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="132" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Three models for the description of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Chomsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Theory, IRE Transactions on</title>
		<imprint>
			<date type="published" when="1956" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="113" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spectral learning of latent-variable PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Shay B Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="223" to="231" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Experiments with spectral learning of latent-variable PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Shay B Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="148" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A new statistical parser based on bigram lexical dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael John</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collins</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th annual meeting on Association for Computational Linguistics</title>
		<meeting>the 34th annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="184" to="191" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Three generative, lexicalised models for statistical parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Head-driven statistical models for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="589" to="637" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Solving the problem of cascading errors: Approximate Bayesian inference for linguistic annotation pipelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning a Part-of-Speech Tagger from Two Hours of Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Mielens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51th annual meeting on Association for Computational Linguistics</title>
		<meeting>the 51th annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="721" to="741" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joshua T Goodman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Massachusetts</publisher>
		</imprint>
		<respStmt>
			<orgName>Harvard University Cambridge</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Parsing Inside-Out. Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SelfTraining PCFG grammars with latent annotations across languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Harper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="832" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Breaking the Resource Bottleneck for Multilingual Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Hwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Weinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of the Workshop on Linguistic Knowledge Acquisition and Representation: Bootstrapping Annotated Language Data. Conference on Language Resources and Evaluation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bayesian inference for PCFGs via Markov Chain Monte Carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="139" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PCFG models of linguistic tree representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="613" to="632" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Applying computational linguistic techniques in a documentary project for Qanjobal (Mayan, Guatemala)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC 2004</title>
		<meeting>LREC 2004</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Experiments in parallel-text based grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 470</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics, page 470</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The estimation of stochastic context-free grammars using the insideoutside algrithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Lary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer, Speech and Language</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="35" to="56" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Probabilistic grammars and hierarchical Dirichlet processes. The handbook of applied Bayesian analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Statistical decision-tree models for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David M Magerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd annual meeting on Association for Computational Linguistics</title>
		<meeting>the 33rd annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="276" to="283" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COMPUTATIONAL LINGUISTICS</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Probabilistic CFG with latent annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Insideoutside reestimation from partially bracketed corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Schabes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th annual meeting on Association for Computational Linguistics</title>
		<meeting>the 30th annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improved Inference for Unlexicalized Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sparse multi-scale grammars for discriminative latent variable parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="867" to="876" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning accurate, compact, and interpretable tree annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elias</forename><surname>Ponvert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A framework for (under) specifying dependency syntax without overloading annotators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naomi</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Saphra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baldridge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.2091</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bayesian symbol-refined tree substitution grammars for syntactic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="440" to="448" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">On estimation and selection for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew A Taddy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1109.4518</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Collapsed Variational Bayesian Inference for PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia, Bulgaria, August</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The Penn Chinese TreeBank: Phrase structure annotation of a large corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="238" />
			<date type="published" when="2005-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
