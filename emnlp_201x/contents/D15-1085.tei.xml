<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Open-Domain Name Error Detection using a Multi-Task RNN</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Open-Domain Name Error Detection using a Multi-Task RNN</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Out-of-vocabulary name errors in speech recognition create significant problems for downstream language processing, but the fact that they are rare poses challenges for automatic detection, particularly in an open-domain scenario. To address this problem, a multi-task recurrent neural network language model for sentence-level name detection is proposed for use in combination with out-of-vocabulary word detection. The sentence-level model is also effective for leveraging external text data. Experiments show a 26% improvement in name-error detection F-score over a system using n-gram lexical features.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most spoken language processing or dialogue sys- tems are based on a finite vocabulary, so occa- sionally a word used will be out of the vocabu- lary (OOV), in which case the automatic speech recognition (ASR) system chooses the best match- ing in-vocabulary sequence of words to cover that region (where acoustic match dominates the deci- sion). The most difficult OOV words to cover are names, since they are less likely to be covered by morpheme-like subword fragments and they often result in anomalous recognition output, e.g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REF: what can we get at</head><p>Litanfeeth HYP: what can we get it leaks on feet While these errors are rare, they create major prob- lems for language processing, since names tend to be important for many applications. Thus, it is of interest to automatically detect such error regions for additional analysis or human correction.</p><p>Named entity recognition (NER) systems have been applied to speech output <ref type="bibr" target="#b21">(Palmer and Ostendorf, 2005;</ref><ref type="bibr" target="#b26">Sudoh et al., 2006</ref>), taking advantage of local contextual cues to names (e.g. titles for person names), but as illustrated above, neighbor- ing words are often affected, which obscures lexi- cal cues to name regions. <ref type="bibr" target="#b22">Parada et al. (2011)</ref> re- duce this problem somewhat by applying an NER tagger to a word confusion network (WCN) based on a hybrid word/fragment ASR system.</p><p>In addition to the problem of noisy context, au- tomatic name error detection is challenging be- cause name errors are rare for a good recognizer. To learn the cues to name errors, it is neces- sary to train from the output of the target rec- ognizer, so machine learning is faced with infre- quent positive examples for which training data is very sparse. In addition, in an open domain sys- tem, automatically-learned lexical context features from one domain may be useless in another.</p><p>In this paper, we address these general problems -detecting rare events in an open-domain task - specifically for name error detection. Prior work addressed the problem of skewed priors by artifi- cially increasing the error rate by holding names out of the vocabulary <ref type="bibr" target="#b2">(Chen et al., 2013)</ref> or by factoring the problem into sentence-level name de- tection and OOV word detection ( <ref type="bibr" target="#b6">He et al., 2014</ref>) (since OOV errors in general are more frequent than name errors). Sentence-level features are also shown to be more robust than local context in di- rect name error prediction <ref type="bibr" target="#b15">(Marin, 2015)</ref>. While these techniques provide some benefit, the use of discrete lexical context cues is sensitive to the lim- ited amount of training data available.</p><p>Our work leverages the factored approach, but improve performance by using a continuous-space sentence representation for predicting presence of a name. Specifically, we modify a recurrent neu- ral network (RNN) language model (LM) to pre- dict both the word sequence and a sentence-level name indicator. Combining the LM objective with name prediction provides a regularization effect in training that leads to improved sentence-level name prediction. The continuous-space model is also effective for leveraging external text resources to improve generalization in the open-domain sce- nario.</p><p>The overall framework for speech recognition and baseline name error detection system is out- lined in Section 2, and the multi-task (MT) RNN approach for sentence-level name prediction is in- troduced in Section 3. Experimental results for both sentence-level name detection and name error detection are presented in Section 4, demonstrat- ing the effectiveness of the approach on test data that varies in its match to the training data. As discussed in Section 5, the sentence-level model is motivated by similar models for other applica- tions. The paper summarizes key findings and dis- cusses potential areas for further improvement in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview and Tasks</head><p>The name error detection task explored in this work is a component in a bidirectional speech- to-speech translation system for English to/from Iraqi Arabic with human-computer dialogue inter- action for error resolution <ref type="bibr" target="#b0">(Ayan et al., 2013</ref>), de- veloped during the DARPA BOLT project. The training data consists of a range of topics associ- ated with activities of military personnel, includ- ing traffic control, military training, civil affairs, medical checkups, and so on. However, the sys- tem is expected to handle open-domain tasks, and thus the evaluation data covers a broader range of topics, including humanitarian aid and disaster re- lief, as well as more general topics such as sports, family and weather. The dialogues often contain mentions of names and places, many of which are OOV words to the ASR system. As illustrated in the previous section, ASR hypotheses necessarily have errors in OOV regions, but because specific names are infrequent, even in-vocabulary names can have these types of error patterns. Therefore, developing a robust name error detector for ASR hypotheses is an important component of the sys- tem to resolve errors and ambiguity.</p><p>Detecting OOV errors requires combining ev- idence of recognizer uncertainty and anomalous word sequences in a local region. For name er- rors, lexical cues to names are also useful, e.g. a person's title, location prepositions, or keywords such as "name". The baseline system for this work uses structural features extracted from a confusion network of ASR hypotheses plus ASR word con- fidence to represent recognizer uncertainty, and word n-gram context to the left and right of the target confusion network slot. These features are combined in a maximum entropy (ME) classifier trained to predict name errors directly. This is the same as the baseline used in ( <ref type="bibr" target="#b6">He et al., 2014;</ref><ref type="bibr" target="#b15">Marin, 2015;</ref>, but with a differ- ent ASR system.</p><p>Training a classifier to predict whether a sen- tence has a name is easier than direct name error prediction, because the positive class is less rare, and it does not require recognizer output so more data can be used (e.g. speech transcripts with- out recognizer output, or written text). In addi- tion, since the words abutting the name are less reliable in a recognition hypothesis, the informa- tion lost by working at the sentence level is mini- mal. The idea of using sentence-level name pre- diction is proposed in ( <ref type="bibr" target="#b6">He et al., 2014</ref>), but in that work the sentence name posterior is a fea- ture in the ME model (optionally with word cues learned by the sentence-level predictor). In our work, the problem is factored to use the acous- tic confusibility and local word class features for OOV error prediction, which is combined with the sentence-level name posterior for name error pre- diction. In other words, only two features (poste- riors) are used in training with the sparse name er- ror prediction data. An ME classifier is then used to combine the two features to predict the word- level name error. The word-level OOV detector is another ME binary classifier; the full set of fea- tures used for the word-level OOV detector can be found in <ref type="bibr" target="#b15">(Marin, 2015)</ref>.</p><p>The main innovation in this work is that we propose to use a multi-task RNN model for the sentence-level name prediction, where the train- ing objective takes into account both the language modeling task and the sentence-level name predic- tion task, as described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multi-task Recurrent Neural Network</head><p>The RNN is a powerful sequential model and has proven to be useful in many natural lan- guage processing tasks, including language mod- eling ( <ref type="bibr" target="#b16">Mikolov et al., 2010</ref>) and word similarity ( <ref type="bibr" target="#b19">Mikolov et al., 2013c</ref>). It also achieves good re- sults for a variety of text classification problems when combined with a convolutional neural net- work (CNN) ( <ref type="bibr" target="#b11">Lai et al., 2015)</ref>. In this paper, we propose an MT RNN for the sentence-level name prediction task, which augments the word predic- tion in the RNN language model with an addi- tional output layer for sentence-level name pre- diction. Formally, the MT RNN is defined over t = 1, . . . , n for a sentence of length n as:</p><formula xml:id="formula_0">x(t) = M z(t), h(t) = f (W x(t) + Rh(t − 1)) , o(t) = s (U h(t) + b 1 ) , y(t) = s (V h(t) + b 2 ) .</formula><p>where z(t) ∈ R V is the 1-of-V encoding of the t-th word in the sentence; x(t) ∈ R d is the d- dimensional continuous word embedding corre- sponding to the t-th word; h(t) ∈ R k is a k- dimensional embedding that summarizes the word sequence through time t; and o(t) and y(t) are re- spectively the output layers for the language mod- eling task and the sentence-level prediction task. The parameters of the model (learned in multi-task training) include: M ∈ R d×V , which is usually referred to as the word embedding matrix; projec- tion matrices U ∈ R V ×d and V ∈ R 2×d ; and bias terms b 1 , b 2 ∈ R d . f and s are respectively the sigmoid and softmax functions. Note that the word sequence associated with a sentence includes start and end symbols.</p><p>The structure of the proposed MT RNN is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. At each time step, the hidden vector is used to predict both the next word and a sentence-level indicator of the presence of a name, providing a probability distribution for both variables. Thus, the hidden vector h t provides a continuous representation of the word history that emphasizes words that are important for pre- dicting the presence of a name. The vector at time n can be thought of as a sentence embed- ding. The sentence-level output y t differs from the word-dependent label predictor typically used in named entity detection (or part-of-speech tagging) in that it is providing a sequentially updated pre- diction of a sentence-level variable, rather than a word-level indicator that specifies the location of a named entity in the sentence. The final predic- tion y n is used as a feature in the name error de- tection system. The sentence-level output y t does not always converge to y n gradually nor is it al- ways monotonic, since the prediction can change abruptly (either positively or negatively) as new words are processed. The sentence-level variable provides a mechanism for capturing long distance context, which is particularly useful for speech ap- plications, where both the name of interest and the words in its immediate context may be in error.</p><p>The training objective is the combination of the log-likelihood of the word sequence and that of the sentence-level name prediction:</p><formula xml:id="formula_1">n t=1 [(1 − λ) log P (w(t)|h(t)) + λ log P (y(t)|h(t))] , (1)</formula><p>where h(t) = [w(1), . . . , w(t − 1)] and λ is the weight on the log-likelihood of the sentence-level name labels.</p><p>Another way to train the model is to predict the sentence-level name label only at the end of the sentence, rather than at every time step. Pre- liminary experiment results show that this model has inferior performance. We argue that train- ing with only the sentence-final name label output can result in unbalanced updates, i.e., information from the language modeling task is used more of- ten than that from the sentence-level name predic- tion task, implying that balancing the use of infor- mation sources is an important design choice for multi-task models.</p><p>The training objective is optimized using stochastic gradient descent (SGD). We also exper- iment with AdaGrad ( <ref type="bibr" target="#b4">Duchi et al., 2011</ref>), which has shown to be more stable and converge faster for non-convex SGD optimization. Since language model training requires a normalization opera- tion each time over the whole vocabulary (~60K) which is computationally intensive, we further speed up training by using noise contrastive esti-mation (NCE) <ref type="bibr" target="#b20">(Mnih and Teh, 2012)</ref>. For all mod- els using NCE, we fix the number of negative sam- ples to 50.</p><p>All the weights are randomly initialized in the range of [−0.1, 0.1]. The hidden layer size k is selected from {50, 100, 200} and the task mixing weight λ is select in {0.2, 0.4, 0.6, 0.8}, based on development set performance. We set the initial learning rate to 1 and 0.1 for SGD with and with- out Adagrad respectively. In our experiments, we observe that models trained with AdaGrad achieve better performance, so we only report the mod- els with AdaGrad in this paper. At each training epoch, we validate the objective on the develop- ment set. The learning rate is reduced after the first time the development set loglikelihood de- creases, and the whole training procedure termi- nates when the development set loglikelihood de- creases for the second time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>There are two types of datasets used in this pa- per: the BOLT dataset and a collection of Reddit discussions. The first dataset was collected dur- ing the DARPA TRANSTAC and BOLT projects. The ASR hypotheses of totally 7088 spoken sen- tences makes up of the training dataset (BOLT- Train) for both sentence-level name prediction and word-level name error detection. There are two development sets: Dev1 and Dev2. Dev1 is used for parameter tuning for all models, and Dev2 is used for training the ME-based word-level name error detector using the word-level OOV posterior and sentence-level name posterior. For RNN mod- els, we tune the hidden layer size and the multi- task weight λ; for the ME-based word-level name error detector, we tune the regularization param- eter. Two test sets are used to evaluate sentence- level name prediction (Test1) and word-level name error detection (Test2), based on the BOLT phase 2 and 3 evaluations, respectively.</p><p>As shown in <ref type="table">Table 1</ref>, the BOLT topics are cate- gorized into three domains: TRANSTAC, HADR and General. The BOLT-Train, Dev1 and Dev2 sets contain only speech from the TRANSTAC do- main, whereas the Test1 and Test2 sets contain all three domains. Detailed data split statistics and domain information are summarized in Ta- ble 2. Note that there are very few positive sam- ples of name errors (roughly 1%), whereas for the  sentence-level name prediction task and the word- level OOV prediction task, the data skewness is somewhat less severe (roughly 8% sentences with names in most of our data sets). In order to address the issue of domain mis- match between the training data and the broad- domain subsets of the test data, we collect text from Reddit.com which is a very active discus- sion forum where users can discuss all kinds of topics. Reddit has thousands of user-created and user-moderated subreddits, each emphasiz- ing a different topic. For example, there is a general ASKREDDIT subreddit for people to ask any questions, as well as subreddits targeted for specific interests, like ASKMEN, ASKWOMEN, ASKSCIENCE, etc. Although the Reddit discus- sions are different from BOLT data, they have a conversational nature and names are often ob- served within the discussions. Therefore, we hy- pothesize that they can help improve sentence- level name prediction in general. We collect data from 14 subreddits that cover different kinds of topics (such as politics, news, book suggestions) and vary in community size. The Stanford Name Entity Recognizer ( <ref type="bibr" target="#b5">Finkel et al., 2005</ref>) is used to detect names in each sentence, and a sentence- level name label is assigned if there are any names present. <ref type="bibr">1</ref> The data are tokenized using Stanford  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sentence-level Name Prediction</head><p>To evaluate the effectiveness of the proposed MT RNN model, we first apply it to the sentence- level name prediction task on ASR hypotheses. For this task, each sample corresponds to a hy- pothesized sentence generated by the ASR system and a ground-truth label indicating whether there are names in that sentence. We compare the MT RNN with four contrasting models for predicing whether a sentence includes a name.</p><p>• BOW + ME. An ME classifier using a bag-of- words (BOW) sentence representation.</p><p>• SG + ME. An ME classifier is used with the sen- tence embedding as features, where the embed- ding uses the skip-gram (SG) model (Mikolov et al., 2013a) to get word-level embeddings (with window size 10) and sentence embeddings are composed by averaging embeddings of all words in the sentence.</p><p>• RNN + ME. A simple RNN LM is trained (i.e., λ in (1) is set to 0), and the hidden layer for the last word in a sentence provides a sentence- level embedding that is used in an ME classifier trained to predict the sentence-level name label.</p><p>• ST RNN. A single-task (ST) RNN model is trained to directly predict the sentence-level name for each word (i.e., λ in (1) is set to 1).</p><p>All models are trained on either BOLT-Train or BOLT-Train + Reddit, and tuned on Dev1 includ- ing the dimension of embeddings, 2 regulariza- tion parameters for the ME classifiers, and so on. The domain-specific F-scores on Test1 are sum- marized in <ref type="table" target="#tab_3">Table 3</ref>   data. The proposed MT RNN achieves the best re- sults on the TRANSTAC and HADR domains, and it has significant overall performance improve- ment over all baseline models. Not surprisingly, the two approaches that used unsupervised learn- ing to obtain a sentence-level embedding (SG + ME, RNN + ME) have the worst performance on the TRANSTAC and HADR domains, with both having very low precision (6-18%), with best pre- cision on the general domain. The BOW + ME and the ST RNN have similar performance in terms of overall F-score, but the BOW + ME model is much better on the TRANSTAC domain, whereas the ST RNN achieves the best results on the General do- main. The main failing of the BOW + ME model is in recall on the General domain, though it also has relatively low recall on the HADR domain. Note that the General domain only accounts for 10% of the Test1 set, so the ST RNN gets lower F-score overall compared with the MT RNN, which per- forms best on the other two domains (90% of the Test1 set). On all domains, the MT RNN greatly improves precision compared to the ST RNN (at the expense of recall), and it improves recall com- pared to the BOW + ME approach (at the expense of precision).</p><p>In order to study the effectiveness of using ex- ternal data, we also train all models on an enlarged training set including extra name-tagged sentences from Reddit. The improvement for each domain due to also using the Reddit data is shown in <ref type="figure" target="#fig_2">Fig. 2</ref> for the three best configurations. (There is no ben- efit in the unsupervised learning cases.) Com-pared with training only on the BOLT data, all three of these models get substantial overall per- formance improvement by utilizing the external domain training data. Since the external training data covers mostly topics in the General domain, the performance gain in that domain is most sig- nificant. For the MT RNN and BOW + ME clas- sifiers, the additional training data primarily ben- efits recall, particulary for the General domain. In contrast, the ST RNN sees an improvement in precision for all domains. One reason that the added training text also benefits the models on the TRANSTAC domain is that over 90% of the words in the speech recognizer vocabulary are not seen in the small set of name-labeled speech training data, which means that the embeddings for these words are random in the RNNs trained only this data and the BOW + ME classifier will never use these words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Word-level Name Error Detection</head><p>We next assess the usefulness of the resulting MT RNN model for word-level name error detection on ASR hypotheses. Here, several word-level name error detection approaches are compared, in- cluding direct name error prediction and factored name/OOV prediction approaches.</p><p>• OOV thresholding. This system simply uses the OOV prediction, but with the posterior threshold tuned for word-level name error based on the Dev1 set.</p><p>• Word Context. This is the baseline ME sys- tem described in Section 2 and also used as a baseline in ( <ref type="bibr" target="#b6">He et al., 2014;</ref>, which directly predicts the word-level name er- ror using WCN structural features, current word Brown class, and up-to trigram left and right word context information.</p><p>• Word Class. This system, from <ref type="bibr" target="#b15">(Marin et al., 2015)</ref>, also directly predicts the word-level name error, but replaces the word n-gram fea- tures with a smaller number of word class fea- tures to address the sparse training problem. The word classes are based on seed words learned from sentence-level name prediction which are expanded to classes using a nearest neighbor distance with RNN embeddings trained on the BOLT data.</p><p>• Word Context + OOV. This system uses an <ref type="bibr">2</ref> regularized ME classifier to predict the word- level name error using two posteriors as fea- tures: a word-dependent posterior from the Word Context system and one from the OOV de- tection system.</p><p>• MT RNN + OOV. This system also uses an <ref type="bibr">2</ref> regularized ME classifier to predict the word- level name error using two posteriors as fea- tures: the same word-dependent OOV posterior as above, and the posterior from the sentence- level name prediction using the MT RNN model described in Section 4.2, which is constant for all positions in the sentence.</p><p>All of the above models are trained on BOLT-Train and tuned on Dev1. The ME classifier used in the Word Context + OOV and the MT RNN + OOV systems are trained on Dev2, with regularization weights and decision boundaries tuned on Dev1. Name error detection results (F-scores) are summarized in <ref type="table" target="#tab_5">Table 4</ref>. The three systems that use discrete, categorical lexical context features (word or word class context) in direct name er- ror prediction have worse results than the OOV thresholding approach overall, as well as on the TRANSTAC subset. Presumably this is due to over-fitting associated with the lexical context fea- tures. The factored MT RNN + OOV system, which uses a continuous-space representation of lexical context, achieves a gain in overall perfor- mance compared to the other systems and a sub- stantial gain in performance on the TRANSTAC domain on which it is trained. Using the Red- dit data further improves the overall F-score, with a slight loss on the TRANSTAC subset but sub- stantial gains in the other domains. Although the performance improvement in the general domain is relatively small compared with sentence-level name prediction, utilizing external data makes the resulting representation more transferable across different domains and tasks. The best MT RNN + OOV system obtains 26% relative improvement over the baseline Word Context system (or 17% improvement over the simple OOV thresholding approach).</p><p>Looking at performance trade-offs in precision and recall, we find that the use of the RNN systems mainly improves precision, though there is also a small gain in recall overall. The added training data benefits recall for all domains, with a small loss in precision for the TRANSTAC and HADR sets. The use of the OOV posterior improves pre- cision but limits recall, particularly for the general domain where recall of the OOV posterior alone   is only 18% vs. 25% for the word context model with the OOV posterior information.</p><p>To better understand some of the challenges of this task, consider the following examples (R=reference, H=hypothesis):</p><p>R1: i'm doing good my name is captain rodriguez H1: i'm doing good my name is captain road radios R2: well it's got flying lizards knights and zombies and shit H2: well it's gotta flying lives there it's nights and some bees and shia R3: i live in a city called omaha H3: i live in a city called omar ASR tokens associated with name errors are un- derlined and italicized; tokens associated with non-name OOV errors are simply underlined. Name errors have a similar character to OOV erors in that they often have anomalous word sequences in the region of the OOV word (examples 1 and 2), which is why the OOV posterior is so useful. However, too much reliance on the OOV posterior leads to wrongly detecting general OOV errors as name errors ('lizards' and 'zombies' in example 2) and missed detection of name errors where the confusion network cues indicate a plausible hy- pothesis ('omaha' in example 3). Examples 1 and 3 illustrate the importance of lexical cues to names ('name . . . captain', 'city called'), but word-based cues are unreliable for the systems trained only on the small amount of domain-specific data. Lever- aging the reddit data allowed the MT RNN system to detect the error in example 1 (HADR domain) that was missed by the word context system. Ex- ample 3 was only detected by the word context system when no OOV posterior is used. Though this example was from the General domain, city names represent an important error class in the TRANSTAC data, so the term 'city' is learned as a useful cue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Sentence Embedding</head><p>As discussed in Section 3, we postulate that by modeling words in a sentence in sequential order and simutaneously predicting sentence categorical information, the resulting hidden vector of the last word should be a good representation of the whole sentence, i.e., a sentence embedding. To provide support for this hypothesis and show the impact of external data, we present the sentence embed- dings learned by the different RNN variants on Test2 using the t-Distributed Stochastic Neighbor Embedding (t-SNE) visualization <ref type="bibr" target="#b28">(van der Maaten and Hinton, 2008)</ref>  As we can see in <ref type="figure" target="#fig_4">Fig. 3a</ref>, the positive and neg- ative sentence embeddings learned by RNN LM are randomly scattered in the space, indicating that embeddings learned via unsupervised train- ing (e.g., solely on word context) may fail to cap- ture the sentence-level indicators associated with a particular task, in this case presence of a name. When the results are ploted in terms of domains, the embeddings are similarly broadly scattered - there is no obvious topic representation in the em- beddings.</p><p>When comparing <ref type="figure" target="#fig_4">Fig. 3b to Fig. 3a</ref>, which is associated with the RNN using a sentence-final name indicator, we can see that a lot of posi- tive vectors have moved to the bottom-left of the space, though there is still a relatively large over- lap between positive and negative embeddings. In <ref type="figure" target="#fig_4">Fig. 3c</ref>, corresponding to the word-level MT RNN, there forms a separable subgroup of positive em- beddings and the overlapping seems to be reduced as well in contrast to <ref type="figure" target="#fig_4">Figs. 3a and 3b</ref>. Finally, most of the positive sentence embeddings produced by the MT RNN model trained with external Red- dit data gather at the botton-left of <ref type="figure" target="#fig_4">Fig. 3d</ref>. In general, the overlap between positive and nega- tive sentences decreases from single task models to multi-task model. The external data make the proposed model produce more well-shaped group- ings of sentence embeddings.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Recently, due to the success of continuous repre- sentation methods, much work has been devoted to studying methods for learning word embeddings that capture semantic and syntactic meaning. Al- though these word embeddings are shown to be successful in some word-level tasks, such as word analogy <ref type="bibr" target="#b18">(Mikolov et al., 2013b;</ref><ref type="bibr">Jeffery Pennington, 2014</ref>) and semantic role labeling <ref type="bibr" target="#b3">(Collobert and Weston, 2008)</ref>, it is still an open question how best to compose the word embeddings effectively and make use of them for text understanding.</p><p>Recent work on learning continuous sentence representations usually compose the word em- beddings using either a convolutional neural net- work (CNN), a tree-structured recursive NN, or a variant of an RNN. A typical CNN-based struc- ture composes word embeddings in a hierachical fashion (alternating between convolutional layers and pooling layers) to form the continuous sen- tence representation for sentence-level classifica- tion tasks <ref type="bibr" target="#b10">(Kim, 2014;</ref><ref type="bibr" target="#b9">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b11">Lai et al., 2015</ref>). These models usually build up the sentence representation directly from the lexi- cal surface representation and rely on the pooling layer to capture the dependencies between words. Another popular method for continuous sentence representation is based on the recursive neural net- work ( <ref type="bibr" target="#b24">Socher et al., 2012;</ref><ref type="bibr" target="#b25">Socher et al., 2013;</ref><ref type="bibr" target="#b27">Tai et al., 2015</ref>). These models use a tree struc- ture to compose a continuous sentence represen- tation and have the advantages of capturing more fine-grained sentential structure due to the use of parsing trees. Note that the RNN-based sequen- tial modeling used in this paper can be viewed as a linearized tree-structure model.</p><p>In this paper, we train the neural network model with a multi-task objective reflecting both the probability of the sequence and the probability that the sequence contains names. The general idea of multitask learning dates back to <ref type="bibr" target="#b1">(Caruana, 1997)</ref>, and is shown to be effective recently for neu- ral network models in different natural language processing tasks. <ref type="bibr" target="#b3">Collobert and Weston (2008)</ref> propose a unified deep convolutional neural net- work for different tasks by using a set of task- independent word embeddings together with a set of task-specific word embeddings. For each task, it uses a unique neural network with its own lay- ers and connections.  propose a different neural network structure for search query classification and document retrieval where lower- level layers and connections are all shared but the high-level layers are task-specific. For tasks con- sidered in <ref type="bibr" target="#b3">(Collobert and Weston, 2008)</ref> and ( ), training samples are task-dependent. Thus, both models are trained following the SGD manner by alternating tasks for each training sam- ples with task-dependent training objectives. In this paper, we combine the language modeling task with the sentence-level name prediction task, and each training sample has labels for both tasks. Therefore, the SGD training can be done with the weighted sum of the task-specific objectives for each training sample, and the language model ob- jective can be thought of as a regularization term. Similar settings of multitask learning for neural network models are employed in phoneme recog- nition for speech <ref type="bibr" target="#b23">(Seltzer and Droppo, 2013)</ref> and speech synthesis ( <ref type="bibr" target="#b29">Wu et al., 2015)</ref> as well, but both of them use equal weights for all tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we address an open domain rare event detection problem, specifically, name er- ror detection on ASR hypotheses. To alleviate the data skewness and domain mismatch prob- lems, we adopt a factored approach (sentence- level name prediction and OOV error prediction) and propose an MT RNN for sentence-level name prediction. The factored model is shown to be more robust to the sparse training problem. For the problem of sentence-level name prediction, the proposed method of combining the language modeling and sentence-level name prediction ob- jectives in an MT RNN achieves the best results among studied models for the domain represented by the training data as well as in the open-domain scenario. Visualization of sentence-level embed- dings show how both the multi-task and the word- level name label update are important for achiev- ing good results. The use of unrelated external training text (which can only be used in sentence- level name prediction) improves all models, par- ticularly for the highly-mismatched general do- main data.</p><p>The improvement in performance associated with using the external text is much smaller on the word-level name error detection task than on the sentence-level name prediction. This seems to be due to the high weight learned for the word-level posterior. For future work, it is worthwhile look- ing into whether continuous word and sentence representations can be combined in the name er- ror detector to achieve further improvement.</p><p>In this work, we proposed a model for learn- ing sentence representations that might be use- ful for other sentence classification tasks, such as review and opinion polarity detection, ques- tion type classification and so on. As discussed in Section 5, there are other models that have been found useful for obtaining continuous sen- tence embeddings. It would be of interest to in- vestigate whether other structures are more or less sensitive to data skew and/or useful for incorporat- ing multi-domain training data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The structure of the proposed MT RNN model, which predicts both the next word o(t) and whether the sentence contains a name y(t) at each time step.</figDesc><graphic url="image-1.png" coords="3,79.09,62.81,204.09,170.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>for training only with the BOLT give useful labels for the Reddit data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: F-scores on Test1 for sentence-level name prediction for models trained with and without Reddit data. The number in the parenthesis indicates the portion of the domain in the data.</figDesc><graphic url="image-2.png" coords="5,307.28,62.81,226.77,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>in Fig. 3 .</head><label>3</label><figDesc>Four models are com- pared, including three RNNs trained on the BOLT data, and the MT RNN model trained on BOLT + Reddit data. Note the visualization method t-SNE does not take the label information into account during the learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: t-SNE visualization of sentence embeddings learned from different RNN models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Data splits and statistics, including the 
percentage of sentences containing names (name 
sentences), the percentage of hypothesized words 
that are name errors (name errors), and the per-
centage of words that are OOVs (OOVs). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>F-scores on Test1 for sentence-level name 
prediction for models trained on BOLT data. 

CoreNLP tools (Manning et al., 2014) and are low-
ered cased after running the Stanford Name Entity 
Recognizer. In total, we obtain 135K sentences 
containing at least one name and 360K sentences 
without names. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>F-scores on Test2 for word-level name 
error prediction. MT RNN is trained on BOLT-
Train, whereas MT RNN  † is trained on BOLT-
Train + Reddit. 

</table></figure>

			<note place="foot" n="1"> The Stanford Name Entity Recognizer achieves 82.3% F-score on the references of Dev1. Thus, it is expected to</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Alex Marin, Ji He, Wen Wang and all reviewers for useful discussion and comment. This material is based on work sup-ported by DARPA under Contract No. HR0011-12-C-0016 (subcontract 27-001389). Any opin-ions, findings and conclusions or recommenda-tions expressed herein are those of the authors and do not necessarily reflect the views of DARPA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Can you give me another word for hyperbaric?&quot; Improving speech translation using targeted clarification questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">F</forename><surname>Ayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Frandsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kathol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bechet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Favre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salletmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hirschberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stoyanchev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8391" to="8395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997-07" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Variable-span out-of-vocabulary named entity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sankaranarayanan</forename><surname>Ananthakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3761" to="3765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Effective data-driven feature learning for detecting name errors in automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT</title>
		<meeting>SLT</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffery</forename><surname>Pennington</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Representation learning using multi-task deep neural networks for semantic classification and information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning phrase patterns for asr error detection using semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Effective use of cross-domain parsing in automatic speech recognition and error detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Marin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>University of Washington</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Tomá˘ S Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">˘</forename><surname>Luká˘ S Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Cernock´ycernock´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Tomá˘ S Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving out-of-vocabulary name resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and language</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="128" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">OOV sensitive named-entity recognition in speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Parada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2085" to="2088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multitask learning in deep neural networks for improved phoneme recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasha</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Droppo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Incorporating speech recognition confidence into discriminative named entity recognition of speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cassia</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
