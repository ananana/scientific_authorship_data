<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:54+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rapid Adaptation of Neural Machine Translation to New Languages</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rapid Adaptation of Neural Machine Translation to New Languages</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="875" to="880"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>875</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. We propose methods based on starting with massively multilingual &quot;seed models&quot;, which can be trained ahead-of-time, and then continuing training on data related to the LRL. We contrast a number of strategies, leading to a novel, simple, yet effective method of &quot;similar-language reg-ularization&quot;, where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similar-language regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>When disaster strikes, news and social media are invaluable sources of information, allowing hu- manitarian organizations to rapidly mitigate crisis situations and save lives ( <ref type="bibr" target="#b23">Vieweg et al., 2010;</ref><ref type="bibr" target="#b15">Neubig et al., 2011;</ref><ref type="bibr" target="#b21">Starbird et al., 2012</ref>). However, language barriers looms large over these efforts, especially when disasters occur in parts of the world that use less common languages. In these cases, machine translation (MT) technology can be a valuable tool, with one widely-heralded suc- cess story being the deployment of Haitian Creole- to-English translation systems during the earth- quakes in Haiti ( <ref type="bibr" target="#b12">Lewis, 2010;</ref><ref type="bibr" target="#b14">Munro, 2010)</ref>.</p><p>However, data-driven MT systems, particularly neural machine translation <ref type="bibr">(NMT;</ref><ref type="bibr" target="#b8">Kalchbrenner and Blunsom (2013)</ref>; <ref type="bibr" target="#b1">Bahdanau et al. (2015)</ref>), re- quire large amounts of training data, and creating high-quality systems in low-resource languages (LRLs) is a difficult challenge where research ef- forts have just begun ( <ref type="bibr" target="#b4">Gu et al., 2018)</ref>. Another hurdle, which to our knowledge has not been cov- ered in previous research, is the time it takes to create such a system. In a crisis situation, time is of the essence, and systems that require days or weeks of training will not be desirable or even fea- sible.</p><p>In this paper we focus on the question: how can we create MT systems for new language pairs as accurately as possible, and as quickly as possible? To examine this question we propose NMT meth- ods at the intersection of cross-lingual transfer learning ( <ref type="bibr" target="#b24">Zoph et al., 2016)</ref> and multilingual train- ing ( <ref type="bibr" target="#b7">Johnson et al., 2016</ref>), two paradigms that, to our knowledge, have not been used together in pre- vious work. Our methods, laid out in §2 follow the process of training a seed model on a large num- ber of languages, then fine-tuning the model to im- prove its performance on the language of interest. We propose a novel method of similar-language regularization (SLR) where training data from a second similar languages is used to help prevent over-fitting to the small LRL dataset.</p><p>In the experiments in §3, we attempt to answer two questions: (1) Which method of creating mul- tilingual systems and adapting them to an LRL is the most effective way to increase accuracy? (2) How can we create the strongest system possible with a bare minimum of training time? The re- sults are sometimes surprising -we first find that a single monolithic model trained on 57 languages can achieve BLEU scores as high as 15.5 with no training data in the new source language whatso- ever. In addition, the proposed method starting with a universal model then fine-tuning with the SLR proves most effective, achieving gains of 1.7 BLEU points averaged over several language pairs compared to previous methods adapting to only the LRL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Training Paradigms</head><p>In this paper, we consider the setting where we have a source LRL of interest, and we want to translate into English. 2 All of our adaptation methods are based on first training on larger data including other languages, then fine-tuning the model to be specifically tailored to the LRL. We first discuss a few multilingual training paradigms from previous literature ( §2.1), then discuss our proposed adaptation methods ( §2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multilingual Modeling Methods</head><p>We use three varieties of multilingual training: Single-source modeling ("Sing.") is the first method, using only parallel data between the LRL of interest and English. This method is straightfor- ward and the resulting model will be most highly tailored to the final test language pair, but the method also has the obvious disadvantage that training data is very sparse. Bi-source modeling ("Bi") trains an MT system with two source languages: one LRL that we would like to translate from, and a second highly related high-resource language (HRL): the helper source language. 3 This method is inspired by <ref type="bibr" target="#b7">Johnson et al. (2016)</ref>, who examine multilingual translation models to/from English and two highly related languages such as Spanish/Portuguese or Japanese/Korean. The advantage of this method is that it allows the LRL to learn from a highly simi- lar helper, potentially increasing accuracy. All-source modeling ("All") trains not only on a couple source languages, but instead creates a uni- versal model on all of the languages that we have at our disposal. In our experiments ( §3.1) this en- tails training systems on 58 source languages, to our knowledge the largest reported in NMT exper- iments. <ref type="bibr">4</ref> This paradigm allows us to train a single 2 Translation into LRLs, is a challenging and interesting problem in it's own right, but beyond the scope of the paper.</p><p>3 "Related" could mean different things: typologically re- lated or having high lexical overlap. In our experiments our LRLs are all selected to have an helper that is highly similar in both aspects, but choosing an appropriate helper when this is not the case is an interesting problem for future work. <ref type="bibr">4</ref> In contrast to <ref type="bibr" target="#b4">Gu et al. (2018)</ref>, who train on 10 languages. <ref type="bibr" target="#b13">Malaviya et al. (2017)</ref>; <ref type="bibr" target="#b22">Tiedemann (2018)</ref> train NMT on over 1,000 languages, but only as a feature extractor for down- stream tasks; MT accuracy itself is not evaluated. model that has wide coverage of vocabulary and syntax of a large number of languages, but also has the drawback in that a single model must be able to express information about all the languages in the training set within its limited parameter bud- get. Thus, it is reasonable to expect that this model may achieve worse accuracy than a model created specifically to handle a particular source language.</p><p>In the following, we will consider adaptation methods that focus on tailoring a more general model (i.e. bi-source or universal) to a more spe- cific model (i.e. single-source or bi-source).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adaptation to New Languages</head><p>As noted in the introduction, there are two major requirements: the accuracy of the system is im- portant and the training time required from when we learn of a need for translation to when we can first start producing adequate results. Throughout the discussion, we will compare various adapta- tion paradigms with respect to these two aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Adaptation by Fine-tuning</head><p>Our first adaptation method, inspired by <ref type="bibr" target="#b24">Zoph et al. (2016)</ref> is based on fine-tuning to the source language of interest. Within our experiments, we will test this setting, but also make two distinctions between the types of adaptation: Seed Model Variety: <ref type="bibr" target="#b24">Zoph et al. (2016)</ref> per- formed experiments taking a bilingual system trained on a different language (e.g. French) and adapting it to a new LRL (e.g. Uzbek). We can also take universal model and adapt it to the new language, a setting that we examine (to our knowl- edge, for the first time) in this work. Warm vs. Cold Start: Another contrast is whether we have training data for the LRL of inter- est while training the original system, or whether we only receive training data after the original model has already been trained. We call the former warm start, and the latter cold start. Intuitively, we expect warm-start training to perform better, as having access to the LRL of interest during the training of the original model will ensure that it can handle the LRL to some extent. However, the cold-start scenario is also of interest: we may want to spend large amounts of time training a strong model, then quickly adapt to a new language that we have never seen before in our training data as data becomes available. For the cold-start models, we start with a model that is only trained on the HRL similar to the LRL (Bi − ), or a model trained  <ref type="table" target="#tab_1">dev  test  HRL  train  aze  5.94k  671  903  tur  182k  bel  4.51k  248  664  rus  208k  glg  10.0k  682 1,007  por  185k  slk  61.5k 2,271 2,445  ces  103k   Table 1</ref>: Data sizes in sentences for LRL/HRL pairs on all languages but the LRL (All − ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Similar-Language Regularization</head><p>One problem with adapting to a small amount of data in the target language is that it will be very easy for the model to over-fit to the small train- ing set. To alleviate this problem, we propose a method of similar language regularization: while training to adapt to the language of interest, we also add some data from another similar HRL that has sufficient resources to help prevent over- fitting. We do this in two ways: Corpus Concatenation: Simply concatenate the data from the two corpora, so that we have a small amount of data in the LRL, and a large amount of data in the similar HRL.</p><p>Balanced Sampling: Every time we select a mini- batch to do training, we either sample it from the LRL, or from the HRL according to a fixed ra- tio. We try different sampling strategies, includ- ing sampling with a 1-to-1 ratio, 1-to-2 ratio, and 1-to-4 ratio for the LRL and HRL respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>We perform experiments on the 58-language-to- English TED corpus ( , which is ideal for our purposes because it has a wide variety of languages over several language families, some high-resourced and some low-resourced. Like , we experiment with Azerbaijani (aze), Belarusian (bel), and Galician (glg) to En- glish, and also additionally add Slovak (slk), a slightly higher resourced language, for contrast. These languages are all paired with a similar HRL: Turkish (tur), Russian (rus), Portuguese (por), and Czech (ces) respectively. Data sizes are shown in <ref type="table">Table 1</ref>. Models are implemented using xnmt (Neu- big et al., 2018), commit 8173b1f, and start with the recipe for training on IWSLT TED 5 . The model consists of an attentional neural machine translation model ( <ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>), using bi-directional LSTM encoders, 128-dimensional <ref type="bibr">5</ref> Found in examples/stanford-iwslt/ word embeddings, 512-dimensional hidden states, and a standard LSTM-based decoder.</p><p>Following standard practice ( <ref type="bibr" target="#b20">Sennrich et al., 2016;</ref><ref type="bibr" target="#b2">Denkowski and Neubig, 2017)</ref>, we break low-frequency words into subwords using the sentencepiece toolkit. <ref type="bibr">6</ref> There are two alter- natives for creating subword units: jointly learning subwords over all source language, or separately learning subwords for each source language, then taking the union of all the subword vocabularies as the vocabulary for the multilingual model. Previ- ous work on multilingual training has preferred the former <ref type="bibr" target="#b17">(Nguyen and Chiang, 2017)</ref>, but in this pa- per we use the latter for two reasons: (1) because data in the LRL will not affect the subword units from the other languages, in the cold-start sce- nario we can postpone creation of subword units for the LRL until directly before we start train- ing on the LRL itself, and (2) we need not be concerned with the LRL being "overwhelmed" by the higher-resourced languages when calculating statistics used in the creation of subword units, be- cause all languages get an equal share. <ref type="bibr">7</ref> In the ex- periments, we use a subword vocabulary of 8,000 for each language.</p><p>We also compare with two additional baselines: phrase-based MT implemented in Moses, 8 and unsupervised NMT implemented in undreamt. <ref type="bibr">9</ref> Moses is trained on the bilingual data only (train- ing multilingually reduced average accuracy), and undreamt is trained on all monolingual data available for the LRL and English. <ref type="table" target="#tab_2">Table 2</ref> shows our main translation results, with warm-start scenarios in the upper half and cold- start scenarios in the lower half.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Does Multilingual Training Help?</head><p>To answer this question, we can compare the warm-start Sing., Bi, and All settings, and find that the an- swer is a resounding yes, gains of 7-13 BLEU points are obtained by going from single-source to bi-source or all-source training, corroborating previous work ( <ref type="bibr" target="#b4">Gu et al., 2018)</ref>. Bi-source mod- els tend to perform slightly better than all-source models, indicating that given identical parameter <ref type="bibr">6</ref> https://github.com/google/ sentencepiece, using the unigram training setting.</p><p>7 Preliminary experiments found both comparable: with scores of 20.1 and 19.4 for separate and joint respectively.</p><p>8 http://statmt.org/moses 9 https://github.com/artetxem/undreamt   capacity, training on a highly resourced language is effective. Comparing with the phrase-based baseline, as noted by <ref type="bibr" target="#b9">Koehn and Knowles (2017)</ref> NMT tends to underperform on low-resource set- tings when trained only on the data available for these languages. However, multilingual training of any variety quickly remedies this issue; all out- perform phrase-based handily.</p><note type="other">Strategy aze/tur bel/rus glg/por slk/ces Avg. Phrase-based 5.</note><note type="other">11.7 18.3 28.8 28.2 21.8 All→Bi 1-1 10.2 18.3 28.8 28.3 21.4 All→Bi 1-2 11.0 17.5 29.1 28.2 21.4 All→Bi 1-4 11.1 17.9 28.5 27.9 21.</note><p>More interestingly, examining the cold-start re- sults, we can see that even systems with no data in the target language are able to achieve non- trivial accuracies, up to 15.5 BLEU on glg-eng. Interestingly, in the cold-start scenario, the All − model bests the Bi − model, indicating that mas- sively multilingual training is more useful in this setting. In contrast, the unsupervised NMT model struggles, achieving a BLEU score of around 0 for all language pairs -this is because unsupervised NMT requires high-quality monolingual embed- dings from the same distribution, which can be trained easily in English, but are not available in the low-resource languages we are considering.</p><p>Does Adaptation Help? Regarding adaptation, we can first observe that regardless of the origi- nal model and method for adaptation, adaptation is helpful, particularly (and unsurprisingly) in the cold-start case. When adapting directly to only the target language ("→Sing."), adapting from the massively multilingual model performs better, in- dicating that information about all input languages is better than just a single language. Next, compar- ing with our proposed method of adding similar  <ref type="figure">Figure 1</ref>: Example of adaptation on the aze-eng and bel-eng development sets language regularization ("→Bi"), we can see that this helps significantly over adapting directly to the LRL, particularly in the cold-start case where we can observe gains of up to 1.7 BLEU points. Finally, in our data setting, corpus concatenation outperforms balanced sampling in both the cold- start and warm-start scenarios. How Can We Adapt Most Efficiently? Finally, we revisit adapting to new languages efficiently, with <ref type="figure">Figure 1</ref> showing BLEU vs. hours training for the aze/tur and bel/rus source language pairs (others were similar). We can see that in all cases the cold-start models (All − →) either outperform or are comparable in final accuracy to the from- scratch single-source and bi-source models. In ad- dition, all of the adapted models converge faster than the bi-source from-scratch trained models, in- dicating that adapting from seed models is a good strategy for rapid construction of MT systems in new languages. Comparing the cold-start adap- tation strategies, we can see that in general, the higher the density of target language training data, the faster the training converges to a solution, but the worse the final solution is. This suggests that there is a speed/accuracy tradeoff in the amount of similar language regularization we apply dur- ing fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>While adapting MT systems to new languages is a long-standing challenge ( <ref type="bibr" target="#b19">Schultz and Black, 2006;</ref><ref type="bibr" target="#b6">Jabaian et al., 2013)</ref>, multilingual NMT is highly promising in its ability to abstract across language boundaries <ref type="bibr" target="#b3">(Firat et al., 2016;</ref><ref type="bibr" target="#b5">Ha et al., 2016;</ref><ref type="bibr" target="#b7">Johnson et al., 2016)</ref>. Results on multi- lingual training for low-resource translation ( <ref type="bibr" target="#b4">Gu et al., 2018;</ref> further demonstrates this potential, although these works do not con- sider adaptation to new languages, the main focus of our work. Notably, we did not examine par- tial freezing of parameters, another method proven useful for cross-lingual adaptation ( <ref type="bibr" target="#b24">Zoph et al., 2016)</ref>; this is orthogonal to our multi-lingual train- ing approach but the two methods could poten- tially be combined. Finally, unsupervised NMT approaches ( <ref type="bibr" target="#b0">Artetxe et al., 2017;</ref><ref type="bibr" target="#b11">Lample et al., 2018</ref><ref type="bibr" target="#b10">Lample et al., , 2017</ref> require no parallel data, but rest on strong assumptions about high-quality comparable monolingual data. As we show, when this assump- tion breaks down these methods fail to function, while our cold-start methods achieve non-trivial accuracies even with no monolingual data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper examined methods to rapidly adapt MT systems to new languages by fine-tuning. In both warm-start and cold-start scenarios, the best re- sults were obtained by adapting a pre-trained uni- versal model to the low-resource language while regularizing with similar languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>3</head><label>3</label><figDesc></figDesc><table>Cold Start 

Bi − 
3.8 
2.5 
8.6 
5.4 
5.1 
All − 
3.7 
3.5 
15.5 
7.3 
7.5 
Bi − →Sing. 
8.7 
11.8 
25.4 
26.8 18.2 
All − →Sing. 
8.8 
15.3 
26.5 
27.6 19.5 
All − →Bi 
10.7 
17.4 
28.4 
28.0 21.2 
All − →Bi 1-1 
10.5 
16.0 
28.0 
28.2 20.7 
All − →Bi 1-2 
10.7 
17.1 
28.3 
27.9 21.0 
All − →Bi 1-4 
11.0 
17.4 
28.4 
27.6 21.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>BLEU for single-source (Sing.), bi-source 
(Bi), and all-source universal (All) models, with 
adapted counterparts. 1-1, 1-2, 1-4 indicate balanced 
sampling from  §2.2. Bold indicates highest score. 

</table></figure>

			<note place="foot" n="1"> Code to reproduce experiments at https://github. com/neubig/rapid-adaptation</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors thank Jaime Carbonell, Xinyi Wang, Rebecca Knowles, Arya McCarthy, and anony-mous reviewers for their constructive comments on this paper. This work is sponsored by Defense Advanced Research Projects Agency Information Innovation Office (I2O). Program: Low Resource Languages for Emergent Incidents (LORELEI). Issued by DARPA/I2O under Contract No. HR0011-15-C0114. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official poli-cies, either expressed or implied, of the U.S. Gov-ernment. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Unsupervised neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11041</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stronger baselines for trustable results in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WNMT</title>
		<meeting>WNMT</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-way, multilingual neural machine translation with a shared attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="866" to="875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Universal neural machine translation for extremely low resource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Toward multilingual neural machine translation with universal encoder and decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Le</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Waibel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04798</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Comparison and combination of lightly supervised approaches for language portability of a spoken language understanding system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bassam</forename><surname>Jabaian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrice</forename><surname>Lefevre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="636" to="648" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Six challenges for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Knowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WNMT</title>
		<meeting>WNMT</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Phrase-based &amp; neural unsupervised machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07755</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Haitian Creole: how to build and ship an MT engine from scratch in 4 days, 17 hours, &amp; 30 minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EAMT</title>
		<meeting>EAMT</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning language representations for typology prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Crowdsourced translation for emergency response in Haiti: the global collaboration of local knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Munro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AMTA Workshop on Collaborative Crowdsourcing for Translation</title>
		<meeting>AMTA Workshop on Collaborative Crowdsourcing for Translation</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Safety information mining-what can NLP do in a disaster</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichiroh</forename><surname>Matsubayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masato</forename><surname>Hagiwara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koji</forename><surname>Murakami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNLP</title>
		<meeting>IJCNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="965" to="973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">XNMT: The extensible neural machine translation toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarguna</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devendra</forename><surname>Singh Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachid</forename><surname>Riad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AMTA</title>
		<meeting>AMTA<address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Transfer learning across low-resource, related languages for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Toan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">When and why are pre-trained word embeddings useful for neural machine translation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devendra</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Felix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL<address><addrLine>New Orleans, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Sarguna Padmanabhan, and Graham Neubig</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Challenges with rapid adaptation of speech translation systems to new language pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning from the crowd: Collaborative filtering techniques for identifying on-the-ground Twitterers during mass disruptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Starbird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Muzny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leysia</forename><surname>Palen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISCRAM</title>
		<meeting>ISCRAM</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Emerging language spaces learned from massively multilingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00273</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microblogging during two natural hazards events: what Twitter may contribute to situational awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Vieweg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><forename type="middle">L</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Starbird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leysia</forename><surname>Palen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
		<meeting>CHI</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1079" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Transfer learning for low-resource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1568" to="1575" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
