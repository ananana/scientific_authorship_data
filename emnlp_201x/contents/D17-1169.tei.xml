<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjarke</forename><surname>Felbo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Media Lab</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Mislove</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iyad</forename><surname>Rahwan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Media Lab</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sune</forename><surname>Lehmann</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">DTU Compute</orgName>
								<orgName type="institution" key="instit2">Technical University of Denmark</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1615" to="1625"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>NLP tasks are often limited by scarcity of manually annotated data. In social media sentiment analysis and related tasks, researchers have therefore used binarized emoticons and specific hashtags as forms of distant supervision. Our paper shows that by extending the distant supervision to a more diverse set of noisy labels, the models can learn richer representations. Through emoji prediction on a dataset of 1246 million tweets containing one of 64 common emojis we obtain state-of-the-art performance on 8 benchmark datasets within emotion, sentiment and sarcasm detection using a single pretrained model. Our analyses confirm that the diversity of our emotional labels yield a performance improvement over previous distant supervision approaches.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A variety of NLP tasks are limited by scarcity of manually annotated data. Therefore, co-occurring emotional expressions have been used for dis- tant supervision in social media sentiment anal- ysis and related tasks to make the models learn useful text representations before modeling these tasks directly. For instance, the state-of-the-art ap- proaches within sentiment analysis of social me- dia data use positive/negative emoticons for train- ing their models ( <ref type="bibr" target="#b5">Deriu et al., 2016;</ref><ref type="bibr" target="#b31">Tang et al., 2014</ref>). Similarly, hashtags such as #anger, #joy, #happytweet, #ugh, #yuck and #fml have in pre- vious research been mapped into emotional cate- gories for emotion analysis <ref type="bibr">(Mohammad, 2012)</ref>.</p><p>Distant supervision on noisy labels often en- ables a model to obtain better performance on the target task. In this paper, we show that extend- ing the distant supervision to a more diverse set of noisy labels enables the models to learn richer rep- resentations of emotional content in text, thereby obtaining better performance on benchmarks for detecting sentiment, emotions and sarcasm. We show that the learned representation of a single pretrained model generalizes across 5 domains. Emojis are not always a direct labeling of emo- tional content. For instance, a positive emoji may serve to disambiguate an ambiguous sentence or to complement an otherwise relatively negative text. <ref type="bibr" target="#b17">Kunneman et al. (2014)</ref> discuss a similar duality in the use of emotional hashtags such as #nice and #lame. Nevertheless, our work shows that emo- jis can be used to classify the emotional content of texts accurately in many cases. For instance, our DeepMoji model captures varied usages of the word 'love' as well as slang such as 'this is the shit' being a positive statement (see <ref type="table" target="#tab_0">Table 1</ref>). We provide an online demo at deepmoji.mit.edu to al- low others to explore the predictions of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions</head><p>We show how millions of read- ily available emoji occurrences on Twitter can be used to pretrain models to learn a richer emotional representation than traditionally obtained through distant supervision. We transfer this knowledge to the target tasks using a new layer-wise fine-tuning method, obtaining improvements over the state- of-the-art within a range of tasks: emotion, sar- casm and sentiment detection. We present multi- ple analyses on the effect of pretraining, including results that show that the diversity of our emoji set is important for the transfer learning potential of our model. Our pretrained DeepMoji model is re- leased with the hope that other researchers can use it for various NLP tasks 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Using emotional expressions as noisy labels in text to counter scarcity of labels is not a new idea <ref type="bibr" target="#b23">(Read, 2005;</ref><ref type="bibr" target="#b10">Go et al., 2009)</ref>. Originally, bi- narized emoticons were used as noisy labels, but later also hashtags and emojis have been used. To our knowledge, previous research has always manually specified which emotional category each emotional expression belong to. Prior work has used theories of emotion such as Ekman's six basic emotions and Plutchik's eight basic emo- tions <ref type="bibr">(Mohammad, 2012;</ref><ref type="bibr" target="#b30">Suttles and Ide, 2013)</ref>.</p><p>Such manual categorization requires an under- standing of the emotional content of each expres- sion, which is difficult and time-consuming for sophisticated combinations of emotional content. Moreover, any manual selection and categoriza- tion is prone to misinterpretations and may omit important details regarding usage. In contrast, our approach requires no prior knowledge of the cor- pus and can capture diverse usage of 64 types of emojis (see <ref type="table" target="#tab_0">Table 1</ref> for examples and <ref type="figure" target="#fig_1">Figure 3</ref> for how the model implicitly groups emojis).</p><p>Another way of automatically interpreting the emotional content of an emoji is to learn emoji embeddings from the words describing the emoji- semantics in official emoji tables <ref type="bibr" target="#b7">(Eisner et al., 2016)</ref>. This approach, in our context, suffers from two severe limitations: a) It requires emojis at test time while there are many domains with limited or no usage of emojis. b) The tables do not cap- ture the dynamics of emoji usage, i.e., drift in an emoji's intended meaning over time.</p><p>Knowledge can be transferred from the emoji dataset to the target task in many different ways. In particular, multitask learning with simultaneous <ref type="bibr">1</ref>  Softmax 1 x C <ref type="figure">Figure 1</ref>: Illustration of the DeepMoji model with S being text length and C the number of classes. training on multiple datasets has shown promis- ing results <ref type="bibr" target="#b4">(Collobert and Weston, 2008)</ref>. How- ever, multitask learning requires access to the emoji dataset whenever the classifier needs to be tuned for a new target task. Requiring access to the dataset is problematic in terms of violat- ing data access regulations. There are also is- sues from a data storage perspective as the dataset used for this research contains hundreds of mil- lions of tweets (see <ref type="table" target="#tab_2">Table 2</ref>). Instead we use trans- fer learning ( <ref type="bibr" target="#b1">Bengio et al., 2012)</ref> as described in §3.3, which does not require access to the original dataset, but only the pretrained classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pretraining</head><p>In many cases, emojis serve as a proxy for the emotional contents of a text. Therefore, pretrain- ing on the classification task of predicting which emoji were initially part of a text can improve per- formance on the target task (see §5.3 for an anal- ysis of why our pretraining helps). Social media contains large amounts of short texts with emojis that can be utilized as noisy labels for pretraining. Here, we use data from Twitter from January 1st 2013 to June 1st 2017, but any dataset with emoji occurrences could be used.</p><p>Only English tweets without URL's are used for the pretraining dataset. Our hypothesis is that the content obtained from the URL is likely to be im- portant for understanding the emotional content of the text in the tweet. Therefore, we expect emo- jis associated with these tweets to be noiser labels than for tweets without URLs, and the tweets with URLs are thus removed.</p><p>Proper tokenization is important for generaliza- tion. All tweets are tokenized on a word-by-word basis. Words with 2 or more repeated characters are shortened to the same token (e.g. 'loool' and 'looooool' are tokenized such that they are treated the same). Similarly, we use a special token for all URLs (only relevant for benchmark datasets), user mentions (e.g. '@acl2017' and '@emnlp2017' are thus treated the same) and numbers. To be in- cluded in the training set the tweet must contain at least 1 token that is not a punctuation symbol, emoji or special token 2 .</p><p>Many tweets contain multiple repetitions of the same emoji or multiple different emojis. In the training data, we address this in the following way. For each unique emoji type, we save a separate tweet for the pretraining with that emoji type as the label. We only save a single tweet for the pretrain- ing per unique emoji type regardless of the number of emojis associated with the tweet. This data pre- processing allows the pretraining task to capture that multiple types of emotional content are asso- ciated with the tweet while making our pretraining task a single-label classification instead of a more complicated multi-label classification.</p><p>To ensure that the pretraining encourages the models to learn a rich understanding of emotional content in text rather than only emotional content associated with the most used emojis, we create a balanced pretraining dataset. The pretraining data is split into a training, validation and test set, where the validation and test set is randomly sam- pled in such a way that each emoji is equally repre- sented. The remaining data is upsampled to create a balanced training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model</head><p>With the millions of emoji occurrences available, we can train very expressive classifiers with lim- ited risk of overfitting. We use a variant of the Long Short-Term Memory (LSTM) model that has been successful at many NLP tasks <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b29">Sutskever et al., 2014</ref>). Our DeepMoji model uses an embedding layer of 256 dimensions to project each word into a vector space. A hyperbolic tangent activation function is used to enforce a constraint of each embedding di- mension being within <ref type="bibr">[−1, 1]</ref>. To capture the con-text of each word we use two bidirectional LSTM layers with 1024 hidden units in each (512 in each direction). Finally, an attention layer that take all of these layers as input using skip-connections is used (see <ref type="figure">Figure 1</ref> for an illustration).</p><p>The attention mechanism lets the model decide the importance of each word for the prediction task by weighing them when constructing the represen- tation of the text. For instance, a word such as 'amazing' is likely to be very informative of the emotional meaning of a text and it should thus be treated accordingly. We use a simple approach inspired by ( <ref type="bibr" target="#b0">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b38">Yang et al., 2016</ref>) with a single parameter pr. input channel:</p><formula xml:id="formula_0">e t = h t w a a t = exp(e t ) T i=1 exp(e i ) v = T i=1 a i h i</formula><p>Here h t is the representation of the word at time step t and w a is the weight matrix for the atten- tion layer. The attention importance scores for each time step, a t , are obtained by multiplying the representations with the weight matrix and then normalizing to construct a probability distribution over the words. Lastly, the representation vector for the text, v, is found by a weighted summation over all the time steps using the attention impor- tance scores as weights. This representation vec- tor obtained from the attention layer is a high-level encoding of the entire text, which is used as input to the final Softmax layer for classification. We find that adding the attention mechanism and skip- connections improves the model's capabilities for transfer learning (see §5.2 for more details).</p><p>The only regularization used for the pretrain- ing task is a L2 regularization of 1E−6 on the embedding weights. For the finetuning additional regularization is applied (see §4.2). Our model is implemented using Theano (Theano Development Team, 2016) and we make an easy-to-use version available that uses Keras (Chollet et al., 2015).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Transfer learning</head><p>Our pretrained model can be fine-tuned to the tar- get task in multiple ways with some approaches 'freezing' layers by disabling parameters updates to prevent overfitting. One common approach is to use the network as a feature extractor <ref type="bibr" target="#b6">(Donahue et al., 2014)</ref>, where all layers in the model are frozen when fine-tuning on the target task except the last layer (hereafter referred to as the 'last' ap- proach). Alternatively, another common approach is to use the pretrained model as an initializa- tion ( <ref type="bibr" target="#b8">Erhan et al., 2010)</ref>, where the full model is unfrozen (hereafter referred to as 'full').</p><p>We propose a new simple transfer learning ap- proach, 'chain-thaw', that sequentially unfreezes and fine-tunes a single layer at a time. This ap- proach increases accuracy on the target task at the expense of extra computational power needed for the fine-tuning. By training each layer separately the model is able to adjust the individual patterns across the network with a reduced risk of overfit- ting. The sequential fine-tuning seems to have a regularizing effect similar to what has been exam- ined with layer-wise training in the context of un- supervised learning <ref type="bibr" target="#b8">(Erhan et al., 2010</ref>).</p><p>More specifically, the chain-thaw approach first fine-tunes any new layers (often only a Softmax layer) to the target task until convergence on a validation set. Then the approach fine-tunes each layer individually starting from the first layer in the network. Lastly, the entire model is trained with all layers. Each time the model converges as measured on the validation set, the weights are reloaded to the best setting, thereby prevent- ing overfitting in a similar manner to early stop- ping <ref type="bibr" target="#b26">(Sjöberg and Ljung, 1995)</ref>. This process is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. Note how only perform- ing step a) in the figure is identical to the 'last' approach, where the existing network is used as a feature extractor. Similarly, only doing step d) is identical to the 'full' approach, where the pre- trained weights are used as an initialization for a fully trainable network. Although the chain-thaw procedure may seem extensive it is easily imple- mented with only a few lines of code. Similarly, the additional time spent on fine-tuning is limited when the target task uses GPUs on small datasets of manually annotated data as is often the case.</p><p>A benefit of the chain-thaw approach is the abil- ity to expand the vocabulary to new domains with little risk of overfitting. For a given dataset up to 10000 new words from the training set are added to the vocabulary. §5.3 contains analysis on the added word coverage gained from this approach.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Emoji prediction</head><p>We use a raw dataset of 56.6 billion tweets, which is then filtered to 1.2 billion relevant tweets (see details in §3.1). In the pretraining dataset a copy of a single tweet is stored once for each unique emoji, resulting in a dataset consisting of 1.6 bil- lion tweets. <ref type="table" target="#tab_2">Table 2</ref> shows the distribution of tweets across different emoji types. To evaluate performance on the pretraining task a validation set and a test set both containing 640K tweets (10K of each emoji type) are used. The remain- ing tweets are used for the training set, which is balanced using upsampling. The performance of the DeepMoji model is evaluated on the pretraining task with the results shown in <ref type="table" target="#tab_3">Table 3</ref>. Both top 1 and top 5 accuracy is used for the evaluation as the emoji labels are noisy with multiple emojis being potentially cor- rect for any given sentence. For comparison we also train a version of our DeepMoji model with smaller LSTM layers and a bag-of-words classi- fier, fastText, that has recently shown competitive results ( <ref type="bibr" target="#b15">Joulin et al., 2016)</ref>. We use 256 dimen- sions for this fastText classifier, thereby making it almost identical to only using the embedding layer from the DeepMoji model. The difference in top 5 accuracy between the fastText classifier (36.2%) and the largest DeepMoji model (43.8%) under- lines the difficulty of the emoji prediction task. As the two classifiers only differ in that the DeepMoji model has LSTM layers and an attention layer be- tween the embedding and Softmax layer, this dif- ference in accuracy demonstrates the importance of capturing the context of each word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Benchmarking</head><p>We benchmark our method on 3 different NLP tasks using 8 datasets across 5 domains. To make for a fair comparison, we compare DeepMoji to other methods that also utilize external data sources in addition to the benchmark dataset. An averaged F1-measure across classes is used for evaluation in emotion analysis and sarcasm detec- tion as these consist of unbalanced datasets while sentiment datasets are evaluated using accuracy.</p><p>An issue with many of the benchmark datasets is data scarcity, which is particularly problem- atic within emotion analysis. Many recent pa- pers proposing new methods for emotion analysis such as <ref type="bibr" target="#b27">(Staiano and Guerini, 2014</ref>) only evaluate performance on a single benchmark dataset, Se- mEval 2007 Task 14, that contains 1250 observa- tions. Recently, criticism has been raised concern- ing the use of correlation with continuous ratings as a measure <ref type="bibr" target="#b2">(Buechel and Hahn, 2016)</ref>, making only the somewhat limited binary evaluation pos- sible. We only evaluate the emotions {Fear, Joy, Sadness} as the remaining emotions occur in less than 5% of the observations.</p><p>To fully evaluate our method on emotion analy- sis against the current methods we thus make use of two other datasets: A dataset of emotions in tweets related to the Olympic Games created by Sintsova et al. that we convert to a single-label classification task and a dataset of self-reported emotional experiences created by a large group of psychologists ( <ref type="bibr" target="#b36">Wallbott and Scherer, 1986)</ref>. See the supplementary material for details on the datasets and the preprocessing. As these two datasets do not have prior evaluations, we eval- uate against a state-of-the-art approach, which is based on a valence-arousal-dominance frame- work ( <ref type="bibr" target="#b2">Buechel and Hahn, 2016)</ref>. The scores ex- tracted using this approach are mapped to the classes in the datasets using a logistic regres- sion with parameter optimization using cross- validation. We release our preprocessing code and hope that these 2 two datasets will be used for fu- ture benchmarking within emotion analysis.</p><p>We evaluate sentiment analysis performance on three benchmark datasets. These small datasets are chosen to emphasize the importance of the transfer learning ability of the evaluated models. Two of the datasets are from SentiStrength (Thel- wall et al., 2010), SS-Twitter and SS-Youtube, and follow the relabeling described in ( <ref type="bibr" target="#b24">Saif et al., 2013</ref>) to make the labels binary. The third dataset is from SemEval 2016 Task4A ( <ref type="bibr" target="#b20">Nakov et al., 2016)</ref>. Due to tweets being deleted from Twitter, the SemEval dataset suffers from data decay, mak- ing it difficult to compare results across papers. At the time of writing, roughly 15% of the training dataset for SemEval 2016 Task 4A was impossible to obtain. We choose not to use review datasets for sentiment benchmarking as these datasets contain so many words pr. observation that even bag-of- words classifiers and unsupervised approaches can obtain a high accuracy ( <ref type="bibr" target="#b15">Joulin et al., 2016;</ref><ref type="bibr" target="#b22">Radford et al., 2017</ref>).</p><p>The current state of the art for sentiment analy- sis on social media (and winner of SemEval 2016 Task 4A) uses an ensemble of convolutional neu- ral networks that are pretrained on a private dataset of tweets with emoticons, making it difficult to replicate ( <ref type="bibr" target="#b5">Deriu et al., 2016)</ref>. Instead we pretrain a model with the hyperparameters of the largest model in their ensemble on the positive/negative emoticon dataset from <ref type="bibr" target="#b10">Go et al. (2009)</ref>. Using this pretraining as an initialization we finetune the model on the target tasks using early stop- ping on a validation set to determine the amount of training. We also implemented the Sentiment- Specific Word Embedding (SSWE) using the em- beddings available on the authors' website ( <ref type="bibr" target="#b31">Tang et al., 2014</ref>), but found that it performed worse <ref type="table">Table 4</ref>: Description of benchmark datasets. Datasets without pre-existing training/test splits are split by us (with splits publicly available). Data used for hyperparameter tuning is taken from the training set.  than the pretrained convolutional neural network. These results are therefore excluded. For sarcasm detection we use the sarcasm dataset version 1 and 2 from the Internet Argu- ment Corpus ( <ref type="bibr" target="#b35">Walker et al., 2012)</ref>. Note that results presented on these benchmarks in e.g. <ref type="bibr" target="#b21">Oraby et al. (2016)</ref> are not directly comparable as only a subset of the data is available online. 3 A state-of-the-art baseline is found by modeling the embedding-based features from <ref type="bibr" target="#b14">Joshi et al. (2016)</ref> alongside unigrams, bigrams and trigrams with an SVM. GoogleNews word2vec embed- dings ( <ref type="bibr" target="#b18">Mikolov et al., 2013</ref>) are used for comput- ing the embedding-based features. A hyperparam- eter search for regularization parameters is carried out using cross-validation. Note that the sarcasm dataset version 2 contains both a quoted text and a sarcastic response, but to keep the models identi- cal across the datasets only the response is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identifier</head><p>For training we use the Adam opti- mizer ( <ref type="bibr" target="#b16">Kingma and Ba, 2015</ref>) with gradient clipping of the norm to 1. Learning rate is set to 1E−3 for training of all new layers and 1E−4 for finetuning any pretrained layers. To prevent overfitting on the small datasets, 10% of the channels across all words in the embedding layer are dropped out during training. Unlike e.g. ( <ref type="bibr" target="#b9">Gal and Ghahramani, 2016)</ref> we do not drop out entire words in the input as some of our datasets contain observations with so few words that it could change the meaning of the text. In addition to the embedding dropout, L2 regularization for the embedding weights is used and 50% dropout is applied to the penultimate layer. <ref type="table" target="#tab_5">Table 5</ref> shows that the DeepMoji model out- performs the state of the art across all benchmark datasets and that our new 'chain-thaw' approach consistently yields the highest performance for the transfer learning, albeit often only slightly better or equal to the 'last' approach. Results are aver- aged across 5 runs to reduce the variance. We test the statistical significance of our results by com- paring the performance of DeepMoji (chain-thaw) vs. the state of the art. Bootstrap testing with 10000 samples is used. On all datasets are our re- sults statistically significantly better than the state of the art with p &lt; 0.001.</p><p>Our model is able to out-perform the state-of-the-art on datasets that originate from domains that differ substantially from the tweets on which it was pretrained. A key difference between the pre- training dataset and the benchmark datasets is the length of the observations. The average number of tokens pr. tweet in the pretraining dataset is 11, whereas e.g. the board posts from the Internet Ar- gument Corpus version 1 ( <ref type="bibr" target="#b21">Oraby et al., 2016)</ref> has an average of 66 tokens with some observations being much longer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Model Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Importance of emoji diversity</head><p>One of the major differences between this work compared to previous papers using distant super- vision is the diversity of the noisy labels used (see §2). For instance, both <ref type="bibr" target="#b5">Deriu et al. (2016)</ref> and <ref type="bibr" target="#b31">Tang et al. (2014)</ref> only used positive and negative emoticons as noisy labels. Other instances of pre- vious work have used slightly more nuanced sets of noisy labels (see §2), but to our knowledge our set of noisy labels is the most diverse yet. To an- alyze the effect of using a diverse emoji set we create a subset of our pretraining data containing tweets with one of 8 emojis that are similar to the positive/negative emoticons used by <ref type="bibr" target="#b31">Tang et al. (2014)</ref> and <ref type="bibr" target="#b13">Hu et al. (2013)</ref> (the set of emoticons and corresponding emojis are available in the sup- plemental material). As the dataset based on this reduced set of emojis contains 433M tweets, any difference in performance on benchmark datasets is likely linked to the diversity of labels rather than differences in dataset sizes.</p><p>We train our DeepMoji model to predict whether the tweets contain a positive or negative emoji and evaluate this pretrained model across the benchmark datasets. We refer to the model trained on the subset of emojis as DeepMoji- PosNeg (as opposed to DeepMoji). To test the emotional representations learned by the two pre- trained models the 'last' transfer learning ap- proach is used for the comparison, thereby only allowing the models to map already learned fea- tures to classes in the target dataset. <ref type="table">Table 6</ref> shows that DeepMoji-PosNeg yields lower performance compared to DeepMoji across all 8 benchmarks, thereby showing that the diversity of our emoji types encourage the model to learn a richer repre- sentation of emotional content in text that is more useful for transfer learning.</p><p>Many of the emojis carry similar emotional <ref type="table">Table 6</ref>: Benchmarks using a smaller emoji set (Pos/Neg emojis) or a classic architecture (stan- dard LSTM). Results for DeepMoji from <ref type="table" target="#tab_5">Table 5</ref> are added for convenience. Evaluation metrics are as in content, but have subtle differences in usage that our model is able to capture. Through hierar- chical clustering on the correlation matrix of the DeepMoji model's predictions on the test set we can see that the model captures many similarities that one would intuitively expect (see <ref type="figure" target="#fig_1">Figure 3</ref>). For instance, the model groups emojis into overall categories associated with e.g. negativity, positiv- ity or love. Similarly, the model learns to differen- tiate within these categories, mapping sad emojis in one subcategory of negativity, annoyed in an- other subcategory and angry in a third one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Model architecture</head><p>Our DeepMoji model architecture as described in §3.2 use an attention mechanism and skip- connections to ease the transfer of the learned rep- resentation to new domains and tasks. Here we compare the DeepMoji model architecture to that of a standard 2-layer LSTM, both compared using the 'last' transfer learning approach. We use the same regularization and training parameters.</p><p>As seen in <ref type="table">Table 6</ref> the DeepMoji model per- forms better than a standard 2-layer LSTM across all benchmark datasets. The two architectures per- formed equally on the pretraining task, suggesting that while the DeepMoji model architecture is in- deed better for transfer learning, it may not neces- sarily be better for single supervised classification task with ample available data.</p><p>A reasonable conjecture is that the improved transfer learning performance is due to two fac- tors: a) the attention mechanism with skip- connections provide easy access to learned low- level features for any time step, making it easy to use this information if needed for a new task b) the improved gradient-flow from the output layer to the early layers in the network due to skip- connections <ref type="bibr" target="#b11">(Graves, 2013</ref>) is important when ad- justing parameters in early layers as part of trans- fer learning to small datasets. Detailed analysis of whether these factors actually explain why our ar- chitecture outperform a standard 2-layer LSTM is left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analyzing the effect of pretraining</head><p>Performance on the target task benefits strongly from pretraining as shown in <ref type="table" target="#tab_5">Table 5</ref> by compar- ing DeepMoji (new) to DeepMoji (chain-thaw). In this section we experimentally decompose the benefit of pretraining into 2 effects: word coverage and phrase coverage. These two effects help regu- larize the model by preventing overfitting (see the supplementary details for an visualization of the effect of this regularization).</p><p>There are numerous ways to express a specific sentiment, emotion or sarcastic comment. Conse- quently, the test set may contain specific language use not present in the training set. The pretraining helps the target task models attend to low-support evidence by having previously observed similar usage in the pretraining dataset. We first exam- ine this effect by measuring the improvement in word coverage on the test set when using the pre- training with word coverage being defined as the % of words in the test dataset seen in the train- ing/pretraining dataset (see <ref type="table" target="#tab_7">Table 7</ref>). An impor- tant reason why the 'chain-thaw' approach outper- forms other transfer learning approaches is can be used to tune the embedding layer with limited risk of overfitting. <ref type="table" target="#tab_7">Table 7</ref> shows the increased word coverage from adding new words to the vocabu- lary as part of that tuning.</p><p>Note that word coverage can be a misleading metric in this context as for many of these small datasets a word will often occur only once in the training set. In contrast, all of the words in the pretraining vocabulary are present in thousands (if not millions) of observations in the emoji pretrain- ing dataset thus making it possible for the model to learn a good representation of the emotional and semantic meaning. The added benefit of pre- training for learning word representations there- fore likely extends beyond the differences seen in <ref type="table" target="#tab_7">Table 7</ref>. To examine the importance of capturing phrases and the context of each word, we evaluate the ac- curacy on the SS-Youtube dataset using a fastText classifier pretrained on the same emoji dataset as our DeepMoji model. This fastText classifier is al- most identical to only using the embedding layer from the DeepMoji model. We evaluate the rep- resentations learned by fine-tuning the models as feature extractors (i.e. using the 'last' transfer learning approach). The fastText model achieves an accuracy of 63% as compared to 93% for our DeepMoji model, thereby emphasizing the im- portance of phrase coverage. One concept that the LSTM layers likely learn is negation, which is known to be important for sentiment analy- sis ( <ref type="bibr" target="#b37">Wiegand et al., 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparing with human-level agreement</head><p>To understand how well our DeepMoji classi- fier performs compared to humans, we created a new dataset of random tweets annotated for senti- ment. Each tweet was annotated by a minimum of 10 English-speaking Amazon Mechanical Turkers (MTurk's) living in USA. Tweets were rated on a scale from 1 to 9 with a 'Do not know' option, and guidelines regarding how to rate the tweets were provided to the human raters. The tweets were selected to contain only English text, no men- tions and no URL's to make it possible to rate them without any additional contextual informa- tion. Tweets where more than half of the eval- uators chose 'Do not know' were removed (98 tweets).</p><p>For each tweet, we select a MTurk rating ran- dom to be the 'human evaluation', and average over the remaining nine MTurk ratings are av- eraged to form the ground truth. The 'senti- ment label' for a given tweet is thus defined as the overall consensus among raters (excluding the randomly-selected 'human evaluation' rating). To ensure that the label categories are clearly sep- arated, we removed neutral tweets in the inter- val [4.5, 5.5] (roughly 29% of the tweets). The remaining dataset consists of 7 347 tweets. Of these tweets, 5000 are used for training/validation and the remaining are used as the test set. Our DeepMoji model is trained using the chain-thaw transfer learning approach. <ref type="table" target="#tab_8">Table 8</ref> shows that the agreement of the random MTurk rater is 76.1%, meaning that the randomly selected rater will agree with the average of the nine other MTurk-ratings of the tweet's polarity 76.1% of the time. Our DeepMoji model achieves 82.4% agreement, which means it is better at cap- turing the average human sentiment-rating than a single MTurk rater. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have shown how the millions of texts on so- cial media with emojis can be used for pretrain- ing models, thereby allowing them to learn repre- sentations of emotional content in texts. Through comparison with an identical model pretrained on a subset of emojis, we find that the diversity of our emoji set is important for the performance of our method. We release our pretrained DeepMoji model with the hope that other researchers will find good use of them for various emotion-related NLP tasks 4 .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the chain-thaw transfer learning approach, where each layer is fine-tuned separately. Layers covered with a blue rectangle are frozen. Step a) tunes any new layers, b) then tunes the 1st layer and c) the next layer until all layers have been fine-tuned individually. Lastly, in step d) all layers are fine-tuned together.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Hierarchical clustering of the DeepMoji model's predictions across categories on the test set. The dendrogram shows how the model learns to group emojis into overall categories and subcategories based on emotional content. The y-axis is the distance on the correlation matrix of the model's predictions measured using average linkage. More details are available in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Example sentences scored by our model. For each text the top five most likely emojis are shown with the model's probability estimates.</figDesc><table>I love mom's cooking 

49.1% 8.8% 
3.1% 
3.0% 
2.9% 

I love how you never reply back.. 

14.0% 8.3% 
6.3% 
5.4% 
5.1% 

I love cruising with my homies 

34.0% 6.6% 
5.7% 
4.1% 
3.8% 

I love messing with yo mind!! 

17.2% 11.8% 8.0% 
6.4% 
5.3% 

I love you and now you're just gone.. 

39.1% 11.0% 7.3% 
5.3% 
4.5% 

This is shit 

7.0% 
6.4% 
6.0% 
6.0% 
5.8% 

This is the shit 

10.9% 9.7% 
6.5% 
5.7% 
4.8% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The number of tweets in the pretraining 
dataset associated with each emoji in millions. 

233.7 
82.2 
79.5 
78.1 
60.8 
54.7 
54.6 
51.7 
50.5 
44.0 
39.5 
39.1 
34.8 
34.4 
32.1 
28.1 

24.8 
23.4 
21.6 
21.0 
20.5 
20.3 
19.9 
19.6 
18.9 
17.5 
17.0 
16.9 
16.1 
15.3 
15.2 
15.0 

14.9 
14.3 
14.2 
14.2 
12.9 
12.4 
12.0 
12.0 
11.7 
11.7 
11.3 
11.2 
11.1 
11.0 
11.0 
10.8 

10.2 
9.6 
9.5 
9.3 
9.2 
8.9 
8.7 
8.6 
8.1 
6.3 
6.0 
5.7 
5.6 
5.5 
5.4 
5.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 : Accuracy of classifiers on the emoji prediction task. d refers to the dimensionality of each LSTM layer. Parameters are in millions.</head><label>3</label><figDesc></figDesc><table>Params 
Top 1 
Top 5 

Random 
− 
1.6% 
7.8% 
fasttext 
12.8 
12.8% 36.2% 
DeepMoji (d = 512) 15.5 
16.7% 43.3% 
DeepMoji (d = 1024) 22.4 
17.0% 43.8% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison across benchmark datasets. Reported values are averages across five runs. Varia-
tions refer to transfer learning approaches in  §3.3 with 'new' being a model trained without pretraining. 

Dataset 
Measure 
State of the art 
DeepMoji 
(new) 
DeepMoji 
(full) 
DeepMoji 
(last) 
DeepMoji 
(chain-thaw) 

SE0714 
F1 
.34 [Buechel] 
.21 
.31 
.36 
.37 
Olympic 
F1 
.50 [Buechel] 
.43 
.50 
.61 
.61 
PsychExp 
F1 
.45 [Buechel] 
.32 
.42 
.56 
.57 

SS-Twitter 
Acc 
.82 [Deriu] 
.62 
.85 
.87 
.88 
SS-Youtube 
Acc 
.86 [Deriu] 
.75 
.88 
.92 
.93 
SE1604 
Acc 
.51 [Deriu] 
.51 
.54 
.58 
.58 

SCv1 
F1 
.63 [Joshi] 
.67 
.65 
.68 
.69 
SCv2-GEN 
F1 
.72 [Joshi] 
.71 
.71 
.74 
.75 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 5 . Reported values are the averages across five runs.</head><label>5</label><figDesc></figDesc><table>Dataset 
Pos/Neg 
emojis 
Standard 
LSTM 
DeepMoji 

SE0714 
.32 
.35 
.36 
Olympic 
.55 
.57 
.61 
PsychExp 
.40 
.49 
.56 

SS-Twitter 
.86 
.86 
.87 
SS-Youtube 
.90 
.91 
.92 
SE1604 
.56 
.57 
.58 

SCv1 
.66 
.66 
.68 
SCv2-GEN 
.72 
.73 
.74 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Word coverage on benchmark test sets 
using only the vocabulary generated by finding 
words in the training data ('own'), the pretrain-
ing vocabulary ('last') or a combination of both 
vocabularies ('full / chain-thaw'). 

Dataset 
Own 
Last 
Full / 
Chain-thaw 

SE0714 
41.9% 
93.6% 
94.0% 
Olympic 
73.9% 
90.3% 
96.0% 
PsychExp 
85.4% 
98.5% 
98.8% 

SS-Twitter 
80.1% 
97.1% 
97.2% 
SS-Youtube 
79.6% 
97.2% 
97.3% 
SE1604 
86.1% 
96.6% 
97.0% 

SCv1 
88.7% 
97.3% 
98.0% 
SCv2-GEN 
86.5% 
97.2% 
98.0% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 8 : Comparison of agreement between clas- sifiers and the aggregate opinion of Amazon Mechanical Turkers on sentiment prediction of tweets.</head><label>8</label><figDesc></figDesc><table>Agreement 

Random 
50.1% 
fastText 
71.0% 
MTurk 
76.1% 
DeepMoji 
82.4% 

</table></figure>

			<note place="foot" n="2"> Details available at github.com/bfelbo/deepmoji</note>

			<note place="foot" n="3"> We contacted the authors, but were unable to obtain the full dataset for neither version 1 or version 2.</note>

			<note place="foot" n="4"> Available with preprocessing code, examples of usage, benchmark datasets etc. at github.com/bfelbo/deepmoji</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Janys Analytics for generously allowing us to use their dataset of human-rated tweets and the associated code to an-alyze it. Furthermore, we would like to thank Max Lever, who helped design the website, and Han Thi Nguyen, who helped code the software that is provided alongside the pretrained model.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning of representations for unsupervised and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th International Conference on Machine learning (ICML)-Workshop on Unsupervised and Transfer Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="17" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Emotion analysis as a regression problem-dimensional models and their implications on emotion representation and metrical evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Buechel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udo</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd European Conference on Artificial Intelligence (ECAI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th International Conference on Machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Swisscheese at semeval-2016 task 4: Sentiment classification using an ensemble of convolutional neural networks with distant supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Deriu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Gonzenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Uzdilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valeria</forename><forename type="middle">De</forename><surname>Luca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of SemEval</title>
		<imprint>
			<biblScope unit="page" from="1124" to="1128" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31th International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">emoji2vec: Learning emoji representations from their description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matko</forename><surname>Bošnjak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Workshop on Natural Language Processing for Social Media</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>SocialNLP</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Dumitru Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Twitter sentiment classification using distant supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richa</forename><surname>Bhayani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">1</biblScope>
			<pubPlace>Stanford</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">CS224N Project Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<title level="m">Generating sequences with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised sentiment analysis with emotional signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiji</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web (WWW)</title>
		<meeting>the 22nd international conference on World Wide Web (WWW)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="607" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Are word embedding-based features useful for sarcasm detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhav</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Carman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The (un)predictability of emotional hashtags in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fa Kunneman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apj</forename><surname>Liebrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">52th Annual Meeting of the Association for Computational Linguistics (ACL). Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">2012. #emotional tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The First Joint Conference on Lexical and Computational Semantics (*SEM)</title>
		<imprint>
			<biblScope unit="page" from="246" to="255" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semeval2016 task 4: Sentiment analysis in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Workshop on Semantic Evaluation (SemEval)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Creating and characterizing a diverse corpus of sarcasm in dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shereen</forename><surname>Oraby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vrindavan</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lena</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernesto</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning to generate reviews and discovering sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01444</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Using emoticons to reduce dependency in machine learning techniques for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Read</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL student research workshop</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="43" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evaluation datasets for twitter sentiment analysis: a survey and a new dataset, the stsgold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harith</forename><surname>Alani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop: Emotion and Sentiment in Social and Expressive Media: approaches and perspectives from AI (ESSEM) at AI*IA Conference</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fine-grained emotion recognition in olympic tweets based on human computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina</forename><surname>Sintsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Claudiu-Cristian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pearl</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Overtraining, regularization and searching for a minimum, with application to neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Sjöberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lennart</forename><surname>Ljung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Control</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1391" to="1407" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Depechemood: A lexicon for emotion analysis from crowd-annotated news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacopo</forename><surname>Staiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Guerini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">52th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semeval2007 task 14: Affective text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Workshop on Semantic Evaluations (SemEval)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="70" to="74" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distant supervision for emotion classification with discrete binary values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Suttles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><surname>Ide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Text Processing and Computational Linguistics (CICLing)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="121" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning sentimentspecific word embedding for twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">52th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<idno>abs/1605.02688</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Theano Development Team. arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sentiment strength detection for the social web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Thelwall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevan</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Paltoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology (JASIST)</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="163" to="173" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sentiment strength detection in short informal text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Thelwall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevan</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Paltoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvid</forename><surname>Kappas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2544" to="2558" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A corpus for research on deliberation and debate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marilyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">E</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Fox Tree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Language Resources and Evaluation (LREC)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="812" to="817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">How universal and specific is emotional experience? evidence from 27 countries on five continents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Harald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><forename type="middle">R</forename><surname>Wallbott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Social Science Council</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="763" to="795" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A survey on the role of negation in sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Balahur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrés</forename><surname>Montoyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Negation and Speculation in Natural Language Processing</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="60" to="68" />
		</imprint>
	</monogr>
	<note>NeSp-NLP)</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
