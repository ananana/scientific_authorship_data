<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Privacy-preserving Neural Representations of Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximin</forename><surname>Coavoux</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Privacy-preserving Neural Representations of Text</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1" to="10"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This article deals with adversarial attacks towards deep learning systems for Natural Language Processing (NLP), in the context of privacy protection. We study a specific type of attack: an attacker eavesdrops on the hidden representations of a neural text classifier and tries to recover information about the input text. Such scenario may arise in situations when the computation of a neural network is shared across multiple devices, e.g. some hidden representation is computed by a user&apos;s device and sent to a cloud-based model. We measure the privacy of a hidden representation by the ability of an attacker to predict accurately specific private information from it and characterize the tradeoff between the privacy and the utility of neural representations. Finally, we propose several defense methods based on modified training objectives and show that they improve the privacy of neural representations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This article presents an adversarial scenario meant at characterizing the privacy of neural representa- tions for NLP tasks, as well as defense methods designed to improve the privacy of those represen- tations. A deep neural network constructs inter- mediate hidden representations to extract features from its input. Such representations are trained to predict a label, and therefore should contain use- ful features for the final prediction. However, they might also encode information about the input that a user wants to keep private (e.g. personal data) and can be exploited for adversarial usages.</p><p>We study a specific type of attack on neural rep- resentations: an attacker eavesdrops on the hidden representations of novel input examples (that are not in the training set) and tries to recover informa- tion about the content of the input text ( <ref type="figure" target="#fig_0">Figure 1)</ref>. A typical scenario where such attacks would oc- cur is when the computation of a deep neural net is shared between several devices ( <ref type="bibr" target="#b13">Li et al., 2017)</ref>. For example, a user's device computes a represen- tation of a textual input, and sends it a to cloud- based neural network to obtain, e.g. the topic of the text or its sentiment. The scenario is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Private information can take the form of key phrases explicitly contained in the text. However, it can also be implicit. For example, demographic information about the author of a text can be pre- dicted with above chance accuracy from linguistic cues in the text itself ( <ref type="bibr" target="#b21">Rosenthal and McKeown, 2011;</ref><ref type="bibr" target="#b20">Preot¸iucPreot¸iuc-Pietro et al., 2015</ref>).</p><p>Independently of its explicitness, some of this private information correlates with the output la- bels, and therefore will be learned by the network. In such a case, there is a tradeoff between the util- ity of the representation (measured by the accu- racy of the network) and its privacy. It might be necessary to sacrifice some accuracy in order to satisfy privacy requirements.</p><p>However, this is not the case of all private in- formation, since some of it is not relevant for the prediction of the text label. Still, private infor- mation might be learned incidentally. This non- intentional and incidental learning also raises pri- vacy concerns, since an attacker with an access to the hidden representations, may exploit them to re- cover information about the input.</p><p>In this paper we explore the following situation: (i) a main classifier uses a deep network to predict a label from textual data; (ii) an attacker eaves- drops on the hidden layers of the network and tries to recover information about the input text of un- seen examples. In contrast to previous work about neural networks and privacy <ref type="bibr">(Papernot et al., 2016;</ref><ref type="bibr" target="#b3">Carlini et al., 2018)</ref> we do not protect the privacy of examples from the training set, but the privacy of unseen examples provided, e.g., by a user.</p><p>An example of a potential application would be a spam detection service with the following con- straints: the service provider does not access ver- batim emails sent to users, only their vector repre- sentations. Theses vector representations should not be usable to gather information about the user's contacts or correspondents, i.e. protect the user from profiling. This paper makes the following contributions: 1</p><p>• We propose a metric to measure the privacy of the neural representation of an input for Natural Language Processing tasks. The met- ric is based on the ability of an attacker to recover information about the input from the latent representation only.</p><p>• We present defense methods designed against this type of attack. The methods are based on modified training objectives and lead to an improved privacy-accuracy tradeoff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Adversarial Scenario</head><p>In the scenario we propose, each example consists of a triple (x, y, z), where x is a natural language text, y is a single label (e.g. topic or sentiment), and z is a vector of private information contained in x. Our base setting has two entities: (i) a main classifier whose role is to learn to predict y from x, (ii) an attacker who learns to predict z from the latent representation of x used by the main classi- fier. We illustrate this setting in <ref type="figure" target="#fig_0">Figure 1</ref>. In order to evaluate the utility and privacy of a specific model, we proceed in three phases: Phase 1. Training of the main classifier on (x, y) pairs and evaluation of its accuracy;</p><p>Phase 2. Generation of a dataset of pairs (r(x), z) for the attacker, r is the representation function of the main classifier (r is defined in Sec- tion 2.1);</p><p>Phase 3. Training of the attacker's network and evaluation of its performance for measuring pri- vacy.</p><p>In the remainder of this section, we describe the main classifier (Section 2.1), and the attacker's model (Section 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text Classifier</head><p>As our base model, we chose a standard LSTM architecture <ref type="bibr" target="#b7">(Hochreiter and Schmidhuber, 1997)</ref> for sequence classification. LSTM-based archi- tectures have been applied to many NLP tasks, including sentiment classification ( <ref type="bibr" target="#b24">Wang et al., 2016)</ref> and text classification ( <ref type="bibr" target="#b26">Zhou et al., 2016)</ref>.</p><p>First, an LSTM encoder computes a fixed-size representation r(x) from a sequence of tokens x = (x 1 , x 2 , . . . , x n ) projected to an embedding space. We use θ r to denote the parameters used to construct r. They include the parameters of the LSTM, as well as the word embeddings. Then, the encoder output r(x) is fed as input to a feedfor- ward network with parameters θ p that predicts the label y of the text, with a softmax output activa- tion. In the standard setting, the model is trained to minimize the negative log-likelihood of y labels:</p><formula xml:id="formula_0">L m (θ r , θ p ) = N i=1 − log P (y (i) |x (i) ; θ r , θ p ),</formula><p>where N is the number of training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attacker's Classifier</head><p>Once the main model has been trained, we assume that its parameters θ r and θ p are fixed. We gen- erate a new dataset made of pairs (r(x), z(x)), where r(x) is the hidden representation used by the main model and z(x) is a vector of private cat- egorical variables. In practice, z is a vector of bi- nary variables, (representing e.g. demographic in- formation about the author). In our experiments, we use the same training examples x for the main classifier and the classifier of the attacker. How- ever, since the attacker has access to the repre- sentation function r parameterized by θ r , they can generate a dataset from any corpus containing the private variables they want to recover. In other words, it is not necessary that they have access to the original training corpus to train their classifier.</p><p>The attacker trains a second feedforward net- work on the new dataset {(r(x (i) ), z (i) )} i≤N . This classifier uses a sigmoid output activation to com- pute the probabilities of each binary variable in z:</p><formula xml:id="formula_1">P (z|r(x); θ a ) = σ(FeedForward(r(x))).</formula><p>It is trained to minimize the negative log- likelihood of z:</p><formula xml:id="formula_2">L a (θ a ) = N i=1 − log P (z (i) |r(x (i) ); θ a ) = N i=1 K j=1 − log P (z (i) j |r(x (i) ); θ a ),</formula><p>assuming that the K variables in z are indepen- dent. Since the parameters used to construct r are fixed, the attacker only acts upon its own parame- ters θ a to optimize this loss.</p><p>We use the performance of the attacker's clas- sifier as a proxy for privacy. If its accuracy is high, then an eavesdropper can easily recover in- formation about the input document. In contrast, if its accuracy is low (i.e. close to that of a most- frequent label baseline), then we may reasonably conclude that r does not encode enough informa- tion to reconstruct x, and mainly contains infor- mation that is useful to predict y.</p><p>In general, the performance of a single attacker does not provide sufficient evidence to conclude that the input representation r is robust to an at- tack. It should be robust to any type of reconstruc- tion method. In the scope of this paper though, we only experiment with a feedforward network reconstructor, i.e. a powerful learner.</p><p>In the following sections, we propose several training method modifications aimed at obfuscat- ing private information from the hidden represen- tation r(x). Intuitively, the aim of these modifica- tions is to minimize some measure of information between r and z to make the prediction of z hard. An obvious choice for that measure would be the Mutual Information (MI) between r and z. How- ever, MI is hard to compute due to the continuous distribution of r and does not lend itself well to stochastic optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Defenses Against Adversarial Attacks</head><p>In this section, we present three training methods designed as defenses against the type of attack we described in Section 2.2. The first two methods are based on two neural networks with rival objective functions (Section 3.1). The last method is meant at discouraging the model to cluster together train- ing examples with similar private variables z (Sec- tion 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Adversarial Training</head><p>First, we propose to frame the training of the main classifier as a two-agent process: the main agent and an adversarial generator, exploiting a set- ting similar to Generative Adversarial Networks (GAN, <ref type="bibr" target="#b6">Goodfellow et al., 2014</ref>). The generator learns to reconstruct examples from the hidden representation, whereas the main agent learns (i) to perform its main task (ii) to make the task of the generator difficult.</p><p>We experiment with two types of generators: a classifier that predicts the binary attributes z(x) used as a proxy for the reconstruction of x (Sec- tion 3.1.1) and a character-based language model that directly optimizes the likelihood of the train- ing examples (Section 3.1.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Adversarial Classification: Multidetasking</head><p>In order not to make r(x) a good representation for reconstructing z, we make two modifications to the training setup of the main model (Phase 1):</p><p>• We use a duplicate adversarial classifier, with parameters θ a , that tries to predict z from r(x). It is trained simultaneously with the main classifier. Its training examples are generated on the fly, and change overtime as the main classifier updates its own parame- ters. This classifier simulates an attack dur- ing training.</p><p>• We modify the objective function of the main classifier to incorporate a penalty when the adversarial classifier is good at reconstruct- ing z. In other words, the main classifier tries to update its parameters so as to confuse the duplicate attacker.</p><p>Formally, for a single data point (x, y, z), the adversarial classifier optimizes:</p><formula xml:id="formula_3">L a (x, y, z; θ a ) = − log P (z|r(x); θ a ),</formula><p>whereas the main classifier optimizes:</p><formula xml:id="formula_4">L m (x, y, z; θ r , θ p ) = − α log P (y|x; θ r , θ p ) − β log P (¬z|r(x); θ a ).</formula><p>The first term of this equation is the log-likelihood of the y labels. The second term is designed to de- ceive the adversary. The hyperparameters α &gt; 0 and β &gt; 0 control the relative importance of both terms.</p><p>As in a GAN, the losses of both classifiers are interdependent, but their parameters are distinct: the adversary can only update θ a and the main classifier can only update θ r and θ p .</p><p>The duplicate adversarial classifier is identical to the classifier used to evaluate privacy after the main model has been trained and its parameters are fixed. However, both classifiers are completely distinct: the former is used during the training of the main model (Phase 1) to take privacy into ac- count whereas the latter is used to evaluate the pri- vacy of the final model (Phase 3), as is described in Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Adversarial Generation</head><p>The second type of generator we use is a character- based LSTM language model that is trained to re- construct full training examples. For a single ex- ample (x; y), the hidden state of the LSTM is ini- tialized with r(x), computed by the main model. The generator optimizes:</p><formula xml:id="formula_5">L g (x, y; θ ; θ r ) = − log P (x|r(x); θ ) = − C i=1 log P (x i |x i−1 1 , r(x); θ ),</formula><p>where θ is the set of parameters of the LSTM generator, x i is the i th character in the document, and C is the length of the document in number of characters. The generator has no control over r(x), and optimizes the objective only by updat- ing its own parameters θ . Conversely, the loss of the main model is modi- fied as follows:</p><formula xml:id="formula_6">L m (x, y; θ r , θ p ) = − α log P (y|x; θ r , θ p ) − βL g (x, y; θ , θ r ).</formula><p>The first term maximizes the likelihood of the y labels whereas the second term is meant at mak- ing the reconstruction difficult by maximizing the loss of the generator. As in the loss function de- scribed in the previous section, α and β control the relative importance of both terms. Once again, the main classifier can optimize the second term only by updating θ r , since it has no control over the parameters of the adversarial generator.</p><p>A key property of this defense method is that it has no awareness of what the private variables z are. Therefore, it has the potential to protect the neural representation against an attack on any pri- vate information. From a broader perspective, the goal of this defense method is to specialize the hid- den representation r(x) to the task at hand (sen- timent or topic prediction) and to avoid learning anything not relevant to it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Declustering</head><p>The last strategy we employ to make the task of the attacker harder is based on the intuition that pri- vate variables z are easier to predict from r when the main model learns implicitly to cluster exam- ples with similar z in the same regions of the rep- resentation space.</p><p>In order to avoid such implicit clustering, we add a term to the training objective of the main model that penalizes pairs of examples (x, x ) that (i) have similar reconstructions z(x) ≈ z(x ) (ii) have hidden representations r(x) and r(x ) in the same region of space. We use the following modi- fied loss for a single example:</p><formula xml:id="formula_7">L m (x, y, z; θ r , θ p ) = − log P (y|x; θ r , θ p ) +α(0.5 − (z, z ))||r(x) − r(x )|| 2 2 ,</formula><p>where (x , z ) is another example sampled uni- formly from the training set, α is a hyperparame- ter controlling the importance of the second term, and (·, ·) ∈ [0, 1] is the normalized Hamming dis- tance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our experiments are meant to characterize the privacy-utility tradeoff of neural representations on text classification tasks, and evaluating if the proposed defense methods have a positive im- pact on it. We first describe the datasets we used (Section 4.1) and the experimental protocol (Section 4.2), then we discuss the results (Sec- tion 4.3). We found that in the normal train- ing regime, where no defense is taken into ac- count, the adversary can recover private informa- tion with higher accuracy than a most frequent class baseline. Furthermore, we found that the de- fenses we implemented have a positive effect on the accuracy-privacy tradeoff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We experiment with two text classification tasks: sentiment analysis (Section 4.1.1) and topic clas- sification (Section 4.1.2). The sizes of each dataset are summarized in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Sentiment Analysis</head><p>We use the Trustpilot dataset (  for sentiment analysis. This corpus contains re- views associated with a sentiment score on a five point scale, and self-reported information about the users. We use the five subcorpora correspond- ing to five areas (Denmark, France, Germany, United Kingdom, United States). We filter examples containing both the birth year and gender of the author of the review and use these variables as the private information. As in previous work on this dataset <ref type="bibr" target="#b8">(Hovy, 2015;</ref>, we bin the age of the author into two categories <ref type="bibr">('under 35' and 'over 45')</ref>. Fi- nally, we randomly split each subcorpus into a training set (80%), a development set (10%) and a test (10%).</p><p>As an additional experimental setting, we use both demographic variables (gender and age) as input to the main model. We do so by adding two additional tokens at the beginning of the input text, one for each variable. It has been shown that those variables can be used to improve text classifica- tion <ref type="bibr" target="#b8">(Hovy, 2015)</ref>. Also, we would like to evalu- ate whether the attacker's task is easier when the variables to predict are explicitly in the input, com- pared to when these information are only poten- tially and implicitly in the input. In other words, this setting simulates the case where private in- formation may be used by the model to improve classification, but should not be exposed too obvi- ously. In the rest of this section, we use RAW to denote the setting where only the raw text is used as input and +DEMO, the setting where the demo- graphic variables are also used as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Topic Classification</head><p>We perform topic classification on two genres of documents: news articles and blog posts.</p><p>News article For topic classification of news ar- ticle, we use two datasets: the AG news corpus 2 <ref type="bibr" target="#b4">(Del Corso et al., 2005</ref>) and the English part of the Deutsche Welle (DW) news corpus (Pappas and Popescu-Belis, 2017).</p><p>For the AG corpus, following <ref type="bibr" target="#b25">Zhang et al. (2015)</ref>, we construct the dataset by extracting doc- uments belonging to the four most frequent topics, and use the concatenation of the 'title' and 'de- scription' fields as the input to the classifier. We randomly split the corpus into a training set (80%), a development set (10%) and a test set (10%). For the DW dataset, we use the 'text' field as input, and the standard split. We kept only documents belonging to the 20 most frequent topics.</p><p>The attacker tries to detect which named enti- ties appear in the input text (each coefficient in z(x) indicates whether a specific named entity oc- curs in the text). For both datasets, we used the named entity recognition system from the NLTK package ( <ref type="bibr" target="#b1">Bird et al., 2009</ref>) to associate each ex- ample with the list of named entities that occur in it. We select the five most frequent named entities with type 'person', and only keep examples con- taining at least one of these named entities. This filtering is necessary to avoid a very unbalanced dataset (since each selected named entity appears usually in very few articles).</p><p>Blog posts We used the blog authorship corpus presented by <ref type="bibr">Schler</ref>   used the age and gender of the author as the private variables. These variables have a very unbalanced distribution in the dataset, we randomly select ex- amples to obtain uniform distributions of private variables. Finally, we split the corpus into a train- ing set (80%), a validation set and a test set (10% each).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Protocol</head><p>Evaluation For the main task, we report a single accuracy measure. For measuring the privacy of a representation, we compute the following metrics:</p><p>• For demographic variables (sentiment analy- sis and blog post topic classification): 1 − X, where X is the average of the accuracy of the attacker on the prediction of gender and age; • For named entities (news topic classifica- tion): 1−F , where F is an F-score computed over the set of binary variables in z that in- dicate the presence of named entities in the input example.</p><p>Training protocol We implemented our model using Dynet ( <ref type="bibr">Neubig et al., 2017</ref>). The feedfor- ward components (both of the main model and of the attacker) have a single hidden layer of 64 units with a ReLU activation. Word embeddings have 32 units. The LSTM encoder has a single layer of varying sizes, since it is expected that the amount of information that can be learned depends on the size of these representations. We used the Adam optimizer ( <ref type="bibr" target="#b12">Kingma and Ba, 2014</ref>) with the default learning rate, and 0.2 dropout rate for the LSTM. We used α = 0.1 for the declustering method, based on preliminary experiments. For the other defense methods, we used α = β = 1 and did not experiment with other values.</p><p>For each dataset, and each LSTM state di- mension <ref type="figure" target="#fig_0">({8, 16, 32, 64, 128})</ref>, we train the main model for 8 epochs (sentiment classification) or 16 epochs (topic classification), and select the model with the best accuracy on the development set. Then, we generate the dataset for the attacker, train the adversarial model for 16 epochs and se- lect the model with the worst privacy on the devel- opment set (i.e. the most successful attacker).</p><p>It has to be noted that we select the models that implement defenses on their accuracy, rather than their privacy or a combination thereof. In prac- tice, we could also base the selection strategy on a privacy budget: selecting the most accurate model with privacy above a certain threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>This section discusses results for the sentiment analysis task (Section 4.3.1) and the topic classi- fication task (Section 4.3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Sentiment Analysis</head><p>How private are neural representations? Be- fore discussing the effect of proposed defense methods, we motivate empirically our approach by showing that adversarial models can recover pri- vate information with reasonable accuracy when the attack is targeted towards a model that imple- ments none of the presented defense methods.</p><p>To do so, we compare the accuracy of adversar- ial models to two types of baselines:</p><p>• As a lower bound, we use the most frequent class baseline.</p><p>• As an upper bound, we trained a classi- fier that can optimize the hidden represen- tations (r) for the attacker's tasks. In other words, this baseline is trained to predict de-</p><formula xml:id="formula_8">Corpus Standard M-Detask. A-Gener. Decl. α = 0.1 Main Priv. Main Priv. Main Priv. Main</formula><p>Priv.</p><p>Germany 85.1 32.2 -0.6 -0.3 -1.3 +0.6 -0.8 +1.9 baseline 78.6 36.  mographic variables from x, as if it were the main task.</p><p>In <ref type="table" target="#tab_2">Table 2</ref>, we compare both baselines to the best adversary in the two settings (RAW and +DEMO) among the models trained with no de- fenses. First of all, we observe that apart from gender on the German dataset, the trained baseline outperforms the most frequent class baseline by a wide margin (8 to 25 absolute difference). Sec- ond of all, the attacker is able to outperform the most frequent class baseline overall, even in the RAW setting. In more details, for age, the adver- sary is well over the baseline in all cases except US. On the other hand, gender seems harder to predict: the adversary outperforms the most fre- quent class baseline only in the +DEMO setting.</p><p>The same pattern is visible for the blog post dataset, also presented in the last line of <ref type="table" target="#tab_2">Table 2</ref>: the best adversaries are 14 points over the base- line for gender and 5 points for age, i.e. almost as good as a model that can fine tune the hidden representations.</p><p>These results justify our approach, since they demonstrate that hidden representations learn pri- vate information about the input, and can be ex- ploited to recover this information with reasonable accuracy.</p><p>Effect of defenses We report results for the main task accuracy and the representation privacy in <ref type="table" target="#tab_4">Ta- ble 3</ref> for the +DEMO setting and in <ref type="table" target="#tab_5">Table 4</ref> for the RAW setting. Recall that the privacy measure</p><formula xml:id="formula_9">Corpus Standard M-Detask. A-Gener. Decl. α = 0.1 Main Priv. Main Priv. Main Priv. Main Priv.</formula><p>Germany 85.5 32.1 +0.3 +0.5 -0.8 +0.9 -1.7 +2.2 baseline 78.6 36.9</p><p>Denmark 82.    <ref type="table" target="#tab_4">Table 3</ref> for details about the metrics.</p><p>(Priv.) is computed by 1 − X where X is the av- erage accuracy of the attacker on gender and age predictions. When this privacy metric is higher, it is more difficult to exploit the hidden repre- sentation of the network to recover information about x. The 'Standard' columns contain the ac- curacy and privacy of the base model described in Section 2. The next columns present the abso- lute variation in accuracy and privacy for the three defense methods presented in Section 3: Multi- detasking, Adversarial Generation, and Decluster- ing. We also report for each corpus the most fre- quent class baseline for the main task accuracy, and the privacy of the most frequent class base- lines on private variables (i.e. the upper bound for privacy). The three modified training methods designed as defenses have a positive effect on privacy. De- spite a model selection based on accuracy, they lead to an improvement in privacy on all datasets, except on the France subcorpus. In most cases, we observe only a small decrease in accuracy, or even an improvement at times (e.g. multidetasking on the Germany dataset, RAW setting), thus improv- ing the tradeoff between the utility and the privacy of the text representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Topic Classification</head><p>We report results on topic classification in <ref type="table" target="#tab_6">Table 5</ref>.</p><p>News articles For the news corpora, the privacy metric is based on the F-score on the binary vari- ables z indicating the presence or absence of a named entity in the text. First of all, we ob- serve that defense methods that explicitly use z (i.e. multidetasking and declustering), have a very positive effect on privacy, but also a detrimental effect on the main task. We hypothesize that this is due to the strong correlations between the main task labels y and the private information z. As a result, improving the privacy of the neural repre- sentations comes at a cost in accuracy.</p><p>In contrast, the adversarial generation defense method lead to an improvement in accuracy, that is quite substantial for the DW corpus. We specu- late that this is due to the secondary term in the ob- jective function of the main model (Section 3.1.2) that helps avoiding overfitting the main task or learning spurious features.</p><p>Blog posts On the blog post dataset, the effects are smaller, which we attribute to the nature of the task of the attacker. The defense methods con- sistently improve privacy and, in one case, accu- racy. The best effects on the tradeoff are achieved with the multidetasking and adversarial generation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>The main result of our experiments is that the de- fenses we propose improve privacy with usually a small effect, either positive or negative, on accu- racy, thus improving the tradeoff between the util- ity and the privacy of neural representations.</p><p>An important direction for future work is the choice of a strategy for model selection. The tradeoff between utility and privacy can be con- trolled in many ways. For example, the impor- tance of both terms in the loss functions in Sec- tion 3.1 can be controlled to favor either privacy or utility. In the scope of this paper, we did not perform thorough hyperparameter tuning, but be- lieve that doing so is important for achieving better results, since the effects of defense method can be more drastic than desired in some cases, as exem- plified on the news corpora <ref type="table" target="#tab_6">(Table 5)</ref>.</p><p>Overall, we found that the multidetasking ap- proach lead to the more stable improvements and should be preferred in most cases, since it is also the less computationnally expensive defense. On the other hand, the adversarial generation method does not require the specification of private vari- ables, and thus is a more general approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>The deployment of machine learning in both academic and industrial contexts raises concerns about adversarial uses of machine learning, as well as concerns about attacks specifically targeted at these algorithms that often rely on large amounts of data, including personal data.</p><p>More generally, the framework of differential privacy <ref type="bibr" target="#b5">(Dwork, 2006</ref>) provides privacy guaran- tees for the problem of releasing information with- out compromising confidential data, and usually involves adding noise in the released information. It has been applied to the training of deep learning models ( <ref type="bibr" target="#b0">Abadi et al., 2016;</ref><ref type="bibr">Papernot et al., 2016;</ref><ref type="bibr" target="#b18">Papernot et al., 2018)</ref>, and Bayesian topic models ( <ref type="bibr" target="#b22">Schein et al., 2018)</ref>.</p><p>The notion of privacy is particularly crucial to NLP, since it deals with textual data, oftentimes user-generated data, that contain a lot of private in- formation. For example, textual data contain a lot of signal about authors <ref type="bibr" target="#b11">(Hovy and Spruit, 2016)</ref>. and can be leveraged to predict demographic vari- ables ( <ref type="bibr" target="#b21">Rosenthal and McKeown, 2011;</ref><ref type="bibr">Preot¸iucPreot¸iucPietro et al., 2015</ref>). Oftentimes, this information is not explicit in the text but latent and related to the usage of various linguistic traits. Our work is based on a stronger hypothesis: this latent infor- mation is still present in vectorial representations of texts, even if the representations have not been supervised by these latent variables. <ref type="bibr" target="#b13">Li et al. (2017)</ref> study the privacy of unsuper- vised representations of images, and measures their privacy with the peak signal to noise ratio between an original image and its reconstruction by an attacker. They find a tradeoff between the privacy of the learned representations and the ac- curacy of an image classification model that uses these representations as inputs. Our setting is complementary since it is applied to NLP tasks, but explores a similar problem in the case of rep- resentations learned with a task supervision.</p><p>A related problem is the unintended memoriza- tion of private data from the training set and has been addressed by <ref type="bibr" target="#b3">Carlini et al. (2018)</ref>. They tackle this problem in the context of text gener- ation (machine translation, language modelling).</p><p>If an attacker has access to e.g. a trained language model, they are likely to be able to generate sen- tences from the training set, since the language model is trained to assign high probabilities to those sentences. Such memorization is problem- atic when the training data contains private infor- mation and personal data. The experimental set- ting we explore is different from these works: we assume that the attacker has access to a hidden layer of the network and tries to recover informa- tion about an input example that is not in the train- ing set.</p><p>In a recent study, <ref type="bibr" target="#b14">Li et al. (2018)</ref> proposed a method based on GAN designed to improve the robustness and privacy of neural representations, applied to part-of-speech tagging and sentiment analysis. They use a training scheme with two agents similar to our multidetasking strategy (Sec- tion 3.1.1), and found that it made neural represen- tations more robust and accurate. However, they only use a single adversary to alter the training of the main model and to evaluate the privacy of the representations, with the risk of overestimat- ing privacy. In contrast, once the parameters of our main model are fixed, we train a new classifier from scratch to evaluate privacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented an adversarial scenario and used it to measure the privacy of hidden repre- sentations in the context of two NLP tasks: senti- ment analysis and topic classification of news arti- cle and blog posts. We have shown that in general, it is possible for an attacker to recover private vari- ables with higher than chance accuracy, using only hidden representations. In order to improve the privacy of hidden representations, we have pro- posed defense methods based on modifications of the training objective of the main model. Empiri- cally, the proposed defenses lead to models with a better privacy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: General setting illustration. The main classifier predicts a label y from a text x, the attacker tries to recover some private information z contained in x from the latent representation used by the main classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>et al. (2006), a collection of blog posts associated with the age and gender of the authors, as provided by the authors themselves. Since the blog posts have no topic annotation, we ran the LDA algorithm (Blei et al., 2003) on the whole collection (with 10 topics). The LDA out- puts a distribution on topics for each blog post. We selected posts with a single dominating topic (&gt; 80%) and discarded the other posts. We binned age into two category (under 20 and over 30). We</figDesc><table>Baselines 
Best adversaries 
Lower bound (most Upper bound 
frequent class) 
(trained) 
+DEMO 

RAW 

Gender 
Age 
Gender Age Gender Age Gender Age 

TP (Denmark) 
61.6 
58.4 
70.5 
78.0 
68.5 
75.3 
62.0 
63.4 
TP (France) 
61.0 
50.1 
69.0 
63.4 
61.0 
57.1 
61.0 
60.6 
TP (Germany) 
75.2 
50.9 
75.2 
75.2 
75.2 
60.4 
75.2 
58.6 
TP (UK) 
58.8 
56.7 
70.0 
76.3 
66.4 
63.5 
59.9 
61.8 
TP (US) 
63.5 
63.7 
74.1 
74.8 
81.3 
74.9 
64.7 
63.9 
Blogs 
50.0 
50.3 
65.7 
56.1 
-
-
63.9 
55.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Comparisons between baselines and best adversaries. All metrics reported in this table are accuracies.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results on the test sets of the Trustpilot 
dataset, +DEMO setting. Main is the accuracy on senti-
ment analysis. Priv. is the privacy measure (i.e. the in-
verse accuracy of the attacker: higher is better, see Sec-
tion 4.2). The baselines are most-frequent class clas-
sifiers. The values reported for the defense methods 
indicate absolute differences with the standard training 
regime (no defense implemented) for both metrics. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results on the test sets of the Trustpilot 
dataset, RAW setting. See Section 4.2 and caption of 
Table 3 for details about the metrics. 

Corpus 
Standard 
M-Detask. 
A-Gener. 
Decl. α = 0.1 
Main Priv. Main 
Priv. Main 
Priv. Main 
Priv. 

AG news 
76.5 33.7 -14.5 +14.5 +0.2 
-7.8 
-2.5 
+8.6 
baseline 
57.8 

DW news 44.3 78.3 
-5.7 +21.7 +5.9 +13.1 
-5.4 +18.4 
baseline 
22.1 

Blogs 
58.3 40.8 
-0.8 
+3.4 +1.1 
+0.9 
-0.2 
+1.2 
baseline 
47.8 49.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Results for topic classification (test sets). See 
Section 4.2 and caption of </table></figure>

			<note place="foot" n="1"> The source code used for the experiments described in this paper is available at https://github.com/ mcoavoux/pnet.</note>

			<note place="foot" n="2"> http://www.di.unipi.it/ ˜ gulli/AG_ corpus_of_news_articles.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers and members of the Cohort for helpful feedback on previous ver-sions of the article. We gratefully acknowledge the support of the European Union under the Horizon 2020 SUMMA project (grant agreement 688139), and the support of Huawei Technologies.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning with differential privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, CCS &apos;16</title>
		<meeting>the 2016 ACM SIGSAC Conference on Computer and Communications Security, CCS &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="308" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Natural Language Processing with Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>O&apos;Reilly Media, Inc</pubPlace>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The secret sharer: Measuring unintended neural network memorization &amp; extracting secrets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jernej</forename><surname>Kos</surname></persName>
		</author>
		<idno>abs/1802.08232</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>´ Ulfar Erlingsson, and Dawn Song</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ranking a stream of news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianna</forename><forename type="middle">M Del</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Gullí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Romani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on World Wide Web, WWW &apos;05</title>
		<meeting>the 14th International Conference on World Wide Web, WWW &apos;05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="97" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Differential privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd International Colloquium on Automata, Languages and Programming, part II (ICALP 2006)</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">4052</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Demographic factors improve classification performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="752" to="762" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">User review sites as a resource for largescale sociolinguistic studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Johannsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web, WWW &apos;15</title>
		<meeting>the 24th International Conference on World Wide Web, WWW &apos;15<address><addrLine>Republic and Canton of Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tagging performance correlates with author age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="483" to="488" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The social impact of natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shannon</forename><forename type="middle">L</forename><surname>Spruit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="591" to="598" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Privynet: A flexible framework for privacy-preserving deep neural network training with A fine-grained privacy control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhen</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Z</forename><surname>Pan</surname></persName>
		</author>
		<idno>abs/1709.06161</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards robust and privacy-preserving text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03980</idno>
		<title level="m">Swabha Swayamdipta, and Pengcheng Yin. 2017. Dynet: The dynamic neural network toolkit</title>
		<meeting><address><addrLine>Yusuke Oda, Matthew Richardson, Naomi Saphra</addrLine></address></meeting>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ulfar Erlingsson</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semisupervised knowledge transfer for deep learning from private training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<idno>abs/1610.05755</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Scalable Private Learning with PATE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erlingsson</surname></persName>
		</author>
		<idno>abs/1802.08908</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multilingual hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Popescu-Belis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1015" to="1025" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An analysis of the user occupational class through twitter content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Preot¸iucpreot¸iuc-Pietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Lampos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Aletras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1754" to="1764" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Age prediction in blogs: A study of style, content, and online behavior in pre-and post-social media generations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="763" to="772" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Locally Private Bayesian Inference for Count Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Schein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><forename type="middle">Steven</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<idno>abs/1803.08471</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Effects of age and gender on blogging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Pennebaker</surname></persName>
		</author>
		<idno>SS-06-03</idno>
	</analytic>
	<monogr>
		<title level="m">Computational Approaches to Analyzing Weblogs-Papers from the AAAI Spring Symposium</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="191" to="197" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention-based LSTM for aspectlevel sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Text classification improved by integrating bidirectional lstm with twodimensional max pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3485" to="3495" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
