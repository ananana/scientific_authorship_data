<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling Online Discourse with Coupled Distributed Topics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Srivatsan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Wojtowicz</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Social and Decision Sciences</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling Online Discourse with Coupled Distributed Topics</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4673" to="4682"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4673</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we propose a deep, globally normalized topic model that incorporates structural relationships connecting documents in socially generated corpora, such as online forums. Our model (1) captures discursive interactions along observed reply links in addition to traditional topic information, and (2) incorporates latent distributed representations arranged in a deep architecture, which enables a GPU-based mean-field inference procedure that scales efficiently to large data. We apply our model to a new social media dataset consisting of 13M comments mined from the popular internet forum Reddit, a domain that poses significant challenges to models that do not account for relationships connecting user comments. We evaluate against existing methods across multiple metrics including perplex-ity and metadata prediction, and qualitatively analyze the learned interaction patterns.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Topic models have become one of the most com- mon unsupervised methods for uncovering latent semantic information in natural language data, and have found a wide variety of applications across the sciences. However, many common models - such as Latent Dirichlet Allocation ( <ref type="bibr" target="#b11">Ng and Jordan, 2003)</ref> -make an explicit exchangeability assump- tion that treats documents as independent samples from a generative prior, thereby ignoring important aspects of text corpora which are generated by non- ergodic, interconnected social systems. While the direct application of such models to datasets such as transcripts of The French Revolution ( <ref type="bibr" target="#b1">Barron et al., 2017</ref>) and discussions on Twitter ( <ref type="bibr" target="#b17">Zhao et al., 2011</ref>) have yielded sensible topics and exciting in- sights, their exclusion of document-to-document interactions imposes limitations on the scope of their applicability and the analyses they support.</p><p>For instance, on many social media platforms, com- ments are short (the average Reddit comment is 10 words long), making them difficult to treat as full documents, yet they do cohere as a collection, sug- gesting that contextual relationships should be con- sidered. Moreover, analysis of social data is often principally concerned with understanding relation- ships between documents (such as question-asking and -answering), so a model able to capture such features is of direct scientific relevance.</p><p>To address these issues, we propose a design that models representations of comments jointly along observed reply links. Specifically, we attach a vec- tor of latent binary variables to each comment in a collection of social data, which in turn connect to each other according to the observed reply-link structure of the dataset. The inferred representa- tions can provide information about the rhetorical moves and linguistic elements that characterize an evolving discourse. An added benefit is that while previous work such as Sequential LDA ( <ref type="bibr" target="#b3">Du et al., 2012</ref>) has focused on modeling a linear progres- sion, the model we present applies to a more gen- eral class of acyclic graphs such as tree-structured comment threads ubiquitous on the web.</p><p>Online data can be massive, which presents a scalability issue for traditional methods. Our ap- proach uses latent binary variables similar to a Re- stricted Boltzmann Machine (RBM); related mod- els such as Replicated Softmax (RS) <ref type="bibr" target="#b14">(Salakhutdinov and Hinton, 2009</ref>) have previously seen suc- cess in capturing latent properties of language, and found substantial speedups over previous methods due to their GPU amenable training procedure. RS was also shown to deal well with documents of significantly different length, another key charac- teristic of online data. While RBMs permit exact in- ference, the additional coupling potentials present in our model make inference intractable. How- ever, the choice of bilinear potentials and latent   <ref type="figure">Figure 1</ref>: DDTM factor graph for an example thread. Each comment is modeled as an ob- served bag-of-words x with top- ics represented by a latent bi- nary vector h. Log-bilinear fac- tors connect the latent and ob- served variables of each com- ment, and the latent variables of parent-child comment pairs along observed reply links. Bi- ases are omitted for clarity.</p><p>features admits a mean-field inference procedure which takes the form of a series of dense matrix multiplications followed by nonlinearities, which is particularly amenable to GPU computation and lets us scale efficiently to large data. Our model outperforms LDA and RS baselines on perplexity and downstream tasks including meta- data prediction and document retrieval when evalu- ated on a new dataset mined from Reddit. We also qualitatively analyze the learned topics and discuss the social phenomena uncovered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>We now present an overview of our model. Specifi- cally, it will take the probabilistic form of an undi- rected graphical model whose architecture mirrors the tree structure of the threads in our data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motivating Dataset</head><p>We evaluate on a corpus mined from Reddit, an internet forum which ranks as the fourth most traf- ficked site in the US (Alexa, 2018) and sees mil- lions of daily comments <ref type="bibr" target="#b13">(Reddit, 2015)</ref>. Discourse on Reddit follows a branching pattern, shown in <ref type="figure">Figure 1</ref>. The largest unit of discourse is a thread, beginning with a link to external content or a natu- ral language prompt, posted to a relevant subreddit based on its subject matter. Users comment in re- sponse to the original post (OP), or to any other comment. The result is a structure which splits at many points into more specific or tangential discussions that while locally coherent may dif- fer substantially from each other. The data reflect features of the underlying memory and network structure of the generating process; comments are serially correlated and highly cross-referential. We treat individual comments as "documents" under the standard topic modeling paradigm, but use ob- served reply structure to induce a tree of documents for every thread.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Description of Discursive Distributed Topic Model</head><p>We now introduce the Discursive Distributed Topic Model (DDTM) (illustrated in <ref type="figure">Figure 1</ref>). For each comment in the thread, DDTM assigns a latent vector of binary random variables (or bits) that col- lectively form a distributed embedding of the topi- cal content of that comment; for instance, one bit might represent sarcastic language while another might track usage of specific acronyms -a given comment could have any combination of those fea- tures. These representations are tied to those of parent and child comments via coupling potentials (see Section 2.3), which allow them to learn dis- cursive properties by inducing a deep undirected network over the thread. In order to encourage the model to use these comment-level representa- tions to learn discursive and stylistic patterns as opposed to simply topics of discussion, we incor- porate a single additional latent vector for the entire thread that interacts with each comment, explain- ing word choices that are mainly topical rather than discursive or stylistic. As we demonstrate in our experiments (see Section 6) the thread-level embed- ding learns distributions more reminiscent of what a traditional topic model would uncover, while the comment-level embeddings model styles of speak-ing and mannerisms that do not directly indicate specific subjects of conversation. The joint proba- bility is defined in terms of an energy function that scores latent embeddings and observed word counts across the tree of comments within a thread using log-bilinear potentials, and is globally normalized over all word count and embedding combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Probability Model</head><p>More formally, consider a thread containing N comments each of size D n with a vocabulary of size K. As depicted in <ref type="figure">Figure 1</ref>, each comment is viewed as a bag-of-words, densely connected via a log-bilinear potential to a latent embedding of size F . Let each comment be represented as as an integer vector x n ∈ Z K where x nk is number of times word k was observed in comment n, and let h n = {0, 1} F be the topic embedding for each comment, and let h 0 = {0, 1} F be the embedding for the entire thread. To model topic transitions, we score the embeddings of parent-child pairs with a separate coupling potential as shown in <ref type="figure">Figure 1</ref> (comments with no parents or children receive ad- ditional start/stop biases respectively). Let replies be represented with sets R, P N , and C N where (n, m) ∈ R and n ∈ P m and m ∈ C n if comment m is a reply to comment n. DDTM assigns prob- ability to a specific configuration of x, h with an energy function scored by the emission (π e ) and coupling (π c ) potentials.</p><formula xml:id="formula_0">E(x, h; θ) = N n=1 π e (h, x, n) Emission Potentials + (n,m)∈R π c (h, n, m)</formula><p>Coupling Potentials</p><formula xml:id="formula_1">π e (h, x, n) = h ⊺ n U x n + x ⊺ n a + D n h ⊺ n b + h ⊺ 0 V x n + D n h ⊺ 0 c π c (h, n, m) = h ⊺ n W h m (1)</formula><p>Note that the bias on embeddings is scaled by the number of words in the comment, which controls for their highly variable length. The joint probabil- ity is computed by exponentiating the energy and dividing by a normalizing constant.</p><formula xml:id="formula_2">p(x, h; θ) = exp(E(x, h; θ)) Z(θ) Z(θ) = x ′ ,h ′ exp(E(x ′ , h ′ ; θ))<label>(2)</label></formula><p>This architecture encourages the model to learn discursive maneuvers via the coupling potentials while separating within-thread variance and across- thread variance through the comment-level and thread-level embeddings respectively. The cou- pling of latent variables makes factored inference impossible, meaning that even the exact computa- tion of the partition function is no longer tractable. This necessitates approximating the gradients for learning which we will now address.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning and Inference</head><p>Inference in this model class in intractable, so as has been done in previous work on topic modeling (Ng and Jordan, 2003) we rely on variational meth- ods to approximate the gradients needed during training as well as the posteriors over the topic bit vectors. Specifically, we will need the gradients of the normalizer and the sum of the energy function over the hidden variables</p><formula xml:id="formula_3">E(x; θ) = log h exp(E(x, h; θ))<label>(3)</label></formula><p>which we refer to as the marginal energy. Follow- ing the approach described for undirected models by Eisner (2011), we approximate these quantities and their gradients with respect to the model pa- rameters θ as we will now describe (thread-level embeddings are omitted in this section for clarity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Normalizer Approximation</head><p>We aim to train our model to maximize the marginal likelihood of the observed comment word counts, conditioned on the reply links. To do this we must compute the gradient of the normalizer Z(θ). However, this quantity is computationally intractable, as it contains a summation over all exponential choices for every word in the thread. Therefore, we must approximate Z(θ). Observe that under Jensen's Inequality, we can form the following lower bound on the normalizer using an approximate joint distribution q (Z) .</p><formula xml:id="formula_4">log Z(θ) = log x,h exp(E(x, h; θ)) ≥ E q (Z) [E(x, h; θ)] − E q (Z) [log q (Z) (x, h; φ, γ)]<label>(4)</label></formula><p>We now define q (Z) as depicted in <ref type="figure" target="#fig_2">Figure 2</ref> as a mean-field approximation that treats all vari- ables as independent. We parameterize q (Z) with   Factor graph of full joint compared to mean- field approximations to joint and posterior.</p><p>φ nf ∈ [0, 1], independent Bernoulli parameters representing the probability of h nf being equal to 1, and γ nk replicated softmaxes representing the probability of a word in x n taking the value k. Note that all words in x n are modeled as samples from this single distribution. The approximation then factors as follows:</p><formula xml:id="formula_5">q (Z) (x, h; φ, γ) = q (Z) (x; γ) · q (Z) (h; φ) q (Z) (x; γ) = N n=1 K k=1 (γ nk ) x nk q (Z) (h; φ) = N n=1 F f =1 (φ nf ) h nf (1 − φ nf ) (1−h nf )<label>(5)</label></formula><p>We optimize the parameters of q (Z) to maximize its variational lower bound, via iterative mean-field updates, which allow us to perform coordinate as- cent over the parameters of q (Z) . Maximizing the lower bound with respect to particular φ nf and γ nk while holding all other parameters frozen, yields the following mean-field update equations (biases omitted for clarity):</p><formula xml:id="formula_6">φ n· = σ U γ n + m∈Cn W φ m + φ ⊺ Pn W γ n· = σ (φ ⊺ n U )<label>(6)</label></formula><p>We iterate over the parameters of q (Z) in an "upward-downward" manner; first updating φ for all comments with no children, then all comments whose children have been updated, and so on up to the root of the thread. Then we perform the same updates in reverse order. After updating all φ, we then update γ simultaneously (the components of γ are independent conditioned on φ). We iterate these upward-downward passes until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Marginal Energy Approximation</head><p>We can now approximate the normalizer, but still need the marginal data likelihood in order to take gradient steps on it and train our model. In order to recover the marginal likelihood, we must next approximate the marginal energy E(x; θ) as it too is intractable. This is due to the coupling potentials, which make the topics across comments dependent even when conditioned on the word counts. To do this, we form an additional variational approxima- tion (see <ref type="figure" target="#fig_2">Figure 2)</ref> to the marginal energy, which we optimize similarly.</p><formula xml:id="formula_7">E(x; θ) = log h exp(E(x, h; θ)) ≥ E q (E) [E(x, h; θ)] − E q (E) [log q (E) (h; ψ)]<label>(7)</label></formula><p>Since q (E) (h; ψ) need only model the hidden units h, we can parameterize it in the same man- ner as q (Z) (h; φ). Note that while these distribu- tions factor similarly, they do not share parame- ters, although we find that in practice, initializing φ ← ψ improves our approximation. We optimize the lower bound on E(x; θ) via a similar coordi- nate ascent strategy, where the mean-field updates take the following form (biases omitted for clarity):</p><formula xml:id="formula_8">ψ n· = σ U h n + m∈Cn W ψ m + ψ ⊺ Pn W<label>(8)</label></formula><p>We can use q (E) to perform inference at test time in our model, as its parameters ψ directly corre- spond to the expected values of the hidden topic embeddings under our approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning via Gradient Ascent</head><p>We train the parameters of our true model p(x, h; θ) via stochastic updates wherein we optimize both ap-proximations on a single datum (i.e. thread) to com- pute the approximate gradient of its log-likelihood, and take a single gradient step on the model param- eters (repeating on all training instances until con- vergence). That gradient is given by the difference in feature expectations under the approximations (entropy terms from the lower bounds are dropped as they do not depend on θ).</p><formula xml:id="formula_9">∇ log p(x; θ) ≈ E q (E) (h;ψ) [∇E(x, h; θ)] − E q (Z) (x ′ ,h;ψ) ∇E(x ′ , h; θ)<label>(9)</label></formula><p>In summary, we use two separate mean-field approximations to compute lower bounds on the marginal energy E(x, h; θ), and its normalizer Z(θ), which lets us approximate the marginal like- lihood p(x; θ). Note that as our estimate on the marginal likelihood is the difference between two lower bounds, it is not a lower bound itself, al- though in practice it works well for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Scalability and GPU Implementation</head><p>Given the magnitude of our dataset, it is essen- tial to be able to train efficiently at scale. Many commonly used topic models such as LDA <ref type="bibr" target="#b11">(Ng and Jordan, 2003)</ref> have difficulty scaling, partic- ularly if trained via MCMC methods. Improve- ments have been shown from online training <ref type="bibr" target="#b7">(Hoffman et al., 2010</ref>), but extending such techniques to model comment-to-comment connections and leverage GPU compute is nontrivial.</p><p>In contrast, our proposed model and mean-field procedure can be scaled efficiently to large data because they are amenable to GPU implementation. Specifically, the described inference procedure can be viewed as the output of a neural network. This is because DDTM is globally normalized with edges parameterized as log-bilinear weights, which re- sults in the mean-field updates taking the form of matrix operations followed by nonlinearities. Therefore, a single iteration of mean-field is equiv- alent to a forward pass through a recursive neu- ral network, whose architecture is defined by the tree structure of the thread. Multiple iterations are equivalent to feeding the output of the network back into itself in a recurrent manner, and optimiz- ing for T iterations is achieved by unrolling the network over T timesteps. This property makes DDTM highly amenable to efficient training on a GPU, and allowed us to scale experiments to a dataset of over 13M total Reddit comments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>We mined a corpus of Reddit threads pulled through the platform's API. Focusing on the twenty most popular subreddits (gifs, today- ilearned, CFB, funny, aww, AskReddit, Black- PeopleTwitter, videos, pics, politics, The_Donald, soccer, leagueoflegends, nba, nfl, worldnews, movies, mildlyinteresting, news, gaming) over a one month period yielded 200, 000 threads consist- ing of 13, 276, 455 comments total. The data was preprocessed by removing special characters, re- placing URLs with a domain-specific token, stem- ming English words using a Snowball English Stemmer <ref type="bibr" target="#b12">(Porter, 2001)</ref>, removing stopwords, and truncating the vocabulary to only include the top 10, 000 most common words. OPs are modeled as a comment at the root of each thread to which all top-level comments respond. This dataset will be made available for public use after publication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines and Comparisons</head><p>We compare to baselines of Replicated Softmax (RS) <ref type="bibr" target="#b14">(Salakhutdinov and Hinton, 2009)</ref> and Latent Dirichlet Allocation (LDA) <ref type="bibr" target="#b11">(Ng and Jordan, 2003)</ref>. RS is a distributed topic model similar to our own, albeit without any coupling potentials. LDA is a lo- cally normalized topic model which defines topics as non-overlapping distributions over words. To en- sure that DDTM does not gain an unfair advantage purely by having a larger embedding space, we divide the dimensions equally between comment- and thread-level. Unless specified 64 bits/topics were used. We experiment with RS and LDA treat- ing either comments or full threads as documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training and Initialization</head><p>SGD was performed using the Adam opti- mizer ( <ref type="bibr" target="#b8">Kingma and Ba, 2015)</ref>. When running in- ference, we found convergence was reached in an average of 2 iterations of updates. Using a sin- gle NVIDIA Titan X (Pascal) card, we were able to train our model to convergence on the training set of 10M comments in less than 30 hours. It is worth noting that we found DDTM to be fairly sen- sitive to initialization. We found best results from Gaussian noise, with comment-level emissions at variance of 0.01, thread-level emissions at 0.0001, and transitions at 0. We initialized all biases to 0 except for the bias on word counts, which we set to the unigram log-probabilities from the train set.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluating Perplexity</head><p>We compare models by perplexity on a held-out test set, a standard evaluation for generative and latent variables models.</p><p>Setup: Due to the use of mean-field approxima- tions for both the marginal energy and normalizer we lose any guarantees regarding the accuracy of our likelihood estimate (both approximations are lower bounds, and therefore their difference is nei- ther a strict lower bound nor guaranteed to be unbi- ased). To evaluate perplexity in a more principled way, we use Annealed Importance Sampling (AIS) to estimate the ratio between our model's normal- izer and the tractable normalizer of a base model from which we can draw true independent samples as described by <ref type="bibr" target="#b15">Salakhutdinov and Murray (2008)</ref>. Note that since the marginal energy is intractable in our model, unlike a standard RBM, we must sample the joint -and not the marginal -intermedi- ate distributions. This yields an unbiased estimate of the normalizer. The marginal energy must still be approximated via a lower bound, but given that AIS is unbiased and empirically low in variance, we can treat the overall estimate as a lower bound on likelihood for evaluation. Using 2000 interme- diate distributions, and averaging over 20 runs, we evaluated per-word perplexity over a set of 50 un- seen threads. Results are shown in <ref type="table">Table 1</ref>.</p><p>Results: DDTM achieves the lowest perplexity at all dimensionalities. Note our ablation with the coupling potentials removed (-cpl), increases per- plexity noticeably, indicating that modeling replies helps beyond simply modeling threads and com- ments jointly, particularly at larger embeddings. For reference, a unigram model achieves 2644.  We find that LDA's approximate perplexity is even worse, likely due to slackness in its lower bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Upvote Regression</head><p>To measure how well embeddings capture comment-level characteristics, we feed them into a linear regression model that predicts the number of upvotes the comment received. Upvotes provide a loose human-annotated measure of likability. We expect that context matters in determining how well received a comment is; the same comment posted in response to different parents may receive a very different number of upvotes. Hence, we expect comment-level embeddings to be more informa- tive for this task when connected via our model's coupling potentials. Setup: We trained a standard linear regressor for each model. The regressor was trained using or- dinary least squares on the entire training set of comments using the model's computed topic em- beddings as input, and the number of upvotes on the comment as the output to predict. As a pre- processing step, we took the log of the absolute number of votes before training. We compared models by mean squared error (MSE) on our test set. Results are shown in <ref type="table" target="#tab_4">Table 2</ref>.</p><p>Results: DDTM achieves lowest MSE. To assess statistical significance, we performed a 500 sample bootstrap of our training set. The standard errors of these replications are small, and a two-sample t-test rejects the null hypothesis that DDTM has an average MSE equal to that of the next best method (p &lt; .001). Note that our model outperforms both comment-and thread-level embeddings, suggesting that modeling these jointly, and modeling the ef- fect of neighboring representations in the comment graph, more accurately learns information relevant to a comment's social impact. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Deletion Prediction</head><p>Comments that are excessively provocative or in violation of site rules are often deleted, either by the author or a moderator. We can measure whether DDTM captures discursive interactions that lead to such intervention by training a logistic classifier that predicts whether any of a given comment's children have been deleted. Setup: For each model, a logistic regression classi- fier was trained stochastically with the Adam opti- mizer on the entire training set of comments using the model's computed topic embeddings as input, and a binary label for whether the comment had any deleted children as the output to predict. We com- pared models by accuracy on our test set. Results are shown in <ref type="table" target="#tab_4">Table 2</ref>.</p><p>Results: DDTM gets the highest accuracy. In- terestingly, thread-level models do better than comment-level ones, which suggests that certain topics or even subreddits may correlate with com- ments being deleted. This makes sense given that subreddits vary in severity of moderation. DDTM's performance also demonstrates that mod- eling comment-to-comment interaction patterns is helpful in predicting when a comment will spawn a deleted future response, which strongly matches our intuition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Document Retrieval</head><p>Finally, while DDTM is not designed to better cap- ture topical structure, we evaluate the extent to which it can still capture this information by per- forming document retrieval, a standard evaluation, for which we treat the subreddit to which a thread was posted as a label for relevance. Note that every comment within the same thread belongs to the same subreddit, which gives thread-level models an inherent advantage at this task. We include this task purely for the purpose of demonstrating that by capturing discursive patterns, DDTM does not lose the ability to model thread-level topics as well.</p><p>Setup: Given a query comment from our held-out test set, we rank the training set by the Dice simi- larity of the hidden embeddings computed by the model. We consider a retrieved comment relevant to the query if they both originate from the same subreddit, which loosely categorizes the seman- tic content. Tuning the number of documents we return allows us to form precision recall curves, which we show in <ref type="figure" target="#fig_4">Figure 3</ref>. Results: DDTM outperforms both comment-level baselines and is competitive with thread-level mod- els, even beating LDA at high levels of recall. This indicates that despite using half of its dimensions to model comment-to-comment interactions DDTM can still do almost as good a job of modeling thread- level semantics as a model using its entire capacity</p><note type="other">Bit # Associated Word Stems by Emission Weight (Higher Score → Lower Score)</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comment-Level</head><p>Bit 1 faq tldrs pms 165 til keyword questions feedback chat pm 2 irl riamverysmart legend omfg riski aboard favr madman skillset tunnel 3 lotta brah ouch spici oof bummer buildup viewership hd uncanni 4 funniest mah tfw teleport fav hoo plz bah whyd dumbest 5 handsom hipster texan hottest whore norwegian shittier scandinavian jealousi douch</p><p>Thread-Level Bit 1 btc gameplay tutori cyclist dev currenc kitti bitcoin rpg crypto 2 url_youtu url_leagueoflegends url_businessinsider url_twitter url_redd url_snopes 3 comey pede macron pg13 maga globalist ucf committe cuck distributor 4 maduro venezuelan ballot puerto catalonia rican quak skateboard venezuela quebec 5 nra scotus opioid cheney nevada metallica marijuana vermont colorado xanax <ref type="table">Table 3</ref>: Words with the highest emission weight for various comment-level and thread-level bits.</p><p>to do so. The gap between comment-level RS and LDA is also consistent with LDA's known issues dealing with sparse data <ref type="bibr" target="#b16">(Sridhar, 2015)</ref>, and lends credence to our theory that distributed topic repre- sentations are better suited to such domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Qualitative Analysis of Topics</head><p>We now offer qualitative analysis of the topic em- beddings learned by our model. Note that since we use distributed embeddings, our bits are more akin to filters than complete distributions over words, and we typically observe as many as half of them active for a single comment. In a sense, we have an exponential number of topics, whose parame- terization simply factors over the bits. Therefore, it can be difficult to interpret them as one would interpret topics learned by a model such as LDA. Furthermore, we find that in practice this effect is correlated with the topic embedding size; the more bits our model has, the less sparse and con- sequently less individually meaningful the bits be- come. Therefore for this analysis, we specifically focus on DDTM trained with 64 bits total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Bits in Isolation</head><p>Directly inspecting the emission parameters, re- veals that the comment-level and thread-level halves of our embeddings capture substantially different aspects of the data (shown in <ref type="table">Table 3</ref>) akin to vertical, within-thread, and horizontal, across-thread sources of variance respectively. The comment-level topic bits tend to reflect styles of speaking, lingo, and memes that are not unique to a particular subject of discourse or even subreddit. For example, comment-level Bit 2 captures many words typical of taunting Reddit comments; reply- ing with "/r/iamverysmart" (a subreddit dedicated to mocking people who make grandiose claims about their intellect) is a common way of jokingly implying that the author of the parent comment takes themselves too seriously -and thus corre- sponds to a certain kind of rhetorical move. Further, it is grouped with other words that indicate related rhetorical moves; calling a user "risky" or a "mad- man" is a common means of suggesting that they are engaging in a pointless act of rebellion. They also cluster at the coarsest level by length (see <ref type="figure">Fig- ure 5</ref>) which we find to correlate with writing style. By contrast, the thread-level bits are more in- dicative of specific topics of discussion, and unsur- prisingly they cluster by subreddit (see <ref type="figure" target="#fig_5">Figure 4</ref>). For example, thread-level Bit 3 captures lexicon used almost exclusively by alt-right Donald Trump supporters as well as the names of various politi- cal figures. Bit 4 highlights words related to civil unrest in Spanish speaking parts of the world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Bits in Combination</head><p>While these distributions over words (particularly for comment-level bits) can seem vague, when mul- tiple bits are active, their effects compound to pro- duce much more specific topics. One can think of the bits as defining soft filters over the space of words, that when stacked together carve out pat- terns not apparent in any of them individually. We now analyze a few sample topic embeddings. To do this, we perform inference as described on a held-out thread, and pass the comment-level topic embedding for a single sampled comment through our emission matrix and inspect the words with the highest corresponding weight (shown in <ref type="table">Table 4</ref>). In generative terminology, these can be thought of as reconstructions of comments.</p><p>These topic embeddings capture more specific conversational and rhetorical moves. For example, Sample # Associated Word Stems by Emission Weight (Higher Score → Lower Score)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comment-Level</head><p>Sample 1 grade grader math age 5th 9th 10th till mayb 7th 2 repost damn dope bamboozl shitload imagin cutest sad legendari awhil 3 heh dawg hmm spooki buddi aye m8 aww fam woah 4 hug merci bless tfw prayer pleas dear bear banana satan 5 chuckl cutest funniest yall bummer oooh mustv coolest ok oop 6 cutest heard coolest funniest havent seen ive craziest stupidest weirdest 7 reev keanu christoph murphi walken vincent chris til wick roger 8 moron douchebag stupid dipshit snitch jackass dickhead idioci hypocrit riddanc 9 technic actual realiz happen escal werent citat practic memo cba 10 reddit shill question background user subreddit answer relev discord guild <ref type="table">Table 4</ref>: Words with the highest emission weight for sample held-out comment reconstructions.</p><p>Sample 6 displays supportive and interested reac- tionary language, which one might expect to see used in response to a post or comment linking to media or describing something intriguing. This is of note given that one of the primary aims of includ- ing coupling potentials was to encourage DDTM to learn "topics" that correspond to responses and interactive behavior, something existing methods are largely not designed for. By contrast, Sample 9 captures a variety of hostile language and insults, which unlike those discussed previously do not de- note membership in a particular online community. As patterns of toxic and hateful behavior on Red- dit are more well-studied ( <ref type="bibr" target="#b2">Chandrasekharan et al., 2017)</ref>, it could be useful to have a tool to analyze precipitous contexts and parent comments, some- thing which we hope systems based on coupling of comment embeddings have the capacity to pro- vide. Sample 10 is of particular interest as it con- sists largely of Reddit terminology. Conversations about the meta of the site can manifest for example in users accusing each other of being "shills" (i.e. accounts paid to astroturf on behalf of external in- terests) or requesting/responding to "guilding", a feature which lets users purchase premium access for each other often in response to a particularly well made comment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Many topic models such as LDA <ref type="bibr" target="#b11">(Ng and Jordan, 2003</ref>) treat documents as independent mixtures, yet this approach fails to model how comments inter- act with one another throughout a larger discourse if such connections exist in the data. Other work has considered modeling hierarchy in topics ( <ref type="bibr" target="#b5">Griffiths et al., 2004</ref>). These models form hierarchical representations of topics themselves, but still treat documents as independent. While this approach can succeed in learning topics of various granulari- ties, it does not explicitly track how topics interact in the context of a nested conversation. Some approaches such as Pairwise-Link-LDA and Link-PSLA-LDA ( <ref type="bibr" target="#b10">Nallapati et al., 2008)</ref> at- tempt to model interactions among documents in an arbitrary graph, albeit with important drawbacks. The former models every possible pairwise link between comments, and the latter models links as a bipartite graph, limiting its ability to scale to large tree-structured threads. Similar work on Topic-Link LDA ( <ref type="bibr" target="#b9">Liu et al., 2009</ref>) models link probabilities conditioned on both topic similar- ity and an authorship model, yet this approach is poorly suited to high volume, semi-anonymous on- line domains. Other studies have leveraged reply- structures on Reddit in the context of predicting persuasion (Hidey and McKeown), but DDTM dif- fers in its generative, unsupervised approach. DDTM's emission potentials are similar to those of Replicated Softmax <ref type="bibr" target="#b14">(Salakhutdinov and Hinton, 2009</ref>), an undirected model based on a Restricted Boltzmann Machine. Unlike LDA-style models, RS does not assign a topic to each word, but instead builds a distributed representation. In this setting, a single word can be likely under two different topics, both of which are present, and lend probability mass to that word. LDA-style models by contrast would require the topics to compete for the word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper we introduce a novel way to learn topic interactions in observed discourse trees, and describe GPU-amenable learning techniques to train on large-scale data mined from Reddit. We demonstrate improvements over previous models on perplexity and downstream tasks, and offer qual- itative analysis of learned discursive patterns. The dichotomy between the two levels of embeddings hints at applications in style-transfer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Perplexity</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Precision vs. recall for document retrieval based on subreddit comparing various models for 1000 randomly selected held-out query comments.</figDesc><graphic url="image-1.png" coords="7,72.00,62.81,226.77,164.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: t-SNE visualization of a random sample of DDTM thread-level embeddings colored by subreddit (not observed in training)</figDesc><graphic url="image-2.png" coords="7,348.38,62.81,136.06,137.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>í± í± í±¥, ℎ; í¼, í»¾ ℎ í± í±¥ í± í±¼ í±¾ í±¾ í±¥, ℎ; í¼ í½ í², í½ í², í½ í², í½ í², í±¥ í± í¼¸í²í¼¸í² ℎ í± í°¸ℎí°¸ℎ; í¼ í½ í², í½ í², í½ í², í½ í²,</head><label></label><figDesc></figDesc><table>Joint Probability 
Variational Approx. to Joint 
Variational Approx. to Post. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance of DDTM vs. Replicated 
Softmax (RS) and Latent Dirichlet Allocation 
(LDA) at predicting upvotes and child deletion. 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Reddit.com traffic, demographics and competitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexa</forename></persName>
		</author>
		<ptr target="https://www.alexa.com/siteinfo/reddit.com.Accessed" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2018" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><forename type="middle">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Spang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dedeo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06867</idno>
		<title level="m">Individuals, institutions, and innovation in the debates of the french revolution</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">You can&apos;t stay here: The efficacy of reddit&apos;s 2015 ban examined through hate speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eshwar</forename><surname>Chandrasekharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umashanthi</forename><surname>Pavalanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Glynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on HumanComputer Interaction</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2017" />
			<publisher>CSCW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Sequential latent dirichlet allocation. Knowledge and information systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wray</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huidong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="475" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">High-level explanation of variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<ptr target="https://www.cs.jhu.edu/~jason/tutorials/variational.html.Accessed" />
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2018" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical topic models and the nested chinese restaurant process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas L Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Persuasive influence detection: The role of argument sequencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hidey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Online learning for latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Topic-link lda: joint models of topic and author community</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Gryc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="665" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint latent topic models for text and citations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ramesh M Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="542" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Snowball: A language for stem-ming algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Porter</surname></persName>
		</author>
		<ptr target="http://snowball.tartarus.org/texts.Accessed" />
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="2018" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reddit</surname></persName>
		</author>
		<ptr target="https://redditblog.com/2015/12/31/reddit-in-2015/.Accessed" />
		<title level="m">Reddit in 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2018" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Replicated softmax: An undirected topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1607" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the quantitative analysis of deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised topic modeling for short texts using distributed representations of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Kumar Rangarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sridhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing</title>
		<meeting>the 1st Workshop on Vector Space Modeling for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="192" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Comparing twitter and traditional media using topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="338" to="349" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
