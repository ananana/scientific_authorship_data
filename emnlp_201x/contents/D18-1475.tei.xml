<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling Localness for Self-Attention Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosong</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Tencent AI Lab</orgName>
								<orgName type="department" key="dep2">Tencent AI Lab</orgName>
								<orgName type="department" key="dep3">Tencent AI Lab</orgName>
								<orgName type="institution" key="instit1">University of Macau</orgName>
								<orgName type="institution" key="instit2">University of Macau</orgName>
								<orgName type="institution" key="instit3">University of Macau</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
							<email>zptu@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Tencent AI Lab</orgName>
								<orgName type="department" key="dep2">Tencent AI Lab</orgName>
								<orgName type="department" key="dep3">Tencent AI Lab</orgName>
								<orgName type="institution" key="instit1">University of Macau</orgName>
								<orgName type="institution" key="instit2">University of Macau</orgName>
								<orgName type="institution" key="instit3">University of Macau</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
							<email>derekfw@umac.mo</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Tencent AI Lab</orgName>
								<orgName type="department" key="dep2">Tencent AI Lab</orgName>
								<orgName type="department" key="dep3">Tencent AI Lab</orgName>
								<orgName type="institution" key="instit1">University of Macau</orgName>
								<orgName type="institution" key="instit2">University of Macau</orgName>
								<orgName type="institution" key="instit3">University of Macau</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
							<email>fandongmeng@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Tencent AI Lab</orgName>
								<orgName type="department" key="dep2">Tencent AI Lab</orgName>
								<orgName type="department" key="dep3">Tencent AI Lab</orgName>
								<orgName type="institution" key="instit1">University of Macau</orgName>
								<orgName type="institution" key="instit2">University of Macau</orgName>
								<orgName type="institution" key="instit3">University of Macau</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
							<email>lidiasc@umac.mo</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Tencent AI Lab</orgName>
								<orgName type="department" key="dep2">Tencent AI Lab</orgName>
								<orgName type="department" key="dep3">Tencent AI Lab</orgName>
								<orgName type="institution" key="instit1">University of Macau</orgName>
								<orgName type="institution" key="instit2">University of Macau</orgName>
								<orgName type="institution" key="instit3">University of Macau</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Tencent AI Lab</orgName>
								<orgName type="department" key="dep2">Tencent AI Lab</orgName>
								<orgName type="department" key="dep3">Tencent AI Lab</orgName>
								<orgName type="institution" key="instit1">University of Macau</orgName>
								<orgName type="institution" key="instit2">University of Macau</orgName>
								<orgName type="institution" key="instit3">University of Macau</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling Localness for Self-Attention Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4449" to="4458"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4449</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Self-attention networks have proven to be of profound value for its strength of capturing global dependencies. In this work, we propose to model localness for self-attention networks, which enhances the ability of capturing useful local context. We cast localness modeling as a learnable Gaussian bias, which indicates the central and scope of the local region to be paid more attention. The bias is then incorporated into the original attention distribution to form a revised distribution. To maintain the strength of capturing long distance dependencies and enhance the ability of capturing short-range dependencies, we only apply localness modeling to lower layers of self-attention networks. Quantitative and qualitative analyses on Chinese⇒English and English⇒German translation tasks demonstrate the effectiveness and universality of the proposed approach.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, a new simple architecture, the TRANS- FORMER ( <ref type="bibr" target="#b19">Vaswani et al., 2017)</ref>, that based solely on attention mechanisms has attracted increas- ing attention in machine translation community. Instead of using complex recurrent or convolu- tional neural networks, TRANSFORMER imple- ments encoder and decoder as self-attention net- works to draw global dependencies between input and output. By further parallel performing (multi- head) and stacking (multi-layer) attentive func- tions, TRANSFORMER has achieved state-of-the- art performance on various translation tasks ( <ref type="bibr" target="#b14">Shaw et al., 2018;</ref><ref type="bibr" target="#b5">Hassan et al., 2018)</ref>.</p><p>One strong point of self-attention networks is the strength of capturing long-range dependencies by explicitly attending to all the signals. In this * Zhaopeng Tu and Derek F. Wong are the co- corresponding authors of the paper. This work was conducted when Baosong Yang was interning at <ref type="bibr">Tencent AI Lab.</ref> way, a representation is allowed to build a direct relation with another long-distance representation. Accordingly, it can serve as the role of RNN and CNN to capture both the short-and long-range re- lations among the representations.</p><p>Self-attention networks fully take into account all the signals with a weighted averaging opera- tion. We argue that such operation disperses the distribution of attention, which results in over- looking the relation of neighboring signals. Re- cent works have shown that self-attention net- works benefit from locality modeling. For ex- ample, <ref type="bibr" target="#b14">Shaw et al. (2018)</ref> introduced relative position encoding to consider the relative dis- tances between sequence elements, which pro- duces substantial improvements on the translation task. <ref type="bibr" target="#b18">Sperber et al. (2018)</ref> modeled the local in- formation by restricting self-attention model to neighboring representations, which boosts perfor- mance on long-sequence acoustic modeling. Al- though not for self-attention, <ref type="bibr" target="#b10">Luong et al. (2015)</ref> proposed a local attention model for translation task, which looks at only a subset of source words at a time. Inspired by these studies, we propose more flexible strategies for modeling localness for self-attention networks in this work.</p><p>Specifically, we cast the localness modeling as a learnable Gaussian bias, in which a central po- sition (i.e. mean of the position) and a dynamic window (i.e. deviation of the distribution) are pre- dicted with the intermediate representations in the self-attention network. Intuitively, the central po- sition and the window respectively denote the cen- ter and the scope of the locality to be paid more attention. The learned Gaussian bias is then in- corporated into the original attention distribution to form a revised distribution, which considers the expected local context. Some researchers may doubt that self-attention networks augmented localness modeling focuses leanings toward local context, which weakens its strength of capturing long-range dependencies. Our extensive analyses can dispel such doubt by showing that the potential problem is compen- sated by multi-layer multi-head self-attention net- works. First, multi-head attention attends to lo- cal regions centered at different positions, which can constitute the complete information of an in- put sequence. Second, we found that self-attention models tend to capture short-range dependencies among neighboring words in lower layers, while capture long-range dependencies beyond phrase boundaries in higher layers. Accordingly, we only apply localness modeling to lower layers.</p><p>We conducted experiments on two widely- used WMT14 English⇒German and WMT17 Chinese⇒English translation tasks. The proposed approach consistently improves translation perfor- mance over the strong TRANSFORMER baseline, demonstrating its effectiveness and universality. In addition, our approach is complementary to the relative position encoding ( <ref type="bibr" target="#b14">Shaw et al., 2018)</ref>, and combining them can further improve translation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Attention model has recently been a basic module of most deep learning models. The mechanism al- lows to dynamically select related representations as needed. In particular, it is very useful for gen- eration models such as machine translation <ref type="bibr" target="#b1">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b10">Luong et al., 2015;</ref> and image captioning ( <ref type="bibr" target="#b23">Xu et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Self-Attention Model</head><p>Recently, self-attention networks ( <ref type="bibr" target="#b19">Vaswani et al., 2017;</ref><ref type="bibr" target="#b14">Shaw et al., 2018;</ref><ref type="bibr" target="#b15">Shen et al., 2018a</ref>) have attracted increasing attention due to their flexibil- ity in parallel computation and dependency mod- eling. Self-attention networks calculate attention weights between each pair of tokens in a single sequence, thus can capture long-range dependency more directly than their RNN counterpart.</p><p>Formally, given an input sequence x = {x 1 , . . . , x I }, each hidden state in the l-th layer is constructed by attending to the states in the (l − 1)-th layer. 1 Specifically, the (l − 1)-th layer H l−1 ∈ R I×d is first transformed into the queries Q ∈ R I×d , the keys K ∈ R I×d , and the values V ∈ R I×d with three separate weight matrices. <ref type="bibr">1</ref> The first layer is the word embedding layer.</p><p>The l-th layer is calculated as:</p><formula xml:id="formula_0">H l = ATT(Q, K) V ,<label>(1)</label></formula><p>where ATT(·) is a dot-product attention model, de- fined as:</p><formula xml:id="formula_1">ATT(Q, K) = sof tmax(energy) (2) energy = QK T √ d ,<label>(3)</label></formula><p>where √ d is the scaling factor with d being the dimensionality of layer states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Motivation</head><p>The self-attention network models the global de- pendencies without regard to their distances, by directly attending to all the positions in an input sequence (i.e. Equation 3). We argue that self- attention can be further improved by taking into account the local context. However, since the con- ventional self-attention models consider all of the words in a sequence, the weighted averaging in- hibits the relation among the neighboring words.</p><p>From a linguistic intuition, when a word x i is aligned to another word x j , we also expect x i to align mainly to the neighboring words of x j , so as to capture the phrasal patterns that contain use- ful local context information. Take <ref type="figure" target="#fig_0">Figure 1</ref> as an example, if "Bush" is aligned to "held" with high probability, we expect the self-attention model to pay more attention to the neighboring words "a talk". Consequently, the model is guided to cap- ture the phrase "held a talk". <ref type="figure" target="#fig_0">Figure 1</ref> shows an example. We first learn a Gaus- sian bias, which is centered around the word "talk" (it is not necessary to be consistent with the orig- inal attention distribution), with a window size being 2 (in practice, it is a float number in our model). The distribution of attention is then regu- larized with the learned Gaussian bias to produce the final distribution, which pays more attention to the local context around the word "talk".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Localness Modeling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Localness Modeling as a Gaussian Bias</head><p>Specifically, a Gaussian bias G is placed to mask the logit similarity energy in Equation 2, namely:</p><formula xml:id="formula_2">ATT(Q, K) = sof tmax(energy + G). (4)</formula><p>The first term is the original dot product self- attention model. G ∈ R I×I is a favor alignment </p><formula xml:id="formula_3">G i,j = − (j − P i ) 2 2σ i 2 ,<label>(5)</label></formula><p>where σ i denotes the standard deviation which is empirically set as σ i = D i 2 , and D i is a window size. Note that, due to the exponential operation in sof tmax function, adding the logit similarity energy with a bias ∈ [0, −∞) approximates to multiplying the attention distribution by a weight ∈ [1, 0). The position and window size can be calculated as:</p><formula xml:id="formula_4">P i D i = I · sigmoid( p i z i ).<label>(6)</label></formula><p>The scalar factor I is used to regulate P i and D i to real value numbers between 0 and the length of input sequence. The predictions are conditioned on two scalar p i and z i respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Central Position Prediction</head><p>Since the prediction of each central position de- pends on its corresponding query vector, 2 we sim- ply apply a feed-forward network to transform Q i into a positional hidden state, which is then mapped into the scalar p i by a linear projection U p ∈ R d , namely:</p><formula xml:id="formula_5">p i = U p T tanh(W p Q i ),<label>(7)</label></formula><p>where W p ∈ R d×d is the model parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Window Size Prediction</head><p>Several alternative strategies are proposed to se- lect the window size. Except a non-parametric ap- proach, the other two define parametric windows.</p><p>Fixed-Window A simple choice is to use a pre- defined window size D, which is a constant num- ber throughout the whole training and testing pro- cess. In this study, following the common practice ( <ref type="bibr" target="#b10">Luong et al., 2015)</ref>, D is set to 10.</p><p>Layer-Specific Window Furthermore, an inter- pretable way to select the window size is to ac- count for the context of the sequence by summa- rizing the information from all the representations in a layer. In this study, we assign the mean of keys K to represent the semantic context. Thus, the unified scalar z of a layer is defined as:</p><formula xml:id="formula_6">z = U T d tanh(W d K),<label>(8)</label></formula><p>where</p><formula xml:id="formula_7">W d ∈ R d×d and U d ∈ R d are learnable parameters.</formula><p>Query-Specific Window The last strategy pro- vides a more flexible manner to differentiate the scope by conditioning on each query. Similar to the prediction of the central position (Equation 7), the query-specific window can be formally ex- pressed as:</p><formula xml:id="formula_8">z i = U d T tanh(W p Q i ).<label>(9)</label></formula><p>Here, U d ∈ R d is a trainable linear projection. Note that, Equations 7 and 9 share same param- eter W p but use different U p and U d . The intu- ition behind this design is that the central position and window size interdependently locate the local scope, hence condition on the same hidden state. The distinct linear projections U p and U d are suffi- cient in distinguishing the two scalars, resulting in a smaller parameter size and faster computational speed than that of the layer-specific model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Incorporating into TRANSFORMER</head><p>We evaluate our model on the advanced TRANS- FORMER model ( <ref type="bibr" target="#b19">Vaswani et al., 2017)</ref>, which builds an encoder-decoder framework merely us- ing attention networks. Both the encoder and de- coder are composed of a stack of L = 6 layers, each of which has two sub-layers. The first is a multi-head self-attention layer, and the second is a position-wise fully connected feed-forward layer.</p><p>In this section, we describe how to apply our ap- proach to TRANSFORMER by adapting to multi- head and multi-layer self-attention networks.</p><p>Adapting to Multi-Head Self-Attention In- stead of performing a single attention function, the multi-head mechanism employs M separate atten- tion models with distinct parameters to jointly at- tend to the information from different representa- tion subspaces at different positions. Accordingly, we assign a distinct Gaussian bias to each attention head, and rewrite Equation 6 as:</p><formula xml:id="formula_9">P m i D m i = I · sigmoid( p m i z m i ),<label>(10)</label></formula><p>where p m i and z m i are trained with distinct parame- ters to predict the central position and window size for the m-th attention head.</p><p>We argue that multi-head self-attention may benefit more from localness modeling. Multi-head attention captures different features by attending to different positions, which complements the lo- calness modeling that may potentially ignore the global information. Experimental results in Ta- ble 5 confirm our hypothesis by showing that lo- calness modeling achieves more significant im- provement when working with multi-head atten- tion than its single-head counterpart.</p><p>Adapting to Multi-Layer Self-Attention Re- cent work shows that different layers capture dif- ferent types of features.</p><p>Anastasopoulos and Chiang (2018) indicated that higher-level layers are more representative than lower-level layers, while <ref type="bibr" target="#b12">Peters et al. (2018)</ref> showed that higher-level layers capture context-dependent aspects of word meaning while lower-level layers model aspects of syntax. One question naturally arises: is it neces- sary to model localness for all layers?</p><p>In this work, we investigate which levels of lay- ers benefit most from the localness modeling. In addition, we visualize the Gaussian biases across layers, to better understand the behaviors of differ- ent attentive layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>To compare with the results reported by previ- ous work <ref type="bibr" target="#b4">(Gehring et al., 2017;</ref><ref type="bibr" target="#b19">Vaswani et al., 2017;</ref><ref type="bibr" target="#b5">Hassan et al., 2018)</ref>, we conducted exper- iments on both Chinese⇒English (Zh⇒En) and English⇒German (En⇒De) translation tasks. For the Zh⇒En task, the models were trained using all of the available parallel corpus from WMT17 dataset with maximum length limited to 50, con- sisting of about 20.62 million sentence pairs. We used newsdev2017 as the development set and newstest2017 as the test set. For the En⇒De task, we trained on the widely-used WMT14 dataset consisting of about 4.56 million sentence pairs. The models were validated on newstest2013 and examined on newstest2014. The Chinese sen- tences were segmented by the word segmentation toolkit Jieba, <ref type="bibr">3</ref> and the English and German sen- tences were tokenized using the scripts provided in Moses. Then, all tokenized sentences were pro- cessed by byte-pair encoding (BPE) to alleviate the Out-of-Vocabulary problem ( <ref type="bibr" target="#b13">Sennrich et al., 2016</ref>) with 32K merge operations for both lan- guage pairs. The 4-gram NIST BLEU score <ref type="bibr" target="#b11">(Papineni et al., 2002</ref>) is used as the evaluation metric.</p><p>We evaluated the proposed approaches on ad- vanced TRANSFORMER model ( <ref type="bibr" target="#b19">Vaswani et al., 2017)</ref>, and implemented on top of an open-source toolkit -THUMT 4 ( ). We fol- lowed <ref type="bibr" target="#b19">Vaswani et al. (2017)</ref> to set the configu- rations and reproduced their reported results on the En⇒De task. We tested both the Base and Big models, which differ at the layer size (512 vs. 1024) and the number of attention heads <ref type="bibr">(8 vs. 16</ref>). All the models were trained on eight NVIDIA P40 GPUs, each of which is allocated a batch of 4096 tokens. In consideration of the computation cost, we studied the variations of the Base model on Zh⇒En task, and evaluated the overall perfor- mance with the Big model on both Zh⇒En and En⇒De translation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>In the first series of experiments, we evaluated the impact of different components on the Zh⇒En validation set using the TRANSFORMER-BASE. First, we investigated the effect of different strate- gies to predict the localness window. Then, we</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Speed Dev Baseline 1.20 22.59 - Fixed 1.14 23.07 + 0.48 Layer-Spec.</p><p>1.07 23.13 + 0.54 Query-Spec.</p><p>1.11 23.13 + 0.54 <ref type="table">Table 1</ref>: Evaluation of various window predic- tion strategies for localness modeling, which is only applied to encoder-side self-attention net- work. "Speed" denotes training speed measured in steps per second.</p><p>examined whether it is necessary to apply local- ness modeling to all the layers. Finally, given that TRANSFORMER consists of encoder and decoder side self-attention as well as encoder-decoder at- tention networks, we checked which types of at- tention networks benefit most from the localness modeling. To eliminate the influence of control variables, we conducted the first two ablation stud- ies on encoder-side self-attention networks only. <ref type="table">Table 1</ref>, all the proposed window prediction strategies consistently improve the model perfor- mance over the baseline, validating the impor- tance of localness modeling in self-attention net- works. Among them, layer-specific and query- specific window outperform 5 their fixed counter- part, showing the benefit that flexible mechanism is able to capture varying local context accord- ing to layer and query information. Moreover, the flexible strategy does not reply on the hand- crafted parameters (e.g. the pre-defined window size), which makes model robustly applicable to other language pairs and NLP tasks. Considering the training speed, we use the query-specific pre- diction mechanism as the default setting in subse- quent experiments.   <ref type="table">Table 3</ref>: Effect of localness modeling on dif- ferent types of attention networks. "Enc" and "Dec" denote the encoder and decoder side self- attention networks respectively, while "Enc-Dec" represents the encoder-decoder attention network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Window Prediction Strategies As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layers to be Applied</head><p>modeling to different combinations of layers, as shown in <ref type="table" target="#tab_1">Table 2</ref>. Clearly, modeling the localness for part of the layers consistently outperforms all layers in terms of the training speed and transla- tion quality, which again validates our claim. Interestingly, the performance generally goes up with the increase of layers from bottom to top (Rows 2-4), while the trend does not hold when reaching the 4th-layer (Row 5). In addition, the lower three layers benefit more from the local- ness modeling than that of the higher three layers (Rows 4 and 6). These results reveal that lower- level layers benefit more from the local context. Accordingly, we only model the localness in the lower three layers in the following experiments.</p><p>Attention Networks to be Applied <ref type="table">Table 3</ref> lists the results of localness modeling on dif- ferent types of attention networks.</p><p>As ob- served, modeling localness for decoder-side self- attention and encoder-decoder attention networks only marginally improves or even harms the trans- lation quality. We attribute the marginal improve- ment of the encoder-decoder attention network to the fact that it exploits the top-layer of en- coder representations, which already embeds use- ful local context. Concerning decoder-side self- attention network,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>pointed out</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Architecture Zh⇒En En⇒De # Para. BLEU # Para. BLEU Existing NMT systems ( <ref type="bibr" target="#b21">Wu et al., 2016)</ref> GNMT n/a n/a n/a 26.30 <ref type="bibr" target="#b4">(Gehring et al., 2017</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>) CONVS2S</head><p>n/a n/a n/a 26.36 <ref type="bibr" target="#b19">(Vaswani et al., 2017</ref>) TRANSFORMER-BASE n/a n/a 65M 27.3 TRANSFORMER-BIG n/a n/a 213M 28.4 <ref type="bibr" target="#b5">(Hassan et al., 2018)</ref> TRANSFORMER-BIG n/a 24.2 n/a n/a</p><p>Our NMT systems this work  Comparing with the existing NMT systems on WMT17 Zh⇒En and WMT14 En⇒De test sets. "# Para." denotes the trainable parameter size of each model (M = million). "↑ / ⇑": significant over the conventional self-attention counterpart (p &lt; 0.05/0.01), tested by bootstrap resampling <ref type="bibr" target="#b8">(Koehn, 2004)</ref>.</p><p>that it tends to only focus on its nearby repre- sentation, which poses difficulties to modeling lo- calness on the decoder side. In the main experi- ments, we only applied localness modeling to the lower three layers of the encoder, which employs a query-specific window prediction strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head><p>In this section, we evaluated the proposed ap- proach on both WMT17 Zh⇒En and WMT14 En⇒De translation tasks, as listed in <ref type="table" target="#tab_3">Table 4</ref>. Our baseline models, both TRANSFORMER-BASE and TRANSFORMER-BIG, outperform the reported re- sults on the same data, which we believe make the evaluation convincing. As seen, modeling local- ness ("Localness") consistently achieves improve- ment across language pairs and model variations, demonstrating the efficiency and universality of the proposed approach. We also re-implemented the relative posi- tion encoding ("Rel Pos") that recently proposed by <ref type="bibr" target="#b14">Shaw et al. (2018)</ref>, which considers the rela- tive distances between sequence elements. Both <ref type="bibr" target="#b14">Shaw et al. (2018)</ref> and our work have shown that explicitly modeling locality for self-attention net- works can improve the model performance. This indicates that it is necessary to enhance the locality modeling for Transformer. Besides, our approach is complementary to theirs, and combining them is able to further improve the translation perfor- mance. We attribute this to the fact that the two models modeling localness from two different as- pects: First, the position embeddings are the same across different positions (if the absolute positions or relative positions are the same) and training ex- amples, our model assigns a distinct localness bias to each position from layer to layer. Second, con- trast to position encoding which learns the locality through the positional information in embeddings, our model directly revises the attention distribu- tion to focus on a local space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>We conducted extensive analyses to better under- stand our model in terms of its compatibility with multi-head and multi-layer attention networks, as well as building the ability of capturing phrasal patterns. All the results are reported on Zh⇒En development set with TRANSFORMER-BASE, un- less otherwise stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Compatibility with Multi-Head Attention</head><p>In this section, we investigated whether multi-head attention and localness modeling are compatible from two perspectives: (1) whether multi-head at- tention benefits more from the localness modeling than its single-head counterpart; and (2) how does multi-head attention work together with localness modeling?   <ref type="table">Table 5</ref>: Evaluation of localness modeling on top of single and multiple attention heads.</p><p>Multi-Head vs. Single-Head The single-head attention and multi-head attention differ at: the former uses a single 512-dimension attention head while the latter uses eight 64-dimension heads. The results in <ref type="table">Table 5</ref> confirm our claim by showing that multi-head attention indeed benefits more from our model than the single-head model (+0.70 vs. +0.13). It should be noted that our model marginally improves the performance un- der single-head setting. One possible reason is that our model focuses more on local context thus may ignore global information, which cannot be com- plemented by the single-head attention. Can Multi-Head Separate Locality? To sim- plistically visualize how heads cooperate in mod- eling localness, we propose an additional paramet- ric model which is assigned a learnable but unified window size for each head, namely head-specific. As a result, the window size D m of the m-th head is calculated as:</p><formula xml:id="formula_10">D m = N · sigmoid(z m ),<label>(11)</label></formula><p>where the scalar z m is a trainable parameter, N = 50 denotes a pre-defined constant number. <ref type="figure" target="#fig_2">Figure 2</ref> visualizes the distribution of the learned window size of each head, verifying that multi-head attention is able to capture diverse in- formation by selecting suitable window sizes for different heads. For example, in the middle-level layers, heads are assigned to consider both the global and local information by regulating the dif- ferent window sizes. One interesting finding is that the distributions of window size are not ex- actly same in different layers, which is explored in more details in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analysis on Multi-Layer Attention</head><p>In this section, we try to answer how does each layer learn the localness. We first investigated how the window size varies across layers. Then we checked the specific behavior of the first word em- bedding layer, which is inconsistent with the trend of other layers.</p><p>The Higher Layer, The Larger Scope <ref type="bibr" target="#b17">Shi et al. (2016)</ref> and <ref type="bibr" target="#b19">Vaswani et al. (2017)</ref> have shown that different layers have the abilities to distinguish and capture diverse syntactic context (e.g. the depen- dents between words). <ref type="figure">Figure 3</ref> shows the dis- tribution of local scopes predicted by each layer. Except the first layer, the higher layers are more likely to pay attention to larger scopes, indicating that self-attention models tend to capture short- term dependencies among neighboring words in lower layers, while capture long-range dependen- cies beyond phrase boundaries in higher layers.</p><p>The Special First Layer Inconsistent with the intuition which the lower layers may focus on lo- cal information, in common, the first layer is as- signed with large scopes of local context. The same phenomenon has also occurred for head- specific model <ref type="figure" target="#fig_2">(Figure 2</ref>). Since the first layer represents word embeddings that are deficient in context, we argue that the self-attention model at first layer has to encode the representations with global context. In addition, experimental results in <ref type="table" target="#tab_1">Table 2</ref> (Row 2) show that despite its large lo- cal size, modeling localness at the first layer is still valid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis on Phrasal Pattern</head><p>As aforementioned, one intuition of our approach is to capture useful phrase patterns. To evalu- ate the accuracy of phrase translations, we calcu- late the improvement of the proposed approaches over multiple N-grams, as shown in <ref type="figure">Figure 4</ref>. Although our models underperform the baseline on unigram translations, they consistently outper- form the baseline on larger granularities, indicat- ing that modeling locality can raise the ability of  self-attention model on capturing the phrasal in- formation. Concerning the two variations, query- specific localness modeling surpasses its layer- specific counterpart on large phrases (i.g. 4-grams to 8-grams). We attribute this to the more model- ing flexibility of query-specific strategy to differ- entiate the scope by conditioning on each query.</p><note type="other">a) Layer 1 b) Layer 2 c) Layer 3 d) Layer 4 e) Layer 5 f) Layer 6 10</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>A successful extension of neural language model is attention mechanism, which can directly capture long-distance dependencies by attending to previ- ously generated words. <ref type="bibr" target="#b2">Daniluk et al. (2017)</ref> pro- posed a key-value-predict attention to separate the key addressing, value reading, and word predict- ing functions explicitly. <ref type="bibr" target="#b7">Im and Cho (2017)</ref> and <ref type="bibr" target="#b18">Sperber et al. (2018)</ref> adopted self-attention net- works for acoustic modeling and natural language inference tasks, respectively. <ref type="bibr" target="#b19">Vaswani et al. (2017)</ref> applied the idea of self- attention to neural machine translation. <ref type="bibr" target="#b15">Shen et al. (2018a)</ref> and <ref type="bibr" target="#b16">Shen et al. (2018b)</ref> proposed to im- prove the self-attention model with directional masks and multi-dimensional features. Although the standard self-attention model can give more bias toward localness, 6 several studies show that explicitly modeling localness for self-attention model can further improve performance. For example, <ref type="bibr" target="#b18">Sperber et al. (2018)</ref> showed that re- stricting the self-attention model on the neigh- boring representations performs better for longer sequences in acoustic modeling and natural lan- guage inference tasks. Closely related to this work, <ref type="bibr" target="#b14">Shaw et al. (2018)</ref> introduced relative po- sition encoding to consider the relative distances between sequence elements. While they modeled localness from static position embedding, we im- prove locality modeling from dynamically revising attention distribution. Experimental results show that the two models are complementary to each other, and combining them can further improve performance.</p><p>Several researches have shown that explicitly modeling phrases is useful for neural machine translation ( <ref type="bibr" target="#b20">Wang et al., 2017;</ref>). Our results confirm these findings. Concerning attention models, <ref type="bibr" target="#b10">Luong et al. (2015)</ref> proposed a modification to look at only a subset of input words at a time. This can be regarded as a "hard" variation of our fixed-window strategy. In this study, we propose more flexible strategies for plac- ing and zooming the local scope, which yield bet- ter results than the fixed scope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we enhanced the ability of captur- ing local context for self-attention networks with a learnable Gaussian bias. We proposed several strategies to learn the scope of the local con- text, and found that a query-specific mechanism yielded the best result due to its more modeling flexibility. Experimental results on widely-used English⇒German and Chinese⇒English transla- tion tasks demonstrate the effectiveness and uni- versality of the proposed approach. By visualiz- ing the scopes of the learned Gaussian biases, we found that the higher the layer, the larger scope the bias, which is consistent with the findings in pre- vious work <ref type="bibr" target="#b17">(Shi et al., 2016;</ref><ref type="bibr" target="#b12">Peters et al., 2018)</ref>.</p><p>As our approach is not limited to specific tasks, it is interesting to validate our model in other tasks, such as reading comprehension, language infer- ence, and stance classification ( <ref type="bibr" target="#b22">Xu et al., 2018)</ref>. Another promising direction is to design more powerful localness modeling techniques, such as incorporating linguistic knowledge (e.g. phrases and syntactic categories). It is also interesting to combine with other techniques ( <ref type="bibr" target="#b14">Shaw et al., 2018;</ref><ref type="bibr" target="#b15">Shen et al., 2018a;</ref><ref type="bibr" target="#b3">Dou et al., 2018;</ref> to further improve the performance of Transformer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the proposed approach. In this example, window size of 2 is used (D = 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Instructions of the learned window size by head-specific parametric model, where colors distinguish the heads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Distribution of the local scopes learned by each attentive layer. The upper figures illustrate the distribution of the predicted pairs of central position (Y-axis) and its correspond window size (X-axis) in each layer, the samples are randomly selected from the development set. The lower figures show the distribution of the window size in each layer. Blue color represents the layer-specific parametric approach, while the query-specific parametric method is indicated in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Evaluation of different layers in the en-
coder, which are implemented as self-attention 
with query-specific localness modeling. 

Enc 
Dec 
Enc-Dec Speed Dev 

× 
× 
1.15 23.29 


× 
1.10 23.27 

× 

1.08 23.33 



1.02 23.19 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> For the input of feed-forward network, we also tried an additive term to consider the weighted context Oi (Equation 1), namely: tanh(WpQi + WoOi). Our experimental results showed that there is no progressive improvement. Among the parametric methods, the first strategy assigns a unified window size to all the hidden states in a layer, so as to consider the context of the sequence, while the second one calculates a distinct window size for each hidden state.</note>

			<note place="foot" n="3"> https://github.com/fxshy/jieba 4 https://github.com/thumt/THUMT</note>

			<note place="foot" n="5"> Although the differences are not always significant, the flexible strategy consistently outperforms its fixed counterpart across language pairs. For example, the query-specific strategy improves performance over the fixed-window model by +0.07 and +0.23 BLEU points on Zh-En and En-De validation sets, respectively.</note>

			<note place="foot" n="6"> As pointed out by one reviewer, in the original selfattention model, there are some considerations about given more bias toward the localness. For example, base on the definition of the positional embeddings, the adjacent words will have more similar positional embeddings compared with more further words. After summing word embeddings and corresponding positional embeddings together, the model would prefer the local words.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by the Na-tional Natural Science Foundation of China (Grant No. 61672555), the Joint Project of Macao Sci-ence and Technology Development Fund and Na-tional Natural Science Foundation of China (Grant No. 045/2017/AFJ) and the Multiyear Research Grant from the University of Macau (Grant No. MYRG2017-00087-FST). We thank the anony-mous reviewers for their insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tied Multitask Learning for Neural Speech Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Frustratingly short attention spans in neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michał</forename><surname>Daniluk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting Deep Representations for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional Sequence to Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Aue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><surname>Chowdhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05567</idno>
	</analytic>
	<monogr>
		<title level="j">Achieving Human Parity on Automatic Chinese to English News Translation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards Neural Phrasebased Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Po Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sitao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Distance-based Self-Attention Network for Natural Language Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinbae</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungzoon</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02047</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Statistical Significance Tests for Machine Translation Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-Head Attention with Disagreement Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attentionbased Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BLEU: A Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Contextualized Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-Attention with Relative Position Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DiSAN: Directional Self-Attention Network for RNN/CNNFree Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bi-Directional Block SelfAttention for Fast and Memory-Efficient Sequence Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkit</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<title level="m">Does String-based Neural MT Learn Source Syntax? In EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Sebastian Stüker, and Alex Waibel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>SelfAttentional Acoustic Models. Interspeech</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Translating Phrases in Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cross-Target Stance Classification with SelfAttention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecile</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Nepal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Sparks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards Bidirectional Hierarchical Representations for Attentionbased Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Accelerating Neural Transformer via an Average Attention Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhuo</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06415</idno>
		<title level="m">THUMT: An Open Source Toolkit for Neural Machine Translation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
