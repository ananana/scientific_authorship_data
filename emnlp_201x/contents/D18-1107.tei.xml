<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Deep Neural Network Sentence Level Classification Method with Context Information</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">NIHR Biomedical Research Centre Institute of Psychiatry, Psychology and Neuroscience Kings College London London</orgName>
								<orgName type="institution">University of Sheffield Sheffield</orgName>
								<address>
									<country>UK, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johann</forename><surname>Petrak</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">NIHR Biomedical Research Centre Institute of Psychiatry, Psychology and Neuroscience Kings College London London</orgName>
								<orgName type="institution">University of Sheffield Sheffield</orgName>
								<address>
									<country>UK, UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angus</forename><surname>Roberts</surname></persName>
							<email>angus.roberts@kcl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">NIHR Biomedical Research Centre Institute of Psychiatry, Psychology and Neuroscience Kings College London London</orgName>
								<orgName type="institution">University of Sheffield Sheffield</orgName>
								<address>
									<country>UK, UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Deep Neural Network Sentence Level Classification Method with Context Information</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="900" to="904"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>900</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In the sentence classification task, context formed from sentences adjacent to the sentence being classified can provide important information for classification. This context is, however, often ignored. Where methods do make use of context, only small amounts are considered, making it difficult to scale. We present a new method for sentence classification , Context-LSTM-CNN, that makes use of potentially large contexts. The method also utilizes long-range dependencies within the sentence being classified, using an LSTM, and short-span features, using a stacked CNN. Our experiments demonstrate that this approach consistently improves over previous methods on two different datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Artificial neural networks (ANN) and especially Deep Neural Networks (DNN) give state-of-the art results for sentence classification tasks. Usu- ally, sentences are treated as separate instances for the task. However, in many situations the sen- tence that is the focus of classification appears in a context that can provide additional informa- tion. For example, in the below sentences from the IEMOCAP dataset, it is difficult to classify M02 as showing excitement, without the prior context:</p><p>• M01: I got it. I got accepted to U.S.C..</p><p>• F01: Oh, for real?</p><p>• M02: Yes! I just found out today. I just got the letter. Our work is motivated by sentence classifica- tion in the text of medical records, in which com- plex judgements may be made across several sen- tences, each adding weight and nuance to a point. We believe, however, that the techniqe is more widely applicable. In order to test generalisability and to allow reproducibility, we therefore present an evaluation of the method with publicy avail- able, non-medical corpora.</p><p>Previous work on using context for sentence classification used LSTM and CNN network lay- ers to encode the surrounding context, giving an improvement in classification accuracy ( <ref type="bibr" target="#b10">Lee and Dernoncourt, 2016)</ref>. However, the use of CNN and LSTM layers imposes a significant computa- tional cost when training the network, especially if the size of the context is large. For this reason, the approach presented in ( <ref type="bibr" target="#b10">Lee and Dernoncourt, 2016</ref>) is explicitly intended for sequential, short- text classification.</p><p>In many cases, however, the context available is of significant size. We therefore introduce a new method, Context-LSTM-CNN 1 , which is based on the computationally efficient FOFE (Fixed Size Ordinally Forgetting) method ( <ref type="bibr" target="#b14">Zhang et al., 2015)</ref>, and an architecture that combines an LSTM and CNN for the focus sentence. The method consis- tently improves over results obtained from either LSTM alone, CNN alone, or these two combined, with little increase in training time.</p><p>This paper makes three contributions: 1) a demonstration of the importance of context in some sentence classification tasks; 2) an adapta- tion of existing datasets for such sentence classifi- cation tasks, in order to support reproducibility of evaluations; 3) a neural architecture for sentence classification that outperforms previous methods, and can include context of arbitrary size without incurring a large computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Since their introduction <ref type="bibr" target="#b3">(Collobert et al., 2011</ref>), CNNs with word embedding language models have become common for text classification tasks <ref type="bibr" target="#b8">(Kim, 2014;</ref><ref type="bibr" target="#b4">Conneau et al., 2017)</ref>. One limi- tation of the original CNN approach is the loss <ref type="bibr">1</ref> The code is publicly available at https://github.com/deansong/contextLSTMCNN of long distance dependencies. In order to deal with this in image and speech recognition tasks, ; <ref type="bibr" target="#b12">Sainath et al. (2015)</ref> combined CNNs with a Recurrent Neural Network (RNN) layer. <ref type="bibr" target="#b15">Zhou et al. (2015)</ref> subsequently applied this to text classification. However, the CNN-RNN approach was originally devised for sequence la- belling, is biased towards later words in the se- quence, and does not perform better than CNN alone. <ref type="bibr" target="#b7">Huynh et al. (2016)</ref> suggested reversing the architecture to first apply the RNN followed by a CNN with pooling to obtain global features. This gave results that improved over CNN-RNN, but not over CNN alone. In this paper, we build on <ref type="bibr" target="#b7">Huynh et al. (2016)</ref>'s approach by replacing the GRU-based RNN ( <ref type="bibr" target="#b2">Cho et al., 2014</ref>) with an LSTM <ref type="bibr" target="#b6">(Hochreiter and Schmidhuber, 1997</ref>) and by using multiple kernel sizes and more features in the sub- sequent CNN layer. <ref type="bibr" target="#b10">Lee and Dernoncourt (2016)</ref> showed that when classifying short texts, accuracy can be boosted by adding a CNN or LSTM derived vector representa- tion of the surrounding context. For long contexts (such as patient records which may include well over 100 sentences), however, this will incur a sig- nificant additional computational cost. In this pa- per, we therefore apply an adaptation of the FOFE encoding ( <ref type="bibr" target="#b14">Zhang et al., 2015</ref>) to encode context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>The In brief, an LSTM layer is used to encode the focus sentence. This is followed by convolutional layers with small-size kernels and max-pooling to extract local features at specific points from the LSTM outputs. In addition to processing the fo- cus sentence, we also encode the full left and right contexts using an adaptation of FOFE applied to our embeddings. This encodes any variable length context into a fixed length embedding, thus allow- ing us to include large contexts without rapidly in- In detail, the full network takes three in- puts. The first is the sequence of words X = (x 1 , x 2 , ...x T ), where T is the length of the sen- tence to be classified, and where each x i is a word embedding for the respective word in this sen- tence. Embeddings are pre-trained by <ref type="bibr">Word2Vec (Mikolov et al., 2013</ref>) on the corpus used for the respective experiment. The embeddings are not updated during the training of our network.</p><p>The second and third inputs are the left and right context, which will connect to the FOFE en- coders. Each context is a sequence of sentences X C = (s 1 , s 2 , ...s N ), where each sentence is a se- quence of word embeddings s n = (x 1 , x 2 , ...x U ) from the same embedding space as X.</p><p>The first component of the inputs, derived from the focus sentence, is processed by a bi-directional LSTM with one layer, in order to capture long- distance dependencies within the sentence. Since LSTMs impose a significant computational cost for very long sequences we only use this layer for the input representing the focus sentence, and not for the left and right contexts.</p><p>The LSTM generates outputs h lstm = (h 1 , h 2 ..., h T ) which are passed on to the convolu- tional layer (CNN) in order to learn local features for different kernel sizes l from the history-aware outputs of the LSTM. For each of several kernel sizes, we generate f different features, to give CNN outputs c l cnn = (c 1 , c 2 , · · · , c T −l+1 ). For each CNN output c l cnn , we use max-overtime pooling to extract the most significant feature, and dropout to make the learned features more robust. We use an adapted version of FOFE to provide information about the left and right contexts of the focus. Instead of the original 1 of k FOFE repre- sentation, we apply FOFE encoding to word2vec embeddings. This gives a weighted sum of the context word embeddings, with weights decreas- ing exponentially with distance from the focus.</p><p>The embedding z for a sentence (x 1 , x 2 , ...x U ) is initialised to z 1 = x 1 , and then calculated recur- sively for u ∈ 2 · · · U as z u = α · z u−1 + x u . The parameter α is the forgetting factor, which con- trols how fast the weights used for words farther away from the start of the sentence diminish. This method is fast and compactly encodes the words of a sentence in a single embedding vector.</p><p>For our use of FOFE, we encode all sentences in the document to left and right of the focus sentence, in two hierarchical steps. First we en- code each context sentence into a FOFE embed- ding z sent , with a slowly-decreasing α sent . Fol- lowing this, the left context FOFE encodings are themselves encoded into a single context embed- ding using a rapidly decreasing α cont . This is cal- |C right | = z sent |C right | and recursively applying the same formula for m ∈ |C right | · · · 2. This gives a heavy bias towards sentences more local to the focus sentence, but only slightly decreases the im- portance of words within each sentence. The final FOFE embeddings for the left and right contexts are then put through a dense linear layer to ob- tain the hidden layer outputs, which are combined with the LSTM-CNN outputs. The concatenated outputs from the dense FOFE layers and from the CNN layer for all kernel sizes are then used as in- put to a final softmax output layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We compare the performance of four different network architectures: 1) CNN only; 2) LSTM only; 3) LSTM-CNN; 4) LSTM context encoded LSTM-CNN (L-LSTM-CNN), in which the one left and right context sentence are encoded by LSTM; and 5) Context-LSTM-CNN (C-LSTM- CNN). We use the following two datasets for eval- uation:</p><p>Interactive Emotional Dyadic Motion Cap- ture Database ( <ref type="bibr" target="#b0">Busso et al., 2008)</ref> 2 (IEMO- CAP). Originally created for the analysis of hu- man emotions based on speech and video, a tran- script of the speech component is available for NLP research. Each sentence in the dialogue is annotated with one of 10 types of emotion. There is a class imbalance in the labelled data, and so we follow the approach of ( <ref type="bibr" target="#b1">Chernykh et al., 2017)</ref>, and only use sentences classified with one of four labels ('Anger', 'Excitement', 'Neutral' and 'Sad- ness'). For this dataset, instead of using left and right contexts, we assign all sentences from one person to one context and all sentences from the other person to the other context. While only the sentences with the four classes of interest are used for classification, all sentences of the dialog are used as the context. This results in a set of 4936 la- belled sentences with average sentence length 14, and average document length is 986.</p><p>Drug-related Adverse Effects (Gurulingappa et al., 2012) 3 (ADE). This dataset contains sen- tences sampled from the abstracts of medical case reports. For each sentence, the annotation indi- cates whether adverse effects of a drug are be- ing described ('Positive') or not ('Negative'). The original release of the data does not contain the document context, which we reconstructed from PubMed <ref type="bibr">4</ref> . Sentences for which the full abstract could not be found were removed, resulting in 20,040 labelled sentences, with average sentence length 21 and average document length 129.  In all experiments, five-fold cross validation was used for evaluation (for comparison with <ref type="bibr" target="#b7">(Huynh et al., 2016)</ref>). For each fold, 50 epochs were run for training using a minibatch size of 64 for each fold, and the Adamax optimization algo-   rithm. To deal with label imbalance in the data, class weights w i for class i were set proportional to max(f i )/f i where f i is the frequency of class i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><note type="other">F-measure Anger (1,103) Sadness (1,084) Neutral (1,708) Excitement (1,041) Negative(14,854) Positive(5,186</note><p>We used word2vec embeddings with 50 dimen- sions (suggesed as sufficient by <ref type="bibr" target="#b9">(Lai et al., 2016)</ref>). For the LSTM, 64 hidden units were used. For the CNN, layers for kernel sizes 2 to 6 were included in the network, and 64 features were used for each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Effect of Forgetting Factors</head><p>We examined the effect of the two context encoder hyperparameters: α cont (context level forgetting factor) and α w (sentence level forgetting factor) on classification performance over the IEMOCAP dataset. We tested both in the range of 0.1 to 1 with an incremental step of 0.1. Results are shown in <ref type="figure" target="#fig_3">Figure 2</ref>. Accuracy improves as α cont increases, but drops at α cont = 1, at which point all context sentence are given equal weight. This may be be- cause context closest to the focus sentence is more important than distant context. Therefore, we se- lect α cont = 0.9 in all experiments.</p><p>For α sent , performance always increases as α sent increases, with best results at α sent = 1, at which point all words in the sentence con- tribute equally in the context code. This implies that for individual sentences in the context, it is more preferable to lose word order, than to down weight any individual word. In all experiments, we therefore set the sentence level forgetting fac- tor to α sent = 1 <ref type="table" target="#tab_1">Table 1</ref> shows the mean and standard deviations for accuracy over the cross validation folds, and training time, for both data sets. CNN alone per- forms better than LSTM alone in both tasks. The combined LSTM-CNN network consistently im- proves performance beyond both CNN alone and LSTM alone. Both context based models (L- LSTM-CNN and C-LSTM-CNN) perform better than non context based models, but note that L- LSTM-CNN increases training time by approxi- mately 1.5x, whereas C-LSTM-CNN shows only a marginal increase in training time, with a large increase in accuracy on the IEMOCAP corpus. <ref type="table" target="#tab_3">Table 2</ref> shows the F1-measure for each class in the two datasets. Again, Context-LSTM-CNN outperforms the other models on all classes for all data sets. C-LSTM-CNN improves on average by 6.28 over L-LSTM-CNN, 10.16 over LSTM- CNN, 11.4 over CNN and 13.29 over LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Results</head><p>We conducted a t-test between L-LSTM-CNN and C-LSTM-CNN. On IEMOCAP, C-LSTM- CNN is significantly better than L-LSTM-CNN (p = 0.002). On ADE, C-LSTM-CNN is not sig- nificantly better than L-LSTM-CNN (p = 0.128). This may because ADE sentences are less context dependent. Alternatively, as the ADE task is rela- tively easy, with all models able to achieve about 90% accuracy, a context based approach might not be able to further improve the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we introduced a new ANN model, Context-LSTM-CNN, that combines the strength of LSTM and CNN with the lightweight context encoding algorithm, FOFE. Our model shows a consistent improvement over either a non-context based model and a LSTM context encoded model, for the sentence classification task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Context-LSTM-CNN model is shown in Fig- ure 1. It is based on the following components: 1. Input layer using word embeddings to encode the words of the focus sentence. 2. Bi-directional LSTM applied to the word em- beddings of the focus sentence. 3. CNN on the outputs of the LSTM. 4. FOFE applied to word embeddings of both left and right context. 5. A final output layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Structure of the C-LSTM-CNN model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>culated starting with z cont 1 = z sent 1 and is calcu- lated for m ∈ 2 · · · |C lef t | as z cont m = α cont · z cont m−1 + z sent m . The right context FOFE encod- ings are encoded in the same way, starting with z cont</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Context level (red line) and sentence level (blue line) forgetting factor test</figDesc><graphic url="image-1.png" coords="4,72.00,185.75,226.77,107.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Average test accuracy and training time. 
Best values are marked as bold, standard devia-
tions in parentheses 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Average test F-measure for each class. Instance numbers in parentheses after class name. Best 
values are marked as bold, standard deviations in parentheses 

</table></figure>

			<note place="foot" n="2"> http://sail.usc.edu/iemocap/iemocap_ release.htm 3 https://sites.google.com/site/ adecorpus/home/document 4 https://www.ncbi.nlm.nih.gov/pubmed/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partially supported by the Euro-pean Union under grant agreement No. 654024 SoBigData.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shrikanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">335</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Emotion recognition from speech with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Chernykh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigoriy</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Prihodko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08071</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsha</forename><surname>Gurulingappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdul</forename><forename type="middle">Mateen</forename><surname>Rajput</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angus</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliane</forename><surname>Fluck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Hofmann-Apitius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Toldo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="885" to="892" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adverse drug reaction classification with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allistair</forename><surname>Willis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Rueger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 26th International Conference on Computational Linguistics</title>
		<meeting>The 26th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How to generate a good word embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="5" to="14" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sequential short-text classification with recurrent and convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional, long short-term memory, fully connected deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasim</forename><surname>Sak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2015th International Conference on Machine Learning</title>
		<meeting>2015th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The fixed-size ordinallyforgetting encoding method for neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A c-lstm neural network for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chonglin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">C M</forename><surname>Lau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
