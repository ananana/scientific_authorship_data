<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convolutional Neural Network Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Quan</forename><surname>Pham</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Mind/Brain Sciences</orgName>
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Kruszewski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Mind/Brain Sciences</orgName>
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Mind/Brain Sciences</orgName>
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Convolutional Neural Network Language Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1153" to="1162"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Convolutional Neural Networks (CNNs) have shown to yield very strong results in several Computer Vision tasks. Their application to language has received much less attention, and it has mainly focused on static classification tasks, such as sentence classification for Sentiment Analysis or relation extraction. In this work, we study the application of CNNs to language modeling, a dynamic, sequential prediction task that needs models to capture local as well as long-range dependency information. Our contribution is twofold. First, we show that CNNs achieve 11-26% better absolute performance than feed-forward neu-ral language models, demonstrating their potential for language representation even in sequential tasks. As for recurrent models, our model outperforms RNNs but is below state of the art LSTM models. Second, we gain some understanding of the behavior of the model, showing that CNNs in language act as feature detectors at a high level of abstraction, like in Computer Vision, and that the model can profitably use information from as far as 16 words before the target.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional Neural Networks (CNNs) are the family of neural network models that feature a type of layer known as the convolutional layer. This layer can extract features by convolving a learnable filter (or kernel) along different positions of a vectorial in- put.</p><p>CNNs have been successfully applied in Com- puter Vision in many different tasks, including ob- ject recognition, scene parsing, and action recogni- tion ( <ref type="bibr">Gu et al., 2015</ref>), but they have received less attention in NLP. They have been somewhat ex- plored in static classification tasks where the model is provided with a full linguistic unit as input (e.g. a sentence) and classes are treated as independent of each other. Examples of this are sentence or docu- ment classification for tasks such as Sentiment Anal- ysis or Topic Categorization ( <ref type="bibr" target="#b15">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b17">Kim, 2014)</ref>, sentence matching ( <ref type="bibr" target="#b13">Hu et al., 2014)</ref>, and relation extraction <ref type="bibr" target="#b26">(Nguyen and Grishman, 2015</ref>). However, their application to sequen- tial prediction tasks, where the input is construed to be part of a sequence (for example, language model- ing or POS tagging), has been rather limited (with exceptions, such as <ref type="bibr" target="#b5">Collobert et al. (2011)</ref>). The main contribution of this paper is a systematic evalu- ation of CNNs in the context of a prominent sequen- tial prediction task, namely, language modeling.</p><p>Statistical language models are a crucial compo- nent in many NLP applications, such as Automatic Speech Recognition, Machine Translation, and In- formation Retrieval. Here, we study the problem under the standard formulation of learning to predict the upcoming token given its previous context. One successful approach to this problem relies on count- ing the number of occurrences of n-grams while using smoothing and back-off techniques to esti- mate the probability of an upcoming word <ref type="bibr" target="#b18">(Kneser and Ney, 1995)</ref>. However, since each individual word is treated independently of the others, n-gram models fail to capture semantic relations between words. In contrast, neural network language mod- els ( <ref type="bibr" target="#b2">Bengio et al., 2006</ref>) learn to predict the up-coming word given the previous context while em- bedding the vocabulary in a continuous space that can represent the similarity structure between words. Both feed-forward <ref type="bibr" target="#b27">(Schwenk, 2007)</ref> and recurrent neural networks ( <ref type="bibr" target="#b23">Mikolov et al., 2010</ref>) have been shown to outperform n-gram models in various se- tups ( <ref type="bibr" target="#b23">Mikolov et al., 2010;</ref><ref type="bibr" target="#b9">Hai Son et al., 2011</ref>). These two types of neural networks make different architectural decisions. Recurrent networks take one token at a time together with a hidden "memory" vector as input and produce a prediction and an up- dated hidden vector for the next time step. In con- trast, feed-forward language models take as input the last n tokens, where n is a fixed window size, and use them jointly to predict the upcoming word.</p><p>In this paper we define and explore CNN-based language models and compare them with both feed- forward and recurrent neural networks. Our results show a 11-26% perplexity reduction of the CNN with respect to the feed-forward language model, comparable or higher performance compared to similarly-sized recurrent models, and lower perfor- mance with respect to larger, state-of-the-art recur- rent language models (LSTMs as trained in <ref type="bibr">Zaremba et al. (2014)</ref>).</p><p>Our second contribution is an analysis of the kind of information learned by the CNN, showing that the network learns to extract a combination of grammat- ical, semantic, and topical information from tokens of all across the input window, even those that are the farthest from the target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Convolutional Neural Networks (CNNs) were orig- inally designed to deal with hierarchical representa- tion in Computer Vision ( <ref type="bibr" target="#b21">LeCun and Bengio, 1995)</ref>. Deep convolutional networks have been success- fully applied in image classification and understand- ing ( <ref type="bibr" target="#b29">Simonyan and Zisserman, 2014;</ref><ref type="bibr" target="#b11">He et al., 2015)</ref>. In such systems the convolutional kernels learn to detect visual features at both local and more abstract levels.</p><p>In NLP, CNNs have been mainly applied to static classification tasks for discovering latent structures in text. <ref type="bibr" target="#b17">Kim (2014)</ref> uses a CNN to tackle sentence classification, with competitive results. The same work also introduces kernels with varying window sizes to learn complementary features at different aggregation levels. <ref type="bibr" target="#b15">Kalchbrenner et al. (2014)</ref> pro- pose a convolutional architecture for sentence repre- sentation that vertically stacks multiple convolution layers, each of which can learn independent convo- lution kernels. CNNs with similar structures have also been applied to other classification tasks, such as semantic matching ( <ref type="bibr" target="#b13">Hu et al., 2014</ref>), relation ex- traction <ref type="bibr" target="#b26">(Nguyen and Grishman, 2015)</ref>, and infor- mation retrieval <ref type="bibr" target="#b28">(Shen et al., 2014</ref>). In contrast, <ref type="bibr" target="#b5">Collobert et al. (2011)</ref> explore a CNN architecture to solve various sequential and non-sequential NLP tasks such as part-of-speech tagging, named entity recognition and also language modeling. This is per- haps the work that is closest to ours in the existing literature. However, their model differs from ours in that it uses a max-pooling layer that picks the most activated feature across time, thus ignoring tempo- ral information, whereas we explicitly avoid doing so. More importantly, the language models trained in that work are only evaluated through downstream tasks and through the quality of the learned word embeddings, but not on the sequence prediction task itself, as we do here.</p><p>Besides being applied to word-based sequences, the convolutional layers have also been used to model sequences at the character level. <ref type="bibr" target="#b16">Kim et al. (2015)</ref> propose a recurrent language model that re- places the word-indexed projection matrix with a convolution layer fed with the character sequence that constitutes each word to find morphological pat- terns. The main difference between that work and ours is that we consider words as the smallest lin- guistic unit, and thus apply the convolutional layer at the word level.</p><p>Statistical language modeling, the task we tackle, differs from most of the tasks where CNNs have been applied before in multiple ways. First, the input typically consists of incomplete sequences of words rather than complete sentences. Second, as a classi- fication problem, it features an extremely large num- ber of classes (the words in a large vocabulary). Fi- nally, temporal information, which can be safely dis- carded in many settings with little impact in perfor- mance, is critical here: An n-gram appearing close to the predicted word may be more informative, or yield different information, than the same n-gram appearing several tokens earlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models</head><p>Our model is constructed by extending a feed- forward language model (FFLM) with convolutional layers. In what follows, we first explain the imple- mentation of the base FFLM and then describe the CNN model that we study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline FFLM</head><p>Our baseline feed-forward language model (FFLM) is almost identical to the original model proposed by <ref type="bibr" target="#b2">Bengio et al. (2006)</ref>, with only slight changes to push its performance as high as we can, producing a very strong baseline. In particular, we extend it with highway layers and use Dropout as regulariza- tion. The model is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> and works as follows. First, each word in the input n-gram is mapped to a low-dimensional vector (viz. embed- ding) though a shared lookup table. Next, these word vectors are concatenated and fed to a highway layer ( <ref type="bibr" target="#b30">Srivastava et al., 2015)</ref>. Highway layers im- prove the gradient flow of the network by computing as output a convex combination between its input (called the carry) and a traditional non-linear trans- formation of it (called the transform). As a result, if there is a neuron whose gradient cannot flow through the transform component (e.g., because the activa- tion is zero), it can still receive the back-propagation update signal through the carry gate. We empiri- cally observed the usage of a single highway layer to significantly improve the performance of the model. Even though a systematic evaluation of this aspect is beyond the scope of the current paper, our empirical results demonstrate that the resulting model is a very competitive one (see Section 4).</p><p>Finally, a softmax layer computes the model pre- diction for the upcoming word. We use ReLU for all non-linear activations, and Dropout ( ) is applied between each hidden layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CNN and variants</head><p>The proposed CNN network is produced by inject- ing a convolutional layer right after the words in the input are projected to their embeddings ( <ref type="figure" target="#fig_1">Figure 2</ref>). Rather than being concatenated into a long vector, the embeddings x i ∈ R k are concatenated transver- sally producing a matrix x 1:n ∈ R n×k , where n is the size of the input and k is the embedding size. This matrix is fed to a time-delayed layer, which convolves a sliding window of w input vectors cen- tered on each word vector using a parameter matrix W ∈ R w×k . Convolution is performed by taking the dot-product between the kernel matrix W and each sub-matrix x i−w/2:i+w/2 resulting in a scalar value for each position i in input context. This value represents how much the words encompassed by the window match the feature represented by the filter W . A ReLU activation function is applied subse- quently so negative activations are discarded. This operation is repeated multiple times using various kernel matrices W , learning different features in- dependently. We tie the number of learned kernels to be the same as the embedding dimensionality k, such that the output of this stage will be another ma- trix of dimensions n × k containing the activations for each kernel at each time step. The number of kernels was tied to the embedding size for two rea- sons, one practical, namely, to limit the hyper pa- rameter search, one methodological, namely, to keep the network structure identical to that of the baseline feed-forward model.</p><formula xml:id="formula_0">1 . . . . . . . . .</formula><p>Next, we add a batch normalization stage imme- diately after the convolutional output, which facil- itates learning by addressing the internal covariate shift problem and regularizing the learned represen- tations ( <ref type="bibr" target="#b14">Ioffe and Szegedy, 2015)</ref>.</p><p>Finally, this feature matrix is directly fed into a fully connected layer that can project the ex- tracted features into a lower-dimensional represen- tation. This is different from previous work, where a max-over-time pooling operation was used to find the most activated feature in the time series. Our choice is motivated by the fact that the max pooling operator loses the specific position where the feature was detected, which is important for word predic- tion.</p><p>After this initial convolutional layer, the network proceeds identically to the FFNN by feeding the pro- duced features into a highway layer, and then, to a softmax output. This is our basic CNN architecture. We also ex- periment with three expansions to the basic model, as follows. First, we generalize the CNN by ex- tending the shallow linear kernels with deeper multi- layer perceptrons, in what is called a MLP Convolu- tion (MLPConv) structure ( <ref type="bibr" target="#b22">Lin et al., 2013)</ref>. This allows the network to produce non-linear filters, and it has achieved state-of-the-art performance in object recognition while reducing the number of total lay- ers compared to other mainstream networks. Con- cretely, we implement MLPConv networks by using another convolutional layer with a 1 × 1 kernel on top of the convolutional layer output. This results in an architecture that is exactly equivalent to sliding a one-hidden-layer MLP over the input. Notably, we do not include the global pooling layer in the origi- nal Network-in-Network structure ( <ref type="bibr" target="#b22">Lin et al., 2013)</ref>.</p><p>Second, we explore stacking convolutional lay- ers on top of each other (Multi-layer CNN or ML- CNN) to connect the local features into broader re- gional representations, as commonly done in com- puter vision. While this proved to be useful for sentence representation <ref type="bibr" target="#b15">(Kalchbrenner et al., 2014</ref>), here we have found it to be rather harmful for lan- guage modeling, as shown in Section 4. It is impor- tant to note that, in ML-CNN experiments, we stack convolutions with the same kernel size and number of kernels on top of each other, which is to be distin- guished from the MLPConv that refers to the deeper structure in each CNN layer mentioned above.</p><p>Finally, we consider combining features learned through different kernel sizes (COM), as depicted in Mapping-5</p><p>Mapping-3</p><p>Hidden layers + Softmax  <ref type="figure" target="#fig_2">Figure 3</ref>. For example, we can have a combination of kernels that learn filters over 3-grams with oth- ers that learn over 5-grams. This is achieved simply by applying in parallel two or more sets of kernels to the input and concatenating their respective out- puts <ref type="bibr" target="#b17">(Kim, 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our model on three English corpora of different sizes and genres, the first two of which have been used for language modeling evaluation before. The Penn Treebank contains one mil- lion words of newspaper text with 10K words in the vocabulary. We reuse the preprocessing and training/test/validation division from <ref type="bibr" target="#b25">Mikolov et al. (2014)</ref>. Europarl-NC is a 64-million word cor- pus that was developed for a Machine Translation shared task ( <ref type="bibr" target="#b3">Bojar et al., 2015)</ref>, combining Europarl data (from parliamentary debates in the European Union) and News Commentary data. We prepro- cessed the corpus with tokenization and true-casing tools from the Moses toolkit ( <ref type="bibr" target="#b19">Koehn et al., 2007</ref></p><note type="other">). The vocabulary is composed of words that occur at least 3 times in the training set and contains approx- imately 60K words. We use the validation and test set of the MT shared task. Finally, we took a sub- set of the ukWaC corpus, which was constructed by crawling UK websites (Baroni et al., 2009). The training subset contains 200 million words and the vocabulary consists of the 200K words that appear more than 5 times in the training subset. The val- idation and test sets are different subsets of the ukWaC corpus, both containing 120K words. We preprocessed the data similarly to what we did for Europarl-NC.</note><p>We train our models using Stochastic Gradient Descent (SGD), which is relatively simple to tune compared to other optimization methods that involve additional hyper parameters (such as alpha in RM- Sprop) while being still fast and effective. SGD is commonly used in similar work <ref type="bibr" target="#b6">(Devlin et al., 2014;</ref><ref type="bibr">Zaremba et al., 2014;</ref><ref type="bibr" target="#b31">Sukhbaatar et al., 2015</ref>). The learning rate is kept fixed during a single epoch, but we reduce it by a fixed proportion every time the val- idation perplexity increases by the end of the epoch. The values for learning rate, learning rate shrinking and mini-batch sizes as well as context size are fixed once and for all based on insights drawn from pre- vious work <ref type="bibr" target="#b9">(Hai Son et al., 2011;</ref><ref type="bibr" target="#b31">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b6">Devlin et al., 2014</ref>) as well as experimentation with the Penn Treebank validation set.</p><p>Specifically, the learning rate is set to 0.05, with mini-batch size of 128 (we do not take the average of loss over the batch, and the training set is shuffled). We multiply the learning rate by 0.5 every time we shrink it and clip the gradients if their norm is larger than 12. The network parameters are initialized ran- domly on a range from -0.01 to 0.01 and the context size is set to 16. In Section 6 we show that this large context window is fully exploited.</p><p>For the base FFNN and CNN we varied em- bedding sizes (and thus, number of kernels) k = 128, 256. For k = 128 we explore the simple CNN, incrementally adding MLPConv and COM varia- tions (in that order) and, alternatively, using a ML- CNN. For k = 256, we only explore the former three alternatives (i.e. all but the ML-CNN). For the kernel size, we set it to w = 3 words for the sim- ple CNN (out of options 3, 5, 7, 9), whereas for the COM variant we use w = 3 and 5, based on experi- mentation on PTB. However, we observed the mod- els to be generally robust to this parameter. Dropout rates are tuned specifically for each combination of model and dataset based on the validation perplex- ity. We also add small dropout (p = 0.05-0.15) when we train the networks on the smaller corpus (Penn Treebank).</p><p>The experimental results for recurrent neural net- work language models, such as Recurrent Neural Networks (RNN) and Long-Short Term Memory models (LSTM), on the Penn Treebank are quoted from previous work; for Europarl-NC, we train our own models (we also report the performance of these in-house trained RNN and LSTM models on the Penn Treebank for reference). Specifically, we train LSTMs with embedding size k = 256 and number of layers L = 2 as well as k = 512 with L = 1, 2. We train one RNN with k = 512 and L = 2. To train these models, we use the published source code from <ref type="bibr">Zaremba et al. (2014)</ref>. Our own models are also implemented in Torch7 for easier comparison. 1 Fi- nally, we selected the best performing convolutional and recurrent language models on Europarl-NC and the Baseline FFLM to be evaluated on the ukWaC corpus.</p><p>For all models trained on Europarl-NC and ukWaC, we speed up training by approximating the softmax with Noise Contrastive Estimation (NCE) ( <ref type="bibr" target="#b8">Gutmann and Hyvärinen, 2010)</ref>, with the parameters being set following previous work <ref type="bibr" target="#b4">(Chen et al., 2015)</ref>. Concretely, for each predicted word, we sample 10 words from the unigram distribution, and the normalization factor is such that ln Z = 9. <ref type="bibr">2</ref> For comparison, we also implemented a simpler version of the FFNN without dropout and highway layers ( <ref type="bibr" target="#b2">Bengio et al., 2006</ref>). These networks have two hidden layers ( <ref type="bibr" target="#b0">Arisoy et al., 2012</ref>) with the size of 2 times the embedding size (k), thus having the same number of parameters as our baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Our experimental results are summarized in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>First of all, we can see that, even though the FFNN gives a very competitive performance, 3 the addition of convolutional layers clearly improves it even further. Concretely, we observe a solid 11-26% reduction of perplexity compared to the feed-forward network after using MLP Convolution, depending on the setup and corpus. CNN alone yields a sizable improvement (5-24%), while MLP- Conv, in line with our expectations, adds another approximately 2-5% reduction in perplexity. A fi- nal (smaller) improvement comes from combining kernels of size 3 and 5, which can be attributed to a more expressive model that can learn patterns of n-grams of different sizes. In contrast to the suc- cessful two variants above, the multi-layer CNN did not help in better capturing the regularities of text, but rather the opposite: the more convolutional lay- ers were stacked, the worse the performance. This also stands in contrast to the tradition of convolu- tional networks in Computer Vision, where using very deep convolutional neural networks is key to having better models. Deep convolution for text representation is in contrast rather rare, and to our knowledge it has only been successfuly applied to sentence representation ( <ref type="bibr" target="#b15">Kalchbrenner et al., 2014</ref>). We conjecture that the reason why deep CNNs may not be so effective for language could be the effect of the convolution on the data: The convolution output for an image is akin to a new, more abstract image, which yet again can be subject to new convolution operations, whereas the textual counterpart may no longer have the same properties, in the relevant as- pects, as the original linguistic input.</p><p>Regarding the comparison with a stronger LSTM, our models can perform competitively under the same embedding dimension (e.g. see k = 256 of k = 512) on the first two datasets. However, the LSTM can be easily scaled using larger models, as shown in <ref type="bibr">Zaremba et al. (2014)</ref>, which gives the best known results to date. This is not an option for our model, which heavily overfits with large hidden layers (around 1000) even with very large dropout values. Furthermore, the experiments on the larger ukWaC corpus show an even clearer advantage for the LSTM, which seems to be more efficient at har- nessing this volume of data, than in the case of the two smaller corpora.</p><p>To sum up, we have established that the results of our CNN model are well above those of sim- ple feed forward networks and recurrent neural net- works. While they are below state of the art LSTMs, they are able to perform competitively with them for small and moderate-size models. Scaling to larger sizes may be today the main roadblock for CNNs to reach the same performances as large LSTMs in language modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Model Analysis</head><p>In what follows, we obtain insights into the inner workings of the CNN by looking into the linguis- tic patterns that the kernels learn to extract and also studying the temporal information extracted by the network in relation to its prediction capacity.</p><p>Learned patterns To get some insight into the kind of patterns that each kernel is learning to de- tect, we fed trigrams from the validation set of the Penn Treebank to each of the kernels, and extracted the ones that most highly activated the kernel, simi- larly to what was done in <ref type="bibr" target="#b15">Kalchbrenner et al. (2014)</ref>. Some examples are shown in <ref type="figure" target="#fig_3">Figure 4</ref>. Since the word windows are made of embeddings, we can ex- pect patterns with similar embeddings to have close activation outputs. This is borne out in the analysis: The kernels specialize in distinct features of the data, including more syntactic-semantic constructions (cf. the "comparative kernel" including as . . . as pat- terns, but also of more than) and more lexical or top- ical features (cf. the "ending-in-month-name" ker- nel). Even in the more lexicalized features, how- ever, we see linguistic regularities at different lev- els being condensed in a single kernel: For instance, the "spokesman" kernel detects phrases consisting of an indefinite determiner, a company name (or the word company itself) and the word "spokesman". We hypothesize that the convolutional layer adds an "I identify one specific feature, but at a high level of <ref type="table" target="#tab_1">Model   k  w  Penn Treebank  Europarl-NC  ukWaC  val test #p val test #p val test #p  FFNN (Bengio et al., 2006)  128  - 156 147 4.5  - - - - - - Baseline FFNN  128  - 114 109 4.5  - - - - - - +CNN  128  3  108 102 4.5  - - - - - - +MLPConv  128  3  102 97 4.5  - - - - - - +MLPConv+COM  128 3+5 96  92  8  - - - - - - +ML-CNN (2 layers)  128  3  113 108  8  - - - - - - +ML-CNN (4 layers)  128  3  130 124  8  - - - - - -</ref>    abstraction" dimension to a feed-forward neural net- work, similarly to what has been observed in image classification ( ).</p><p>Temporal information To the best of our knowl- edge, the longest context used in feed-forward lan- guage models is 10 tokens <ref type="bibr" target="#b10">(Hai Son et al., 2012</ref>), where no significant change in terms of perplexity was observed for bigger context sizes, even though in that work only same-sentence contexts were con- sidered. In our experiments, we use a larger context size of 16 while removing the sentence boundary limit (as commonly done in n-gram language mod- els) such that the network can take into account the words in the previous sentences.</p><p>To analyze whether all this information was effectively used, we took our best model, the CNN+MLPConv+COM model with embedding size of 256 (fifth line of second block in <ref type="table" target="#tab_1">Table 1</ref>), and we identified the weights in the model that map the convolutional output (of size n × k) to a lower di- mensional vector (the "mapping" layer in <ref type="figure" target="#fig_1">Figure 2)</ref>. Recall that the output of the convolutional layer is a matrix indexed by time step and kernel index con- taining the activation of the kernel when convolved with a window of text centered around the word at the given time step. Thus, output units of the above mentioned mapping predicate over an ensem- ble of kernel activations for each time step. We can identify the patterns that they learn to detect by extracting the time-kernel combinations for which they have positive weights (since we have ReLU ac- tivations, negative weights are equivalent to ignor- ing a feature). First, we asked ourselves whether these units tend to be more focused on the time steps closer to the target or not. To test this, we calculated the sum of the positive weights for each position in time using an average of the mappings that corre- spond to each output unit. The results are shown in   Next, we checked that the information extracted from the positions that are far in the past are actu- ally used for prediction. To measure this, we arti- ficially lesioned the network so it would only read the features from a given range of time steps (words in the context). To lesion the network we manually masked the weights of the mapping that focus on times outside of the target range by setting them to zero. We started using only the word closest to the final position and sequentially unmasked earlier po- sitions until the full context was used again. The re- sult of this experiment is presented in <ref type="figure" target="#fig_5">Figure 6</ref>, and it confirms our previous observation that positions that are the farthest away contribute to the predic- tions of the model. The perplexity drops dramati- cally as the first positions are unmasked, and then decreases more slowly, approximately in the form of a power law (f (x) ∝ x −0.9 ). Even though the ef- fect is smaller, the last few positions still contribute to the final perplexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we have investigated the potential of Convolutional Neural Networks for one prominent NLP task, language modeling, a sequential predic-tion task. We incorporate a CNN layer on top of a strong feed-forward model enhanced with modern techniques like Highway Layers and Dropout. Our results show a solid 11-26% reduction in perplexity with respect to the feed-forward model across three corpora of different sizes and genres when the model uses MLP Convolution and combines kernels of dif- ferent window sizes. However, even without these additions we show CNNs to effectively learn lan- guage patterns that allow it to significantly decrease the model perplexity.</p><p>In our view, this improvement responds to two key properties of CNNs, highlighted in the analysis. First, as we have shown, they are able to integrate information from larger context windows, using in- formation from words that are as far as 16 positions away from the predicted word. Second, as we have qualitatively shown, the kernels learn to detect spe- cific patterns at a high level of abstraction. This is analogous to the role of convolutions in Computer Vision. The analogy, however, has limits; for in- stance, a deeper model stacking convolution layers harms performance in language modeling, while it greatly helps in Computer Vision. We conjecture that this is due to the differences in the nature of vi- sual vs. linguistic data. The convolution creates sort of abstract images that still retain significant proper- ties of images. When applied to language, it detects important textual features but distorts the input, such that it is not text anymore.</p><p>As for recurrent models, even if our model out- performs RNNs, it is well below state-of-the-art LSTMs. Since CNNs are quite different in nature, we believe that a fruitful line of future research could focus on integrating the convolutional layer into a recurrent structure for language modeling, as well as other sequential problems, perhaps capturing the best of both worlds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of baseline FFLM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Convolutional layer on top of the context matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Combining kernels with different sizes. We concatenate the outputs of 2 convolutional blocks with kernel size of 5 and 3 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Some example phrases that have highest activations for 8 example kernels (each box), extracted from the validation set of the Penn Treebank. Model trained with 256 kernels for 256-dimensional word vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The distribution of positive weights over context positions, where 1 is the position closest to the predicted word.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Perplexity change over position, by incrementally revealing the Mapping's weights corresponding to each position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. As could be expected, positions that are close to the token to be predicted have many active units (local context is very informative; see positions 2-4). However, surprisingly, positions that are actually far from the target are also quite active. It seems like the CNN is putting quite a lot of effort on characterizing long-range dependencies. Next, we checked that the information extracted from the positions that are far in the past are actually used for prediction. To measure this, we artificially lesioned the network so it would only read the features from a given range of time steps (words in the context). To lesion the network we manually masked the weights of the mapping that focus on times outside of the target range by setting them to zero. We started using only the word closest to the final position and sequentially unmasked earlier positions until the full context was used again. The result of this experiment is presented in Figure 6, and it confirms our previous observation that positions that are the farthest away contribute to the predictions of the model. The perplexity drops dramatically as the first positions are unmasked, and then decreases more slowly, approximately in the form of a power law (f (x) ∝ x −0.9 ). Even though the effect is smaller, the last few positions still contribute to the final perplexity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Results on Penn Treebank and Europarl-NC. Figure of merit is perplexity (lower is better). Legend: k: embedding size 

(also number of kernels for the convolutional models and hidden layer size for the recurrent models); w: kernel size; val: results on 

validation data; test: results on test data; #p: number of parameters in millions; L: number of layers. 

no matter how 
are afraid how 
question is how 
remaining are how 
to say how 

as little as 
of more than 
as high as 
as much as 
as low as 

a merc spokesman 
a company spokesman 
a boeing spokesman 
a fidelity spokesman 
a quotron spokeswoman 

amr chairman robert 
chief economist john 
chicago investor william 
exchange chairman john 
texas billionaire robert 

would allow the 
does allow the 
still expect ford 
warrant allows the 
funds allow investors 

more evident among 
a dispute among 
bargain-hunting among 
growing fear among 
paintings listed among 

facilities will substantially 
which would substantially 
dean witter actually 
we 'll probably 
you should really 

have until nov. 
operation since aug. 
quarter ended sept. 
terrible tuesday oct. 
even before june 

</table></figure>

			<note place="foot" n="1"> Available at https://github.com/quanpn90/NCE CNNLM. 2 We also experimented with Hierarchical Softmax (Mikolov et al., 2011) and found out that the NCE gave better performance in terms of speed and perplexity.</note>

			<note place="foot" n="3"> In our experiments, increasing the number of fully connected layers of the FFNN is harmful. Two hidden layers with highway connections is the best setting we could find.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Marco Baroni and three anonymous re-viewers for fruitful feedback. This project has re-ceived funding from the European Union's Hori-zon 2020 research and innovation programme un-der the Marie Sklodowska-Curie grant agreement No 655577 (LOVe); ERC 2011 Starting Independent Research Grant n. 283554 (COMPOSES) and the Erasmus Mundus Scholarship for Joint Master Pro-grams. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used in our research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ebru</forename><surname>Arisoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuvana</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the Ngram Model? On the Future of Language Modeling for HLT</title>
		<meeting>the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the Ngram Model? On the Future of Language Modeling for HLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The WaCky wide web: A collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><surname>Ferraresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eros</forename><surname>Zanchetta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="209" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sébastien</forename><surname>Senécal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fréderic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="137" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hokamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1" to="46" />
		</imprint>
	</monogr>
	<note>Findings of the 2015 workshop on statistical machine translation</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recurrent neural network language model training with noise contrastive estimation for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip C</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5411" to="5415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1370" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1512.07108</idno>
		<title level="m">Xingxing Wang, and Gang Wang. 2015. Recent advances in convolutional neural networks. CoRR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Noisecontrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AISTATS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structured output layer neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Hai Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Oparin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5524" to="5527" />
		</imprint>
	</monogr>
	<note>2011 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Measuring the influence of long range dependencies with neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Hai Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT</title>
		<meeting>the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m">Deep residual learning for image recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>abs/1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014-06-22" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classification</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
	<note>ICASSP95</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions</title>
		<meeting>the 45th annual meeting of the ACL on interactive poster and demonstration sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Cernock`nock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">HonzaČernock`yHonzaˇHonzaČernock`HonzaČernock`y, and Sanjeev Khudanpur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011-01" />
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
	<note>2011 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7753</idno>
		<title level="m">Learning longer memory in recurrent neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Relation extraction: Perspective from convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Continuous space language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="492" to="518" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A latent semantic model with convolutional-pooling structure for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management</title>
		<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>abs/1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2431" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
