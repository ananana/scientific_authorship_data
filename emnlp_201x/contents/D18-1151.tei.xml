<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Targeted Syntactic Evaluation of Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Marvin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Cognitive Science</orgName>
								<orgName type="institution" key="instit1">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit2">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
							<email>tal.linzen@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Cognitive Science</orgName>
								<orgName type="institution" key="instit1">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit2">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Targeted Syntactic Evaluation of Language Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1192" to="1202"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1192</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a dataset for evaluating the gram-maticality of the predictions of a language model. We automatically construct a large number of minimally different pairs of En-glish sentences, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items. We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. In an experiment using this data set, an LSTM language model performed poorly on many of the constructions. Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM&apos;s accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online. This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A language model (LM) defines a probability dis- tribution over sequences of words. Recent techno- logical advances have led to an explosion of neural network-based LM architectures. The most pop- ular ones are based on recurrent neural networks (RNNs) <ref type="bibr" target="#b5">(Elman, 1990;</ref><ref type="bibr" target="#b21">Mikolov et al., 2010)</ref>, in particular Long Short-Term Memory networks (LSTMs) <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber, 1997)</ref>. While a large number of alternative architectures have been proposed in the past few years, LSTMs are still highly competitive ( <ref type="bibr" target="#b20">Melis et al., 2018)</ref>.</p><p>Language models are typically evaluated using perplexity: it is considered desirable for an LM to assign a high probability to held-out data from the same corpus as the training data. This mea- sure conflates multiple sources of success (or fail- ure) in predicting the next word: common collo- cations, semantics, pragmatics, syntax, and so on. The quality of the syntactic predictions made by the LM is arguably particularly difficult to mea- sure using perplexity: since most sentences are grammatically simple and most words can be pre- dicted from their local context, perplexity rewards LMs primarily for collocational and semantic pre- dictions.</p><p>We propose to supplement perplexity with a metric that assesses whether the probability dis- tribution defined by the model conforms to the grammar of the language. Following previous work ( <ref type="bibr" target="#b16">Lau et al., 2017;</ref><ref type="bibr" target="#b19">Linzen et al., 2016;</ref><ref type="bibr" target="#b11">Gulordava et al., 2018)</ref>, we suggest that given two sentences that differ minimally from each other, one of which is grammatical and the other which is not, it is desirable for the model to assign a higher probability to the grammatical one.</p><p>The value of this approach can be illustrated with a recent study by <ref type="bibr" target="#b36">Tran et al. (2018)</ref>, where a standard LSTM language model was compared to an attention-only LM without recurrence ( <ref type="bibr" target="#b38">Vaswani et al., 2017)</ref>. Although the attention-only model had somewhat better perplexity on the valida- tion set, when the models were tested specifically on challenging subject-verb agreement dependen- cies, the attention-only model made three times as many errors as the LSTM. In other words, the LSTM learned more robust syntactic representa- tions, but this advantage was not reflected in its av- erage perplexity on the corpus, since syntactically challenging sentences are relatively infrequent.</p><p>Previous work on targeted syntactic evaluation of language models has identified syntactically challenging sentences in corpora ( <ref type="bibr" target="#b19">Linzen et al., 2016;</ref><ref type="bibr" target="#b11">Gulordava et al., 2018)</ref>. While evaluation on naturally occurring examples is appealing, this approach has its limitations (see Section 2). In particular, syntactically challenging examples are sparsely represented in a corpus, their identifica-tion requires a clean parsed corpus, and naturally occurring sentences are difficult to control for con- founds. We contrast the naturalistic approach with a constructed dataset, which allows us to exam- ine a much larger range of specific grammatical phenomena than has been possible before. We use templates to automatically create our test sen- tences, making it possible to generate a large test set while maintaining experimental control over our materials as well as a balanced number of ex- amples of each phenomenon.</p><p>We test three LMs on the data set we develop: an n-gram baseline, an RNN LM trained on an unannotated corpus, and an RNN LM trained on a multitask objective: language modeling and Com- binatory Categorial Grammar (CCG) supertagging <ref type="bibr" target="#b1">(Bangalore and Joshi, 1999</ref>). We also conduct a human experiment using the same materials. The n-gram baseline largely performed at chance, suggesting that good performance on the task re- quires syntactic representations. The RNN LMs performed well on simple cases, but struggled on more complex ones. Multi-task training with a su- pervised syntactic objective improved the perfor- mance of the RNN, but it was still much weaker than humans. This suggests that our data set is challenging, especially when explicit syntactic su- pervision is not available, and can therefore moti- vate richer language modeling architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of the approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Grammaticality and LM probability</head><p>How should grammaticality be captured in the probability distribution defined by an LM? The most extreme position would be that a language model should assign a probability of zero to un- grammatical sentences. For most applications, some degree of error tolerance is desirable, and it is not practical to assign a sentence a proba- bility of exactly zero. 1 Following <ref type="bibr" target="#b19">Linzen et al. (2016)</ref> and <ref type="bibr" target="#b11">Gulordava et al. (2018)</ref>, our desider- atum for the language model is more modest: if two closely matched sentence differ only in their grammaticality, the probability of the grammati- cal sentence should be higher than the probability of the ungrammatical one. For example, the fol- lowing minimal pair illustrates the fact that third-person present English verbs agree with the num- ber of their subject:</p><p>(1) Simple agreement:</p><p>a. The author laughs. b. *The author laugh.</p><p>We expect the probability of (1a) to be higher than the probability of (1b). Previous work has simpli- fied this setting further by comparing the proba- bility that the LM assigns to a single word that is the locus of ungrammaticality. In (1), for exam- ple, the LM would be fed the first two words of the sentence, and would be considered successful on the task if it predicts P (laughs) &gt; P (laugh). The prediction setting is only applicable when the locus of ungrammaticality is a single word, rather than, say, the interaction between two words; moreover, the information needed to make the grammaticality decision needs to be available in the left context of the locus of grammaticality. These conditions do not always hold. Negative po- larity items (NPIs), for example, are words like any and ever that can only be used in the scope of negation. <ref type="bibr">2</ref> The grammaticality of placing a par- ticular quantifier in the beginning of the sentences in (2) depends on whether the sentence contains an NPI later on:</p><p>(2) Simple NPI:</p><p>a. No students have ever lived here. b. *Most students have ever lived here.</p><p>It would not be possible to compare these two sen- tences using the prediction task. In the current pa- per, we use the more general setting and compare the probability of the two complete sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data set construction</head><p>Previous work has used syntactically complex sen- tences identified from a parsed corpus. This ap- proach has several limitations. If the corpus is automatically parsed, the risk of a parse error in- creases with the complexity of the construction <ref type="bibr" target="#b2">(Bender et al., 2011</ref>). If the test set is restricted to sentences with gold parses, it can be difficult or impossible to find a sufficient number of exam- ples of syntactically challenging cases. Moreover, using naturally occurring sentences can introduce confounds that may complicate the interpretation of the experiments <ref type="bibr" target="#b7">(Ettinger et al., 2018)</ref>.</p><p>To circumvent these issues, we use templates to automatically construct a large number of En- glish sentence pairs (∼350,000). Our data set in- cludes three phenomena that linguists consider to be sensitive to hierarchical syntactic structure <ref type="bibr" target="#b8">(Everaert et al., 2015;</ref><ref type="bibr" target="#b41">Xiang et al., 2009)</ref>: subject- verb agreement (described in detail in Sections 4.1 and 4.2), reflexive anaphora (Section 4.3) and neg- ative polarity items (Section 4.4).</p><p>The templates can be described using non- recursive context-free grammars. We specify the preterminal symbols that make up a syntactic con- struction and have different terminal symbols that those preterminals could be mapped to. For ex- ample, the template for the simple agreement con- struction illustrated in (1) consists of the following rules:</p><formula xml:id="formula_0">(3) a. Simple agreement → D MS MV b. D → the c. MS → {author, pilot, . . .} d. MV → {laughs, smiles, . . .}</formula><p>We generate all possible combinations of the ter- minals. The Supplementary Materials provide a full description of all our templates. 3 While these examples are somewhat artificial, our goal is to isolate the syntactic capabilities of the model; it is in fact beneficial to minimize the semantic or collocational cues that can be used to identify the grammatical sentence. Gulordava et al. took this approach further and constructed "colorless green ideas" test cases by substituting random content words into sentences from a cor- pus. We take a more moderate position and avoid combinations that are very implausible or violate selectional restrictions (e.g., the apple laughs). We do this by having separate templates for animate and inanimate subjects and verbs so that the re- sulting sentences are always reasonably plausible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related work</head><p>Targeted evaluation: LM evaluation data sets using challenging prediction tasks have been pro- posed in the context of semantics and discourse comprehension ( <ref type="bibr" target="#b42">Zweig and Burges, 2011;</ref><ref type="bibr" target="#b23">Paperno et al., 2016)</ref>. Evaluation sets consisting of chal-lenging syntactic constructions have been con- structed for parser evaluation ( <ref type="bibr" target="#b27">Rimell et al., 2009;</ref><ref type="bibr" target="#b22">Nivre et al., 2010;</ref><ref type="bibr" target="#b2">Bender et al., 2011)</ref>, and mini- mal pair approaches have been proposed for eval- uating image captioning ( <ref type="bibr" target="#b32">Shekhar et al., 2017)</ref> and machine translation systems <ref type="bibr" target="#b31">(Sennrich, 2017)</ref>, but no data sets exist that target a range of syntactic constructions for language model evaluation.</p><p>Acceptability judgments: <ref type="bibr" target="#b16">Lau et al. (2017)</ref> compared the ability of different LMs to pre- dict graded human acceptability judgments. The forced-choice approach used in the current pa- per has been shown to be effective in human acceptability judgment experiments <ref type="bibr" target="#b33">(Sprouse and Almeida, 2017)</ref>. In some early work, neural net- works were trained explicitly to predict acceptabil- ity judgments ( <ref type="bibr" target="#b17">Lawrence et al., 1996;</ref><ref type="bibr" target="#b0">Allen and Seidenberg, 1999</ref>); Post (2011) likewise trained a classifier on top of a parser to predict grammatical- ity. <ref type="bibr" target="#b40">Warstadt et al. (2018)</ref> use a transfer learning approach, where an unsupervised model is fine- tuned on acceptability prediction. Our work dif- fers from those studies in that we do not advocate providing any explicit grammaticality signal to the LM at any point ("no negative evidence").</p><p>Syntax in LMs: There have been several pro- posals over the years to incorporate explicit syn- tax into LMs to overcome the inability of n-gram LMs to model long-distance dependencies <ref type="bibr" target="#b14">(Jurafsky et al., 1995;</ref><ref type="bibr" target="#b28">Roark, 2001;</ref><ref type="bibr" target="#b24">Pauls and Klein, 2012)</ref>. While RNN language models can in prin- ciple model longer dependencies <ref type="bibr" target="#b21">(Mikolov et al., 2010;</ref><ref type="bibr" target="#b19">Linzen et al., 2016)</ref>, in practice it can still be beneficial to inject syntax into the model. This can be done by combining it with a supervised parser ( <ref type="bibr" target="#b4">Dyer et al., 2016</ref>) or other multi-task learning ob- jectives ( <ref type="bibr" target="#b6">Enguehard et al., 2017</ref>). Our work is or- thogonal to this area of research, but can be seen as providing a potential opportunity to underscore the advantage of such syntax-infused models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data set composition</head><p>This section describes all of the types of sentence pairs included in our data set, which include exam- ples of subject-verb agreement (Sections 4.1 and 4.2), reflexive anaphoras (Section 4.3) and nega- tive polarity items (Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Subject-verb agreement</head><p>Determining the correct number of the verb is triv- ial in examples such as (1) above, in which the sentence only contains a single noun. By contrast, in cases where there are multiple nouns in the sen- tence, identifying which of them is the subject of a given verb requires understanding the structure of the sentence. In particular, the relevant subject is not necessarily the first noun of the sentence:</p><p>(4) Agreement in a sentential complement:</p><p>a. The bankers knew the officer smiles. b. *The bankers knew the officer smile.</p><p>Here the verb smiles needs to agree with the em- bedded subject officer rather than the main clause subject bankers. The subject is also not necessar- ily the most recent noun before the verb: when the subject is modified by a phrase, a distracting noun ("attractor") often intervenes in the linear or- der of the sentence between the head of the subject and the verb. Two examples of such modifiers are prepositional phrases and relative clauses (RCs):</p><p>(5) Agreement across a prepositional phrase: a. The farmer near the parents smiles. b. *The farmer near the parents smile.</p><p>(6) Agreement across a subject relative clause: a. The officers that love the skater smile. b. *The officers that love the skater smiles.</p><p>We include all four possible configurations of noun number for each type of minimal pair; for (5), these would be: 4 Sentences where the two nouns conflict in num- ber are expected to be more challenging, but in- terpretable errors may certainly occur even when they do not. For example, the model may use the heuristic that sentences with multiple nouns are likely to have a plural verb (a heuristic that (i) a. The farmer near the parent smiles.</p><p>b. *The farmer near the parent smile.</p><p>would be effective for coordination); alternatively, it might prefer singular verbs to plural ones regard- less of whether the subject is singular or plural, simply because the singular form of the verb is more frequent. Next, in verb phrase (VP) coordination, both of the verbs need to agree with the subject:</p><formula xml:id="formula_1">(8) Short VP coordination:</formula><p>a. The senator smiles and laughs. b. *The senator smiles and laugh.</p><p>We had both singular and plural subjects. The number of the verb immediately adjacent to the subject was always grammatical. This problem can in principle be solved with a trigram model (smiles and laughs is likely to be a more frequent trigram than smiles and laugh); to address this po- tential concern, we also included a coordination condition with a longer dependency:</p><formula xml:id="formula_2">(9) Long VP coordination:</formula><p>The manager writes in a journal every day and likes/*like to watch television shows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Agreement and object relative clauses</head><p>We go into greater depth in object relative clauses, which most clearly require a hierarchical represen- tation. In (10) and (11), the model needs to be able to distinguish the embedded subject (parents) from the main clause subject (farmer) when mak- ing its predictions:</p><p>(10) Agreement across an object relative clause: a. The farmer that the parents love swims. b. *The farmer that the parents love swim.</p><p>(11) Agreement in an object relative clause: a. The farmer that the parents love swims. b. *The farmer that the parents loves swims.</p><p>In keeping with the minimal pair approach, we never introduce two agreement errors at the same time: either the embedded verb or the main verb is incorrectly inflected, but not both.</p><p>We include a number of variations on the pat- tern in (11). First, we delete the relativizer that, with the hypothesis that the absence of an overt cue to structure will make the task more difficult:</p><p>(12) The farmer the parents love/*loves swims.</p><p>In another condition, we replace the main sub- ject with an inanimate noun and keep the embed-ded subject animate. We base this manipulation on human experimental work showing that sim- ilar nouns (for example, two animate nouns) are more likely to cause confusion during comprehen- sion than dissimilar nouns, such as an animate and an inanimate noun (Van Dyke, 2007):</p><p>(13) The movies that the author likes are/*is good.</p><p>For a complete list of all the types of minimal pairs we include, see the Supplementary Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Reflexive anaphora</head><p>A reflexive pronoun such as himself needs to have an antecedent from which it derives its interpreta- tion. The pronoun needs to agree in number (and gender) with its antecedent:</p><formula xml:id="formula_3">(14) Simple reflexive:</formula><p>a. The senators embarrassed themselves. b. *The senators embarrassed herself.</p><p>There are structural conditions on the nouns to which a reflexive pronoun can be bound. One of these conditions requires the antecedent to be in the same clause as the reflexive pronoun. For ex- ample, (15b) cannot refer to a context in which the pilot embarrassed the bankers: Likewise, in the following minimal pair, sentence (16b) is ungrammatical, because the reflexive pro- noun themselves, which is part of the main clause, cannot be bound to the noun phrase the architects, which is inside an embedded clause:</p><p>(16) Reflexive across an object relative clause: a. The manager that the architects like doubted himself. b. *The manager that the architects like doubted themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Negative polarity items</head><p>Negative polarity items, introduced in example (2) above, are words that (to a first approximation) need to occur in the context of negation. Crucially for the purposes of the present work, the scope of negation is structurally defined. In particular the negative noun phrase needs to c-command the NPI: the syntactic non-terminal node that domi- nates the negative noun phrase must also domi- nate the NPI. This is the case in (17a), but not in (17b), where the negative noun phrase is too deep in the tree to c-command the NPI ever ( <ref type="bibr" target="#b41">Xiang et al., 2009;</ref><ref type="bibr" target="#b8">Everaert et al., 2015</ref>).</p><p>(17) NPI across a relative clause: a. No authors that the security guards like have ever been famous. b. *The authors that no security guards like have ever been famous.</p><p>All of the nouns and verbs in the NPI cases were plural. As in some of the agreement cases, we in- cluded a variant of <ref type="formula">(17)</ref> in which the subject was inanimate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental setup</head><p>To show how our challenge set can be used to eval- uate the syntactic performance of LMs, we trained three LMs with increasing levels of syntactic so- phistication. All of the LMs were trained on a 90 million word subset of Wikipedia (Gulordava et al., 2018). Our n-gram LM and LSTM LM do not require annotated data. The third model is also an LSTM LM, but it requires syntactically anno- tated data (CCG supertags).</p><p>N-gram model: We trained a 5-gram model on the same 90M word corpus using the SRILM toolkit <ref type="bibr" target="#b34">(Stolcke, 2002</ref>) which backs off to smaller n-grams using Kneser-Ney smoothing.</p><p>Single-task RNN: The RNN LM had two layers of 650 LSTM units, a batch size of 128, a dropout rate of 0.2, and a learning rate of 20.0, and was trained for 40 epochs (following the hyperparam- eters of Gulordava et al. 2018).</p><p>Multi-task RNN: In multi-task learning, the system is trained to optimize an objective func- tion that combines the objective functions of sev- eral tasks. We combine language modeling with CCG supertagging, a task that predicts for each word in the sentence its CCG supertag (Bangalore and Joshi, 1999; <ref type="bibr" target="#b18">Lewis et al., 2016</ref>). We sim- ply sum the two objective functions with equal weights ( <ref type="bibr" target="#b6">Enguehard et al., 2017)</ref>. Early stopping in this model is based on the combined loss on language modeling and supertagging. Supertags provide a large amount of syntactic information about the word; the sequence of supertags of a sentence strongly constrains the possible parses of the sentence. We use supertagging as a "scaffold" task ( <ref type="bibr" target="#b35">Swayamdipta et al., 2017)</ref>: our goal is not to produce a competitive supertagger, but to induce better syntactic representations, which would then lead to improved language modeling. We used CCG-Bank (Hockenmaier and Steedman, 2007) as our CCG corpus.</p><p>Human evaluation: We designed a human ex- periment on Amazon Mechanical Turk that mir- rored the task that was given to the LMs: both ver- sions of a minimal pair were shown on the screen at the same time, and participants were asked to judge which one of them was more acceptable (for details, see the Supplementary Materials). We em- phasize that we do not see human performance on complex syntactic dependencies as setting an up- per bound on the performance that we should ex- pect from an LM. There is a rich literature showing that humans make mistakes such as subject-verb agreement errors; in fact, most of the phenomena we test were inspired by work in psycholinguistics that studies these errors <ref type="bibr" target="#b3">(Bock and Miller, 1991;</ref><ref type="bibr" target="#b25">Phillips et al., 2011</ref>). At the same time, while we do not see a reason not to aspire for 100% accu- racy, we are interested in comparing LM and hu- man errors: if the errors are similar, the two sys- tems may be using similar representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Local agreement: The overall accuracy per condition can be seen in <ref type="table">Table 1</ref>. The n-gram LM's accuracy was only 79% for simple agree- ment and agreement in a sentential complement, both of which can be solved entirely using local context. This is because not all subject and verb combinations in our materials appeared verbatim in the 90M word training corpus; for those combi- nations, the model fell back on unigram probabili- ties, which in this context amounts to selecting the more frequent form of the verb. Both RNNs performed much better than the n-gram model on the simple agreement case (single-task: 94%; multi-task: 100%), reflecting these models' ability to generalize beyond the spe- cific bigrams that occurred in the corpus. Ac- curacy on agreement in a sentential complement was also very high (single-task: 99%; multi-task: 93%). This indicates that the RNNs do not rely on the heuristic whereby the first noun of the sentence is likely to be its subject. They did slightly worse but still very well on short VP coordination (both 90%); this dependency is also local, albeit across the word and.</p><p>Non-local agreement: The accuracy of the n-gram model on non-local dependencies (long VP coordination and agreement across a phrase or a clause) was very close to 50%. This sug- gests that local collocational information is not useful in these conditions. The single-task RNN also performed much more poorly on these con- ditions than on the local agreement conditions, though for the most part its accuracy was better than chance. Humans did worse on these depen- dencies as well, but their accuracy did not drop as sharply as the RNNs' (human accuracies ranged from 82% to 88%). In most of these cases, multi- task learning was very helpful; for example, accu- racy in long VP coordination increased from 61% to 81%. Still, both RNNs performed poorly on agreement across an object RC, especially with- out that, whereas humans performed comparably on all non-local dependencies.</p><p>Agreement inside an object RC: This case is particularly interesting, because this dependency is purely local (see <ref type="formula">(11)</ref>), and the interference is from the distant sentence-initial noun. Although this configuration is similar to the sentential com- plement case, performance was worse both in RNNs and humans. However, RNNs performed better than humans, at least when the sentence in- cluded the overt relativizer that. This suggests that interference is sensitive to proximity in RNNs but to syntactic status in humans -humans appear to be confusing the main clause subject and the em- bedded subject ( <ref type="bibr" target="#b39">Wagers et al., 2009</ref>).</p><p>Reflexive anaphora: The RNNs' performance was significantly worse on simple reflexives (83%) than on simple agreement (94%), and did not dif- fer between the single-task and multi-task mod- els. By contrast, human performance did not dif- fer between subject-verb agreement and reflexive anaphoras. The surprisingly poor performance for this adjacent dependency seems to be due to an asymmetry in accuracy between himself and them- selves on the one hand (100% accuracy in the multi-task RNN) and herself on the other hand (49% accuracy). <ref type="bibr">5</ref>   pronouns in the structurally complex case in which the dependency was across a relative clause (55% compared to 87% in humans).</p><note type="other">Across a prepositional phrase 0.57 0.69 0.50 0.85 44800 Across a subject relative clause 0.56 0.74 0.50 0.88 22400 Across an object relative clause 0.50 0.57 0.50 0.85 44800 Across an object relative (no that) 0.52 0.52 0.50 0.82 44800 In an object relative clause 0.84 0.89 0.50 0.78 44800 In an object relative (no that) 0.71 0.81 0.50 0.</note><p>NPIs: The dependency in simple NPIs spans only four words, so the n-gram model could in principle capture it. In practice, the n-gram model systematically selected the wrong answer, sug- gesting that it backed off to comparing the bi- grams no students and most students, the first of which is presumably less frequent. Surprisingly, the n-gram model's accuracy was higher than 50% on NPIs across a relative clause, a dependency that spans more than five words. In this case, the bi- grams that the and the chef (for example) happen to be more frequent than the that no and no chef. This difference was apparently strong enough to make up for the low-frequency bigram at the start of the sentence. The RNNs did poorly on this task. The accu- racy of the single-task model was around 40%. The multi-task did somewhat better on the simple NPIs (48%) and much better on the NPIs across a relative clause (73%). At the same time, an exam- ination of the plot of log probability of each word in a sentence ( <ref type="figure">Figure A.1</ref> in the Supplementary Materials) suggests that the single-task RNN is in ber representation learned for herself was not robust. An- other possibility is that gender bias reduces the probability of an anaphoric relation between herself and words such as surgeon ( <ref type="bibr" target="#b29">Rudinger et al., 2018).</ref> fact able to differentiate between the grammatical and ungrammatical sentences when it reaches the NPI, but this difference does not offset the overall probability advantage of the ungrammatical sen- tence (which is likely due to non-grammatical col- locational factors). In any case, the fact that the n-gram baseline did not perform at chance sug- gests that there are non-syntactic cues to this task, complicating the interpretation of the performance of other LMs.</p><p>Perplexity: The perplexity of the n-gram model on the Wikipedia test data was 157.5, much higher than the perplexity of the single-task RNN (78.65) and the multi-task <ref type="bibr">RNN (61.10)</ref>. In other words, perplexity tracked accuracy on our syntactic data set -an unsatisfying outcome given our goal of dissociating perplexity and our syntactic evalua- tion method, but an expected one given that each model was conditioned on richer information than the previous one. In previous work, perplexity and syntactic judgment accuracy have been found to be partly dissociable ( <ref type="bibr" target="#b15">Kuncoro et al., 2018;</ref><ref type="bibr" target="#b36">Tran et al., 2018)</ref>.</p><p>Lexical variation and frequency: There was considerable lexical variation in the results; we have mentioned the surprising asymmetry be- tween himself and herself above. As another case study, we examine variation in the results of the simple agreement condition in the single-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main</head><p>Embedded Single-task Multi-task Humans Example sentence subject subject</p><p>Across an objective relative clause:  <ref type="table">Table 2</ref>: Accuracy within and across an object relative clause (only in the cases in which the main subject was animate and the relativizer that was present). The subject that the verb is expected to agree with is underlined.</p><formula xml:id="formula_4">Singular</formula><p>task RNN. Accuracy varied by verb, ranging from is and are, which had 100% accuracy, to swims, where accuracy was only 60% (recall that average accuracy was 94%). This may be a frequency ef- fect: either the LM is learning less robust number representations for infrequent verbs, or the tail of the distribution over the vocabulary is more frag- ile during word prediction. <ref type="bibr" target="#b24">Pauls and Klein (2012)</ref> propose normalizing for unigram frequency when deriving acceptability judgments from an LM. Our preliminary experiments with this method did not significantly improve overall performance; regard- less of the effectiveness of this method, such cor- rections should arguably not be necessary in an LM that adequately captures grammaticality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Case study: agreement and object relative clauses</head><p>The overall results in <ref type="table">Table 1</ref> were averaged over all of the possible number configurations within each condition. In this section, we take a closer look at agreement in sentences with an object RC (see <ref type="table">Table 2</ref>). This kind of finer-grained analy- sis helps explain the cases in which the LMs are failing, and might reveal some of the patterns or heuristics the LMs are using. Performance in agreement across an object RC was poor. Both RNNs made attraction errors: they often preferred the verb that agreed in number with the irrelevant embedded subject to the verb that agreed with the correct main subject. The multi- task RNN showed greater symmetry between the simpler singular/singular and plural/plural cases, whereas the single-task RNN performed poorly even in these cases, often preferring a singular verb when both subjects were plural. This default preference for singular verbs matches the behavior of younger children <ref type="bibr" target="#b9">(Franck et al., 2004</ref>).</p><p>Performance in agreement within an object RC was better; still, the single-task RNN made the most errors when both subjects were singular, per- haps due to a heuristic in which a sentence with multiple subjects is likely to have a plural verb (as in coordination sentences). By contrast, the multi- task model seemed to have a general bias towards singular subjects in this condition. Incidentally, the human results with object RCs were also unex- pected: while attraction errors when the two sub- jects differ in number are to be expected ( <ref type="bibr" target="#b39">Wagers et al., 2009)</ref>, our participants made a sizable num- ber of errors even when both subjects were plural.</p><p>Despite the generally poor performance in ob- ject RCs, Figures A.2 and A.3 in the Supple- mentary Materials show that the single-task RNN is typically assigning a higher probability to the grammatical word of a minimal pair than to the ungrammatical word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>We have described a template-based data set for targeted syntactic evaluation of language models. The data set consists of pairs of sentences that are matched except for their grammaticality; we con- sider a language model to capture the relevant as- pects of the grammar of the language if it assigns a higher probability to the grammatical sentence than to the ungrammatical one.</p><p>An RNN language model performed very well on local subject-verb agreement dependencies, significantly outperforming an n-gram baseline. This suggests that the task is a viable evalua- tion strategy. Even on simple cases, however, the RNN's accuracy was sensitive to the partic- ular lexical items that occurred in the sentence; this would not be expected if its syntactic repre- sentations were fully abstract. The RNN's per- formance degraded markedly on non-local depen- dencies, approaching chance levels on agreement across an object relative clause. Multi-task train- ing with a syntactic objective (CCG supertagging) mitigated this drop in performance for some but not all of the dependencies we tested. We con- jecture that the benefits of the inductive bias con- ferred by multi-task learning will be amplified when the amount of training data is limited.</p><p>Our results contrast with the results of <ref type="bibr" target="#b11">Gulordava et al. (2018)</ref>, who obtained a prediction accu- racy of 81% on English sentences from their test corpus and 74% on constructed sentences modeled after sentences from the corpus. It is likely that our sentences are more syntactically challenging than the ones they were able to find in the relatively small manually annotated treebank they used.</p><p>One limitation of our approach is that it is not always clear what constitutes a minimal grammati- cality contrast. In the subject-verb agreement case, the contrast was clear: the two present-tense forms of the verb, e.g., laugh vs. laughs. Our NPI ma- nipulations, on the other hand, were less success- ful: the members of the contrasts differed not only in their syntactic structure but also in low-level n-gram probabilities, making the performance on this particular contrast harder to interpret.</p><p>We emphasize that the goal of this article was not to advocate for LSTMs in particular as an ef- fective architecture for modeling syntax; indeed, our results show that LSTM language models are far from matching naive annotators' performance on this task, let alone performing at 100% accu- racy. We hope that our data set, and future ex- tensions to other phenomena and languages, will make it possible to measure progress in syntactic language modeling and will lead to better under- standing of the syntactic generalizations captured by language models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 15 )</head><label>15</label><figDesc>Reflexive in a sentential complement: a. The bankers thought the pilot embar- rassed himself. b. *The bankers thought the pilot embar- rassed themselves.</figDesc></figure>

			<note place="foot" n="1"> Nor is it possible to have a threshold such that all grammatical sentences have probability higher than and all ungrammatical sentences have probability lower than , for the simple reason that there is an infinite number of grammatical sentences (Lau et al., 2017).</note>

			<note place="foot" n="2"> In practice, the conditions that govern the distribution of NPIs are much more complicated, but this first approximation will suffice for the present purposes. For a review, see Giannakidou (2011).</note>

			<note place="foot" n="3"> The code, the data set and the Supplementary Materials can be found at https://github.com/ BeckyMarvin/LM_syneval.</note>

			<note place="foot" n="4"> The slash notation indicates the word that differs between the grammatical and ungrammatical sentence; for example, in (7a), the full sentence pair would be:</note>

			<note place="foot" n="5"> This may be because himself and themselves are significantly more frequent than herself, and consequently the num</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgments</head><p>We would like to thank Ming Xiang for sharing materials from human experiments that inspired many of our test cases. We also thank Brian Roark and the JHU Computational Psycholinguistics lab for discussion, and Brian Leonard for help con-ducting the human experiment.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The emergence of grammaticality in connectionist networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Seidenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Emergentist approaches to language: Proceedings of the 28th Carnegie symposium on cognition</title>
		<editor>Brian MacWhinney</editor>
		<meeting><address><addrLine>Mahwah, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Lawrence Erlbaum Associates</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="115" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Supertagging: An approach to almost parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Bangalore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="237" to="265" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Parser evaluation over local and non-local deep dependencies in a large corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Flickinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Oepen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="397" to="408" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Broken agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="93" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="199" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring the syntactic abilities of RNNs with multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>´ Emile Enguehard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Linzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Assessing composition in sentence vector representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allyson</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1790" to="1801" />
		</imprint>
		<respStmt>
			<orgName>Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Structures, not strings: Linguistics as part of the cognitive sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Everaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Marinus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Huybregts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Chomsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><forename type="middle">J</forename><surname>Berwick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bolhuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="729" to="743" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Normal and pathological development of subject-verb agreement in speech production: A study on French children</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Franck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephany</forename><surname>Cronel-Ohayon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><surname>Chillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><forename type="middle">H</forename><surname>Frauenfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Hamann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Rizzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Zesiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neurolinguistics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="147" to="180" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Negative and positive polarity items: Variation, licensing, and compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Giannakidou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semantics: An international handbook of natural language meaning</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Mouton de Gruyter</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1660" to="1712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Colorless green recurrent networks dream hierarchically</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Gulordava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1195" to="1205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="355" to="396" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using a stochastic context-free grammar as a language model for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuck</forename><surname>Wooters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Fosler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><surname>Tajchaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="189" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LSTMs can learn syntax-sensitive dependencies well, but modeling structure makes them better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1426" to="1436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Grammaticality, acceptability, and probability: A probabilistic view of linguistic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalom</forename><surname>Lappin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1202" to="1247" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Can recurrent neural networks learn natural language grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><forename type="middle">C</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santliway</forename><surname>Fong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1853" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">LSTM CCG parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Assessing the ability of LSTMs to learn syntax-sensitive dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="521" to="535" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH 2010)</title>
		<meeting>the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH 2010)<address><addrLine>Makuhari, Chiba, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Evaluation of dependency parsers on unbounded dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">Gómez</forename><surname>Rodríguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="833" to="841" />
		</imprint>
	</monogr>
	<note>Coling 2010 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The LAMBADA dataset: Word prediction requiring a broad discourse context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Quan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Fernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1525" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large-scale syntactic language modeling with treelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Pauls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="959" to="968" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Grammatical illusions and selective fallibility in real-time language comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Wagers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><forename type="middle">F</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Syntax and Semantics</title>
		<editor>Jeffrey T. Runner</editor>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="153" to="186" />
			<date type="published" when="2011" />
			<publisher>Emerald</publisher>
		</imprint>
	</monogr>
	<note>editor, Experiments at the Interfaces</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Judging grammaticality with tree substitution grammar derivations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="217" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unbounded dependency recovery for parser evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="813" to="821" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Probabilistic top-down parsing and language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="276" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gender bias in coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">How grammatical is characterlevel neural machine translation? Assessing MT quality with contrastive translation pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="376" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">FOIL it! Find one mismatch between image and language caption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yauhen</forename><surname>Klimovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurélie</forename><surname>Herbelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="255" to="265" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Design sensitivity and statistical power in acceptability judgment experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Sprouse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<pubPlace>Glossa</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SRILM-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proccedings of the Seventh International Conference on Spoken Language Processing</title>
		<meeting>cedings of the Seventh International Conference on Spoken Language essing</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="901" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Frame-semantic parsing with softmax-margin segmental RNNs and a syntactic scaffold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09528</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The importance of being recurrent for modeling hierarchical structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Interference effects from grammatically unavailable constituents during sentence processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><forename type="middle">A</forename><surname>Van Dyke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="407" to="430" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Agreement attraction in comprehension: Representations and processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Wagers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><forename type="middle">F</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="206" to="237" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bowman</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12471</idno>
	</analytic>
	<monogr>
		<title level="m">Neural network acceptability judgments</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Illusory licensing effects across dependency types: ERP evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain and Language</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="55" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">The Microsoft Research sentence completion challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<pubPlace>Microsoft</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
