<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Sparse Word Representations with Distributional Inference for Semantic Composition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kober</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">TAG laboratory</orgName>
								<orgName type="institution">University of Sussex Brighton</orgName>
								<address>
									<postCode>BN1 9RH</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">TAG laboratory</orgName>
								<orgName type="institution">University of Sussex Brighton</orgName>
								<address>
									<postCode>BN1 9RH</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reffin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">TAG laboratory</orgName>
								<orgName type="institution">University of Sussex Brighton</orgName>
								<address>
									<postCode>BN1 9RH</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">TAG laboratory</orgName>
								<orgName type="institution">University of Sussex Brighton</orgName>
								<address>
									<postCode>BN1 9RH</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Sparse Word Representations with Distributional Inference for Semantic Composition</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1691" to="1702"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Distributional models are derived from co-occurrences in a corpus, where only a small proportion of all possible plausible co-occurrences will be observed. This results in a very sparse vector space, requiring a mechanism for inferring missing knowledge. Most methods face this challenge in ways that render the resulting word representations uninterpretable, with the consequence that semantic composition becomes hard to model. In this paper we explore an alternative which involves explicitly inferring un-observed co-occurrences using the distribu-tional neighbourhood. We show that distribu-tional inference improves sparse word representations on several word similarity benchmarks and demonstrate that our model is competitive with the state-of-the-art for adjective-noun, noun-noun and verb-object compositions while being fully interpretable.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The aim of distributional semantics is to derive meaning representations based on observing co- occurrences of words in large text corpora. However not all plausible co-occurrences will be observed in any given corpus, resulting in word representa- tions that only capture a fragment of the meaning of a word. For example the verbs "walking" and "strolling" may occur in many different and pos- sibly disjoint contexts, although both verbs would be equally plausible in numerous cases. This sub- sequently results in incomplete representations for both lexemes. In addition, models based on counting co-occurrences face the general problem of sparsity in a very high-dimensional vector space. The most common approaches to these challenges have in- volved the use of various techniques for dimension- ality reduction <ref type="bibr" target="#b7">(Bullinaria and Levy, 2012;</ref><ref type="bibr" target="#b21">Lapesa and Evert, 2014)</ref> or the use of low-dimensional and dense neural word embeddings ( <ref type="bibr" target="#b28">Mikolov et al., 2013;</ref><ref type="bibr" target="#b37">Pennington et al., 2014</ref>). The common prob- lem in both of these approaches is that composition becomes a black-box process due to the lack of inter- pretability of the representations. Count-based mod- els are therefore a very attractive line of work with regards to a number of important long-term research challenges, most notably the development of an ade- quate model of distributional compositional seman- tics. In this paper we propose the use of distribu- tional inference (DI) to inject unobserved but plau- sible distributional semantic knowledge into the vec- tor space by leveraging the intrinsic structure of the distributional neighbourhood. This results in richer word representations and furthermore mitigates the sparsity effect common in high-dimensional vector spaces, while remaining fully interpretable. Our contributions are as follows: we show that typed and untyped sparse word representations, enriched by distributional inference, lead to performance im- provements on several word similarity benchmarks, and that a higher-order dependency-typed vector space model, based on "Anchored Packed Depen- dency Trees (APTs)" <ref type="bibr" target="#b45">(Weir et al., 2016)</ref>, is com- petitive with the state-of-the-art for adjective-noun, noun-noun and verb-object compositions. Using our method, we are able to bridge the gap in perfor- mance between high dimensional interpretable mod-els and low dimensional non-interpretable models and offer evidence to support a possible explanation of why high-dimensional models usually perform worse, together with a simple, practical method for over-coming this problem. We furthermore demon- strate that intersective approaches to composition benefit more from distributional inference than com- position by union and highlight the ability of com- position by intersection to disambiguate the mean- ing of a phrase in a local context. The remainder of this paper is structured as follows: we discuss related work in section 2, followed by an introduction of the APT framework for semantic composition in section 3. We describe distributional inference in section 4 and present our experimental work, together with our results in section 5. We con- clude this paper and outline future work in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our method follows the distributional smoothing approach of <ref type="bibr" target="#b10">Dagan et al. (1994)</ref> and <ref type="bibr" target="#b11">Dagan et al. (1997)</ref>. In these works the authors are con- cerned with smoothing the probability estimate for unseen words in bigrams. This is achieved by mea- suring which unobserved bigrams are more likely than others on the basis of the Kullback-Leibler di- vergence between bigram distributions. This has led to significantly improved performance on a lan- guage modelling for speech recognition task, as well as for word-sense disambiguation in machine trans- lation ( <ref type="bibr" target="#b10">Dagan et al., 1994;</ref><ref type="bibr" target="#b11">Dagan et al., 1997</ref>). More recently <ref type="bibr" target="#b34">Pad√≥ et al. (2013)</ref> used a distribu- tional approach for smoothing derivationally related words, such as oldish -old, as a back-off strategy in case of data sparsity. However, none of these approaches have used distributional inference as a general technique for directly enriching sparse dis- tributional vector representations, or have explored its behaviour for semantic composition. Compositional models of distributional semantics have become an increasingly popular topic in the re- search community. Starting from simple pointwise additive and multiplicative approaches to composi- tion, such as <ref type="bibr" target="#b29">Mitchell and Lapata (2008;</ref>, and <ref type="bibr" target="#b4">Blacoe and Lapata (2012)</ref>, to tensor based models, such as <ref type="bibr" target="#b3">Baroni and Zamparelli (2010)</ref>, <ref type="bibr">Coecke et al. (2010)</ref>, <ref type="bibr" target="#b17">Grefenstette et al. (2013)</ref> and <ref type="bibr" target="#b35">Paperno et al. (2014)</ref>, and neural network based approaches, such as <ref type="bibr" target="#b40">Socher et al. (2012)</ref>, <ref type="bibr" target="#b22">Le and Zuidema (2015)</ref>, <ref type="bibr" target="#b31">Mou et al. (2015)</ref> and <ref type="bibr" target="#b41">Tai et al. (2015)</ref>. <ref type="bibr" target="#b48">Zanzotto et al. (2015)</ref> provide a decompositional analysis of how similarity is affected by distributional compo- sition, and link compositional models to convolu- tion kernels. Most closely related to our approach of composition are the works of <ref type="bibr" target="#b42">Thater et al. (2010)</ref>, <ref type="bibr" target="#b43">Thater et al. (2011)</ref> and <ref type="bibr">Weeds et al. (2014)</ref>, which aim to provide a general model of compositionality in a typed distributional vector space. In this paper we adopt the approach to distributional composition introduced by <ref type="bibr" target="#b45">Weir et al. (2016)</ref>, whose APT frame- work is based on a higher-order dependency-typed vector space, however they do not address the issue of sparsity in their work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>Distributional vector space models can broadly be categorised into untyped proximity based models and typed models ( <ref type="bibr" target="#b1">Baroni and Lenci, 2010)</ref>. Examples of the former include Deer- <ref type="bibr">wester et al. (1990)</ref>; <ref type="bibr" target="#b26">Lund and Burgess (1996)</ref>; <ref type="bibr" target="#b9">Curran (2004)</ref>; <ref type="bibr" target="#b38">Sahlgren (2006)</ref>; <ref type="bibr" target="#b6">Bullinaria and Levy (2007)</ref> and <ref type="bibr" target="#b44">Turney and Pantel (2010)</ref>. These models count the number of times every word in a large corpus co-occurs with other words within a specified spatial context window, without leveraging the structural information of the text. Typed models on the other hand, take the grammatical relation between two words for a co-occurrence event into account. Early proponents of that approach are <ref type="bibr" target="#b18">Grefenstette (1994)</ref> and <ref type="bibr" target="#b25">Lin (1998)</ref>. More recent work by <ref type="bibr" target="#b33">Pad√≥ and Lapata (2007)</ref>, <ref type="bibr" target="#b14">Erk and Pad√≥ (2008)</ref> and <ref type="bibr" target="#b45">Weir et al. (2016)</ref> uses dependency paths to build a structured vector space model. In both kinds of models, the raw counts are usually transformed by Positive Pointwise Mutual Informa- tion (PPMI) or a variant of it <ref type="bibr" target="#b8">(Church and Hanks, 1990;</ref><ref type="bibr" target="#b32">Niwa and Nitta, 1994;</ref><ref type="bibr" target="#b39">Scheible et al., 2013;</ref><ref type="bibr" target="#b23">Levy and Goldberg, 2014</ref>). In the following we will give an explanation of the theory of composition with APTs as introduced by <ref type="bibr" target="#b45">Weir et al. (2016)</ref>, which we adopt in this paper. In addition to direct relations between two words, the APT model also considers inverse and higher order relations. Inverse relations are denoted with a horizontal bar above the dependency relation, such as amod for an inverse adjectival modifier. Higher order dependencies are separated by a colon as in the second order distributional feature dobj:nsubj. The example below illustrates how raw text is processed to retrieve elementary representations in our APT model. As an example we consider a lowercased corpus consisting of the sentences:</p><p>we folded the clean clothes i like your clothes we bought white shoes yesterday he folded the white sheets</p><p>We dependency parse the raw sentences and, following <ref type="bibr" target="#b45">Weir et al. (2016)</ref>, align and aggregate the resulting parse trees according to their dependency type as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. For example the lexeme clothes has the distributional features amod:dry and dobj:nsubj:we among others. Over a large corpus, this results in a very high-dimensional and sparse vector space, which due to its typed nature is much sparser than for untyped models.</p><p>we  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Composition with APTs</head><p>Composition is linguistically motivated by the prin- ciple of compositionality, which states that the meaning of a complex expression is fully deter- mined by its structure and the meanings of its con- stituents (Frege, 1884). Many simple approaches to semantic composition neglect the structure and lose information in the composition process. For exam- ple, the phrases house boat and boat house have the exact same representation when composition is done via a pointwise arithmetic operation. Despite performing well in a number of studies, this com- mutativity is not desirable for a fine grained un- derstanding of the semantics of natural language. When performing composition with APTs, we adopt the method introduced by <ref type="bibr" target="#b45">Weir et al. (2016)</ref> which views distributional composition as a process of con- textualisation. For composing the adjective white with the noun clothes via the dependency relation amod we need to consider how the adjective inter- acts with the noun in the vector space. The distri- butional features of white describe things that are white via their first order relations such as amod, and things that can be done to white things, such as bought via amod:dobj in the example above. <ref type="table">Table 1</ref> shows a number of features extracted from the aligned dependency trees in <ref type="figure" target="#fig_0">Figure 1</ref> and high- lights that adjectives and nouns do not share many features if only first order dependencies would be considered. However through the inclusion of in- verse and higher order dependency paths we can ob- serve that the second order features of the adjective align with the first order features of the noun. For composition, the adjective white needs to be offset by its inverse relation to clothes 1 making it distribu- tionally similar to a noun that has been modified by white. Offsetting can be seen as shifting the current viewpoint in the APT data structure and is necessary for aligning the feature spaces for composition <ref type="bibr" target="#b45">(Weir et al., 2016)</ref>. We are then in a position to compose the offset representation of white with the vector for clothes by the union or the intersection of their fea- tures. <ref type="table" target="#tab_2">Table 2</ref> shows the resulting feature spaces of the composed vectors. It is worth noting that any arith- metic operation can be used to combine the counts of the aligned features, however for this paper we use pointwise addition for both composition func- tions. One of the advantages of this approach to composition is that the inherent interpretability of count-based models naturally expands beyond the word level, allowing us to study the distributional se- mantics of phrases in the same space as words. Due to offsetting one of the constituents, the composition operation is not commutative and hence avoids iden- tical representations for house boat and boat house. However, the typed nature of our vector space re-white clothes</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distributional Features</head><p>Offset Features (by amod) Co-occurrence Count Distributional Features Co-occurrence Count amod:shoes :shoes 1 amod:clean 1 amod:dobj:bought dobj:bought 1 dobj:like 1 amod:dobj:folded dobj:folded 1 dobj:folded 1 amod:dobj:nsubj:we dobj:nsubj:we 1 dobj:nsubj:we 1 <ref type="table">Table 1</ref>: Example feature spaces for the lexemes white and clothes extracted from the dependency tree of <ref type="figure" target="#fig_0">Figure 1</ref>. Not all features are displayed for space reasons. Offsetting amod:shoes by amod results in an empty dependency path, leaving just the word co-occurrence :shoes as feature.  sults in extreme sparsity, for example while the un- typed VSM has 130k dimensions, our APT model can have more than 3m dimensions. We therefore need to enrich the elementary vector representations with the distributional information of their nearest neighbours to ease the sparsity effect and infer miss- ing information. Due to the syntactic nature of our composition operation it is not straightforward to apply common dimensionality reduction techniques such as SVD, as the type information needs to be preserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Composition by union</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Distributional Inference</head><p>Following <ref type="bibr" target="#b10">Dagan et al. (1994)</ref> and <ref type="bibr" target="#b11">Dagan et al. (1997)</ref>, we propose a simple unsupervised al- gorithm for enriching sparse vector representations with their nearest neighbours. We show that our distributional inference algorithm improves perfor- mance for untyped and typed models on several word similarity benchmarks, as well as being com- petitive with the state-of-the-art on semantic com- position. As shown in algorithm 1 below, we iter- ate over all word vectors w in a given distributional model M , and add the vector representations of the nearest neighbours n, determined by cosine similar- ity, to the representation of the enriched word vector w . The parameter Œ± in line 4 scales the contribu- tion of the original word vector to the resulting en- riched representation. In this work we always chose Œ± to be identical to the number of neighbours used for distributional inference. For example, if we used 10 neighbours for DI, we would set Œ± = 10, which we found sufficient to prevent the neighbours from dominating the vector representation. In our exper- iments we kept the input distributional model fixed, however it is equally possible to update the given model in an online fashion, adding some amount of stochasticity to the enriched word vector repre- sentations. There is a number of possibilities for the neighbour retrieval function neighbours() and we explore several options in this paper. The algo- rithm furthermore is agnostic to the input distribu- tional model, for example it is possible to use com- pletely different vector space models for querying neighbours and enrichment.</p><formula xml:id="formula_0">Algorithm 1 Distributional Inference 1: procedure DIST INFERENCE(M ) 2: init M 3:</formula><p>for all w in M do 4:</p><formula xml:id="formula_1">w ‚Üê w √ó Œ± 5:</formula><p>for all n in neighbours(M, w) do end for 10:</p><p>return M 11: end procedure</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Static Top n Neighbour Retrieval</head><p>The perhaps simplest way is to choose the top n most similar neighbours for each word in the vec- tor space and enrich the respective vector represen- tations with them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Density based Neighbour Retrieval</head><p>This approach has its roots in kernel density esti- mation <ref type="bibr" target="#b36">(Parzen, 1962)</ref>, however instead of defining a static global parzen window, we set the window size for every word individually, depending on the distance to its nearest neighbour, plus a threshold. For example if the cosine distance between the target vector and its top neighbour is 0.5, we use a window size of 0.5 + for that word. In our experiments we typically define to be proportional to the distance of the nearest neighbour (e.g. = 0.5 √ó 0.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WordNet based Neighbour Retrieval</head><p>Instead of leveraging the intrinsic structure of our distributional vector space, we retrieve neighbours by querying WordNet <ref type="bibr">(Fellbaum, 1998)</ref>, and treat synsets with agreeing PoS tags as the nearest neigh- bours of any target vector. This restricts the retrieved neighbours to synonyms only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Our model is based on a cleaned October 2013 Wikipedia dump, which excludes all pages with fewer than 20 page views, resulting in a corpus of approximately 0.6 billion tokens <ref type="bibr" target="#b47">(Wilson, 2015)</ref>. The corpus is lowercased, tokenised, lemmatised, PoS tagged and dependency parsed with the Stan- ford NLP tools, using universal dependencies <ref type="bibr" target="#b12">de Marneffe et al., 2014</ref>). We then build our APT model with first, second and third or- der relations. We remove distributional features with a count of less than 10, and vectors containing fewer than 50 non-zero entries. The raw counts are subse- quently transformed to PPMI weights. The untyped vector space model is built from the same lower- cased, tokenised and lemmatised Wikipedia corpus. We discard terms with a frequency of less than 50 and apply PPMI to the raw co-occurrence counts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shifted PPMI</head><p>We explore a range of different values for shifting the PPMI scores as these have a significant impact on the performance of the APT model. The effect of shifting PPMI scores for untyped vector space models has already been explored in <ref type="bibr" target="#b23">Levy and Goldberg (2014)</ref>, and <ref type="bibr" target="#b24">Levy et al. (2015)</ref>, thus we only present results for the APT model. As shown in equation 1, PMI is defined as the log of the ratio of the joint probability of observing a word w and a context c together, and the product of the respec- tive marginals of observing them separately. In our APT model, a context c is defined as a dependency relation together with a word.</p><formula xml:id="formula_2">P M I(w, c) = log P (w, c) P (w)P (c) SP P M I(w, c) = max(P M I(w, c) ‚àí log k, 0) (1)</formula><p>As PMI is negatively unbounded, PPMI is used to ensure that all values are greater than or equal to 0. Shifted PPMI (SPPMI) subtracts a constant from any PMI score before applying the PPMI threshold.</p><p>We experiment with values of 1, 5, 10, 40 and 100 for the shift parameter k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Word Similarity Experiments</head><p>We first evaluate our models on 3 word similar- ity benchmarks, MEN ( <ref type="bibr" target="#b5">Bruni et al., 2014</ref>), which is testing for relatedness (e.g. meronymy or holonymy) between terms, SimLex-999 ( <ref type="bibr" target="#b20">Hill et al., 2015)</ref>, which is testing for substitutability (e.g. syn- onymy, antonymy, hyponymy and hypernymy), and WordSim-353 ( <ref type="bibr" target="#b16">Finkelstein et al., 2001</ref>), where we use the version of <ref type="bibr" target="#b0">Agirre et al. (2009)</ref>, who split the dataset into a relatedness and a substitutability sub- set. <ref type="bibr" target="#b2">Baroni and Lenci (2011)</ref> have shown that un- typed models are typically better at capturing relat- edness, whereas typed models are better at encoding substitutability. Performance is measured by com- puting Spearman's œÅ between the cosine similarities of the vector representations and the corresponding aggregated human similarity judgements. For these experiments we keep the number of neighbours that a word vector can consume fixed at 30. This value is based on preliminary experiments on WordSim-353 (see <ref type="figure" target="#fig_2">Figure 2</ref>) using the static top n neighbour re- trieval function and a PPMI shift of k = 40. <ref type="figure" target="#fig_2">Figure 2</ref> shows that distributional inference improves perfor- mance for any number of neighbours over a model without DI (marked as horizontal dashed lines for each WordSim-353 subset) and peaks at a value of 30. Performance slightly degrades with more neigh- bours. For the untyped VSM we use a symmetric window of 5 on either side of the target word.  <ref type="table" target="#tab_4">Table 3</ref> highlights the effect of the SPPMI shift parameter k, while keeping the number of neigh- bours fixed at 30 and using the static top n neigh- bour retrieval function. For the APT model, a value of k = 40 performs best (except for SimLex-999, where smaller shifts give better results), with a per- formance drop-off for larger shifts. In our experi- ments we find that a shift of k = 1 results in top per- formance for the untyped vector space model. It ap- pears that shifting the PPMI scores in the APT model has the effect of cleaning the vectors from noisy PPMI artefacts, which reinforces the predominant sense, while other senses get suppressed. Sub- sequently, this results in a cleaner neighbourhood around the word vector, dominated by a single sense. This explains why distributional inference slightly degrades performance for smaller values of k. <ref type="table">Table 4</ref> shows that distributional inference suc- cessfully infers missing information for both model types, resulting in improved performance over mod- els without the use of DI on all datasets. The im- provements are typically larger for the APT model, suggesting that it is missing more distributional knowledge in its elementary representations than un- typed models. The density window and static top n neighbour retrieval functions perform very similar, however the static approach is more consistent and never underperforms the baseline for either model type on any dataset. The WordNet based neigh- bour retrieval function performs particularly well on SimLex-999. This can be explained by the fact that antonyms, which frequently happen to be among the nearest neighbours in distributional vector spaces, are regarded as dissimilar in SimLex-999, whereas the WordNet neighbour retrieval function only re- turns synonyms. The results furthermore confirm the effect that untyped models perform better on datasets modelling relatedness, whereas typed mod- els work better for substitutability tasks ( <ref type="bibr" target="#b2">Baroni and Lenci, 2011</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Composition Experiments</head><p>Our approach to semantic composition as described in section 3 requires the dimensions of our vector space models to be meaningful and interpretable. However, the problem of missing information is am- plified in compositional settings as many compati- ble dimensions between words are not observed in the source corpus. It is therefore crucial that dis- tributional inference is able to inject some of the missing information in order to improve the com- position process. For the experiments involving se- mantic composition, we enrich the elementary rep- resentations of the phrase constituents before com- position. We first conduct a qualitative analysis for our APT model and observe the effect of distributional inference on the nearest neighbours of composed adjective-noun, noun-noun and verb-object com- pounds. In these experiments, we show how dis- tributional inference changes the neighbourhood in which composed phrases are embedded, and high- light the difference between composition by union and composition by intersection. For this exper- iment we use the static top n neighbour retrieval function with 30 neighbours and k = 40. <ref type="table" target="#tab_3">Table 5</ref> shows a small number of example phrases together with their top 3 nearest neighbours, com- puted from the union of all words in the Wikipedia corpus and all phrase pairs in the Mitchell and La- pata (2010) dataset. As can be seen, nearest neigh- bours of phrases can be either single words or other composed phrases. Words or phrases marked with "*" in <ref type="table" target="#tab_3">Table 5</ref> mean that DI introduced, or failed to downrank, a spurious neighbour, while boldface means that performing distributional inference re- sulted in a neighbourhood more coherent with the query phrase than without DI.    downrank unrelated neighbours introduced by dis- tributional inference. For example large quantity is incorrectly introduced as a top ranked neighbour for the phrase small house, due to the proximity of small and large in the vector space. The phrases market leader and television programme are two ex- amples of incoherent neighbours, which the compo- sition function was unable to downrank and where DI could not improve the neighbourhood. Compo- sition by intersection on the other hand vastly ben- efits from distributional inference. Due to the in- creased sparsity induced by the composition pro- cess, a neighbourhood without DI produces numer- ous spurious neighbours as in the case of the verb have as a neighbour for win battle. Distributional inference introduces qualitatively better neighbours for almost all phrases. For example, government leader and opposition member are introduced as top ranked neighbours for the phrase party leader, and stress importance and underline are introduced as new top neighbours for the phrase emphasise need.</p><p>These results show that composition by union does not have the ability to disambiguate the meaning of a word in a given phrasal context, whereas composi- tion by intersection has that ability but requires dis- tributional inference to unleash its full potential. For a quantitative analysis of distributional in- ference for semantic composition, we evaluate our model on the composition dataset of <ref type="bibr" target="#b30">Mitchell and Lapata (2010)</ref>, consisting of 108 adjective-noun, 108 noun-noun, and 108 verb-object pairs. The task is to compare the model's similarity estimates with the human judgements by computing Spearman's œÅ. For comparing the performance of the different neighbour retrieval functions, we choose the same parameter settings as in the word similarity experi- ments (k = 40 and using 30 neighbours for DI). <ref type="table" target="#tab_8">Table 6</ref> shows that the static top n and den- sity window neighbour retrieval functions perform very similar again. The density window retrieval function outperforms static top n for composition by intersection and vice versa for composition by union. The WordNet approach is competitive for composition by union, but underperfoms the other approaches for composition by intersection signifi- cantly. For further experiments we use the static top n approach as it is computationally cheap and easy to interpret due to the fixed number of neighbours. <ref type="table" target="#tab_8">Table 6</ref> also shows that while composition by in- tersection is significantly improved by distributional  <ref type="table">Union (with DI)  Intersection  Intersection (with DI)  national  AN  government, regime,  government, regime*,  federal assembly,  federal assembly, government,  government  ministry  european state*  government, monopoly  local office  small  AN  house, public building,  house, public building,  apartment, cottage,  cottage, apartment, cabin  house  building  large quantity*  cabin  party</ref> NN leader, market leader, leader, government leader, party official, NDP, government leader, party official, leader government leader market leader* leader opposition member training NN programme, action programme, programme, action programme*, training college, trainee, training college, programme television programme television programme* education course education course, seminar win battle VO win, win match, ties win, win match, fight war win match, win, have fight war, fight, win match emphasise VO emphasise, underline, emphasise, underline, emphasise, prioritize, emphasise, stress importance, need underscore underscore negate underline inference, composition by union does not appear to benefit from it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Composition by Union or Intersection</head><p>Both model types in this study support composition by union as well as composition by intersection. In untyped models, composition by union and com- position by intersection can be achieved by point- wise addition and pointwise multiplication respec- tively. The major difference between composition in the APT model and the untyped model is that in the former, composition is not commutative due to offsetting the modifier in a dependency relation (see section 3). <ref type="bibr" target="#b4">Blacoe and Lapata (2012)</ref> showed that an intersective composition function such as point- wise multiplication represents a competitive and ro- bust approach in comparison to more sophisticated composition methods. For the final set of experi- ments on the Mitchell and Lapata (2010) dataset, we present results the APT model and the untyped model, using composition by union and composi- tion by intersection, with and without distributional inference. We compare our models with the best performing untyped VSMs of <ref type="bibr" target="#b30">Mitchell and Lapata (2010)</ref>, and Blacoe and Lapata (2012), the best performing APT model of <ref type="bibr" target="#b45">Weir et al. (2016)</ref>, as well as with the recently published state-of-the-art methods by <ref type="bibr" target="#b19">Hashimoto et al. (2014)</ref>, and <ref type="bibr" target="#b46">Wieting et al. (2015)</ref>, who are using neural network based approaches. For our models, we use the static top n approach as neighbour retrieval function and tune the remaining parameters, the SPPMI shift k (1, 5, 10, 40, 100) and the number of neighbours (10, 30, 50, 100, 500, 1000, 5000), for both model types, and the sliding window size for the untyped VSM (1, 2, 5), on the development portion of the Mitchell and Lapata (2010) dataset. We keep the vector con- figuration (k and window size) fixed for all phrase types and only tune the number of neighbours used for DI individually. The best vector configuration for the APT model is achieved with k = 10 and for the untyped VSM with k = 1. For composition by intersection best performance on the dev set was achieved with 1000 neighbours for ANs, 10 for NNs and 50 for VOs with DI. For composition by union, top performance was obtained with 100 neighbours for ANs, 30 neighbours for NNs and 50 for VOs. The best results for the untyped model on the dev set are achieved with a symmetric window size of 1 and using 5000 neighbours for ANs, 10 for NNs and 1000 for VOs with composition by pointwise multi- plication, and 30 neighbours for ANs, 5000 for NNs and 5000 for VOs for composition by pointwise ad- dition. The validated numbers of neighbours on the development set show that the problem of missing information appears to be more severe for seman- tic composition than for word similarity tasks. Even though a neighbour at rank 1000 or lower does not appear to have a close relationship to the target word, it still can contribute useful co-occurrence informa- tion not observed in the original vector. <ref type="table" target="#tab_7">Table 7</ref> shows that composition by intersection with distributional inference considerably improves upon the best results for APT models without distribu- tional inference and for untyped count-based mod- els, and is competitive with the state-of-the-art neu- ral network based models of <ref type="bibr" target="#b19">Hashimoto et al. (2014)</ref> and <ref type="bibr" target="#b46">Wieting et al. (2015)</ref>. Distributional inference also improves upon the performance of an untyped VSM where composition by pointwise multiplica- tion is outperforming the models of <ref type="bibr" target="#b30">Mitchell and Lapata (2010)</ref>, and Blacoe and Lapata (2012).    <ref type="table" target="#tab_7">Table 7</ref>: Results for the <ref type="bibr" target="#b30">Mitchell and Lapata (2010)</ref> dataset. Results in brackets denote the performance of the respective models without the use of distributional inference. Underlined means best within group, boldfaced means best overall. the APT model based on composition by union and the untyped model based on composition by point- wise addition. The reason, as pointed out in the dis- cussion for <ref type="table" target="#tab_3">Table 5</ref>, is that the composition function has no disambiguating effect and thus cannot elim- inate unrelated neighbours introduced by distribu- tional inference. An intersective composition func- tion on the other hand is able to perform the disam- biguation locally in any given phrasal context. This furthermore suggests that for the APT model it is not necessary to explicitly model different word senses in separate vectors, as composition by intersection is able to disambiguate any word in context individ- ually. Unlike the models of <ref type="bibr" target="#b19">Hashimoto et al. (2014)</ref> and <ref type="bibr" target="#b46">Wieting et al. (2015)</ref>, the elementary word rep- resentations, as well as the representations for com- posed phrases and the composition process in our models are fully interpretable 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>One of the major challenges in count-based mod- els is dealing with sparsity and missing information. To address this challenge, we contribute an unsu- pervised algorithm for enriching sparse word rep- resentations by exploiting the distributional neigh- bourhood. We have demonstrated its benefit to typed <ref type="bibr">2</ref> We release the APT vectors and our code at https:// github.com/tttthomasssss/apt-toolkit. and untyped vector space models on a range of tasks and have shown that with distributional inference our APT model is competitive with the state-of-the- art for adjective-noun, noun-noun and verb-object compositions while being fully interpretable. With our method, we are able to bridge the gap in perfor- mance between low-dimensional non-interpretable and high-dimensional interpretable representations. Lastly, we have investigated the different behaviour of composition by union and composition by inter- section and have shown that an intersective com- position function, together with distributional in- ference, has the ability to locally disambiguate the meaning of a phrase.</p><p>In future work we aim to scale our approach to se- mantic composition with distributional inference to longer phrases and full sentences. We furthermore plan to investigate whether the number of neigh- bours required for improving elementary vector rep- resentations remains as high for other compositional tasks and longer phrases as in this study.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Aligned Packed Dependency Tree representation of the example sentences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Effect of the number of neighbours on WordSim-353.</figDesc><graphic url="image-1.png" coords="6,72.00,113.20,226.80,156.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4 :</head><label>4</label><figDesc>Neighbour retrieval function comparison. Boldface means best performance on a dataset per VSM type. *) With 3 significant figures, the density window approach (0.713) is slightly better than the baseline without DI (0.708), static top n (0.710) and WordNet (0.710).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Comparison of composition by union and composition by intersection. Not all features are displayed for space reasons.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 shows that composition by union is unable towith DI without DI with DI without DI with DI without DI with</head><label>5</label><figDesc></figDesc><table>APTs 

MEN 
SimLex-999 
WordSim-353 (rel) 
WordSim-353 (sub) 
without DI DI 
k = 1 
0.54 
0.52 
0.31 
0.30 
0.34 
0.27 
0.62 
0.60 
k = 5 
0.64 
0.65 
0.35 
0.36 
0.56 
0.51 
0.74 
0.73 
k = 10 
0.63 
0.66 
0.35 
0.36 
0.56 
0.55 
0.75 
0.74 
k = 40 
0.63 
0.68 
0.30 
0.32 
0.55 
0.61 
0.75 
0.76 
k = 100 
0.61 
0.67 
0.26 
0.29 
0.47 
0.60 
0.71 
0.72 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Effect of the magnitude of the shift parameter k in SPPMI on the word similarity tasks. Boldface means best performance 

per dateset. 

APTs (k = 40) 
No Distributional Inference Density Window Static Top n WordNet 
MEN 
0.63 
0.67 
0.68 
0.63 
SimLex-999 
0.30 
0.32 
0.32 
0.38 
WordSim-353 (rel) 
0.55 
0.62 
0.61 
0.56 
WordSim-353 (sub) 
0.75 
0.78 
0.76 
0.77 
Untyped VSM (k = 1) No Distributional Inference Density Window Static Top n WordNet 
MEN* 
0.71 
0.71 
0.71 
0.71 
SimLex-999 
0.30 
0.29 
0.30 
0.36 
WordSim-353 (rel) 
0.60 
0.64 
0.64 
0.52 
WordSim-353 (sub) 
0.70 
0.73 
0.72 
0.67 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Nearest neighbours AN, NN and VO pairs in the Mitchell and Lapata (2010) dataset, with and without distributional 

inference. Words and phrases marked with * denote spurious neighbours, boldfaced words and phrases mark improved neighbours. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 furthermore shows that DI has a smaller effect on</head><label>7</label><figDesc></figDesc><table>APTs 

No Distributional Inference 
Density Window 
Static Top n 
WordNet 
intersection 
union 
intersection union intersection union intersection union 
Adjective-Noun 
0.10 
0.41 
0.31 
0.39 
0.25 
0.40 
0.12 
0.41 
Noun-Noun 
0.18 
0.42 
0.34 
0.38 
0.37 
0.45 
0.24 
0.36 
Verb-Object 
0.17 
0.36 
0.36 
0.36 
0.34 
0.35 
0.25 
0.36 
Average 
0.15 
0.40 
0.34 
0.38 
0.32 
0.40 
0.20 
0.38 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 : Neighbour retrieval function. Underlined means best performance per phrase type, boldface means best average perfor-</head><label>6</label><figDesc></figDesc><table>mance overall. 

</table></figure>

			<note place="foot" n="1"> The inverse of amod is just amod.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was funded by UK EPSRC project EP/IO37458/1 "A Unified Model of Compositional and Distributional Compositional Semantics: The-ory and Applications". We would like to thank our anonymous reviewers for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A study on similarity and relatedness using distributional and wordnet-based approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kravalova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributional memory: A general framework for corpus-based semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="673" to="721" />
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How we blessed distributional semantic evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of GEMS Workshop, GEMS &apos;11</title>
		<meeting>GEMS Workshop, GEMS &apos;11<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Cambridge, MA, October</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1183" to="1193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A comparison of vector-based representations for semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Blacoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="546" to="556" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extracting semantic representations from word co-occurrence statistics: A computational study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Bullinaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">P</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="page" from="510" to="526" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and svd. Behavior Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Bullinaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">P</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="890" to="907" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mathematical foundations for a compositional distributional model of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">Ward</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
		<idno>abs/1003.4394</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990-03" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Word association norms, mutual information, and lexicography</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">From Distributional to Semantic Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Curran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Similarity-based estimation of word cooccurrence probabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Las Cruces, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1994-06" />
			<biblScope unit="page" from="272" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Similarity-based methods for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-07" />
			<biblScope unit="page" from="56" to="63" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Universal stanford dependencies: A cross-linguistic typology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katri</forename><surname>Haverinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Language Resources Association (ELRA). ACL Anthology Identifier</title>
		<meeting><address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05" />
			<biblScope unit="page" from="14" to="1045" />
		</imprint>
	</monogr>
	<note>Proceedings of LREC</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Soc. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A structured vector space model for word meaning in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Pad√≥</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<meeting><address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10" />
			<biblScope unit="page" from="897" to="906" />
		</imprint>
	</monogr>
	<note>Proceedings of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">WordNet: an electronic lexical database</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gottlob Frege. 1884. Die Grundlagen der Arithmetik: Eine logisch mathematische Untersuchung√ºberUntersuchung¬®Untersuchung√ºber den Begriff der Zahl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW, WWW &apos;01</title>
		<meeting>WWW, WWW &apos;01<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>W. Koebner</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
	<note>Placing search in context: The concept revisited</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-step regression learning for compositional distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Zhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of IWCS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Explorations in Automatic Thesaurus Discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Grefenstette</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Norwell, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Jointly learning word representations and composition functions using predicate-argument structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">October. Association for Computational Linguistics</title>
		<meeting><address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1544" to="1555" />
		</imprint>
	</monogr>
	<note>Proceedings of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="665" to="695" />
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A large scale evaluation of distributional semantic models: Parameters, interactions and model selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriella</forename><surname>Lapesa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Evert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="531" to="545" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The forest convolutional network: Compositional distributional semantics with a neural chart and without binarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1155" to="1164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic retrieval and clustering of similar words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1998-08" />
			<biblScope unit="page" from="768" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Producing high-dimensional semantic spaces from lexical cooccurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curt</forename><surname>Burgess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods, Instruments, &amp; Computers</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="208" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-System Demonstrations</title>
		<meeting>ACL-System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="236" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1429" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Discriminative neural sentence modeling by tree-based convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="2315" to="2325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Co-occurrence vectors from corpora vs. distance vectors from dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiki</forename><surname>Niwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiko</forename><surname>Nitta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Coling, COLING &apos;94</title>
		<meeting>Coling, COLING &apos;94<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="304" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dependencybased construction of semantic space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Pad√≥</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="199" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Derivational smoothing for syntactic distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Pad√≥</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ja≈à</forename><surname>Snajder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Britta</forename><surname>Zeller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<meeting><address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="731" to="735" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A practical and linguistically-motivated approach to compositional distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nghia The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="90" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On estimation of a probability density function and mode</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Parzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The Word-space model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>University of Stockholm (Sweden</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Uncovering distributional differences between synonyms and antonyms in a word space model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silke</forename><surname>Scheible</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvia</forename><surname>Springorum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Federation of Natural Language Processing</title>
		<meeting><address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="489" to="497" />
		</imprint>
	</monogr>
	<note>Proceedings of IJCNLP</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Contextualizing semantic representations using syntactically enriched vector models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>F√ºrstenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="948" to="957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Word meaning in context: A simple and effective vector model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>F√ºrstenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Federation of Natural Language Processing</title>
		<meeting><address><addrLine>Chiang Mai, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11" />
			<biblScope unit="page" from="1134" to="1143" />
		</imprint>
	</monogr>
	<note>Proceedings of IJCNLP</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Distributional composition using higher-order dependency vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 2nd Workshop on Continuous Vector Space Models and their Compositionality<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-01" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
	<note>From frequency to meaning: Vector space models of semantics</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Aligning packed dependency trees: a theory of composition for distributional semantics. Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kober</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1608.07115" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">From paraphrase database to compositional paraphrase model and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="345" to="358" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">The unknown perils of mining wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Wilson</surname></persName>
		</author>
		<ptr target="https://blog.lateral.io/2015/06/the-unknown-perils-of-mining-wikipedia/" />
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Squibs: When the whole is not greater than the combination of its parts: A &quot;decompositional&quot; look at compositional distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Massimo</forename><surname>Zanzotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Ferrone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="165" to="173" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
