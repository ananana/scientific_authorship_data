<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Noise-Clustered Distant Supervision for Relation Extraction: A Nonparametric Bayesian Perspective</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics (Peking University) Ministry of Education</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics (Peking University) Ministry of Education</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Noise-Clustered Distant Supervision for Relation Extraction: A Nonparametric Bayesian Perspective</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1808" to="1813"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>For the task of relation extraction, distant supervision is an efficient approach to generate labeled data by aligning knowledge base with free texts. The essence of it is a challenging incomplete multi-label classification problem with sparse and noisy features. To address the challenge, this work presents a novel nonparametric Bayesian formulation for the task. Experiment results show substantially higher top-precision improvements over the traditional state-of-the-art approaches.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>To efficiently generate structured relation informa- tion from free texts, the research on distantly su- pervised Relation Extraction (RE) ( <ref type="bibr" target="#b12">Mintz et al., 2009;</ref><ref type="bibr" target="#b15">Riedel et al., 2013;</ref><ref type="bibr" target="#b10">Hoffmann et al., 2011</ref>) has been attracting much attention, because it can greatly reduce the manual annotation for training. It essentially based on the assumption that the re- lation between two entities in a Knowledge Base (KB), is also likely hold within a sentence that mentions the two entities in free texts. This as- sumption plays a crucial role in distant supervi- sion, which is quite effective in real applications.</p><p>However, the assumption of distant alignment can also lead to the noisy training corpus prob- lem <ref type="bibr" target="#b7">(Fan et al., 2014</ref>), which is challenging for the task as follows: i) Noisy features. Not all relations existed in a KB keep the same mean- ing of that relation for the corresponding entities in a free text. For example, the second relation mention in <ref type="figure">Figure 1</ref> does not explicitly describe any relation instance, so features extracted from this sentence can be noisy. Such analogous cases commonly exist in feature extraction. ii) Incom- plete labels. Similar to noisy features, the gener- <ref type="figure">Figure 1</ref>: Aligned Example ( <ref type="bibr" target="#b7">Fan et al., 2014</ref>): the relation instances related to the entity pair BarackObama, U.S. in the KB, and its men- tions in the free text. ated label can be incomplete due to the incomplete knowledge base ( <ref type="bibr" target="#b16">Ritter et al., 2013</ref>). For exam- ple, the fourth relation mention in <ref type="figure">Figure 1</ref> should be labeled by the relation Senate-of. However, the corresponding relation instance (Senate-of(Barack Obama, U.S.)) is missing in the knowledge base. Such analogous cases are also common in real ap- plications. iii) Sparse features. Sophisticated features extracted from the mentions can result in a large number of sparse features ( <ref type="bibr" target="#b15">Riedel et al., 2013)</ref>. The generalization ability of feature based prediction models will be badly hurt, when the fea- tures do not match between testing and training.</p><p>To tackle the problem, we develop a novel dis- tant supervision approach from a nonparametric Bayesian perspective ( <ref type="bibr" target="#b2">Blei et al., 2016)</ref>, along with the previously most effective research line <ref type="bibr" target="#b13">(Petroni et al., 2015</ref>) of using matrix completion <ref type="bibr" target="#b7">(Fan et al., 2014</ref>) for relation extraction. Our goal is to design a noise-tolerant relation extraction model for distantly supervised corpus with noise and sparsity problems. Different from <ref type="bibr" target="#b7">(Fan et al., 2014</ref>) as one state-of-the-art method in this line, we model noisy data corpus using adaptive vari- ance modeling approach , based on Dirichlet Process ( <ref type="bibr" target="#b1">Blei and Jordan, 2004</ref>) in- stead of a fixed way of controlling complex noise weighting. To the best of our knowledge, we are the first to apply this technique on relation extrac- tion with distant supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>The essence of the task is a multi-label classifica- tion problem <ref type="bibr" target="#b3">(Cabral et al., 2011</ref>) with noisy pat- terns <ref type="bibr" target="#b8">(Han and Sun, 2014)</ref>. One simple way, to solve the problem, is to learn separate classifiers for each of relation labels, using n samples with d features, by optimizing b ∈ R 1×1 and w ∈ R d×1 ,</p><formula xml:id="formula_0">argmin b,w l(y train , [1 X train ] b w ),<label>(1)</label></formula><p>where 1 is the all-one column vector; X train ∈ R n×d and y train ∈ R n×1 are the correspond- ing feature matrix and label vector respectively. However, label correlations are not considered in the above formulation. To jointly consider feature correlations and label correlations, <ref type="bibr" target="#b3">(Cabral et al., 2011</ref>) formulated the multi-label classification as a matrix completion problem. As a powerful frame- work, it has been successfully applied to relation extraction task with distant supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Previous Formulation</head><p>The work in <ref type="bibr" target="#b7">(Fan et al., 2014</ref>) first adopted the mentioned framework, as a general joint learning and inference framework <ref type="bibr" target="#b3">(Cabral et al., 2011</ref>), to learn noise-tolerant distant supervision for rela- tion extraction. It achieves the state-of-the-art per- formance. Suppose we have a training corpus, in- cluding n instances (entity pairs) including both training and test data, with d-dimensional features and t relation labels, which is built according to the basic alignment assumption. The task can be modeled with a sparse matrix Z ∈ R n×(d+t) , de- fined as</p><formula xml:id="formula_1">Z = X train Y train X test Y test ,<label>(2)</label></formula><p>where each row in Z represents entity pair, and each column represents noisy textual feature in X or incomplete relation label in Y . In such a way, relation extraction is transformed into a problem of completing the unknown labels in Y test for the test data X test in Z. The rational of this model- ing is that noisy features and incomplete labels are semantically correlated, which can be explained in an underlying low-rank structure ( <ref type="bibr" target="#b15">Riedel et al., 2013)</ref>. Taking noise into consideration, Z is fur- ther defined as</p><formula xml:id="formula_2">Z = Z * + E,<label>(3)</label></formula><p>where Z * is the underlying low-rank matrix</p><formula xml:id="formula_3">Z * = X * train Y * train X * test Y * test ,<label>(4)</label></formula><p>and E is the error (noise) matrix</p><formula xml:id="formula_4">E = E X train E Y train E Xtest 0 .<label>(5)</label></formula><p>This error (noise) modeling approach has been successfully applied to distantly supervised rela- tion extraction. However, it still has clear lim- itations. The noise model is limited to a single source without considering the intrinsic clustering structures of data. In addition, the true rank is usu- ally hard to determine, for adaptively modeling the correlations among features and labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Nonparametric Bayesian Modeling</head><p>The use of nonparametric Bayesian modeling has been widely adopted in Natural Language Pro- cessing (NLP) ( <ref type="bibr" target="#b4">Chen et al., 2014</ref>). Instead of im- posing assumptions that might be wrong, it "lets the data speak for itself", without requiring opti- mizing parameters blindly by hands <ref type="bibr" target="#b1">(Blei and Jordan, 2004</ref>). To take advantage of these merits, we here adopt it for the task, with the following moti- vations: Motivation 1: Adaptive Noise-Clustered At- tention. The goal is to find an adaptive cluster specific noise parameterization for the complex noisy corpus, without making overly strong as- sumptions about the noise distribution in real ap- plications.</p><p>Motivation 2: Adaptive Latent Feature Space Selection. The goal is to automatically find better dense representations of latent entity-pair, feature and label without pre-specifying the rank values by laboriously retraining models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Nonparametric Bayesian Formulation</head><p>We develop a novel formulation for distantly su- pervised relation extraction, using a nonparamet- ric Bayesian approach, based on the Dirichlet Pro- cess, which can been seen as an infinite Dirich- let distribution, with clustering effect for modeling categorical variables adaptively.</p><p>Noise component modeling. Instead of using a single fixed noise model, we redefine E = [ε i,j ] ∈ R n×(d+t) in Eq.(5). ε i,j is modeled by a summa- tion of infinite noise models ( ,</p><formula xml:id="formula_5">p(ε i,j ) = ∞ k=1 θ k N (ε i,j |0, σ k ),<label>(6)</label></formula><p>where θ k is the mixing proportion for the k-th gaussian component N (ε i,j |0, σ k ) with mean zero and variance σ k . The θ is obtained from the stick- breaking process ( <ref type="bibr" target="#b1">Blei and Jordan, 2004</ref>), with</p><formula xml:id="formula_6">∞ k=1 θ k = 1, θ k = β k k−1 l=1 (1 − β l ),<label>(7)</label></formula><p>where β k are independent draws from the beta dis- tribution β(1, α). As a result, the noise entries will cluster themselves into K groups without re- quiring a complicated model selection procedure.</p><p>Since a mixture of Gaussians can approximate any continuous probability distribution ( ), this structural noise formulation can adapt much wider range of real noises than previous for- mulation <ref type="bibr" target="#b7">(Fan et al., 2014</ref>) for relation extraction.</p><p>Low-rank component modeling. Different from <ref type="bibr" target="#b7">(Fan et al., 2014</ref>), instead of directly min- imizing the rank of Z * in Eq. <ref type="formula" target="#formula_2">(3)</ref>, we decom- pose Z * into two low-rank matrices U and V , from probabilistic perspective <ref type="bibr" target="#b17">(Salakhutdinov and Mnih, 2007)</ref>. This modeling approach can lead to a more flexible way of estimating the optimal rank values for latent feature spaces. To determine the appropriate rank automatically, we adopt the Au- tomatic Relevance Determination (ARD) method ( <ref type="bibr" target="#b0">Babacan et al., 2012</ref>) by imposing a prior on each dimmension (column) of U and V . Specifically, we impose the Gaussian priors with variance λ r on the r-th columns of U and V , i.e., u .r and v .r :</p><formula xml:id="formula_7">p(U|λ) = R r=1 N (u .r |0, λ r I U ), p(V|λ) = R r=1 N (v .r |0, λ r I V ), λ r ∼ IG(a 1 , b 1 ),<label>(8)</label></formula><p>where IG is an Inverse Gamma distribution for modeling the variance λ r . Considering a column as latent factor in U or V with a zero mean in the prior, a very small variance indicates that this column will shrink to zero. Thus, the irrelevant columns hurting the performance will be elimi- nated adaptively, without pre-specifying the rank values by retraining models laboriously as in the previous modeling <ref type="bibr" target="#b7">(Fan et al., 2014</ref>) for the task.</p><p>Prediction component modeling. We can leverage the above presented low-rank component for U, V and noise component for ε i,j , to build Eq.(9) for prediction. Different from the state- of-the-art multi-label classification framework as adopted in <ref type="bibr" target="#b7">(Fan et al., 2014</ref>), for simplicity, we design noise model for features and labels jointly,</p><formula xml:id="formula_8">p(y i,j ) = N (y i,j | u i. v T j. low−rank component , ε i,j noise component ),<label>(9)</label></formula><p>where u i. and v j. are defined in Eq. <ref type="formula" target="#formula_7">(8)</ref> as rows of U and V respectively. For each interaction between entity-pair and feature (or relation), ε i,j as defined in Eq.(6) can be injected into Eq.(9) ( ) by</p><formula xml:id="formula_9">ε i,j = σ z ij , σ z ij ∼ IG(a 0 , b 0 ), z ij = k ∼ M ult(θ k ),<label>(10)</label></formula><p>where θ k is modeled in Eq. <ref type="formula" target="#formula_6">(7)</ref>; Mult is a Multino- mial distribution.</p><p>The mechanism of the introduced clustered noise component for relation extraction can be easily understood through considering its role in the Gaussian distribution. As shown in Eq.(9), ε i,j is used to control the variance. Large vari- ance value means low confidence, and the small value means high confidence, for fitting y i,j with u i. v T j. . The variance parameter ε i,j , generated by noise component Eq. <ref type="bibr">(7,</ref><ref type="bibr">10)</ref>, serves as a confidence parameter for training instance. In the algebra view of likelihood, variance parameter is just the weight of training instance (i.e., the interaction be- tween "entity pair and feature" or "entity pair and label"), measuring the importance for its contribu- tion to the total likelihood. We can treat this mech- anism as an importance weighting mechanism, for selecting noisy interactions y ij with different clus- tering structures adaptively.</p><p>In this mechanism, for each y i,j in noisy corpus, it allows 1 → 0 (noisy feature) for features, and al- low 1 → 0 (label with no supportive features) or 0 → 1 (incomplete label) for labels. In addition, for the task, we expect that our method can auto- matically adjust the importance weight for reduc- ing the effect of common features, to differentiate two instances with different labels. To achieve the goal, in matrix Z, we fit both "1" (observed) and "0" for training labels as discriminative supervi- sion, while we only fit "1" (observed) for features.</p><p>Dataset #training #testing %more than one label #features #relation labels NYT'10 4,700 1,950 7.5% 244,903 51 NYT' <ref type="bibr">13</ref> 8,077 3,716 0% 1,957 51 <ref type="table">Table 1</ref>: Statistics about the two widely used datasets.   Learning. To combine Eqs. <ref type="formula" target="#formula_6">(7)</ref>- <ref type="formula" target="#formula_0">(10)</ref>, we can construct the full Bayesian model. The goal turns to infer the posterior of all involved variables:</p><formula xml:id="formula_10">p(U, V, λ, σ, z, β|X observed , Y observed ), (11)</formula><p>where X observed , Y observed are the observed bi- nary features (fitting 1) and labels (fitting both 1 and 0). Variational inference is adopted as shown in .</p><p>Prediction. After learning 1 , we use the expec- tation E <ref type="figure">(P (y i,j )</ref>) in Eq.(9) to complete the entries in Y test . Finally, we can acquire Top-N predicted relations via ranking the values E(P (y i,j )), given entity pair i, for different relations j.</p><p>We evaluate our method on two widely used datasets as shown in <ref type="table">Table 1</ref> with the same setting in <ref type="figure" target="#fig_0">(Fan et al., 2014)</ref>.</p><p>Dataset. NYT'10, was developed by ( <ref type="bibr" target="#b14">Riedel et al., 2010)</ref>. NYT'13, was also released by ( <ref type="bibr" target="#b15">Riedel et al., 2013)</ref>, in which they only regarded the lexicalized dependency path between two entities as features. Both are automatically generated by aligning Freebase to New York Times corpus.</p><p>Parameter setting. For all the conducted ex- periments, the model hyperparameters are fixed without further tuning: a 0 = b 0 = 10 −4 , a 1 = b 1 = 0.1 and α = 1.</p><p>Model comparison. <ref type="bibr">Since (Fan et al., 2014</ref>) achieves the state-of-the-art performance on the two datasets, we mainly compare our method with that in the same setting, to verify the effective- ness. NYT'10 dataset: <ref type="table" target="#tab_1">Table 2</ref> indicates that our model achieves the highest precision performance among all of the competitors. Although the re- call performance is not competitive, the F1 score is also comparable to DRMC-b. <ref type="figure" target="#fig_0">Figure 2</ref>(a) fur- ther shows the strong precision performance when the recall is not large. NYT'13 dataset: <ref type="figure" target="#fig_0">Figure  2</ref>(b) illustrates that our approach outperforms the state-of-the-art methods, which shows that our ap- proach can maintain a fairly high precision even when recall is larger. In addition, in practical ap- plications, we also concern about the precision on</p><formula xml:id="formula_11">Top-N NFE- 13 DRMC- b DRMC- 1 Our</formula><p>Top-100 62.9% 82.0% 80.0% 92.0% Top-200 57.1% 77.0% 80.0% 88.2% Top-500 37.2% 70.2% 77.0% 86.3% Average 52.4% 76.4% 79.0% 88.8%  <ref type="table">Table 4</ref>: Results at the highest F1 point in the Precision-Recall (P-R) curve on NYT'13 dataset. DRMC-b(1) <ref type="bibr" target="#b7">(Fan et al., 2014</ref>).</p><p>Top-N predicted instances. <ref type="table" target="#tab_2">Table 3</ref> shows that our model achieves much significant improvements on that. Moreover, <ref type="table">Table 4</ref> shows that our method can achieve the best F1, compared with the baselines. NYT'10 and NYT'13 have different perfor- mance records, which could be explained as fol- lows. From the dataset perspective, NYT'10 is a dataset with multi-label instances, which is more complex than NYT'13 only having single label in- stances. This is one reason of why the trends are quite different between them. More essentially, we further discuss the differences from the model mechanism perspective, to explain the reasons. In <ref type="bibr" target="#b7">(Fan et al., 2014</ref>)'s work, it has no explicit noise modeling mechanism. The noise is modeled im- plicitly as the error of cost functions. From the probabilistic view, that error is sampled from sin- gle Gaussian with zero mean and fixed variance. In contrast, our method uses infinite Gaussian with automatically learnt variance. It may cause over- fitting for complex dataset with sparse features. In addition, we guess the reason is that in <ref type="bibr" target="#b7">(Fan et al., 2014</ref>)'s work, they use two separate cost func- tions for features and labels, while in our work we use one unified noise component for both of them, which shows the promising precision performance in NYT'10 when recall is less than 0.4.</p><p>In addition, in our experiments, we found that early stopping is crucial for achieving good re- sults while model learning. This also verifies that the potential overfitting problem should be further considered while using the more flexible nonpara- metric method for NLP task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Our work is closest to <ref type="bibr" target="#b7">(Fan et al., 2014</ref>), since we focus on the same noisy corpus problem. Al- though from different perspectives, we study it along with the same line of using matrix factoriza- tion ( <ref type="bibr" target="#b13">Petroni et al., 2015</ref>) for relation extraction. In this line, ( <ref type="bibr" target="#b15">Riedel et al., 2013</ref>) initially considered the task as a matrix factorization problem. Their method consists of several models, such as PCA ( <ref type="bibr" target="#b6">Collins et al., 2001</ref>) and collaborative filtering <ref type="bibr" target="#b11">(Koren, 2008)</ref>. However, the data noise brought by the assumption of distant supervision ( <ref type="bibr" target="#b12">Mintz et al., 2009)</ref>, is not considered in the work. Another line addressing the problem uses deep neural networks ( <ref type="bibr" target="#b20">Zeng et al., 2015;</ref>. The differ- ence is that it is a supervised learning approach, while our focused one is a joint learning approach with transductive style, in which both training and test data are exploited simultaneously. In addi- tion, <ref type="bibr" target="#b9">(Han and Sun, 2016)</ref> explored Markov logic technique to enrich supervision knowledge, which can incorporate indirect supervision globally. Our method could be further augmented by that idea, using additional logical constraint to reduce the uncertainty for the clustered noise modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, building on recent advances from the nonparametric Bayesian literature, we reformulate the task of relation extraction with distant super- vision, based on the adaptive variance learning with intrinsic clustering structures. For the task, it can solve the sparsity problem via the learnt low-rank dense representations and can allow fit- ting noisy corpus through adaptive variance ad- justment. Meanwhile, it can avoid turning a large number of parameters. Experiments suggest sub- stantially higher top-precision than the competi- tors. In the future work, we plan to develop more sophisticated noise models for features and labels separately, and try to explore logical information, particularly in this context of nonparametric noise modeling, for further benefiting this task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Precision-Recall curve on NYT'10 and NYT'13 datasets. DRMC-b(1) (Fan et al., 2014).</figDesc><graphic url="image-2.png" coords="4,81.41,141.35,216.01,151.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results at the highest F1 point in the 
Precision-Recall (P-R) curve on NYT'10 dataset. 
Mintz (Mintz et al., 2009); Hoffmann (Hoffmann 
et al., 2011); Surdeanu (Surdeanu et al., 2012); 
DRMC-b(1) (Fan et al., 2014); 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Precision of Top-N predicted instances 
on NYT'13 dataset. NFE-13 (Riedel et al., 2013); 
DRMC-b(1) (Fan et al., 2014). 

Models 
P 
R 
F1 
DRMC-b 47.70% 49.58% 48.62% 
DRMC-1 67.99% 50.42% 57.90% 
Our 
66.46% 53.30% 59.16% 

</table></figure>

			<note place="foot" n="1"> We implement the system for relation extraction, based on the code at http://peixianc.me/amf codes.zip.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Our work is supported by National Natural Science Foundation of <ref type="bibr">China (No.61370117 &amp; No.61433015)</ref>. The corresponding author of this paper is Houfeng Wang.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sparse bayesian methods for low-rank matrix estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Derin</forename><surname>Babacan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Luessi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3964" to="3977" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Variational methods for the dirichlet process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning<address><addrLine>Banff, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
		<idno>abs/1601.00670</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Matrix completion for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><forename type="middle">Silveira</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><forename type="middle">Paulo</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bernardino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems<address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="190" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A joint model for unsupervised chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaohong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="854" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bayesian adaptive matrix factorization with automatic model selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevin</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dityan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1284" to="1292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A generalization of principal components analysis to the exponential family</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems<address><addrLine>British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="617" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Miao Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">Fang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semantic consistency: A local subspace based method for distant supervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="718" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Global distant supervision for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2950" to="2956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledge-based weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics<address><addrLine>Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the International Conference on Knowledge Discovery and Data Mining<address><addrLine>Las Vegas, Nevada, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 47th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CORE: context-aware open relation extraction with factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><forename type="middle">Del</forename><surname>Corro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1763" to="1773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Knowledge Discovery in Databases, European Conference</title>
		<meeting>Machine Learning and Knowledge Discovery in Databases, European Conference<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>Conference of the North American Chapter of the Association of Computational Linguistics<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling missing data in distant supervision for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="367" to="378" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems<address><addrLine>British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distantly supervised neural network model for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Chinese Computational Linguistics and Natural Language Processing</title>
		<meeting>Chinese Computational Linguistics and Natural Language Processing<address><addrLine>Guangzhou, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="253" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust principal component analysis with complex noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
