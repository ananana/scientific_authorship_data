<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LIA: A Natural Language Programmable Personal Assistant</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
							<email>ilabutov@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15217</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Srivastava</surname></persName>
							<email>ssrivastava@cmu.edu tom.mitchell@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15217</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15217</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LIA: A Natural Language Programmable Personal Assistant</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations)</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations) <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="145" to="150"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>145</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present LIA, an intelligent personal assistant that can be programmed using natural language. Our system demonstrates multiple competencies towards learning from human-like interactions. These include (i) the ability to be taught reusable conditional procedures, (ii) ability to be taught new knowledge about the world (concepts in an ontology) and (iii) the ability to be taught how to ground that knowledge in a set of sensors and effectors. Building such a system highlights design questions regarding the overall architecture that such an agent should have, as well as questions about parsing and grounding language in situ-ational contexts. We outline key properties of this architecture, and demonstrate a prototype that embodies them in the form of a personal assistant on an Android device.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Today's conversational assistants such as Alexa have the capacity to act on a small number of pre-programmed natural language commands (e.g., "What is the weather going to be like to- day?"). However, advances in semantic pars- ing and broader language technologies present the possibility of designing conversational interfaces that enable users to instruct (i.e., program) their assistants using language, similar to how humans teach new tasks to one another. For example, if a user wants Alexa to have a new functionality such as "whenever there is an important email I haven't seen within an hour, read it out to me", she should be able to instruct it verbally. This instruction may include explaining what constitutes an "important email". This, in turn, may involve a description such as "important emails are from colleagues", which may require further background knowledge defining "colleagues", "friends", etc. When hu- mans teach other humans, such knowledge is of- ten imparted naturally through explanations, e.g., <ref type="figure">Figure 1</ref>: Architecture overview: LIA interacts with the environment through a set of sensors and effectors, which are mapped to API's of other Android applications. End- users interact with the agent through a text (or voice) in- terface. User utterances are mapped through a Semantic Parser to logical forms. A Dialog Manager module guides user interactions by grounding logical forms to actions, or asking questions based on possible control flow branches "my colleagues would have a CMU affiliation" or "Tom is a colleague". If AI assistants could be taught in a similar fashion, this could effectively make every computer user a programmer.</p><p>Towards this end, we present a prototype for a personal assistant, LIA (for Learning from In- struction Agent), which demonstrates some of these capabilities. LIA resides on a typical mo- bile Android device. It can perceive the external environment through a set of sensors (e.g., sen- sors for detecting new emails, reading the calen- dar, reading current time, etc.) and perform ac- tions to change the environment through its effec- tors (e.g., send a message, set an alarm, change the calendar, etc.). The set of sensors and effectors are mapped to functions calls of API's for correspond- ing Android applications (see <ref type="figure">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Core competencies</head><p>LIA demonstrates three core competencies that we consider key for learning from instruction:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Learning Procedures</head><p>Among the main use-cases of being able to teach an agent is being able to define condition-action rules and procedures, such as the following: Here, the condition and the effect (action) are highlighted in green and red respectively. The condition in each example requires a check that has to be grounded in the perception sensors. If the condition is satisfied, the required processing con- sists of calling the execution of actions grounded in the effectors. Giving a conversational assistant the capacity to learn rules verbally opens the pos- sibility of teaching more complex and personal- ized rules, especially compared to visual program- ming tools such as IFTTT and Zapier 1 . LIA can ask questions if it cannot parse specifics parts of a user-statement (e.g., if it cannot understand the if- condition, see <ref type="figure" target="#fig_0">Figure 2</ref> for an example). Another advantage of a conversational setting is that LIA can take initiative when certain things are left am- biguous by the user (e.g., ask the user what to do if there is a conflict on the calendar for the last rule in the list above) -an issue that cannot be coped with in traditional programming environments.</p><formula xml:id="formula_0">&gt;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning World Knowledge</head><p>LIA can be taught knowledge about the world by the user (e.g., concepts and ontologies) that can be used as building blocks in teaching new programs.</p><p>A key advantage of a conversational interface for teaching new programs is that it allows the user to be naturally expressive about data (i.e., variables/constants) by modelling them after real- world concepts. For example, instead of saying: By relying on the concept of "my project team" (instead of listing its members), the second ex- pression is more efficient and natural. It is also better from a programming perspective: if team members change later, the rule will not have to be redefined. LIA enables users to refer to arbitrary concepts such as a project team or colleague, by declaratively teaching it about them. For example:</p><p>&gt; Oscar is on my project team &gt; Everyone on my project team is a colleague</p><p>The above examples are akin to defining formal data-structures, containing class and field defini- tions, instance creation and definitions of the natu- ral concept hierarchy (class inheritance). Because the object-oriented programming (OOP) paradigm is designed to model the real world, LIA uses it as the underlying knowledge model that the user can build and modify naturally using language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Grounding Knowledge to Perception</head><p>Not all knowledge can be easily conveyed through crisp extensional definitions such as in the exam- ple of project team above. Common concepts such as important email or meeting request are difficult to declaratively define. In a conventional program- ming paradigm, the developer may opt to create special functions for grounding such "fuzzy" con- cepts using machine learning models (i.e., classi- fiers) that are grounded in attributes of examples (e.g., emails) observed through perceptual sensors. A conversational programming paradigm offers a natural interface for teaching "fuzzy" and "crisp" concepts alike. Instead of defining hard rules for detecting important emails for example, the user may instead opt to provide descriptions that char- acterize the concept statistically:</p><p>&gt; An important email will usually be from a colleague's email address &gt; its subject may contain words like urgent or important</p><p>By grounding such natural language descriptions to observable attributes of emails, such descrip- tions can be used to build classification models for concepts such as 'important emails'.</p><p>Example Interaction: <ref type="figure" target="#fig_0">Figure 2</ref> shows an exam- ple interaction exemplifying these abilities in LIA, and also outlining its working. A video demon- stration of the system can be seen at http://y2u. be/YfKqpT0apQw. Next, we describe how these abilities are implemented in LIA, and highlight salient features of its architecture. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Architecture and System Overview</head><p>An instructable conversational assistant can be thought of as a new type of programming inter- face that allows end-users to compose core func- tionalities (over domains such as email, calendar, etc.) into programs through natural language di- alog. Just like conventional programming lan- guages, this needs answers to design questions such as: "how are new components (functions) imported into the instructable agent", and "how do these components communicate (e.g., what are the data types, how do variables get created and passed between functions)". As examples in this paper illustrate, allowing end-users teach an assis- tant through conversation brings new challenges to the design of a software architecture that facilitates programming via dialog. We outline five features that we see as fundamental to any system which can be "programmed" through conversation, and describe how LIA implements them:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Verbally Referencing and Passing Data between Sensors and Effectors</head><p>In a conventional programming language, for- mally declared variables allow one to explicitly store and reference information later in the pro- gram. A conversational programming interface needs to allow for a similar mechanism by allow- ing users to refer and reuse data during instruc- tion through verbal references. For example, con- sider the following instructions that the user can give while teaching a new procedure "If there is a meeting request, put it on my calendar": This example illustrates the requirement for pass- ing data between two components (email and cal- endar APIs), which requires reference resolution on the part of LIA's semantic parser (e.g., which "event" did the user refer to in this context?).</p><formula xml:id="formula_1">&gt;</formula><p>LIA solves the problem of interpreting users' utterances and that of resolving references to vari- ables (e.g., "subject of the received email" or "assistant's email address") jointly. Reference resolution is context-dependent, and is difficult to solve using rule-based heuristics. The prob- lem of semantic parsing and variable resolution is addressed by LIA using a machine learning based approach. For example, if the user men- tions "Tom's email", and there are multiple con- tacts named Tom, the agent can use its conversa- tional context (e.g., most recently mentioned enti- ties) and world knowledge to help and resolve the reference -both are naturally incorporated as fea- tures; weights for these are learned continuously through interactions with the user.</p><p>LIA uses a synchronous CFG-based parser im- plemented using SEMPRE <ref type="bibr" target="#b1">(Berant et al., 2013)</ref>, and an underlying frame-based meaning repre- sentation (i.e., a frame consists of an intent such as CREATE NEW CONCEPT and any arguments such as the name of the concept), allowing nested frames for certain intents (currently the IF THEN intent). The parser has an underlying log-linear parameterization of the frame/utterance pairs, with weights that can be learned offline and updated on- line during interactions with the user. LIA uses the following two classes of features to represent utterance/frame pairs:</p><p>• Lexical/logical form features: these include indicator features for derivation rules used in the parse, as well as the conjunction of non- terminals in the derivation with the part-of- speech tags spanned by the derivation.</p><p>• Variable resolution features: these include in- dicator features that fire if the resolved refer- ence matches only partially to the variable name (e.g., if the user mentions "affiliation" rather than "university affiliation"), and a feature that indicates whether the variable was mentioned recently in the conversational context. The variable resolution features are very power- ful in that they allow the incorporation of external context to help the agent resolve references by re- lying on the aggregate information from all sen- sor and effectors of the physical device. For ex- ample, a reference to a particular person may be ambiguous when interpreted in isolation, but may naturally resolve to the person who recently sent a text-message or an email. External information such as this, can be incorporated into the variable resolution features in a scalable way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generalizing Programs from a Single Example</head><p>In conventional programming, explicit functions serve as reusable building blocks and must be expressed via specialized syntax to declare what parts of the procedure can be generalized to differ- ent arguments. In a conversational setting, the user is not likely to be explicit about what parts of what they teach should generalize -this knowledge is often implicit based on the context of the taught program. Thus, programs taught via conversation need to be intelligent in automatically generaliz- ing to future invocations with different arguments where appropriate (e.g., if the user taught the agent how to "tell colleagues to...", the same procedure should correctly generalize to "tell friends to...". When a user teaches a new procedure to LIA, the interaction is always grounded in the specific context within which the user was teaching it. To explain, when teaching how to "tell my boss that I will be late", the user will narrate the sequence of instructions to the agent that repeat arguments from the original command, e.g., &gt; Set the recipient to my boss 's email address &gt; Then set its subject to I will be late</p><p>Here, "boss" and "I will be late" are arguments repeated from the original utterance that is being taught. In a conventional programming language, the programmer would write a function that would explicitly indicate which parts of the procedure are placeholders and would be replaced with ar- guments in any future invocations of the program. In a conversational setting, the agent must have the capacity to automatically identify what parts of the taught program are placeholders and should be substituted with different arguments in the future. LIA's algorithm is based on <ref type="bibr" target="#b0">Azaria et al. (2016)</ref> - it identifies matches between the command being taught and the references to entities made in the program; it then uses this information to store a templated version of the taught program that can be re-used for future invocations with different ar- guments, e.g.,:"tell my friends I am on my way".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Define New Knowledge</head><p>In a conventional programming language, data structures and their relationships (e.g., inheri- tance) must be declared formally. On the other hand, LIA infers the data types and relationships declared by the user from natural language state- ments (e.g., "most colleagues have a university affiliation" declares a new field 'university affili- ation', and "everyone on the cmu team is a col- league" creates an class-inheritance relation be- tween a member of a cmu team and a colleague). These are identified through a set of manually de- fined syntactic patterns in the semantic parser.</p><p>LIA represents an agent's knowledge in a tradi- tional object-oriented paradigm: the agent's world consists of classes (referred to as concepts), and instances (referred to as objects). Classes can ex- tend (i.e., be inherited by) at most one other class, while instances can instantiate multiple classes. Further, if a concept (or instance) in LIA extends other concepts, it also inherits all of its fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Grounding New Knowledge in Sensors and Effectors</head><p>An important component of an intelligent assis- tant is the ability to ground language and abstract concepts in observable perception, through sen- sors and effectors. We envision enabling the agent to learn concepts (such as important emails) from a combination of explanations, and examples of the concept. This is motivated by our recent re- search on using natural language to define fea- ture functions for learning tasks ( <ref type="bibr" target="#b2">Srivastava et al., 2017)</ref>, and also work on using declarative knowl- edge in natural language explanations to super- vise training of classifiers ( <ref type="bibr" target="#b3">Srivastava et al., 2018)</ref>. Using semantic parsing, we can map natural lan- guage statements to predicates in a logical lan- guage, which are grounded in sensor-effector ca- pabilities of the personal agent. These may enable the user to:</p><p>1. Mention specific attributes that characterize a concept (e.g., define a boolean feature that checks whether an email comes from a col- league) 2. Assert fuzzy statistical constraints specifying relationships between such feature and labels (e.g., 'emails from my colleagues are usually important')</p><p>In combination, these capabilities can poten- tially allow the agent to be taught classifiers for fuzzy concepts from a blend of natural language explanations of these concepts, and labeled or un- labeled data.</p><p>Using these explanations and unlabeled data, an automated learner can output a classifier that can predict the class for a new instance. The system can currently be used to train classifiers for a small number of restricted domains. The classifier learn- ing component is currently a standalone module (separate from rest of LIA). We plan to make this publicly accessible in the future. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Mixed-Initiative Interactions</head><p>In a conventional programming language, the pro- grammer anticipates all possible outcomes of var- ious API calls made in a program, wrapping these calls with control-flow statements (if/then/else blocks) to account for different return flows. Con- versely, end-user programmers must not be re- quired to be explicit about all possible program flows, but rather must inherently be in the form of a mixed-initiative dialog. LIA does this by be- ing pro-active in identifying possible control flow branches based on the instructions the user has provided while teaching. Consider an example: Here, the question creates a control flow branch, from which point on the user instructs a sequence of actions that would be triggered only if the con- dition that the agent asked about was true. One of the key challenges in providing this mixed- initiative strategy is scaling it to multiple sen- sors and effectors, where the API for each sen- sor/effector could trigger a set of potential con- trol flow branches based on the internal execution paths of the individual sensor/effector methods.</p><p>LIA's architecture facilitates a generic way of integrating new sensor/effector classes by auto- matically discovering possible outputs of the API method calls through a static analysis of the API source code. This static code analysis registers this information as possible control flow branches and uses it during the dialog with the user to ask what to do when these control flow branches are reached during execution. Of course, not every possible output of a particular API call (e.g., checking the user's availability) requires asking the user what to White boxes denote branches that were explicitly prompted by the agent by asking a ques- tion on what to do in a given situation (see <ref type="figure" target="#fig_0">Fig 2)</ref> do. It is up to the API developer to decide what ex- ecution paths inside the API call warrant generat- ing a question to the user. To communicate this in- formation explicitly and in a standard way, we re- quire that the API methods always return an object of a special output return type that both (i) contains the message to the user and (ii) encodes the infor- mation about whether the execution path resulting in this return value was successful or resulted in an error, and whether the agent should probe the user on what to do when this branch point is reached. <ref type="figure" target="#fig_4">Figure 4</ref> illustrates a full program taught by the user ("if there is an important meeting request then put it on my calendar") with two conditional branches; both branches were prompted by the agent explicitly asking the user for what to do in two scenarios: the user is not available at a partic- ular time, and the original email does not contain a time/date. See <ref type="figure" target="#fig_4">Figure 4</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Outlook</head><p>We have presented LIA, a natural language pro- grammable assistant that allows users to teach it new procedures, define new concepts and con- cept hierarchies, and ground these to observable attributes and actions in the agent's repertoire of sensors and effectors. Currently, LIA is limited in some ways. For example, newly taught com- mands are incorporated as new grammar rules in the semantic parser. As a result, future invocations of the command that exhibit lexical and syntactic variations may not correctly parse. Also, currently LIA only uses conversational and lexical features to resolve ambiguities in grounding frames to the entities in the knowledge base. The mechanism of incorporating features, however, is general and fu- ture efforts can also incorporate external features from multiple sensors (e.g., your location, person you talked to most recently, etc.).</p><p>We believe that computers that can be interac- tively instructed from natural language present an exciting new area, which can have significant im- plications for both learning and language research, as well as engender a range of creative applica- tions. We hope that through our demonstration, we can engage the community in this direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example interaction sequence with LIA. Annotations on the right summarize different parts of the conversation and outline LIA's working</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Check if there is a time or date mentioned in the message &gt; Then set the time of the new event to that time</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Knowledge View in user interface, which displays procedures taught by a user, along with utilized sensors and effectors</figDesc><graphic url="image-32.png" coords="5,307.28,62.81,226.76,123.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>&gt;</head><label></label><figDesc>Check that I am available tomorrow at 2pm ... &gt; What should I do if you are not available?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Program flow with conditional branches for an example program visualized in terms of the API method calls. Dashed lines show data (variables) passed between different method calls. White boxes denote branches that were explicitly prompted by the agent by asking a question on what to do in a given situation (see Fig 2)</figDesc><graphic url="image-33.png" coords="6,72.00,62.81,226.76,236.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Screenshot of LIA being taught a new procedure on an Android mobile device</figDesc><graphic url="image-34.png" coords="6,352.91,62.81,126.97,142.67" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Instructable intelligent personal agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Azaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2681" to="2689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint concept learning and semantic parsing from natural language explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1527" to="1536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Zero-shot learning of classifiers from natural language quantification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
