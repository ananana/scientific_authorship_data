<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">De-Conflated Semantic Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taher</forename><surname>Pilehvar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technology Lab Department of Theoretical and Applied Linguistics</orgName>
								<orgName type="institution">University of Cambridge Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technology Lab Department of Theoretical and Applied Linguistics</orgName>
								<orgName type="institution">University of Cambridge Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">De-Conflated Semantic Representations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1680" to="1690"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>One major deficiency of most semantic representation techniques is that they usually model a word type as a single point in the semantic space, hence conflating all the meanings that the word can have. Addressing this issue by learning distinct representations for individual meanings of words has been the subject of several research studies in the past few years. However, the generated sense representations are either not linked to any sense inventory or are unreliable for infrequent word senses. We propose a technique that tackles these problems by de-conflating the representations of words based on the deep knowledge that can be derived from a semantic network. Our approach provides multiple advantages in comparison to the previous approaches, including its high coverage and the ability to generate accurate representations even for infrequent word senses. We carry out evaluations on six datasets across two semantic similarity tasks and report state-of-the-art results on most of them.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modeling the meanings of linguistic items in a machine-interpretable form, i.e., semantic represen- tation, is one of the oldest, yet most active, areas of research in Natural Language Processing (NLP). The field has recently experienced a resurgence of interest with neural network-based models that view the representation task as a language modeling prob- lem and learn dense representations (usually re- ferred to as embeddings) by efficiently processing massive amounts of texts. However, either in its conventional count-based form <ref type="bibr">(Turney and Pantel, 2010)</ref> or the recent predictive approach, the prevail- ing objective of representing each word type as a single point in the semantic space has a major limita- tion: it ignores the fact that words can have multiple meanings and conflates all these meanings into a sin- gle representation. This objective can have negative impacts on accurate semantic modeling, e.g., seman- tically unrelated words that are synonymous to dif- ferent senses of a word are pulled towards each other in the semantic space <ref type="bibr" target="#b14">(Neelakantan et al., 2014</ref>). For example, the two semantically-unrelated words squirrel and keyboard are pulled towards each other in the semantic space for their similarities to two dif- ferent senses of mouse, i.e., rodent and computer in- put device.</p><p>Recently, there has been a growing interest in ad- dressing the meaning conflation deficiency of word representations. A series of techniques have been developed to associate a word to multiple points in the semantic space by clustering its contexts in a given text corpus and learning distinct rep- resentations for individual clusters <ref type="bibr" target="#b14">(Reisinger and Mooney, 2010;</ref><ref type="bibr" target="#b9">Huang et al., 2012</ref>). However, these techniques usually assume a fixed number of word senses per word type, disregarding the fact that the number of senses per word can range from one (monosemy) to dozens. <ref type="bibr" target="#b14">Neelakantan et al. (2014)</ref> tackled this issue by allowing the number to be dy- namically adjusted for each word during training. However, the approach and all the other clustering- based techniques still suffer from the fact that the computed sense representations are not linked to any sense inventory, a linking which would require large amounts of sense-annotated data <ref type="bibr" target="#b0">(Agirre et al., 2006</ref>). In addition, because of their dependence on knowledge derived from a text corpus, these tech- niques are generally unable to learn accurate repre- sentations for word senses that are infrequent in the underlying corpus.</p><p>Knowledge-based techniques tackle these issues by deriving sense-specific knowledge from exter- nal sense inventories, such as WordNet <ref type="bibr">(Fellbaum, 1998)</ref>, and learning representations that are linked to the sense inventory. These approaches either use sense definitions and employ Word Sense Disam- biguation (WSD) to gather sense-specific contexts <ref type="bibr">Iacobacci et al., 2015)</ref> or take advantage of the properties of WordNet, such as synonymy and direct semantic relations <ref type="bibr" target="#b15">(Rothe and Schütze, 2015)</ref>. However, the non-optimal WSD techniques and the shallow utilization of knowl- edge from WordNet do not allow these techniques to learn accurate and high-coverage semantic repre- sentations for all senses in the inventory.</p><p>We propose a technique that de-conflates a given word representation into its constituent sense repre- sentations by exploiting deep knowledge from the semantic network of WordNet. Our approach pro- vides the following three main advantages in com- parison to the past work: (1) our representations are linked to the WordNet sense inventory and, accord- ingly, the number of senses for a word is a dynamic parameter which matches that defined by WordNet; (2) the deep exploitation of WordNet's semantic net- work allows us to obtain accurate semantic repre- sentations, even for word senses that are infrequent in generic text corpora; and (3) our methodology in- volves only minimal parameter tuning and can be ef- fectively applied to any sense inventory that is view- able as a semantic network and to any word repre- sentation technique. We evaluate our sense repre- sentations in two tasks: word similarity (both in- context and in-isolation) and cross-level semantic similarity. Experimental results show that the pro- posed technique can provide consistently high per- formance across six datasets, outperforming the re- cent state of the art on most of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">De-Conflated Representations</head><p>Preliminaries. Our proposed approach takes a set of pre-trained word representations and uses the graph structure of a semantic lexical resource in or- der to de-conflate the representations into those of word senses. Therefore, our approach firstly re- quires a set of pre-trained word representations (e.g., word embeddings). Any model that maps a given word to a fixed-size vector representation (i.e., vec- tor space model) can be used by our approach. In our experiments, we opted for a set of publicly available word embeddings (cf. §3.1).</p><p>Secondly, we require a lexical resource whose se- mantic relations allow us to view it as a graph G = (V, E) where each vertex in the set of vertices V cor- responds to a concept and edges in E denote lexico- semantic relationships among these vertices. Each concept c ∈ V is mapped to a set of word senses by a mapping function µ(c) : c → {s 1 , . . . , s l }. Word- Net, the de facto community standard sense inven- tory, is a suitable resource that satisfies these prop- erties. WordNet can be readily represented as a se- mantic graph in which vertices are synsets and edges are the semantic relations that connect these synsets (e.g., hypernymy and meronymy). The mapping function in WordNet maps each synset to the set of synonymous words it contains (i.e., word senses).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview of the approach</head><p>Our goal is to compute a semantic representation that places a given word sense in an existing seman- tic space of words. We achieve this by leveraging word representations as well as the knowledge de- rived from WordNet. The gist of our approach lies in its computation of a list of sense biasing words for a given word sense. To this end, we first analyze the semantic network of WordNet and extract a list of most representative words that can effectively pin- point the semantics of individual synsets ( §2.2). We then leverage an effective technique which learns se- mantic representations for individual word senses by placing the senses in the proximity of their corre- sponding sense biasing words ( §2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Determining sense biasing words</head><p>Algorithm 1 shows the procedure we use to extract from WordNet a list of sense biasing words for a Algorithm 1 Get sense biasing words for synset y t</p><p>Require:</p><formula xml:id="formula_0">Graph G = (V, E) of vertices V = {y i } m i=1</formula><p>(of m synsets) and edges E (semantic relationships between synsets) Require: Function µ(y i ) that returns for a given synset y i the words it contains Require: Target synset y t ∈ V for which a sense biasing word sequence is required Ensure: The sequence B t of sense biasing words for synset y t 1: B t ← () 2: for all word w in µ(y t ) do 3:</p><formula xml:id="formula_1">B t ← B t ∪ (w) 4: for y i ∈ V : y i = y t do 5: p i ← PERSONALIZEDPAGERANK(y i , y t , G) 6: (y * h ) m−1 h=1 ← SORT(V \{y t }) according to scores p i 7: for h : 1 to m − 1 do 8: for all word w in µ(y * h ) do 9:</formula><p>if w / ∈ B t then 10:</p><formula xml:id="formula_2">B t ← B t ∪ (w) 11: return sequence B t</formula><p>given target synset y t . The algorithm receives as its inputs the semantic graph of WordNet and the map- ping function µ(·), and outputs an ordered list of bi- asing words B t for y t . The list comprises the most semantically-related words to synset y t which can best represent and pinpoint its meaning. We lever- age a graph-based algorithm for the computation of the sense biasing words.</p><p>Specifically, we use the Personalized PageRank (Haveliwala, 2002, PPR) algorithm which has been extensively used by several NLP applications <ref type="bibr" target="#b18">(Yeh et al., 2009;</ref><ref type="bibr">Niemann and Gurevych, 2011;</ref>. To this end, we first represent the se- mantic network of WordNet as a row-stochastic tran- sition matrix M ∈ R m×m where m is the number of synsets in WordNet (|V |). The cell M ij of M is set to the inverse of the degree of i if there is a seman- tic relationship between synsets i and j and to zero otherwise. We compute the PPR distribution for a target synset y t by using the power iteration method P t+1 = (1 − σ)P 0 + σMP t , where σ is the damp- ing factor (usually set to 0.85) and P 0 is a one-hot initialization vector with the corresponding dimen- sion of y t being set to 1.0. The weight p i in line 5 is the value of the i th dimension of the PPR vector P computed for the synset y t . This weight can be seen as the importance of the corresponding synset # Sense biasing words 1 dactyl, finger, toe, thumb, pollex, body part, nail, minimus, tarsier, webbed, extremity, appendage 2 figure, cardinal number, cardinal, integer, whole number, numeration system, number system, system of numeration, large integer, constituent, element, digital <ref type="table">Table 1</ref>: The top sense biasing words for the synsets containing the anatomical (#1) and numerical (#2) senses of the noun digit.</p><p>of the i th dimension (i.e., y i ) to y t . When applied to a semantic network, such as the WordNet graph, this importance can be interpreted as semantic rel- evance. Hence, the value of p i denotes the extent of semantic relatedness between y i and y t . We use this notion and retrieve a list of most semantically- related words to y t . To achieve this, we sort the synsets {y * ∈ V :</p><formula xml:id="formula_3">y * = y t } according to their PPR values {p i } m−1 i=1</formula><p>(line 6). We then iterate (lines 7-10) the sorted list (y * ) and for each synset y * h append the list B t with all the words in y * h (i.e., µ(y * h )). However, in order to ensure that the words in the target synset y t appear as the most representative words in B t , we first assign these words to the list (line 3). Finally, the algorithm returns the ordered list B t of sense biasing words for the target synset y t . <ref type="table">Table 1</ref> shows a sample of top biasing words ex- tracted for the two senses of the noun digit: the numerical and the anatomical senses. <ref type="bibr">1</ref> We explain in §2.3 how we use the sense biasing lists to learn sense-specific representations. Note that the size of the list is equal to the total number of strings in WordNet. However, we observed that taking a very small portion of the top-ranking elements in the lists is enough to generate representations that per- form very similarly to those generated when using the full-sized lists (please see §3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning sense representations</head><p>Let V be the set of pre-trained d-dimensional word representations. Our objective here is to compute a set V * = {v * s 1 , . . . , v * sn } of representations for n word senses {s 1 , . . . , s n } in the same d-dimensional semantic space of words. We achieve this for each sense s i by de-conflating the representation v s i of its corresponding lemma and biasing it towards the representations of the words in B i . Specifically, we obtain a representation v * s i for a word sense s i by solving:</p><formula xml:id="formula_4">arg min v * s i α d(v * s i , v s i ) + b ij ∈B i δ ij d(v * s i , v b ij ) (1)</formula><p>where v s i and v b ij are the respective word repre- sentations (∈ V) of the lemma of s i and the j th biasing word in the list of biasing words for s i , i.e, B i . The distance d <ref type="bibr">(v, v )</ref> between vectors v and v is measured by squared Euclidean distance</p><formula xml:id="formula_5">v − v 2 = k (v k − v k ) 2 .</formula><p>The first term in For- mula 1 requires the representation of the word sense s i (i.e., v * s i ) to be similar to that of its corresponding lemma, i.e., v s i , whereas the second term encour- ages v * s i to be in the proximity of its biasing words in the semantic space. The above criterion is simi- lar to the frameworks of Das and Smith (2011) and <ref type="bibr">Faruqui et al. (2015)</ref> which, though being convex, is usually solved for efficiency reasons by an iterative method proposed by <ref type="bibr" target="#b2">Bengio et al. (2007)</ref>. Follow- ing these works, we obtain the below equation for computing the representation of a word sense s i :</p><formula xml:id="formula_6">v * s i = αv s i + b ij ∈B i δ ij v b ij α + j δ ij .<label>(2)</label></formula><p>We define δ ij as e −λr(i,j)</p><formula xml:id="formula_7">|B i |</formula><p>where r(i, j) denotes the rank of the word b ij in the list B i . This is essen- tially an exponential decay function that gives more importance to the top-ranking biasing words for s i . The hyperparameter α denotes the extent to which v * s i is kept close to its corresponding lemma repre- sentation v s i . Following <ref type="bibr">Faruqui et al. (2015)</ref>, we set α to 1. The only parameter to be tuned in our experiments is λ. We discuss the tuning of this pa- rameter in §3.1. The representation of a synset y i can be accordingly calculated as the centroid of the vectors of its associated word senses, i.e.,</p><formula xml:id="formula_8">{ v y i v y i : v y i = s∈µ(y i ) ˆ v * s , ˆ v * s = v * s v * s }. (3)</formula><p>As a result of this procedure, we obtain the set V * of n sense representations in the same semantic space as word representations in V. In fact, we now have a unified semantic space which enables a direct comparison of the two types of linguistic items. In  crappie, trout, guitar, shad, walleye, bassist, angler, catfish, trombone, percussion, piano, drummer, saxophone, jigs, fish 2 baritone, piano, guitar, trombone, saxophone, cello, percussion, tenor, saxophonist, clarinet, pianist, vocals, solos, harmonica 3 fish, trout, shrimp, anglers, fishing, bait, guitar, salmon, shark, fisherman, lakes, seafood, drummer, whale, fisheries <ref type="table">Table 2</ref>: Ten most similar words to the word bass (#1) and two of its senses: music (#2) and fish (#3).</p><p>Figure 1: The illustration of the word digit and two of its com- puted senses in our unified 2-d semantic space. §3.3 we evaluate our approach in the word to sense similarity measurement framework. We show in <ref type="table">Table 2</ref> the closest words to the word bass and two of its senses, music and fish, 2 in our unified semantic space. We can see in row #1 a mix- ture of both meanings when the word representation is used whereas the closest words to the senses (rows #2 and #3) are mostly in-domain and specific to the corresponding sense.</p><p>To exhibit another interesting property of our sense representation approach, we depict in <ref type="figure" target="#fig_1">Figure  1</ref> the word digit and its numerical and anatomical senses (from the example in <ref type="table">Table 1</ref>) in a 2-d seman- tic space, along with a sample set of words in their proximity. <ref type="bibr">3</ref> We can see that the word digit is placed in the semantic space in the neighbourhood of words from the numerical domain (lower left of the figure), mainly due the dominance (Sanderson and Van Ri- jsbergen, 1999) of this sense in the general-domain corpus on which the word embeddings in our ex- periments were trained (cf.</p><note type="other">§3.1). However, upon de-conflation, the emerging anatomical sense of the word is shifted towards the region in the semantic space which is occupied by anatomical words (up- per right of the figure). A clustering-based sense representation technique would have failed in accu- rately representing the infrequent anatomical mean- ing of digit by analyzing a general domain corpus (such as the one used here). But our sense repre- sentation technique, thanks to its proper usage of knowledge from a sense inventory, is effective in unveiling and accurately modeling less frequent or domain-specific senses of a given word.</note><p>Please note that any vector space model represen- tation technique can be used for the pre-training of word representations in V. Also, the list of sense biasing words can be obtained for larger sense in- ventories, such as FreeBase ( <ref type="bibr" target="#b3">Bollacker et al., 2008)</ref> or BabelNet ( <ref type="bibr">Navigli and Ponzetto, 2012)</ref>. We leave the exploration of further ways of computing sense biasing words to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We benchmarked our sense representation approach against several recent techniques on two standard tasks: word similarity ( §3.2), for which we eval- uate on both in-isolation and in-context similarity datasets, and cross-level semantic similarity ( §3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental setup</head><p>Pre-trained word representations. As our word representations, we used the 300-d Word2vec ( <ref type="bibr" target="#b14">Mikolov et al., 2013</ref>) word embeddings trained on the Google News dataset 4 mainly for their popular- ity across different NLP applications. However, our approach is equally applicable to any count-based representation technique ( <ref type="bibr">Baroni and Lenci, 2010;</ref><ref type="bibr">Turney and Pantel, 2010)</ref> or any other embedding approach ( <ref type="bibr">Pennington et al., 2014;</ref><ref type="bibr" target="#b12">LeCun et al., 2015)</ref>. We leave the evaluation and comparison of various word representation techniques with differ- ent training approaches, objectives, and dimension- alities to future work.</p><p>Parameter tuning. Recall from §2.3 that our pro- cedure for learning sense representations needs only one parameter to be tuned, i.e., λ. We did not per- form an extensive tuning on the value of this param- eter and set its value to 1 /5 after trying four differ- ent values (1, 1 /2, 1 /5, and 1 /10) on a small validation dataset. We leave the more systematic tuning of the parameter and the choice of alternative decay func- tions (cf. §2.3) to future work.</p><p>The size of the sense biasing words lists. Also recall from §2.2 that the extracted lists of sense bias- ing words were originally as large as the total num- ber of unique strings in WordNet (around 150K in ver. 3.0). But, given that we use an exponential de- cay function in our learning algorithm (cf. §2.3), the impact of the low-ranking words in the list is negligible. In fact, we observed that taking a very small portion of the top-ranking words, i.e., the top 25, produces similarity scores that are on par with those generated when the full lists were considered. Therefore, we experimented with the down-sized lists which enabled us to generate very quickly sense representations for all word senses in WordNet.  <ref type="formula" target="#formula_6">2015)</ref>, we show the results of the EM+RETERO system which performs most consis- tently across different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Word similarity</head><p>Benchmarks. As our word similarity benchmark, we considered five datasets: RG-65 <ref type="bibr" target="#b15">(Rubenstein and Goodenough, 1965)</ref>, YP-130 ( <ref type="bibr">Yang and Powers, 2005</ref>), MEN-3K ( <ref type="bibr">Bruni et al., 2014</ref>), SimLex- 999 ( <ref type="bibr">Hill et al., 2015, SL-999)</ref>, and Stanford Con- textual Word Similarity <ref type="figure" target="#fig_1">(Huang et al., 2012, SCWS)</ref>. The latter benchmark provides for each word a con- text that triggers a specific meaning of it, making the dataset very suitable for the evaluation of sense rep- resentation. For each datasets, we list the results that are reported by any of our comparison systems.</p><p>Similarity measurement. For the SCWS dataset, we follow the past works <ref type="bibr" target="#b14">(Reisinger and Mooney, 2010;</ref><ref type="bibr" target="#b9">Huang et al., 2012</ref>) and report the results ac- cording to two system configurations: (1) AvgSim: where the similarity between two words is computed as the average of all the pairwise similarities be- tween their senses, and (2) AvgSimC: where each pairwise sense similarity is weighted by the rele- vance of each sense to its corresponding context. For all the other datasets, since words are not provided with any context (they are in isolation), we measure the similarity between two words as that between their most similar senses. In all the experiments, we use the cosine distance as our similarity measure. <ref type="table" target="#tab_1">Tables 4 and 3</ref> show the results of our system, DE- CONF, and the comparison systems on the SCWS and the other four similarity datasets, respectively. In both tables we also report the word vectors base- line, whenever they are available, which is computed by directly comparing the corresponding word rep- resentations of the two words (∈ V). Note that the word-based baseline does not apply to the approach of <ref type="bibr">Pilehvar and Navigli (2015)</ref> as it is purely based on the semantic network of WordNet and does not use any pre-trained word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Experimental results</head><p>We can see from the tables that our sense rep- resentations obtain considerable improvements over those of words across the five datasets. This high- lights the fact that the de-conflation of word rep- resentations into those of their individual meanings has been highly beneficial. On the SCWS dataset, DECONF outperforms all the recent state-of-the-art sense representation techniques (in their best set- tings) which proves the effectiveness of our ap- proach in capturing the semantics of specific mean- ings of the words. The improvement is consistent across both system configurations (i.e., AvgSim and AvgSimC). Moreover, the state-of-the-art WordNet- based approach of <ref type="bibr" target="#b15">Rothe and Schütze (2015)</ref> uses the same initial word vectors as DECONF does (cf. §3.1). Hence, the improvement we obtain indicates that our approach has made better use of the sense- specific knowledge encoded in WordNet.</p><p>As seen in <ref type="table" target="#tab_1">Table 3</ref> our approach shows com- petitive performance on the other four datasets. The YP-130 dataset focuses on verb similarity, whereas SimLex-999 contains verbs and adjectives and MEN-3K has word pairs with different parts of speech (e.g., a noun compared to a verb). The results we obtain on these datasets exhibit the reliability of our approach in modeling non-nominal word senses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Discussion</head><p>The similarity scale of the SimLex-999 dataset is different from our other word similarity benchmarks in that it assigns relatively low scores to antonymous pairs. For instance, sunset-sunrise and man-woman in this dataset are assigned the respective similari- ties of 2.47 and 3.33 (in a <ref type="bibr">[0,</ref><ref type="bibr">10]</ref> similarity scale) which is in the same range as the similarity between word pairs with slight domain relatedness, such as head-nail (2.47), air-molecule (3.05), or succeed-try (3.98). In fact, we observed that tweaking the simi- larity scale of our system in a way that it diminishes the similarity scores between antonyms can result in a significant performance improvement on this dataset. To this end, we performed an experiment in which the similarity of a word pair was simply divided by 3 whenever the two words belonged to synsets that were linked by an antonymy relation in WordNet. <ref type="bibr">5</ref> We observed that the performance on the SimLex-999 dataset increased to 61.6 (from 54.2) and 59.1 (from 51.7) according to Pearson (r × 100) and Spearman (ρ × 100) correlation scores, respec- tively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>Sense-based score Word-based score r ρ r ρ  Initial word vectors 65.1 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cross-Level semantic similarity</head><p>In addition to the word similarity benchmark, we evaluated the performance of our representations in the cross-level semantic similarity measurement framework. For this, we opted for the SemEval- 2014 task on Cross-Level Semantic Similarity (Ju- rgens et al., 2014, CLSS). The word to sense simi- larity subtask of this task, with 500 instances in its test set, provides a suitable benchmark for the eval- uation of sense representation techniques. For a word sense s and a word w, we compute the similarity score according to four different strate- gies: the similarity of s to the most similar sense of w (MaxSim), the average similarity of s to indi- vidual senses of w (AvgSim), the direct similarity of s to w when the latter is modeled as its word repre- sentation (Sense-to-Word or S2W) or as the centroid of its senses' representations (Sense to aggregated word senses or S2A). For this task, we can only com- pare against the publicly-available sense representa- tions of <ref type="bibr">Iacobacci et al. (2015)</ref>, <ref type="bibr" target="#b15">Rothe and Schütze (2015)</ref>, <ref type="bibr">Pilehvar and Navigli (2015)</ref> and  which are linked to the WordNet sense inven- tory. <ref type="table">Table 5</ref> shows the results on the word to sense dataset of the SemEval-2014 CLSS task, according to Pearson (r × 100) and Spearman (ρ × 100) cor- relation scores and for the four strategies. As can be seen from the low overall performance, the task is a very challenging benchmark with many Word- Net out-of-vocabulary or slang terms and rare us- ages. Despite this, DECONF provides consistent im- provement over the comparison sense representation techniques according to both measures and for all the strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Experimental results</head><p>Across the four strategies, S2A proves to be the most effective for DECONF and the representations of <ref type="bibr" target="#b15">Rothe and Schütze (2015)</ref>. The representations of  perform best with the S2W strat-   egy whereas those of Iacobacci et al. <ref type="formula" target="#formula_6">(2015)</ref> do not show a consistent trend with relatively low perfor- mance across the four strategies. Also, a comparison of our results across the S2W and S2A strategies re- veals that a word's aggregated representation, i.e., the centroid of the representations of its senses, is more accurate than its original word representation.</p><p>Our analysis showed that the performance of the approaches of <ref type="bibr" target="#b15">Rothe and Schütze (2015)</ref> and <ref type="bibr">Iacobacci et al. (2015)</ref> were hampered partly due to their limited coverage. In fact, the former was un- able to model around 35% of the synsets in WordNet 1.7.1, mainly for its shallow exploitation of knowl- edge from WordNet, whereas the latter approach did not cover around 15% of synsets in WordNet 3.0.  provide near-full coverage for word senses in WordNet. However, the relatively low performance of their system shows that the us- age of glosses in WordNet and the automated dis- ambiguation have not resulted in accurate sense rep- resentations. Thanks to its deep exploitation of the underlying resource, our approach provides more re- liable representations and full coverage for all word senses and synsets in WordNet.</p><p>The three best-performing systems in the task are Meerkat Mafia ( <ref type="bibr" target="#b11">Kashyap et al., 2014</ref>) (r = 37.5, ρ = 39.3), SimCompass ( <ref type="bibr" target="#b1">Banea et al., 2014</ref>) (r = 35.4, ρ = 34.9), and SemantiKLUE ( <ref type="bibr">Proisl et al., 2014</ref>) (r = 17.9, ρ = 18.8). Note that these systems are specifically designed for the cross-level similar- ity measurement task. For instance, the best-ranking system in the task leverages a compilation of several dictionaries, including The American Heritage Dic- tionary, Wiktionary and WordNet, in order to handle slang terms and rare usages, which leads to its com- petitive performance ( <ref type="bibr" target="#b11">Kashyap et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Learning semantic representations for individual senses of words has been an active area of research for the past few years. Based on the way they view the problem, the recent techniques can be classified into two main branches: (1) those that, similarly to our work, extract knowledge from external sense in- ventories for learning sense representations; and (2) those techniques that cluster the contexts in which a word appears in a given text corpus and learn distinct representations for individual clusters.</p><p>Examples for the first branch include the ap- proaches of , <ref type="bibr" target="#b10">Jauhar et al. (2015)</ref> and <ref type="bibr" target="#b15">Rothe and Schütze (2015)</ref>, all of which use WordNet as an external resource and obtain sense representations for this sense inventory.  uses the content words in the definition of a word sense and WSD. However, the sole us- age of glosses as sense-distinguishing contexts and the non-optimal WSD make the approach inaccu- rate, particularly for highly polysemous words with similar senses and for word senses with short def- initions. Similarly, <ref type="bibr" target="#b15">Rothe and Schütze (2015)</ref> use only polysemy and synonymy properties of words in WordNet along with a small set of semantic re-lations. This significantly hampers the reliability of the technique in providing high coverage (discussed further in §3.3.1). Our approach improves over these works by exploiting deep knowledge from the se- mantic network of WordNet, coupled with an effec- tive training approach. ADW <ref type="bibr">(Pilehvar and Navigli, 2015</ref>) is another WordNet-based approach which ex- ploits only the semantic network of this resource and obtains interpretable sense representations. Other work in this branch include <ref type="bibr">SensEmbed (Iacobacci et al., 2015)</ref> and Nasari <ref type="bibr" target="#b4">(Camacho-Collados et al., 2015;</ref><ref type="bibr" target="#b5">Camacho-Collados et al., 2016)</ref> which are based on the BabelNet sense inventory <ref type="bibr">(Navigli and Ponzetto, 2012</ref>). The former technique first disambiguates words in a given corpus with the help of a knowledge-based WSD system and then uses the generated sense-annotated corpus as train- ing data for Word2vec. Nasari combines structural knowledge from the semantic network of BabelNet with corpus statistics derived from Wikipedia for representing BabelNet synsets. However, the ap- proach falls short of modeling non-nominal senses as Wikipedia, due to its very encyclopedic nature, does not cover verbs, adjectives, or adverbs.</p><p>The second branch, which is usually referred to as multi-prototype representation, is often associ- ated with clustering. <ref type="bibr" target="#b14">Reisinger and Mooney (2010)</ref> proposed one of the recent pioneering techniques in this branch. Other prominent work in the cate- gory include topical word embeddings ( ) which use latent topic models for assigning topics to each word in a corpus and learn topic- specific word representations, and the technique pro- posed by <ref type="bibr" target="#b9">Huang et al. (2012)</ref> which incorporates "global document context." <ref type="bibr" target="#b16">Tian et al. (2014)</ref> mod- ified the Skip-gram model in order to learn multi- ple embeddings for each word type. Despite the fact that these techniques do not usually take advantage of the knowledge encoded in structured knowledge resource, they generally suffer from two disadvan- tages. The first limitation is that they usually make an assumption that a given word has a fixed number of senses, ignoring the fact that polysemy is highly dynamic across words that can range from monose- mous to highly ambiguous with dozens of associ- ated meanings <ref type="bibr" target="#b14">(McCarthy et al., 2016)</ref>. <ref type="bibr" target="#b14">Neelakantan et al. (2014)</ref> tackled this issue by estimating the number of senses for a word type during the learn- ing process. However, all techniques in the second branch suffer from another disadvantage that their computed sense representations are not linked to any sense inventory, a linking which itself would require the existence of high coverage sense-annotated data <ref type="bibr" target="#b0">(Agirre et al., 2006</ref>).</p><p>Another notable line of research incorporates knowledge from external resources, such as PPDB ( <ref type="bibr">Ganitkevitch et al., 2013)</ref> and WordNet, to improve word embeddings ( <ref type="bibr">Yu and Dredze, 2014;</ref><ref type="bibr">Faruqui et al., 2015)</ref>. Neither of the two techniques however, provide representations for word senses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We put forward a sense representation technique, namely DECONF, that provides multiple advantages in comparison to the recent state of the art: (1) the number of word senses in our technique is flexi- ble and the computed representations are linked to word senses in WordNet; (2) DECONF is effective in providing accurate representation of word senses, even for those senses that do not usually appear fre- quently in generic text corpora; and (3) our approach is general in that it can be readily applied to any set of word representations and any semantic network without the need for extensive parameter tuning. Our experimental results showed that DECONF can outperform recent state of the art on several datasets across two tasks. The computed representations for word senses in WordNet 3.0 are released at https: //pilehvar.github.io/deconf/. We in- tend to apply our technique to the task of harmo- nizing biomedical terms in the PheneBank project. As future work, we plan to investigate the possibil- ity of using larger semantic networks, such as Free- Base and BabelNet, which would also allow us to apply the technique to languages other than English. We also plan to evaluate the performance of our ap- proach with other decay functions as well as with other initial word representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Comparison systems.</head><label></label><figDesc>We compared our results against nine other sense representation techniques: the WordNet-based approaches of Pilehvar and Nav- igli (2015), Chen et al. (2014), Rothe and Schütze (2015), Jauhar et al. (2015), and Iacobacci et al. (2015) and the clustering-based approaches of Huang et al. (2012), Tian et al. (2014), Neelakan- tan et al. (2014), and Liu et al. (2015) (please see §4 for more details). We also compared against the approach of Faruqui et al. (2015) which uses knowl- edge derived from WordNet for improving word rep- resentations. From the different configurations pre- sented in (Faruqui et al., 2015) we chose the sys- tem that uses GloVe (Pennington et al., 2014) with all WordNet relations which is their best perform- ing monolingual system. As for the approach of Jauhar et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>5 :</head><label>5</label><figDesc>Evaluation results on the word to sense similarity test dataset of the SemEval-14 task on Cross-Level Semantic Similarity, according to Pearson (r × 100) and Spearman (ρ × 100) correlations. We show results for four similarity computation strategies (see §3.3). The best results per strategy are shown in bold whereas they are underlined for the best strategies per system. Systems marked with * are evaluated on a slightly smaller dataset (474 of the original 500 pairs) so as to have a fair comparison with Rothe and Schütze (2015) and Chen et al. (2014) that use older versions of WordNet (1.7.1 and 1.7, respectively).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Pearson (r × 100) and Spearman (ρ × 100) correlation scores on four standard word similarity benchmarks. For each 

benchmark, we show the results reported by any of the comparison systems along with the scores for their corresponding initial 

word representations (word-based). 

Approach 
Score 
AvgSim AvgSimC 

DECONF 
70.8 
71.5 
Rothe and Schütze (2015) (best) 
68.9 
69.8 
Neelakantan et al. (2014) (best) 
67.3 
69.3 
Chen et al. (2014) 
66.2 
68.9 
Liu et al. (2015) (best) 
− 
68.1 
Huang et al. (2012) 
62.8 
65.7 
Tian et al. (2014) (best) 
− 
65.7 
Iacobacci et al. (2015) 
62.4 
− 
Jauhar et al. (2015) 
− 
58.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Spearman correlation scores (ρ × 100) on the Stan-

ford Contextual Word Similarity (SCWS) dataset. We report 

the AvgSim and AvgSimC scores (cf.  §3.2) for each system, 

where available. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> The first and third senses of the noun digit in WordNet 3.0.</note>

			<note place="foot" n="2"> The first and fourth senses in WordNet 3.0, respectively defined as &quot;the lowest part of the musical range&quot; and &quot;the lean flesh of a saltwater fish of the family Serranidae.&quot;</note>

			<note place="foot" n="3"> We used the t-SNE algorithm (van der Maaten and Hinton, 2008) for dimensionality reduction. 4 https://code.google.com/archive/p/word2vec/</note>

			<note place="foot" n="5"> We chose 3 so as to transform a pair with high similarity score (around 9.0) to one with slight semantic similarity (around 3.0) in the [0, 10] similarity scale of SimLex-999. We also tested for other values in [2, 6] an observed similar performance gains.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors gratefully acknowledge the support of the MRC grant No. MR/M025160/1 for PheneBank.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evaluating and optimizing the parameters of an unsupervised graph-based wsd algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oier</forename><surname>López De Lacalle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Graph Based Methods for Natural Language Processing, TextGraphs-1</title>
		<meeting>the First Workshop on Graph Based Methods for Natural Language Processing, TextGraphs-1</meeting>
		<imprint>
			<publisher>Oier López de Lacalle, and Aitor Soroa</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="57" to="84" />
		</imprint>
	</monogr>
	<note>Random walks for knowledgebased Word Sense Disambiguation</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Simcompass: Using deep learning word embeddings to assess cross-level similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Banea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<editor>Baroni and Lenci2010] Marco Baroni and Alessandro Lenci</editor>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="673" to="721" />
		</imprint>
	</monogr>
	<note>Distributional memory: A general framework for corpus-based semantics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>MIT Press. chapter Label Propagation and Quadratic Criterion</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bollacker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data</title>
		<editor>Bruni et al.2014] Elia Bruni, Nam Khanh Tran, and Marco Baroni</editor>
		<meeting>the 2008 ACM SIGMOD International Conference on Management of Data<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="47" />
		</imprint>
	</monogr>
	<note>Multimodal distributional semantics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">NASARI: a Novel Approach to a SemanticallyAware Representation of Items</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Camacho-Collados</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL<address><addrLine>Denver, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="577" />
		</imprint>
	</monogr>
	<note>José Camacho-Collados, Mohammad Taher Pilehvar, and Roberto Navigli</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">NASARI: Integrating explicit knowledge and corpus statistics for amultilingual representation of concepts and entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Camacho-Collados</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">240</biblScope>
			<biblScope unit="page" from="36" to="64" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified model for word sense representation and disambiguation</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<editor>Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar, Chris Dyer, Eduard Hovy, and Noah A. Smith</editor>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Doha, Qatar; Portland, Oregon, USA; Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1606" to="1615" />
		</imprint>
	</monogr>
	<note>Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">WordNet: An Electronic Database</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<editor>MA. [Ganitkevitch et al.2013] Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch</editor>
		<meeting>NAACL-HLT<address><addrLine>Cambridge; Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="758" to="764" />
		</imprint>
	</monogr>
	<note>PPDB: The paraphrase database</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SimLex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haveliwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on World Wide Web</title>
		<editor>Hill et al.2015] Felix Hill, Roi Reichart, and Anna Korhonen</editor>
		<meeting>the 11th International Conference on World Wide Web<address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="665" to="695" />
		</imprint>
	</monogr>
	<note>Topicsensitive PageRank</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd</title>
		<editor>Ignacio Iacobacci, Mohammad Taher Pilehvar, and Roberto Navigli</editor>
		<meeting>the 53rd<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ontologically grounded multisense representation learning for semantic vector space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Sujay Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<editor>Jurgens et al.2014] David Jurgens, Mohammad Taher Pilehvar, and Roberto Navigli</editor>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Beijing, China; Denver, Colorado; Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="17" to="26" />
		</imprint>
	</monogr>
	<note>Proceedings of the 8th International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Meerkat Mafia: Multilingual and Cross-Level Semantic Textual Similarity systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kashyap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tat-Seng Chua, and Maosong Sun</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2418" to="2424" />
		</imprint>
	</monogr>
	<note>Topical word embeddings</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<editor>Thomas Proisl, Stefan Evert, Paul Greiner, and Besim Kabashi</editor>
		<meeting><address><addrLine>Scottsdale, Arizona; Doha, Qatar; Oxford, United Kingdom; Doha, Qatar; Dublin, Ireland; Los Angeles</addrLine></address></meeting>
		<imprint>
			<publisher>California</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
	<note>Proceedings of the Ninth International Conference on Computational Semantics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">AutoExtend: Extending word embeddings to embeddings for synsets and lexemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze ; Herbert Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<editor>Sanderson and Van Rijsbergen1999] Mark Sanderson and C. J. Van Rijsbergen</editor>
		<meeting>ACL<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1965" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="440" to="465" />
		</imprint>
	</monogr>
	<note>Communications of the ACM</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A probabilistic model for learning multiprototype word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<editor>Turney and Pantel2010] Peter D. Turney and Patrick Pantel</editor>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
		</imprint>
	</monogr>
	<note>From frequency to meaning: Vector space models of semantics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Measuring semantic similarity in the taxonomy of WordNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">] L</forename><forename type="middle">J</forename><surname>Hinton2008</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Hinton ; David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Powers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-eighth Australasian Conference on Computer Science</title>
		<meeting>the Twenty-eighth Australasian Conference on Computer Science<address><addrLine>Newcastle, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="315" to="322" />
		</imprint>
	</monogr>
	<note>Visualizing high-dimensional data using t-SNE</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">WikiWalk: Random walks on Wikipedia for semantic relatedness</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing</title>
		<editor>Yu and Dredze2014] Mo Yu and Mark Dredze</editor>
		<meeting>the 2009 Workshop on Graph-based Methods for Natural Language Processing<address><addrLine>Suntec, Singapore; Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="545" to="550" />
		</imprint>
	</monogr>
	<note>Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
