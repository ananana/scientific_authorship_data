<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CROWD-IN-THE-LOOP: A Hybrid Approach for Annotating Semantic Roles</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
							<email>chenguang.wang@ibm.com, alan.akbik@zalando.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution" key="instit1">IBM Research -Almaden ‡</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution" key="instit1">IBM Research -Almaden ‡</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Chiticariu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution" key="instit1">IBM Research -Almaden ‡</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution" key="instit1">IBM Research -Almaden ‡</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution" key="instit1">IBM Research -Almaden ‡</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution" key="instit1">IBM Research -Almaden ‡</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution" key="instit1">IBM Research -Almaden ‡</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zalando</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution" key="instit1">IBM Research -Almaden ‡</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berlin</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution" key="instit1">IBM Research -Almaden ‡</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CROWD-IN-THE-LOOP: A Hybrid Approach for Annotating Semantic Roles</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1913" to="1922"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Crowdsourcing has proven to be an effective method for generating labeled data for a range of NLP tasks. However, multiple recent attempts of using crowdsourcing to generate gold-labeled training data for semantic role labeling (SRL) reported only modest results, indicating that SRL is perhaps too difficult a task to be effectively crowdsourced. In this paper, we postulate that while producing SRL annotation does require expert involvement in general , a large subset of SRL labeling tasks is in fact appropriate for the crowd. We present a novel workflow in which we employ a classifier to identify difficult annotation tasks and route each task either to experts or crowd workers according to their difficulties. Our experimental evaluation shows that the proposed approach reduces the workload for experts by over two-thirds, and thus significantly reduces the cost of producing SRL annotation at little loss in quality.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic role labeling (SRL) is the task of label- ing the predicate-argument structures of sentences with semantic frames and their roles ( <ref type="bibr" target="#b2">Baker et al., 1998;</ref><ref type="bibr" target="#b19">Palmer et al., 2005</ref>). It has been found useful for a wide variety of NLP tasks such as question-answering <ref type="bibr" target="#b22">(Shen and Lapata, 2007)</ref>, in- formation extraction <ref type="bibr" target="#b5">(Fader et al., 2011</ref>) and ma- chine translation ( <ref type="bibr" target="#b18">Lo et al., 2013)</ref>. A major bot- tleneck impeding the wide adoption of SRL is the need for large amounts of labeled training data to * The work was done while the author was at IBM Re- search -Almaden.</p><p>capture broad-coverage semantics. Such data re- quires trained experts and is highly costly to pro- duce ( <ref type="bibr" target="#b15">Hovy et al., 2006</ref>).</p><p>Crowdsourcing SRL Crowdsourcing has shown its effectiveness to generate labeled data for a range of NLP tasks ( <ref type="bibr" target="#b23">Snow et al., 2008;</ref><ref type="bibr" target="#b14">Hong and Baker, 2011;</ref><ref type="bibr" target="#b8">Franklin et al., 2011)</ref>. A core ad- vantage of crowdsourcing is that it allows the an- notation workload to be scaled out among large numbers of inexpensive crowd workers. Not sur- prisingly, a number of recent SRL works have also attempted to leverage crowdsourcing to gen- erate labeled training data for SRL and investi- gated a variety of ways of formulating crowd- sourcing tasks ( <ref type="bibr" target="#b7">Fossati et al., 2013;</ref><ref type="bibr" target="#b21">Pavlick et al., 2015;</ref>). All have found that crowd feedback generally suffers from low inter- annotator agreement scores and often produces in- correct labels. These results seem to indicate that, regardless of the design of the task, SRL is simply too difficult to be effectively crowdsourced. Proposed Approach We observe that there are significant differences in difficulties among SRL annotation tasks, depending on factors such as the complexity of a specific sentence or the difficulty of a specific semantic role. We therefore postulate that a subset of annotation tasks is in fact suitable for crowd workers, while others require expert in- volvement. We also postulate that it is possible to use a classifier to predict whether a specific task is easy enough for crowd workers.</p><p>Based on these intuitions, we propose CROWD- IN-THE-LOOP, a hybrid annotation approach that involves both crowd workers and experts: All an- notation tasks are passed through a decision func- tion (referred to as TASKROUTER) that classi- fies them as either crowd-appropriate or expert- required, and sent to crowd or expert annotators accordingly. Refer to <ref type="figure" target="#fig_0">Figure 1</ref> for an illustration of this workflow. We conduct an experimental evaluation that shows (1) that we are able to design a classifier that can distinguish between crowd-appropriate and expert-required tasks at very high accuracy (96%), and (2) that our proposed workflow allows us to pass over two-thirds of the annotation work- load to crowd workers, thereby significantly re- ducing the need for costly expert involvement. Contributions In detail, our contributions are:</p><p>• We propose CROWD-IN-THE-LOOP, a novel approach for creating annotated SRL data with both crowd workers and experts. It re- duces overall labeling costs by leveraging the crowd whenever possible, and maintains an- notation quality by involving experts when- ever necessary.</p><p>• We propose TASKROUTER, an annotation task decision function (or classifier), that classifies each annotation task into one of two categories: expert-required or crowd- appropriate. We carefully define the classifi- cation task, discuss features and evaluate dif- ferent classification models.</p><p>• We conduct a detailed experimental evalua- tion of the proposed workflow against several baselines including standard crowdsourcing and other hybrid annotation approaches. We analyze the strengths and weaknesses of each approach and illustrate how expert involve- ment is required to address errors made by crowd workers.</p><p>Outline This paper is organized as follows: We first conduct a baseline study of crowdsourcing SRL annotation, and analyze the difficulties of re- lying solely on crowd workers (Section 2). Based on this analysis, we define the classification prob- lem for CROWD-IN-THE-LOOP, discuss the design of our classifier, and evaluate its accuracy (Sec- tion 3). We then employ this classifier in the pro- posed CROWD-IN-THE-LOOP approach and com- paratively evaluate it against a number of crowd- sourcing and hybrid workflows (Section 4). We discuss related work (Section 5) and conclude the study in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Crowdsourcing SRL</head><p>We first conduct a baseline study of crowdsourcing SRL. We illustrate how we design and create an- notation tasks, how we gather and interpret crowd feedback, and analyze the results of the study to determine the applicability of crowdsourcing for producing SRL annotation. SRL formalism. In this study, and throughout the paper, we use the PROPBANK formalism of SRL ( <ref type="bibr" target="#b19">Palmer et al., 2005</ref>), which defines verb- specific frames (BUY.01, BUY.02), frame-specific core roles (A0 to A5), and frame-independent non- core roles (for temporal, location and other con- texts).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Annotation Task Design</head><p>To design the annotation task, we replicate a setup proposed in previous work ) in which crowd workers are employed to curate the output of a statistical SRL system. This setup gen- erates annotation tasks as following:</p><p>Sentence And many fund managers have built up cash levels and say they will be buying stock this week.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>What is being bought in this sentence? Is it: "stock"?</p><p>buy.01  Step 1. We use a statistical SRL system to predict SRL labels for a set of sentences (see <ref type="figure" target="#fig_0">Figure 1)</ref>. While state-of-the-art SRL will predict many cor- rect labels, some predicted labels will be incorrect, and some labels will be missing. Annotation tasks are therefore designed to detect and correct preci- sion and recall errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Options</head><p>Step 2. We generate two types of annotation tasks for the study, namely CONFIRMPREDICTION and ADDMISSING tasks: (1) The first, CONFIRMPRE- DICTION tasks, ask users to confirm, reject or cor- rect each predicted frame or role. This type of task addresses precision issues in the SRL. We present to workers a human-readable question- answer pair <ref type="bibr" target="#b12">(He et al., 2015</ref>) for each predicted la- bel, an example of which is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. <ref type="formula" target="#formula_1">(2)</ref> The second, ADDMISSING tasks, address po- tentially missing annotation, i.e. recall issues in the SRL. We generate a question without a sug- gested answer and ask workers to either confirm that this role does not appear in the sentence, or supply the correct span. We identify potentially missing annotation using PropBank frame defini- tions; any unseen core role in a sentence is consid- ered potentially missing. We use a manually created mapping of frame- roles to questions to generate these tasks. See Ta- ble 1 for a mapping of the roles of the BUY.01 frame to questions.</p><p>Step 3. Each question is presented to crowd work- ers together with the sentence and a set of answer options. Example annotation tasks are illustrated in Figures 2 and 3. A task thus is defined as fol- lows:</p><p>Definition 1 Annotation Task: A task consists of a sentence, a human readable question regarding a predicted label, and a set of answer options.</p><p>We collect worker responses to these tasks. If the majority of crowd workers agrees on a correction, we remove or correct incorrectly predicted labels <ref type="bibr">Frame</ref>  <ref type="table">Table 2</ref>: Tasks in our crowdsourcing study by ratio of how many workers agreed on an answer. If all five workers agree, the majority answer is correct in 93% of cases. If fewer work- ers agree, the precision of the majority answer decreases.</p><p>for CONFIRMPREDICTION tasks and add new la- bels for ADDMISSING tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Crowdsourcing Study</head><p>We conduct a crowdsourcing study consisting of 2,549 annotation tasks, generated by running a state-of-the-art SRL system (Akbik and Li, 2016) over 250 randomly selected gold-labeled sen- tences from the English training dataset in the CoNLL-2009 shared task <ref type="bibr" target="#b10">(Hajič et al., 2009</ref>). We generated tasks using our question mappings from the predicted labels. This setup allows us to com- pare crowd feedback to gold labels and determine how often the crowd provides incorrect answers. Human Annotators For crowd annotators, we employ five native speakers of English from UP- WORK 1 , selected using the following procedure: We required workers to complete a short tutorial 2 , followed by 20 annotation tasks, which we eval- uated against the gold data. We used the results to select the best-scoring 5 of 7 applicants. We then asked them to complete the remaining label- ing tasks. The study was conducted in a span of three weeks. Crowd workers were paid a fixed sum for the completion of the study, which re- sulted in a cost of 2 cents per worker per task. In total, workers estimated an average of 9 hours to complete the full task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Analysis</head><p>We gather crowd feedback and compare the major- ity answer for each task with the gold label. Refer to <ref type="table">Table 2</ref> for an overview of results. We make several observations:</p><p>The more workers agree, the better the an- swer. Generally, we note that majority answers tend to be more often correct if more workers agree. Specifically, as <ref type="table">Table 2</ref> shows, all 5 annota-</p><formula xml:id="formula_0">Type Frame A0 A1 A2 A3 A4 LOC TMP</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONFIRM-Expert-required 134 (38%) 280 (32%) 382 (32%) 73 (33%) 6 (43%) 8 (50%) 36 (35%) 120 (36%) PREDICTION Crowd-appropriate 222 (62%) 608 (68%) 797 (68%) 146 (67%) 8 (57%) 8 (50%) 67 (65%) 211 (64%)</head><p>ADD- Expert-required 0 82 (31%) 54 (33%) 240 (37%) 99 (34%) 72 (38%) 0 0 MISSING Crowd-appropriate 0 186 (69%) 111 (67%) 405 (63%) 190 (66%) 120 (62%) 0 0 tors agreed in 1,801 out of all 2,549 tasks (71%). Of these tasks, the majority answer was correct in 1,679 cases, and incorrect in 122, yielding a pre- cision of 93% for tasks in full agreement. If only 4 out of 5 agree (i.e. one annotator provided a dif- ferent answer), the precision drops to 86%. If only three annotators agree on an answer, the precision is even lower, at 67%. Furthermore, we note 34 cases in which there was no majority answer (no agreement by at least 3 workers). We therefore see a direct correlation between agreement scores and the validity of majority answers.</p><p>Even if all workers agree, errors are made.</p><p>We also note that all 5 crowd workers sometimes unanimously agree an incorrect annotation, in a total of 122 cases. To illustrate such a case, con- sider the example in <ref type="figure" target="#fig_2">Figure 3</ref>: In our study, all 5 workers incorrectly selected yes as answer. How- ever, (perhaps somewhat counterintuitively to non- experts) under the PropBank paradigm it is the "phone representative" that provide explicit help in this sentence, not "Vanguard."</p><p>Characteristics of difficult annotation tasks. As illustrated in  some tasks of this type require above-average ex- pert involvement, such as confirmation questions that pertain to the frame label or higher numbered roles (A3 and A4). The second row lists results for ADDMISSING tasks. Here, we note that again higher order roles tend to be above average expert- required <ref type="bibr">3</ref> . However, while the breakdown in <ref type="table" target="#tab_2">Ta- ble 3</ref> indicates some general trends for the diffi- culty of annotation tasks, the question type itself does not suffice to determine whether an individ- ual instance requires expert involvement or not. Summary. Our crowdsourcing study supports the initial hypothesis that a portion of SRL tasks is in fact appropriate for crowd workers, but also shows that identifying such tasks is not straightforward since neither crowd agreement scores nor the an- notation task type is sufficient indicators of diffi- cult tasks. We investigate this problem further in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TASKROUTER: Annotation Task Classification</head><p>Our study shows that some annotation tasks are appropriate for crowd workers, while others are not. In this section, we define a classification prob- lem in which we wish to classify each task into one of the two following classes:</p><p>Definition 2 Crowd-appropriate: A task for which: (1) All crowd workers agree on the answer.</p><p>The agreed-upon answer is correct.</p><p>Definition 3 Expert-required: A task that is not crowd-appropriate.</p><p>According to these definitions, our crowdsourcing study found that the task in <ref type="figure" target="#fig_1">Figure 2</ref> is crowd- appropriate, i.e. easy enough for the crowd to pro- vide correct and consistent answers, while the task in <ref type="figure" target="#fig_2">Figure 3</ref> is considered expert-required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Features</head><p>To solve the task classification problem, we note two groups of distinct features (see <ref type="table" target="#tab_5">Table 4</ref>): Task-level features X g capture the general dif- ficulty of a labeling task, as defined by a frame or role type. The intuition here is that cer- tain frames/roles are inherently difficult for non- experts, and that annotation tasks related to such frames/roles should be handled by experts. In the BUY.01 frame for instance, buyer (A0) is a simple crowd-appropriate semantic concept, while bene- factive (A4) generally produces lower agreement scores. Task-level features therefore include the frame and role labels themselves, as well as the complexity of each question, measured in features such as the question word (what, how, when etc.), its length measured in number of tokens, and all tokens, lemmas and POS-tags in the question. Sentence-level features X l capture complexity associated with the specific task instance. The in- tuition is that some sentences are more complex and more difficult to understand than others. In such sentences, even roles with generally crowd- appropriate definitions might be incorrectly an- swered by non-experts. We capture the complexity of a sentence with features such as its length (num- ber of tokens in sentence), the numbers of frames, roles, verbs, and nouns in the sentence, as well as all tokens, lemmas and POS-tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classification Model</head><p>In addition to task-and sentence-level features, we present a classifier that also models the interplay between multiple annotation tasks generated from the same sentence. The intuition here is that there is an interdependence between labeling decisions in the same sentence. For instance, the presence of a difficult role may alter the interpretation of a sentence and make other labeling decisions more  complicated. We thus propose a fuzzy classifica- tion model with two layers ( <ref type="bibr" target="#b16">Ishibuchi et al., 1995)</ref> of SVM classifiers ( <ref type="bibr" target="#b26">Wang et al., 2016)</ref>, which in- troduces the context of the task using fuzzy indica- tors to model the interplay between the two groups of features. Specifically, we train a local-layer SVM classi- fier L l using the sentence-level features X l (com- puted from sentences). We also train a global- layer SVM classifier L g using the task-level fea- tures X g (computed from tasks). We refer to the predictions of the local and global classifiers as fuzzy indicators and we incorporate them as addi- tional features to the fuzzy two-layer SVM clas- sifier L f as follows. Given task a i among all tasks a 1 to a n for a sentence s, the first layer of the fuzzy classifier, consists of applying the local-layer classifier using the sentence-level fea- tures of s. The second layer of the fuzzy classifier consists in applying the global-layer classifier n times, each time using task-level features for task a j , 1 ≤ j ≤ n, resulting in n + 1 values: one local-layer indicator, and n global-layer indica- tors. Our final fuzzy classifier model uses the n+1 local and global indicators as features, in addition to the sentence-and task-level features of a i .</p><p>Note that the classification of task a i considers features from other tasks a j from the same sen- tence, but more efficiently than placing all task- level features of all tasks into a single feature vec- tor. Formally, the objective function of the fuzzy two-layer SVM classification model L f is:</p><formula xml:id="formula_2">max α 1 T α − 1 2 α T YK(X f T X f )Yα (1) s.t. y T α = 0, 0 ≤ α ≤ C1.</formula><p>where</p><formula xml:id="formula_3">K(X f T X f ) is the fuzzy two- layer RBF kernel function, X f = [X gT , X l T , Y 1 gT , · · · , Y j gT , · · · , Y n gT , Y l T ]</formula><p>is the fuzzy two-layer feature matrix, n is the number of annotation tasks generated from a sentence, Y j g represents the j-th fuzzy indicator generated by the j-th global classifier L g j , Y l is the fuzzy indicator generated by the local classifier L l , Y is the label matrix, 1 is a vector of all ones and C is a positive trade-off parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation</head><p>To evaluate the accuracy of TASKROUTER we use the standard measure of accuracy for binary clas- sifiers. As  in which we train an SVM with (1) task-level fea- tures, (2) sentence-level features, (3) all features, and (4) our proposed fuzzy two-layer classifier.</p><p>Data. We use the dataset created in our crowd- sourcing study (see Section 2.2), which consists of 2,549 annotation tasks labeled as either expert- required or crowd-appropriate according to our definitions and the results of the study. We lever- age five-fold cross validation to train the classifiers over a training split (80%).</p><p>Results. The cross validation results are listed in <ref type="table" target="#tab_6">Table 5</ref>. Our proposed classifier outperforms all baselines and reaches a classification accuracy of 0.96. Interestingly, we also note that task- level features are significantly more important than sentence-level features, as the setup SVM task outperforms SVM sentence by 6 accuracy points. Furthermore, our proposed approach outperforms SVM task+sentence , indicating a positive impact of modeling the global interplay of annotation tasks. These experiments confirm our initial postula- tion that it is possible to train a high quality classi- fier to detect expert-required tasks. We refer to the best performing setup as TASKROUTER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CROWD-IN-THE-LOOP Study</head><p>Having created TASKROUTER, we now execute our proposed CROWD-IN-THE-LOOP workflow and comparatively evaluate it against a number of crowdsourcing and hybrid approaches. We wish to determine <ref type="formula">(1)</ref> to what degree does having the crowd in the loop reduce the workload of experts? (2) How does the quality of the produced anno- tated data compare to purely crowdsourced or ex- pert annotations?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Approaches</head><p>We evaluate the following approaches: 1. Baseline without curation The first is a simple baseline in which we use the output of SRL as-is, i.e. with no additional curation either by the crowd or experts. We list this method to show the quality of the starting point for the curation workload. 2. CROWD (Crowdsourcing) The second base- line is a standard crowdsourcing approach as de- scribed in Section 2, i.e. without experts. We send all annotation tasks (100%) to the crowd and gather crowd feedback to correct labels in three different settings. We correct all labels based on majority vote, i.e., if at least 3 (CROWD min3 ), 4 (CROWD min4 ) or all 5 (CROWD all5 ) out of 5 an- notators agree on an answer. 3. HYBRID (Crowdsourcing + Expert cura- tion) In this setting, we replicate the approach pro- posed by : After first executing crowdsourcing (i.e. sending 100% of the tasks to the crowd), we identify all tasks in which crowd workers provided conflicting answers. These tasks are sent to experts for additional curation (ex- pert answers are used for curation instead of the crowd response). We use three definitions of what constitutes a conflicting answer: (1) We consider all answers in which at least a majority (3 out of 5) agreed as crowd-appropriate and send the rest (2.2%) to experts. We refer to this setup as HYBRID min3 . <ref type="formula" target="#formula_1">(2)</ref> Only tasks where 4 out of 5 agreed are crowd-appropriate, the remaining 9.9% go to experts (HYBRID min4 ). (3) Any task in which there is no unanimous agreement (27.3%) is deemed expert-required (HYBRID all5 ). 4. CROWD-IN-THE-LOOP This setup is the pro- posed approach in which we use TASKROUTER trained over a holdout training set to split annota- tion tasks into crowd and expert groups. In our experiments, TASKROUTER determines the fol- lowing partitions: 66.4% of tasks to the crowd, the remaining 33.6% to experts. To give an indi- cation of the lower bound of the approach given these partitions, we list results for two settings: (1) CROWD-IN-THE-LOOP Random , a lower bound setting in which we randomly split into these par- titions. (2) CROWD-IN-THE-LOOP T askRouter , the proposed setting in which we use TASKROUTER to perform this split.</p><p>Refer to <ref type="table" target="#tab_9">Table 6</ref> for an overview of these exper- iments. The WORKLOAD columns indicate what percentage of tasks is sent to crowd and experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>Data We use the dataset created in the crowd- sourcing study in Section 2, consisting of 2,549 annotation tasks labeled either as expert-required  or crowd-appropriate 4 . As shown in Section 3.3, we use 80% of the dataset to train TASKROUTER under cross validation, and conduct the compara- tive evaluation using the remaining 20%.</p><p>Human annotators &amp; curation We simulate an expert annotator using the CoNLL-2009 gold SRL labels and reuse the crowd answers from the study for crowd annotators. For each setting, we gather crowd and expert answers to the annotation tasks, and interpret the answers to curate the SRL labels that were produced by the statistical SRL system. After curation, we evaluate the resulting labeled sentences against gold-labeled data to de- termine the annotation quality in terms of preci- sion, recall and F 1 -score. Evaluation Metrics Next to the quality of result- ing annotations, we are interested to evaluate how effectively we integrate the crowd. We measure this in two metrics. <ref type="formula">(1)</ref> One is the percentage of tasks that go to the crowd and to experts respec- tively. Note that in the HYBRID setup, some tasks go to both crowd workers and experts, so that the percentages can add up to over a hundred percent. This information is illustrated in column WORK- LOAD in <ref type="table" target="#tab_9">Table 6</ref>. <ref type="formula" target="#formula_1">(2)</ref> The second is the over- all validity of crowd feedback, referred to as cor- rectness, measured as the ratio of correct answers among all answers retrieved from the crowd. We provide two values for correctness in <ref type="table" target="#tab_9">Table 6</ref>, un- der column CORRECTNESS: The first is the cor- rectness only over crowd feedback. Note that this value is the same for all CROWD and HYBRID se- tups since in these approaches 100% of annotation tasks are passed to the crowd. The second named hybrid is the overall correctness of the resolved an- swers with both expert and crowd feedback. <ref type="bibr">4</ref> We will release the dataset soon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head><p>The results of our experiments are summarized in <ref type="table" target="#tab_9">Table 6</ref>. We make the following observations:</p><p>CROWD-IN-THE-LOOP significantly increases annotation quality. Our evaluation shows that CROWD-IN-THE-LOOP produces SRL annotation with significantly higher quality compared to crowdsourcing or hybrid scenarios. With a result- ing F 1 -score of 0.94, it outperforms the best per- forming crowdsourcing setup (0.90) by 4 points. More importantly, our proposed approach also outperforms other hybrid approaches that partially leverage experts. It outperforms the best hy- brid approach (0.91) by 3 points, indicating that TASKROUTER is better to select expert-required tasks than a method with only crowd agreement.</p><p>Significantly less expert involvement required.</p><p>In our experiments, more than two-thirds of all tasks were determined to be crowd-appropriate by TASKROUTER. This considerably reduces the need for expert involvement compared to expert labeling, while still maintaining relatively high an- notation quality. In particular, our approach com- pares favorably to other hybrid setups in which a similar partition of tasks is completed by experts.</p><p>Since TASKROUTER is more capable to choose expert-required tasks than previous approaches, we achieve higher overall quality at similar levels of expert involvement.</p><p>Crowd workers more effective. As the correct- ness column in <ref type="table" target="#tab_9">Table 6</ref> shows, the selection of tasks by TASKROUTER is more appropriate for the crowd in general. Their average correctness increases to 0.92, compared to 0.84 if the crowd completes 100% of the tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion and Outlook</head><p>The proposed approach far outperforms crowd- sourcing and hybrid approaches in terms of an- notation quality. In particular, even at similar levels of expert involvement, it outperforms the HYBRID all5 approach. However, we also note that with an F 1 -score of 0.94, our approach does not yet reach the quality of gold annotated data.</p><p>Insights for further improving quality. To fur- ther improve the quality of generated SRL training data, future work may (1) investigate additional features ( <ref type="bibr" target="#b25">Wang et al., 2015</ref>) and classification models to improve the TASKROUTER to better dis- tinguish between crowd-appropriate and expert- required tasks, and (2) experiment with other SRL crowdsourcing designs to make more tasks crowd- appropriate. Nevertheless, we suspect that a small decrease in quality cannot be fully avoided if large amounts of non-experts are involved in a labeling task such as SRL. Given such involvement of non- experts, we believe that our proposed approach is a compelling way for increasing crowdsourcing quality while keeping expert costs relatively low.</p><p>Flexible trade-off of costs vs quality. Another avenue for research is to experiment with classi- fier parameters that allow us to more flexibly con- trol the trade-off between how many experts we wish to involve and what annotation quality we desire (e.g., active learning ( <ref type="bibr" target="#b24">Wang et al., 2017)</ref>). This may be helpful to scenarios in which costs are fixed, or where one aims to compute the costs for generating annotated data of specific quality.</p><p>Use for SRL domain adaptation. One intended avenue for study is to apply our approach to gen- erate training data for a specific textual domain for which little or no SRL training data currently ex- ists. We believe that due to its relatively lower costs, our approach may be an ideal candidate for practical domain adaptation of SRL.</p><p>Applicability to other NLP crowdsourcing tasks. Finally, while in this paper we focused on the task of generating labeled training data for SRL, we believe that our proposed approach may be applicable to other NLP tasks that have only reported moderate results to-date. To study this applicability, one would first need to conduct a similar study as in Section 2 to identify crowd- appropriate and expert-required tasks and attempt the training of a classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Crowdsourcing SRL Annotation Different ap- proaches have been adapted to formulate SRL tasks for non-expert crowd workers ( <ref type="bibr" target="#b14">Hong and Baker, 2011</ref>). Typical tasks include selecting an- swers from a set of candidates ( <ref type="bibr" target="#b7">Fossati et al., 2013)</ref>, marking text passages that contain spe- cific semantic roles <ref type="bibr" target="#b6">(Feizabadi and Padó, 2014)</ref>, and constructing question-answer pairs <ref type="bibr" target="#b12">(He et al., 2015</ref><ref type="bibr" target="#b13">(He et al., , 2016</ref>. However, a particular challenge is that SRL annotation tasks are often complex and crowdsourcing inevitably leads to low-quality an- notations ( <ref type="bibr" target="#b21">Pavlick et al., 2015)</ref>. Instead of attempting to design a better anno- tation task, our proposed approach addresses this problem by accepting that a certain portion of an- notation tasks is too difficult for the crowd. We create a classifier to identify such tasks and involve experts whenever necessary. Routing Tasks Recent approaches have been de- veloped to address the task routing problem in crowdsourcing ( <ref type="bibr" target="#b4">Bragg et al., 2014;</ref><ref type="bibr" target="#b3">Bozzon et al., 2013;</ref><ref type="bibr" target="#b11">Hassan and Curry, 2013)</ref>. As workers vary in skill and tasks vary in difficulty, prior rec- ommended approaches often consider the match between the task content and workers' profiles. However, these approaches are difficult to apply to the particular context of SRL annotation since we only distinguish between either experts famil- iar with PropBank, or non-expert crowd workers.</p><p>Rather than routing tasks to the most appro- priate workers, our proposed approach determines which SRL tasks are appropriate for crowdsourc- ing, and sends the remaining ones to experts. Human-in-the-loop Methods Our method is sim- ilar in the spirit of human-in-the-loop learn- ing ( <ref type="bibr" target="#b9">Fung et al., 1992;</ref>. Human- in-the-loop learning generally aims to leverage hu- mans to complete easy commonsense tasks, such as the recognition of objects in images ( <ref type="bibr" target="#b20">Patterson et al., 2013)</ref>. Recent work also proposed human- in-the-loop parsing <ref type="bibr" target="#b13">(He et al., 2016</ref>) to include hu- man feedback into parsing. However, unlike these approaches, we aim to combine both experts and non-experts to address the difficulty of some SRL annotation tasks, while leveraging the crowd for the majority of tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed CROWD-IN-THE-LOOP an approach for creating high-quality annotated data for SRL that leverages both crowd and ex- pert workers. We conducted a crowdsourcing study and analyzed its results to design a classi- fier to distinguish between crowd-appropriate and expert-required tasks. Our experimental evalua- tion showed that our proposed approach signif- icantly outperforms baseline crowdsourcing and hybrid approaches, and successfully limits the need for expert involvement while achieving high annotation quality.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of proposed CROWD-IN-THE-LOOP approach for curating SRL annotations.</figDesc><graphic url="image-1.png" coords="2,72.00,55.61,460.00,103.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example annotation task, consisting of a sentence with predicted role labels, a human readable question regarding to one label, and a set of answer options. By answering, crowd workers curate a prediction made by the SRL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of an annotation task where crowd workers unanimously provided an incorrect answer in our study (see 2.3). This task is classified as expert-required.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Breakdown of annotation tasks by question types and semantic labels, and proportion of expert-required tasks 

(formally defined in Section 3). Percentages in each cell add up to 100%. On average, 34% of tasks are expert required. Task 
types that lie above this average are highlighted bold. For instance, 38% of all frame confirmation questions are expert-required, 
indicating that this question type is of above-average difficulty. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 ,</head><label>3</label><figDesc></figDesc><table>we break down annotation 
tasks by question types and semantic labels to gain 
a better understanding of which tasks are difficult 
for the crowd. The first row in the table lists results 
for CONFIRMPREDICTION tasks. We note that 

Sentence 
And Vanguard, among other groups, said it was adding more 
phone representatives today to help investors get through. 

Question 
Who is helping in this sentence? Is it: "Vanguard"? 

help.01 

Answer Options 
Yes 
No, who is helping is not mentioned 
No, who is helping is mentioned here: copy and paste text 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Features for annotation task classification.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 shows, we evaluate four setups</head><label>5</label><figDesc></figDesc><table>Approach 

Accuracy 

SVM task : Task features only 
0.91 
SVMsentence: Sentence features only 0.87 
SVM task+sentence : All features 
0.94 
TASKROUTER: Fuzzy two-layer 
0.96  *  

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 : Performance of classifiers trained with five-</head><label>5</label><figDesc></figDesc><table>fold cross validation on training set. The improvements of 

TASKROUTER over other classifiers are significant at the  *  

0.05 level, paired t-test. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>1918</head><label>1918</label><figDesc></figDesc><table>Approach 

ANNOTATION QUALITY WORKLOAD 
CORRECTNESS 
P 
R 
F1 
crowd 
expert 
crowd-only hybrid 

Baseline without curation 
0.86 0.83 0.85 
0% 
0% 
-
-

CROWDmin3 
0.92 0.88 0.90 
100.0% 0% 
0.84 
0.84 
CROWDmin4 
0.89 0.85 0.87 
100.0% 0% 
0.84 
0.84 
CROWD all5 
0.87 0.84 0.85 
100.0% 0% 
0.84 
0.84 

HYBRIDmin3 
0.90 0.86 0.88 
100.0% 2.2% 
0.84 
0.84 
HYBRIDmin4 
0.91 0.87 0.89 
100.0% 9.9% 
0.84 
0.86 
HYBRID all5 
0.93 0.89 0.91 
100.0% 27.3% 
0.84 
0.88 

CROWD-IN-THE-LOOP Random 
0.92 0.88 0.90 
66.4% 
33.6% 
0.83 
0.89 
CROWD-IN-THE-LOOP T askRouter 
0.96  *  0.92  *  0.94  *  
66.4% 
33.6% 
0.92  *  
0.95  *  

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Comparative evaluation of different approaches for generating gold-standard SRL annotation. The improvements of 

CROWD-IN-THE-LOOP T askRouter over other approaches are significant at the  *  0.05 level, paired t-test. 

</table></figure>

			<note place="foot" n="1"> https://www.upwork.com/ 2 The tutorial is available upon request.</note>

			<note place="foot" n="3"> Note that there are no ADDMISSING questions for frames since our SRL predicts a label for each verb in a sentence. Also there are no missing optional arguments since we ask missing argument questions only for core roles.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">K-srl: Instancebased learning for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="599" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards semi-automatic generation of proposition banks for low-resource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Vishwajeet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="993" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The berkeley framenet project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Collin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John B</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="86" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Choosing the right crowd: expert finding in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bozzon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Brambilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ceri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Silvestri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuliano</forename><surname>Vesci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Extending Database Technology</title>
		<meeting>the 16th International Conference on Extending Database Technology</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="637" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Parallel task routing for crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Bragg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Kolobov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second AAAI Conference on Human Computation and Crowdsourcing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Identifying relations for open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Crowdsourcing annotation of non-local semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadat</forename><surname>Parvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Feizabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Padó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="226" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Outsourcing framenet to the crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Fossati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Giuliano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Tonelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="742" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Crowddb: answering queries with crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Michael J Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kossmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukriti</forename><surname>Kraska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reynold</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 ACM SIGMOD International Conference on Management of data</title>
		<meeting>the 2011 ACM SIGMOD International Conference on Management of data</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="61" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Human-in-the-loop machine control loop. US Patent 5</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Patrick T Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">A</forename><surname>Norgate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Dilts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangaswamy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ravindran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page">180</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Antònia</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaň</forename><surname>Stěpánek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A capability requirements approach for predicting worker performance in crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umairul</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Curry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Collaborative Computing: Networking, Applications and Worksharing (Collaboratecom), 2013 9th International Conference Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="429" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Question-answer driven semantic role labeling: Using natural language to annotate natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="643" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human-in-the-loop parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">How good is the crowd at real wsd?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisup</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collin</forename><forename type="middle">F</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th linguistic annotation workshop</title>
		<meeting>the 5th linguistic annotation workshop</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="30" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ontonotes: the 90% solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology Conference of the NAACL</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Selecting fuzzy if-then rules for classification problems using genetic algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisao</forename><surname>Ishibuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Nozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naohisa</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideo</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on fuzzy systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="260" to="270" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09823</idno>
		<title level="m">Dialogue learning with human-in-the-loop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving machine translation into chinese by tuning against chinese meant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Kiu</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meriem</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWSLT 2013, 10th International Workshop on Spoken Language Translation</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The proposition bank: An annotated corpus of semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="106" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bootstrapping fine-grained classifiers: Active learning with a crowd in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horn</forename><surname>Serge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belongie</forename><surname>Pietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Perona James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (Workshop)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Framenet+: Fast paraphrastic tripling of framenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Travis</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="408" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Using semantic roles to improve question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods on Natural Language Processing-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cheap and fast-but is it good?: evaluating non-expert annotations for natural language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Active learning for black-box semantic role labeling with neural factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Chiticariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>page to appear</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Incorporating world knowledge to document clustering via heterogeneous information networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>El-Kishky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1215" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Text classification with heterogeneous information network kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2130" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
