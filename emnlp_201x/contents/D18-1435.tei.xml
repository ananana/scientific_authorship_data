<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Entity-aware Image Caption Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spencer</forename><surname>Whitehead</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
							<email>sfchang@cs.columbia.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Entity-aware Image Caption Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4013" to="4023"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4013</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Current image captioning approaches generate descriptions which lack specific information , such as named entities that are involved in the images. In this paper we propose a new task which aims to generate informative image captions, given images and hashtags as input. We propose a simple but effective approach to tackle this problem. We first train a convolutional neural networks-long short term memory networks (CNN-LSTM) model to generate a template caption based on the input image. Then we use a knowledge graph based collective inference algorithm to fill in the template with specific named entities retrieved via the hashtags. Experiments on a new benchmark dataset collected from Flickr show that our model generates news-style image descriptions with much richer information. Our model outperforms unimodal baselines significantly with various evaluation metrics. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As information regarding emergent situations dis- seminates through social media, the information is presented in a variety of data modalities (e.g. text, images, and videos), with each modality providing a slightly different perspective. Images have the capability to vividly represent events and entities, but without proper contextual information they become less meaningful and lose utility. While images may be accompanied by associated tags or other meta-data, which are inadequate to con- vey detailed events, many lack the descriptive text to provide such context. For example, there are 17,285 images on Flickr from the Women's March on January 21, 2017, 2 most of which contain only a few tags and lack any detailed text descriptions. The absence of context leaves individuals with no knowledge of details such as the purpose or loca- tion of the march.</p><p>Image captioning offers a viable method to di- rectly provide images with the necessary contex- tual information through textual descriptions. Ad- vances in image captioning ( <ref type="bibr" target="#b31">Xu et al., 2015;</ref><ref type="bibr" target="#b6">Fang et al., 2015;</ref><ref type="bibr" target="#b12">Karpathy and Fei-Fei, 2015;</ref><ref type="bibr" target="#b28">Vinyals et al., 2015)</ref> are effective in generating sentence- level descriptions. However, sentences generated by these approaches are usually generic descrip- tions of the visual content and ignore background information. Such generic descriptions do not suf- fice in emergent situations as they, essentially, mir- ror the information present in the images and do not provide detailed descriptions regarding events and entities present in, or related to, the images, which is imperative to understanding emergent sit- uations. For example, given the image in <ref type="figure" target="#fig_0">Fig- ure 1</ref>, the state-of-the-art automatically generated caption is 'Several women hold signs in front of a building.', which is lacking information regarding relevant entities (e.g. ' Junior doctors', 'Tories').</p><p>In our work, we propose an ambitious task: entity-aware image caption generation: auto- matically generate an image description that in- corporates specific information such as named en- tities, relevant to the image, given limited text in- formation, such as associated tags and meta-data (e.g. time of photo and geo-location). Our ap- proach to this task generally follows three steps. First, instead of directly generating a sentence for an image, we generate a template sentence with fillable slots by training an image captioning ar- chitecture on image-caption pairs, where we re- place the entities from the captions with slot types indicating the type of entity that should be used  Second, given the associated tags of an im- age, we apply entity discovery and linking (EDL) methods to extract specific entities from pre- vious posts that embed the same tags. Fi- nally, we select appropriate candidates for each slot based upon the entity type and frequency. For example, we select the person name 'Ju- nior doctors' to fill in the slot &lt;Person&gt; be- cause it co-occurs frequently with other entities such as 'Tories' in the text related to the tags #N HS, #JuniorDoctorsStrike. This frame- work offers a distinct advantage in that it is very flexible, so more advanced captioning or EDL methods as well as other data can be used almost interchangeably within the framework.</p><p>To the best of our knowledge, we are the first to incorporate contextual information into image captioning without large-scale training data or top- ically related news articles and the generated im- age captions are closer to news captions. <ref type="figure" target="#fig_0">Figure 1</ref> shows the overall framework of our pro- posed model. Given an image with associated tags and other meta-data, such as geographical tags and EXIF data, <ref type="bibr">3</ref> we first feed the image into a template caption generator to generate a sen- tence composed of context words, such as "stand", and slots, such as &lt;person&gt;, to represent miss- ing specific information like named entities (Sec- tion 3). The template caption generator, which fol- lows the encoder-decoder model ( <ref type="bibr" target="#b2">Cho et al., 2014</ref>) with a CNN encoder and LSTM decoder <ref type="bibr" target="#b28">(Vinyals et al., 2015)</ref>, is trained using news image-template caption pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach Overview</head><p>We then retrieve topically-related images from the Flickr database, which have the same tags as the input image. Next, we apply EDL algorithms to the image titles to extract entities and link them to external knowledge bases to retrieve their fine- grained entity types. Finally, for each slot gener- ated by the template generator, we choose to fill the slot with the appropriate candidate based on entity type and frequency (Section 4).</p><p>Language models (LM) are widely used to gener- ate text <ref type="bibr" target="#b30">(Wen et al., 2015;</ref><ref type="bibr" target="#b24">Tran and Nguyen, 2017)</ref> and play a crucial role in most of the existing im- age captioning approaches ( <ref type="bibr" target="#b28">Vinyals et al., 2015;</ref><ref type="bibr" target="#b31">Xu et al., 2015</ref>). These models, learned from large-scale corpora, are able to predict a probabil- ity distribution over a vocabulary. However, LM struggle to generate specific entities, which occur sparsely, if at all, within training corpora. More- over, the desired entity-aware captions may con- tain information not directly present in the image alone. Unless the LM is trained or conditioned on data specific to the emergent situation of inter- est, the LM alone cannot generate a caption that incorporates the specific background information. We address this issue by only relying on the LM to generate abstract slot tokens and connecting words or phrases, while slot filling is used to incorpo- rate specific information. This approach allows the LM to focus on generating words or phrases with higher probability, since each slot token ef- fectively has a probability equal to the sum of all the specific entities that it represents, thereby cir- cumventing the issue of generating lower proba- bility or out-of-vocabulary (OOV) words.</p><p>In this section, we describe a novel method to train a model to automatically generate tem- plate captions with slots as 'placeholders' for spe- cific background information. We first present the schemas which define the slot types (Section 3.1) and the procedure to acquire training data for tem- plate generation (Section 3.2). Finally, we intro- duce the model for template caption generation (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Template Caption Definition</head><p>Named entities are the most specific informa- tion which cannot be easily learned by LM. Thus, in this work, we define slots as place- holders for entities with the same types. We use the fine grained entity types defined in DB- pedia <ref type="bibr">4 (Auer et al., 2007)</ref> to name the slots because these types are specific enough to dif- ferentiate between a wide range of entities and still general enough so that the slots have higher probabilities in the language model. For exam- ple, Person is further divided into Athlete, Singer, Politician, and so on. Therefore, a template caption like 'Athlete celebrates after scoring.' can be generated by the language model through leveraging image features, where the slot Athlete means a sports player (e.g., Cristiano Ronaldo, Lionel Messi).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Acquisition of Image-Template Caption Pairs</head><p>High quality training data is crucial to train a template caption generator. However, the image-caption datasets used in previous work, such as Microsoft Common Objects in Con- text (MS COCO) ( <ref type="bibr" target="#b17">Lin et al., 2014</ref>) and Flickr30K (Rashtchian et al., 2010), are not suit- able for this task because they are designed for non-specific caption generation and do not contain detailed, specific information such as named enti- ties. Further, manual creation of captions is expen- sive. In this work, we utilize news image-caption pairs, which are well written and can be easily col- lected. We use the example in <ref type="figure" target="#fig_1">Figure 2</ref> to describe our procedure to convert image-caption to image- template caption pairs: preprocessing, compres- sion, generalization. Preprocessing: We first apply the following pre-processing steps: (1) remove words in paren- theses, such as '(C)' and '(R)' in <ref type="figure" target="#fig_1">Figure 2</ref>, because they usually represent auxiliary information and are not aligned with visual concepts in images; (2) if a caption includes more than one sentence, we choose the longer one. Based on our observation, shorter sentences usually play the role of back- ground introduction, which are not aligned with the key content in images; (3) remove captions with less than 10 tokens because they tend to be not informative enough. The average length of the news image captions is 37 tokens.</p><p>Compression:</p><p>The goal of compression is to make news captions short and aligned with im- ages as much as possible by keeping informa-tion related to objects and concepts in the im- ages, which are usually subjects, verbs and ob- jects in sentences. In this paper, we propose a simple but efficient compression method based on dependency parsing. We do not use other com- plicated compressions ( <ref type="bibr" target="#b15">Kuznetsova et al., 2014</ref>) because our simple method achieves comparative results on image caption dataset. We first ap- ply the Stanford dependency parser (De Marn- effe and Manning, 2008) on preprocessed cap- tions.</p><p>Then, we traverse the parse tree from the root (e.g.'pours') via &lt;governor, grammatical relations, dependent&gt; triples using breadth-first search. We decide to keep a dependent or not based on its grammati- cal relation with the governor. Based on our ob- servations, among the 50 grammatical relations in the Stanford dependency parser, we keep the de- pendents that have the following grammatical re- lations with their governors: nsubj, obj, iobj, dobj, acomp, det, neg, nsubjpass, pobj, predet, prep, prt, vmod, nmod, cc.</p><p>Generalization: The last step for preparing training data is to extract entities from captions and replace them with the slot types we defined in Section 3.1. We apply Stanford CoreNLP name tagger ( <ref type="bibr" target="#b18">Manning et al., 2014</ref>) to the cap- tions to extract entity mentions of the following types: Person, Location, Organization, and Miscellaneous. Next, we use an English Entity Linking algorithm ( <ref type="bibr" target="#b20">Pan et al., 2015</ref>) to link the entity mentions to DBpedia and retrieve their fine-grained types. <ref type="bibr">5</ref> We choose the higher level type if there are multiple fine-grained types for a name. For example, the entity types of Manchester United, Eric Bailly, and Jesse Lingard are Soc- cerTeam, Athlete, and Athlete, respectively. For entity mentions that cannot be linked to DBpedia, we use their coarse-grained entity types, which are the outputs of name tagger.</p><p>Finally, we replace the entities in the com- pressed captions with their corresponding slots:</p><p>Generalized Template: &lt;Athlete&gt; pours champagne over &lt;Athlete&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generation Model</head><p>Using the template caption and image pairs (S, I) as training data, we regard the template cap- tion generation as a regular image captioning <ref type="bibr">5</ref> This yields a total of 95 types after manually cleaning.</p><p>task. Thus, we adapt the encoder-decoder archi- tecture which is successful in the image captioning task ( <ref type="bibr" target="#b28">Vinyals et al., 2015;</ref><ref type="bibr" target="#b31">Xu et al., 2015)</ref>. Our model ( <ref type="figure" target="#fig_2">Figure 3</ref>) is most similar to the one pro- posed in ( <ref type="bibr" target="#b28">Vinyals et al., 2015)</ref>. Note, other cap- tioning methods may easily be used instead.</p><p>Encoder: Similar to previous work <ref type="bibr" target="#b28">(Vinyals et al., 2015;</ref><ref type="bibr" target="#b31">Xu et al., 2015;</ref><ref type="bibr">Karpathy and FeiFei, 2015;</ref><ref type="bibr" target="#b27">Venugopalan et al., 2017)</ref>, we encode images into representations using a ResNet ( ) model pre-trained on the ImageNet dataset ( <ref type="bibr" target="#b4">Deng et al., 2009</ref>) and use the outputs be- fore the last fully-connected layer.</p><p>Decoder: We employ a Long Short Term Mem- ory (LSTM) <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997)</ref> based language model to decode image represen- tations into template captions. We provide the LSTM the image representation, I, as the initial hidden state. At the t th step, the model predicts the probabilities of words/slots, y t , based on the word/slot generated at last time step, y t−1 , as well as the hidden state, s t . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Template Caption Entity Population</head><p>With the generated template captions, our next step is to fill in the slots with the appropriate spe- cific entities to make the caption complete and entity-aware. In this section, we expand our method to extract candidate entities from contex- tual information (i.e., images in Flickr with the same tags). Once we extract candidate entities, we apply the Quantified Collective Validation (QCV) algorithm ( <ref type="bibr" target="#b29">Wang et al., 2015)</ref>, which constructs a number of candidate graphs and performs collec- tive validation on those candidate graphs to choose the appropriate entities for the slots in the template caption. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Candidate Entity Retrieval</head><p>Users on social media typically post images with tags that are event-related (e.g. #occupywall- street), entity-related (e.g. #lakers), or topic- related (e.g. #basketball). On our Flickr test- ing dataset, the average number of tags associated with an image is 11.5 and posts with the same tags likely share common entities. Therefore, given an image and its tags, we retrieve images from Flickr with the same tags by a window size of seven- day based on taken date of the photo, and then utilize the textual information accompanying the retrieved images as context. We filter out the high frequency hashtags(&gt; 200 in testing dataset). Be- cause some common tags, such as '#concert', ap- pear in lots of posts related to different concerts. Given the related text, we apply EDL algo- rithms ( <ref type="bibr" target="#b20">Pan et al., 2015</ref>) to extract named entities and link them to DBpedia to obtain their entity types. For each entity type, we rank the candi- dates based on their frequency in the context and only keep the top 5 candidate entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantified Collective Validation</head><p>Each slot in the template must be filled with an en- tity from its corresponding candidate entities. We can regard this step as an entity linking problem, whose goal is to choose an entity from several can- didates given an entity mention. We utilize the QCV algorithm to construct a number of candidate graphs for a given set of slots ( <ref type="bibr" target="#b29">Wang et al., 2015)</ref>, where each combination of candidate entities sub- stituted into the slots yields a different graph <ref type="figure" target="#fig_3">(Fig- ure 4)</ref>. For each candidate combination graph, G i c , we compute the edge weights between each pair of candidates in the graph as</p><formula xml:id="formula_0">H r = f c h ct max(f c h , f ct )<label>(1)</label></formula><p>where r ∈ E(G i c ) is an edge in G i c , c h and c t are the head candidate and tail candidate of the edge, f c h ct is the co-occurrence frequency of the pair of candidates, and f c h and f ct are the individual frequencies of head candidate and tail candidate, respectively. For example, in <ref type="figure" target="#fig_3">Figure 4</ref>, Colney (Location) and Junior doctors (Person) co- occur frequently, therefore the edge between them has a larger weight.</p><p>We compute the summed edge weight, ω(G i c ), for each G i c by</p><formula xml:id="formula_1">ω(G i c ) = r∈E(G i c ) H r<label>(2)</label></formula><p>and select the combination of candidates with the largest ω(G i c ) to fill in the set of slots. As a result of this process, given the tem- plate: '&lt;Person&gt; holding signs protest against &lt;Organization&gt; outside &lt;Building&gt; in &lt;Location&gt;.', we obtain an entity-aware cap- tion: 'Junior doctors holding signs protest against Tories outside Norfolk and Norwich University Hospital in Colney'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Post-processing</head><p>Some images in Flickr have EXIF data which gives the date that an image is taken. We convert this information into the format such as 'April 26 2016' and add it to the generated captions as post- processing, by which we obtain the complete cap- tion: 'Junior doctors holding signs protest against Tories outside Norfolk and Norwich University Hospital in Colney on April 26 2016.'. We leave the generated caption without adding date infor- mation if it is not available. For those slots that cannot be filled by names, we use general words to replace them, such as using the word 'Athlete' to replace the slot Athlete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data</head><p>We require images with well-aligned, news-style captions for training. However, we want to test our model on real social media data and it is difficult to collect these informative captions for social me- dia data. Therefore, we acquire training data from news outlets and testing data from social media. We select two different topics, social events and sports events, as our case studies. Template Generator Training and Testing. To train the template caption generator, we col- lect 43,586 image-caption pairs from Reuters 6 , us- ing topically-related keywords <ref type="bibr">7</ref> as queries. We do not use existing image caption datasets, such as MSCOCO ( <ref type="bibr" target="#b17">Lin et al., 2014</ref>), because they do not contain many named entities. After the compres- sion and generalization procedures (Section 3.2) we keep 37,384 images and split them into train, development, and test sets. <ref type="table">Table 1</ref> shows the statistics of the datasets.</p><p>Entity-aware Caption Testing. Since news images do not have associated tags, for the pur- pose of testing our model in a real social media setting, we use images from Flickr for our cap- tion evaluation <ref type="bibr">8</ref> , which is an image-centric, repre- sentative social media platform. We use the same keywords as for template generator training to re- trieve multi-modal data with Creative Commons license <ref type="bibr">9</ref> , for social and sports events. We choose the images that already have news-style descrip- tions from users and manually confirm they are well-aligned. In total, we collect 2,594 images for evaluation. For each image, we also obtain the tags (30,148 totally) and meta-data, such as EXIF and geotag data, when they are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Models for Comparison</head><p>We compare our entity-aware model with the fol- lowing baselines: CNN-RNN-raw. We use the model proposed by <ref type="bibr" target="#b28">Vinyals et al. (2015)</ref> to train an image caption- ing model on the raw news image-caption pairs, and apply to Flickr testing data directly. CNN-RNN-compressed. We use the model pro- posed by <ref type="bibr" target="#b28">Vinyals et al. (2015)</ref> to train a model on the compressed news image-caption pairs. Text-summarization. We apply SumBasic sum- marization algorithms <ref type="bibr" target="#b25">(Vanderwende et al., 2007)</ref>, that is a summarization for multiple documents based on frequency of word and semantic content units, to text documents retrieved by hashtag. Entity-aware. We apply trained template genera- tor on Flickr testing data, and then fill in the slots with extracted background information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation Metrics</head><p>We use three standard image captioning evalua- tion metrics, BLEU ( <ref type="bibr" target="#b21">Papineni et al., 2002</ref>) and METEOR <ref type="bibr" target="#b5">(Denkowski and Lavie, 2014</ref>), RE- OUGE <ref type="bibr" target="#b16">(Lin, 2004</ref>) and CIDEr ( <ref type="bibr" target="#b26">Vedantam et al., 2015)</ref>, to evaluate the quality of both the gener- ated templates and generated captions. BLEU is a metric based on correlations at the sentence level. METEOR is a metric with recall weighted higher than precision and it takes into account stemming as well as synonym matching. ROUGE is pro- posed for evaluation of summarization and relies highly on recall. CIDEr metric downweights the n-grams common in all image captions, which are similar to tf-idf. Since the goal of this task is to generate entity-aware descriptions, we also mea- sure the entity F1 scores for the final captions, where we do fuzzy matching to manually count the overlapped entities between the system out- put and reference captions. Besides, we do human evaluation with a score in range of 0 to 3 using the criteria as follows: Score 0: generated caption is not related to the ground-truth; Score 1: generated caption is topically-related to the ground-truth, but has obvious errors; Score 2: generated caption is topically-related to the ground-truth, and has over- lapped named entities; Score 3: generated caption well describes the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Template Evaluation</head><p>Raw: new york giants kicker josh brown kicks a field goal during the first quarter against the san francisco 49ers at metlife stadium.</p><p>Generated Coarse Template: &lt;Person&gt; runs with ball against &lt;Location&gt; &lt;Location&gt; in the first half of their &lt;Miscellaneous&gt; football game in &lt;Location&gt; Generated Fine Template: &lt;Footballteam&gt; kicker &lt;Player&gt; kicks a field goal out of the game against the &lt;Footballteam&gt; at &lt;Organization&gt; stadium <ref type="figure">Figure 5</ref>: Example of generated template. <ref type="table" target="#tab_3">Table 2</ref> shows the performances of tem- plate generator based on coarse-grained and fine- grained type respectively, and <ref type="figure">Figure 5</ref> shows an example of the template generated. Coarse tem- plates are the ones after we replace names with these coarse-grained types. Entity Linking classi- fies names into more fine-grained types, so the cor- responding templates are fine templates. The gen- eralization method of replacing the named entities with entity types can reduce the vocabulary size significantly, which reduces the impact of sparse named entities in training data. The template generation achieves close performance with state- of-the-art generic image captioning on MSCOCO dataset ( <ref type="bibr" target="#b31">Xu et al., 2015)</ref>. The template generator based on coarse-grained entity type outperforms the one based on fine-grained entity type for two reasons: (1) fine template relies on EDL, and in- correct linkings import noise; (2) named entities usually has multiple types, but we only choose one during generalization. Thus the caption, 'Bob Dy- lan performs at the Wiltern Theatre in Los Ange- les', is generalized into '&lt;Writer&gt; performs at the &lt;Theater&gt; in &lt;Loaction&gt;', but the cor- rect type for Bob Dylan in this context should be Artist. <ref type="table" target="#tab_5">Table 4</ref> shows the comparison between our model and the baselines. The scores are much lower than traditional caption generation tasks such as COCO, because we use the real captions as ground-truth. Our model outperforms all the baselines on all metrics except BLEU-4, where Text-summarization model achieves better score. Generally, the model based on textual features (Text-summarization) has better performance than vision-based models (CNN-RNN-raw and CNN- RNN-compressed). It indicates textual sum- marization algorithm is more effective when it involves specific knowledge generation. Text- summarization model generates results from doc- uments retrieved by hashtags, so it tends to in- clude some long phrases common in those docu- ments. However the templates generated by our model is based on the language model trained from the news captions, which has different style with Flickr captions. It results in that Text- summarization model achieves better BLEU-4 score. Our model improves CIDEr score more sig- nificantly compared with other metrics, because CIDEr downweights the n-grams common in all captions, where more specific information such as named entities contribute more to the scores. The experimental results demonstrate that our model is effective to generate image captions with specific knowledge. <ref type="figure" target="#fig_4">Figure 6</ref> shows some examples of the captions generated by the entity-aware model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Flickr Caption Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Analysis</head><p>Good Examples: (A) in <ref type="figure" target="#fig_4">Figure 6</ref> describes the events in the images well ('performs') and include correct, well-placed entities, such as 'Little Dragon' and 'House of Blues'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach Vocabulary BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE CIDEr</head><p>Raw-caption 10,979 15.1 11.  Entity-aware(ours) baltimore orioles starting pitcher joe kelly pitches in the first inning against the balti- more orioles at baltimore June 1 2016</p><p>Human joe kelly of the boston red sox pitches in a game against the baltimore orioles at oriole park at camden yards on june 1, 2016 in baltimore, maryland C CNN-RNN-compressed protestors gesture and hold signs during a protest against what demonstrators call police brutality in mckinney , texas . Text-summarization</p><p>Toronto, Canada September 9, 2017.</p><p>Entity-aware(ours) supporters of an ban protest outside the toronto international film festival in toronto September 9 2017 Human patrick patterson at the premiere of the carter effect, 2017 toronto film festival   Relation Error of Filled Entities: Some of our errors result from ignoring of relations be- tween entities. In Example (B) of <ref type="figure" target="#fig_4">Figure 6</ref> our model generate a good template, but con- nects 'Joe Kelly', who is actually a pitcher of 'Res Sox', with 'Baltimore Orioles' incor- rectly. One possible solution is to incorporate relation information when the model fills in the slots with entities.</p><p>Template Error: Another category of errors re- sults from wrong templates generated by our model. Examples (C) in <ref type="figure" target="#fig_4">Figure 6</ref> is about a film festival, but the model generates a tem- plate about protest, which is not related to the image. One potential improvement is to incorporate the information from associ- ated tags, such as the number of tags and the named entity types related to the tags, as features during template caption genera- tion to make generated templates dynami- cally change according to the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>The goal of image captioning is to automatically generate a natural language sentence given an im- age. ( <ref type="bibr" target="#b13">Kulkarni et al., 2013;</ref><ref type="bibr" target="#b32">Yang et al., 2011;</ref><ref type="bibr" target="#b19">Mitchell et al., 2012</ref>) perform object recognition in images and fill hand-made templates with the rec- ognized objects. ( <ref type="bibr" target="#b14">Kuznetsova et al., 2012</ref><ref type="bibr" target="#b15">Kuznetsova et al., , 2014</ref>) retrieve similar images, parse associated captions into phrases, and compose them into new sen- tences. Due to the use of static, handmade tem- plates, these approaches are unable to generate a variety of sentence realizations, which can result in poorly generated sentences and requires one to manually create more templates to extend the gen- eration. Our approach overcomes this by dynami- cally generating the output. More recent work utilizes neural networks and applies an encoder-decoder model ( <ref type="bibr" target="#b2">Cho et al., 2014</ref>). <ref type="bibr" target="#b28">Vinyals et al. (2015)</ref> use a CNN to encode images into a fixed size vector representation and a LSTM to decode the image representations into a sentence. <ref type="bibr" target="#b31">Xu et al. (2015)</ref> incorporate an atten- tion mechanism ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref> and at- tend to the output from a convolutional layer of a CNN to produce the image representations for the decoder. Instead of encoding a whole image as a vector, <ref type="bibr" target="#b11">(Johnson et al., 2016</ref>) apply R-CNN object detection ( <ref type="bibr" target="#b8">Girshick et al., 2014</ref>), match text snip- pets to the regions of the image detected by the R- CNN, and use a recurrent neural network (RNN) language model, similar to ( <ref type="bibr" target="#b28">Vinyals et al., 2015)</ref>, to generate a description of each region.</p><p>The surface realization for state-of-the-art neu- ral approaches is impressive, but, in the context of generating entity-aware captions, these methods fall short as they heavily rely on training data for language modeling. ( <ref type="bibr" target="#b23">Tran et al., 2016</ref>) leverage face and landmark recognition to generate cap- tions containing named persons, but such large- scale training is difficult. Consequently, OOV words like named entities, which are a quintessen- tial aspect of entity-aware captions because OOV words typically represent entities or events, are difficult to generate due to low training probabil- ities. Some work has been done to incorporate novel objects into captions ( <ref type="bibr" target="#b27">Venugopalan et al., 2017)</ref>, but this does not address the need to gen- erate entity-aware captions and incorporate con- textual information; rather, it gives the ability to generate more fine-grained entities within cap- tions that still lack the necessary context. <ref type="bibr" target="#b7">(Feng and Lapata, 2013</ref>) also generates a caption with named entities, but from associated news articles, in which there is much more textual context than our setting. Our approach uses neural networks to generate dynamic templates and then fills in the templates with specific entities. Thus, we are able to combine the sentence variation and sur- face realization quality of neural language model- ing and the capability to incorporate novel words of template-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>In this paper we propose a new task which aims to automatically generate entity-aware image de- scriptions with limited textual information. Exper- iments on a new benchmark dataset collected from Flickr show that our approach generates more in- formative captions compared to traditional image captioning methods. Moreover, our two-step ap- proach can easily be applied to other language generation tasks involving specific information.</p><p>In the future, we will expand the entity-aware model to incorporate the relations between can- didates when the model fills in the slots, which can avoid the cases such as 'Cristiano Ronaldo of Barcelona'. We will also make further research on context-aware fine-grained entity typing to train a better template generator. Another research direc- tion based on this work is to develop an end-to-end neural architecture to make the model more flexi- ble without generating a template in the middle.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FlickrFigure 1 :</head><label>1</label><figDesc>Figure 1: The overall framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Procedure of Converting News Captions into Templates.</figDesc><graphic url="image-3.png" coords="3,318.19,400.48,91.57,62.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: LSTM language generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Knowledge graph construction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Examples of generated entity-aware caption.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Comparison of Template Generator with coarse/fine-grained entity type. Coarse</head><label>2</label><figDesc></figDesc><table>Template is 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Comparison between our entity-aware model and baseline models on various topics. (  *  We make 
human evaluation on 259 images randomly selected.) 

</table></figure>

			<note place="foot" n="1"> Datasets and programs: https://github.com/ dylandilu/Entity-aware-Image-Captioning 2 https://en.wikipedia.org/wiki/2017 Women%27s March</note>

			<note place="foot" n="3"> EXIF data contains meta-data tags of photos such as date, time, and camera settings.</note>

			<note place="foot" n="4"> We use the sixth level entity types in Yago ontology (Wordnet types only).</note>

			<note place="foot" n="6"> https://www.reuters.com/ 7 Social Events: concert, festival, parade, protest, ceremony; Sports Events: Soccer, Basketball, Soccer, Baseball, Ice Hockey</note>

			<note place="foot" n="8"> https://www.flickr.com/ 9 https://creativecommons.org/licenses/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the U.S. DARPA AIDA Program No. FA8750-18-2-0014 and U.S. ARL NS-CTA No. W911NF-09-2-0053. The views and conclusions contained in this document are those of the authors and should not be inter-preted as representing the official policies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The semantic web</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third International Conference on Learning Representations</title>
		<meeting>the third International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Stanford typed dependencies manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine De</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2009 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth workshop on statistical machine translation</title>
		<meeting>the ninth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Hao Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2015 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Automatic caption generation for news images. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="797" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE</title>
		<meeting>the 2014 IEEE</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densecap: Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4565" to="4574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2015 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Babytalk: Understanding and generating simple image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Visruth</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagnik</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2891" to="2903" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Collective generation of natural image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Treetalk: Composition and compression of trees for image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Transactions of the Association of Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 European Conference on Computer Vision</title>
		<meeting>the 2014 European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations</title>
		<meeting>52nd annual meeting of the association for computational linguistics: system demonstrations</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Midge: Generating image descriptions from computer vision detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xufeng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alyssa</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kota</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised entity linking with abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics-Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics-Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Collecting image annotations using amazon&apos;s mechanical turk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference of the North American Chapter of the Association for Computational Linguistics-Human Language Technologies Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</title>
		<meeting>the 2010 Conference of the North American Chapter of the Association for Computational Linguistics-Human Language Technologies Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rich image captioning in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Carapcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Thrasher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Sienkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the 2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Natural language generation for spoken dialogue system using rnn encoder-decoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le-Minh</forename><surname>Van-Khanh Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Beyond sumbasic: Taskfocused summarization with sentence simplification and lexical expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisami</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1606" to="1618" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Captioning images with diverse objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2017 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2015 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Language and domain independent entity linking with quantified collective validation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Guang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantically conditioned lstm-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 International Conference on Machine Learning</title>
		<meeting>the 2015 International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Corpus-guided sentence generation of natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching</forename><forename type="middle">Lik</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiannis</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
