<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SGNMT-A Flexible NMT Decoding Platform for Quick Prototyping of New Models and Search Strategies</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Hasler</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">SDL Research</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Saunders</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Byrne</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">SDL Research</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SGNMT-A Flexible NMT Decoding Platform for Quick Prototyping of New Models and Search Strategies</title>
					</analytic>
					<monogr>
						<title level="j" type="main">EMNLP System Demonstrations</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="25" to="30"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper introduces SGNMT, our experimental platform for machine translation research. SGNMT provides a generic interface to neural and symbolic scoring modules (predictors) with left-to-right semantic such as translation models like NMT, language models, translation lattices , n-best lists or other kinds of scores and constraints. Predictors can be combined with other predictors to form complex decoding tasks. SGNMT implements a number of search strategies for traversing the space spanned by the predictors which are appropriate for different predic-tor constellations. Adding new predictors or decoding strategies is particularly easy, making it a very efficient tool for proto-typing new research ideas. SGNMT is actively being used by students in the MPhil program in Machine Learning, Speech and Language Technology at the University of Cambridge for course work and theses, as well as for most of the research work in our group.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We are developing an open source decoding framework called SGNMT, short for Syntactically Guided Neural Machine Translation. <ref type="bibr">1</ref> The soft- ware package supports a number of well-known frameworks, including TensorFlow 2 ( <ref type="bibr" target="#b0">Abadi et al., 2016)</ref>, <ref type="bibr">OpenFST (Allauzen et al., 2007)</ref>, Blocks/Theano ( <ref type="bibr" target="#b4">Bastien et al., 2012;</ref><ref type="bibr">van Merriënboer et al., 2015)</ref>, and NPLM ( <ref type="bibr" target="#b21">Vaswani et al., 2013</ref>). The two central concepts in the SGNMT tool are predictors and decoders. Predic- tors are scoring modules which define scores over the target language vocabulary given the current internal predictor state, the history, the source sentence, and external side information. Scores from multiple, diverse predictors can be combined for use in decoding.</p><p>Decoders are search strategies which traverse the space spanned by the predictors. SGNMT provides implementations of common search tree traversal algorithms like beam search. Since de- coders differ in runtime complexity and the kind of search errors they make, different decoders are appropriate for different predictor constellations.</p><p>The strict separation of scoring module and search strategy and the decoupling of scoring modules from each other makes SGNMT a very flexible decoding tool for neural and symbolic models which is applicable not only to machine translation. SGNMT is based on the OpenFST- based Cambridge SMT system <ref type="bibr" target="#b1">(Allauzen et al., 2014</ref>). Although the system is less than a year old, we have found it to be very flexible and easy for new researchers to adopt. Our group has already integrated SGNMT into most of its research work.</p><p>We also find that SGNMT is very well-suited for teaching and student research projects. In the 2015-16 academic year, two students on the Cambridge MPhil in Machine Learning, Speech and Language Technology used SGNMT for their dissertation projects. <ref type="bibr">3</ref> The first project involved using SGNMT with OpenFST for applying sub- word models in SMT ( <ref type="bibr" target="#b5">Gao, 2016)</ref>. The second project developed automatic music composition by LSTMs where WFSAs were used to define the space of allowable chord progressions in 'Bach' chorales <ref type="bibr" target="#b20">(Tomczak, 2016)</ref>. The LSTM provides the 'creativity' and the WFSA enforces constraints Predictor Predictor state initialize(·) predict next() consume(token) NMT State vector in the GRU or LSTM layer of the decoder network and current context vector.</p><p>Run encoder network to compute annotations. Forward pass through the decoder to compute the posterior given the current decoder GRU or LSTM state and the context vector.</p><p>Feed back token to the NMT network and update the decoder state and the context vector.</p><p>FST ID of the current node in the FST. Load FST from the file system, set the predic- tor state to the FST start node.</p><p>Explore all outgoing edges of the current node and use arc weights as scores.</p><p>Traverse the outgoing edge from the current node labelled with token and update the predictor state to the target node. n-gram Current n-gram history Set the current n-gram history to the begin-of- sentence symbol.</p><p>Return the LM scores for the current n-gram history.</p><p>Add token to the cur- rent n-gram history.</p><p>Word For &lt;/s&gt; use the log- probability of the cur- rent number of UNKs given λ. Use zero for all other tokens.</p><p>Increase internal counter by 1 if token is UNK. <ref type="table">Table 1</ref>: Predictor operations for the NMT, FST, n-gram LM, and counting modules.</p><p>that the chorales must obey. This second project in particular demonstrates the versatility of the ap- proach. For the current, 2016-17 academic year, SGNMT is being used heavily in two courses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Predictors</head><p>SGNMT consequently emphasizes flexibility and extensibility by providing a common interface to a wide range of constraints or models used in MT research. The concept facilitates quick prototyp- ing of new research ideas. Our platform aims to minimize the effort required for implementation; decoding speed is secondary as optimized code for production systems can be produced once an idea has been proven successful in the SGNMT frame- work. In SGNMT, scores are assigned to partial hypotheses via one or many predictors. One pre- dictor usually has a single responsibility as it rep- resents a single model or type of constraint. Pre- dictors need to implement the following methods:</p><p>• initialize(src sentence) Initialize the predictor state using the source sentence.</p><p>• get state() Get the internal predictor state.</p><p>• set state(state) Set the internal pre- dictor state.</p><p>• predict next() Given the internal pre- dictor state, produce the posterior over target tokens for the next position.  • consume(token) Update the internal pre- dictor state by adding token to the current history.</p><p>The structure of the predictor state and the im- plementations of these methods differ substan- tially between predictors. Tab. 2 lists all predictors which are currently implemented. Tab. 1 summa- rizes the semantics of this interface for three very common predictors: the neural machine transla- tion (NMT) predictor, the (deterministic) finite state transducer (FST) predictor for lattice rescor- ing, and the n-gram predictor for applying n-gram language models. We also included two examples (word count and UNK count) which do not have a natural left-to-right semantic but can still be repre- sented as predictors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Example Predictor Constellations</head><p>SGNMT allows combining any number of pre- dictors and even multiple instances of the same predictor type. In case of multiple predictors we combine the predictor scores in a linear model. The following list illustrates that various interest- ing decoding tasks can be formulated as predictor combinations.</p><p>• nmt: A single NMT predictor represents pure NMT decoding.</p><p>• nmt,nmt,nmt: Using multiple NMT pre- dictors is a natural way to represent ensem- ble decoding <ref type="bibr" target="#b6">(Hansen and Salamon, 1990;</ref><ref type="bibr" target="#b19">Sutskever et al., 2014</ref>) in our framework.</p><p>• fst,nmt: NMT decoding constrained to an FST. This can be used for neural lat- tice rescoring (  or other kinds of constraints, for example in the context of source side simplification in MT ( ) or chord progres- sions in 'Bach' <ref type="bibr" target="#b20">(Tomczak, 2016)</ref>. The fst pre- dictor can also be used to restrict the output of character-based or subword-unit-based NMT to a large word-level vocabulary encoded as FSA.</p><p>• nmt,rnnlm,srilm,nplm: Combining NMT with three kinds of language mod- els: An RNNLM ( <ref type="bibr">Zaremba et al., 2014</ref>), a Kneser-Ney n-gram LM ( <ref type="bibr" target="#b9">Heafield et al., 2013;</ref><ref type="bibr" target="#b18">Stolcke et al., 2002</ref>), and a feedforward neural network LM ( <ref type="bibr" target="#b21">Vaswani et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoder Description greedy</head><p>Greedy decoding. beam</p><p>Beam search as described in <ref type="bibr" target="#b3">Bahdanau et al. (Bahdanau et al., 2015)</ref>. dfs Depth-first search. Efficiently enumer- ates the complete search space, e.g. for exhaustive FST-based rescoring. restarting Similar to DFS but with better admissi- ble pruning behaviour. astar A* search <ref type="bibr" target="#b13">(Russell and Norvig, 2003)</ref>. The heuristic function can be defined via predictors. sepbeam</p><p>Associates hypotheses in the beam with only one predictor. Efficiently approxi- mates system-level combination. syncbeam Beam search which compares hypothe- ses after consuming a special synchro- nization symbol rather than after each iteration. bucket</p><p>Multiple beam search passes with small beam size. Can have better pruning be- haviour than standard beam search. vanilla Fast beam search decoder for (ensem- bled) NMT. This implementation is similar to the decoder in <ref type="bibr">Blocks (van Merriënboer et al., 2015</ref>) but can only be used for NMT as it bypasses the pre- dictor framework. <ref type="table">Table 3</ref>: Currently implemented decoders.</p><p>• nmt,ngramc,wc: MBR-based NMT fol- lowing <ref type="bibr" target="#b16">Stahlberg et al. (2017)</ref> with n-gram posteriors extracted from an SMT lattice (ngramc) and a simple word penalty (wc).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Decoders</head><p>Decoders are algorithms to search for the highest scoring hypothesis. The list of pre- dictors determines how (partial) hypothe- ses are scored by implementing the meth- ods initialize(·), get state(), set state(·), predict next(), and consume(·). The Decoder class implements versions of these methods which apply to all predictors in the list.</p><p>initialize(·) is always called prior to decoding a new sentence. Many popular search strategies can be described via the remaining methods get state(), set state(·), predict next(), and consume(·). Algs. 1 and 2 show how to define greedy and beam decoding in this way. <ref type="bibr">45</ref> Tab. 3 contains a list of currently implemented decoders. The UML diagram in <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the relation between decoders and predictors. P ←predict next()</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>(t, c) ← arg max (t ,c )∈P c 6:</p><formula xml:id="formula_0">h ← h · t 7:</formula><p>consume(t) 8: until t = &lt;/s&gt; 9: return h NMT batch decoding The flexibility of the pre- dictor framework comes with degradation in de- coding time. SGNMT provides two ways of speeding up pure NMT decoding, especially on the GPU. The vanilla decoding strategy exposes the beam search implementation in <ref type="bibr">Blocks (van Merriënboer et al., 2015</ref>) which processes all ac- tive hypotheses in the beam in parallel. We also implemented a beam decoder version which de- codes multiple sentences at once (batch decoding) rather than in a sequential order. Batch decoding is potentially more efficient since larger batches can make better use of GPU parallelism. The key con- cepts of our batch decoder implementation are:</p><p>• We use a scheduler running on a separate CPU thread to construct large batches of computation (GPU jobs) from multiple sen- tences and feeding them to the jobs queue.</p><p>• The GPU is operated by a single thread which communicates with the CPU scheduler thread via queues containing jobs. This thread is only responsible for retrieving jobs in the jobs queue, computing them, and putting them in the jobs results queue, minimizing the down-time of GPU computation.</p><p>• Yet another CPU thread is responsible for processing the results computed on the GPU </p><formula xml:id="formula_1">H next ← ∅ 5:</formula><p>for all (h, c, s) ∈ H do 6: set state(s)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>P ←predict next() 8:</p><formula xml:id="formula_2">H next ← H next ∪ (t ,c )∈P (h · t , c + c , s) 9:</formula><p>end for 10:</p><formula xml:id="formula_3">H ← ∅ 11:</formula><p>for all (h, c, s) ∈ n-best(H next ) do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>set state(s)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>consume(h |h| )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>H ← H ∪ {(h, c, get state())}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15:</head><p>end for 16: until Best hypothesis in H ends with &lt;/s&gt; 17: return Best hypothesis in H in the job results queue, e.g. by getting the n-best words from the posteriors. Processed jobs are sent back to the CPU scheduler where they are reassembled into new jobs. This decoder is able to translate the WMT English-French test sets news-test2012 to news- test2014 on a Titan X GPU with 911.6 words per second with the word-based NMT model de- scribed in . <ref type="bibr">6</ref> This decoding speed seems to be slightly faster than sequential decoding with high-performance NMT decoders like Marian-NMT (Junczys-Dowmunt et al., 2016) with reported decoding speeds of 865 words per second. <ref type="bibr">7</ref> However, batch decoding with Marian- NMT is much faster reaching over 4,500 words per second. <ref type="bibr">8</ref> We think that these differences are mainly due to the limited multithreading support and performance in Python especially when using external libraries as opposed to the highly opti- mized C++ code in Marian-NMT. We did not push for even faster decoding as speed is not a major design goal of SGNMT. Note that batch decoding bypasses the predictor framework and can only be used for pure NMT decoding.</p><p>Ensembling with models at multiple tokeniza- tion levels SGNMT allows masking predictors with alternative sets of modelling units. The con- version between the tokenization schemes of dif- ferent predictors is defined with FSTs. This makes it possible to decode by combining scores from both a subword-unit (BPE) based NMT ( <ref type="bibr" target="#b14">Sennrich et al., 2016</ref>) and a word-based NMT model with character-based NMT, masking the BPE-based and word-based NMT predictors with FSTs which transduce character sequences to BPE or word se- quences. Masking is transparent to the decod- ing strategy as predictors are replaced by a spe- cial wrapper (fsttok) that uses the masking FST to translate predict next() and consume() calls to (a series of) predictor calls with alter- native tokens. The syncbeam variation of beam search compares competing hypotheses only af- ter consuming a special word boundary symbol rather than after each token. This allows com- bining scores at the word level even when using models with multiple levels of tokenization. Joint decoding with different tokenization schemes has the potential of combining the benefits of the dif- ferent schemes: character-and BPE-based mod- els are able to address rare words, but word-based NMT can model long-range dependencies more efficiently.</p><p>System-level combination We showed in Sec. 2.1 how to formulate NMT ensembling as a set of NMT predictors. Ensembling averages the individual model scores in each decoding step. Alternatively, system-level combination decodes the entire sentence with each model separately, and selects the best scoring complete hypothesis over all models. In our experiments, system-level combination is not as effective as en- 1080), (b) a different training and test set, (c) a slightly differ- ent network architecture, and (d) words rather than subword units.</p><p>8 https://marian-nmt.github.io/ features/ sembling but still leads to moderate gains for pure NMT. However, a trivial implementation which selects the best translation in a postprocessing step after separate decoding runs is slow. The sepbeam decoding strategy reduces the runtime of system-level combination to the single system level. The strategy applies only one predictor rather than a linear combination of all predictors to expand a hypothesis. The single predictor is linked by the parent hypothesis. The initial stack in sepbeam contains hypotheses for each predictor (i.e. system) rather than only one as in normal beam search. We report a moderate gain of 0.5 BLEU over a single system on the Japanese-English ASPEC test set ( <ref type="bibr" target="#b12">Nakazawa et al., 2016</ref>) by combining three BPE-based NMT models from <ref type="bibr" target="#b16">Stahlberg et al. (2017)</ref> using the sepbeam decoder.</p><p>Iterative beam search Normal beam search is difficult to use in a time-constrained setting since the runtime depends on the target sentence length which is a priori not known, and it is therefore hard to choose the right beam size beforehand. The bucket search algorithm sidesteps the problem of setting the beam size by repeatedly perform- ing small beam search passes until a fixed com- putational budget is exhausted. Bucket search pro- duces an initial hypothesis very quickly, and keeps the partial hypotheses for each length in buckets. Subsequent beam search passes refine the initial hypothesis by iteratively updating these buckets. Our initial experiments suggest that bucket search often performs on a similar level as standard beam search with the benefit of being able to support hard time constraints. Unlike beam search, bucket search lends itself to risk-free (i.e. admissible) pruning since all partial hypotheses worse than the current best complete hypothesis can be discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper presented our SGNMT platform for prototyping new approaches to MT which involve both neural and symbolic models. SGNMT sup- ports a number of different models and constraints via a common interface (predictors), and vari- ous search strategies (decoders). Furthermore, SGNMT focuses on minimizing the implementa- tion effort for adding new predictors and decoders by decoupling scoring modules from each other and from the search algorithm. SGNMT is ac- tively being used for teaching and research and we welcome contributions to its development, for ex- ample by implementing new predictors for using models trained with other frameworks and tools.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Reduced UML class diagram.</figDesc><graphic url="image-1.png" coords="4,72.00,62.81,453.53,95.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Currently implemented predictors.</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> http://ucam-smt.github.io/sgnmt/html/ 2 SGNMT relies on the TensorFlow fork available at https://github.com/ehasler/tensorflow</note>

			<note place="foot" n="3"> http://www.mlsalt.eng.cam.ac.uk/Main/ CurrentMPhils</note>

			<note place="foot" n="4"> Formally, predict next() in Algs. 1 and 2 returns pairs of tokens and their costs. 5 String concatenation is denoted with ·.</note>

			<note place="foot" n="6"> Theano 0.9.0, cuDNN 5.1, Cuda 8 with CNMeM, Intel R  Core i7-6700 CPU 7 Note that the comparability is rather limited since even though we use the same beam size (5) and vocabulary sizes (30k), we use (a) a slightly slower GPU (Titan X vs. GTX</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the U.K. Engineering and Physical Sciences Research Council (EPSRC grant EP/L027623/1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martın</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pushdown automata in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrì</forename><surname>De Gispert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gonzalo</forename><surname>Iglesias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="687" to="723" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">OpenFst: A general and efficient weighted finite-state transducer library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Schalkwyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Implementation and Application of Automata</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="11" to="23" />
		</imprint>
	</monogr>
	<note>Wojciech Skut, and Mehryar Mohri</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Theano: New features and speed improvements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Variable length word encodings for neural translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiameng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
<note type="report_type">MPhil dissertation</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural network ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Salamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="993" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Source sentence simplification for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>De Gispert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Waite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Byrne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Computer Speech &amp; Language</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adrì a de Gispert, and Bill Byrne. 2017. A comparison of neural models for word ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Tomalin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INLG</title>
		<meeting><address><addrLine>Santiago de Compostela, Spain</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scalable modified Kneser-Ney language model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Pouzyrevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="690" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Is neural machine translation ready for deployment? a case study on 30 translation directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01108</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bart Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Warde-Farley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00619</idno>
		<title level="m">Chorowski, and Yoshua Bengio. 2015. Blocks and fuel: Frameworks for deep learning</title>
		<imprint>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ASPEC: Asian scientific paper excerpt corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Yaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyotaka</forename><surname>Uchimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<meeting><address><addrLine>Portoroz, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2204" to="2208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Norvig</surname></persName>
		</author>
		<title level="m">Artificial Intelligence: A Modern Approach</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>2 edition. Pearson Education</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient left-to-right hierarchical phrase-based translation with improved reordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Siahbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baskaran</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1089" to="1099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural machine translation by minimising the Bayes-risk with respect to syntactic translation lattices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>De Gispert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="362" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Syntactically guided neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Waite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="299" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SRILM-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">2002</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Tomczak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Bachbot. MPhil dissertation</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Decoding with largescale neural language models improves translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinggong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1387" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
