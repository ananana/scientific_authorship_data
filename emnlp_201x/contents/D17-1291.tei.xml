<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Identifying Semantically Deviating Outlier Documents *</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglei</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Wang</surname></persName>
							<email>wang.chi@microsoft.com, lance.m.kaplan.civ@mail.mil</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangbo</forename><surname>Tao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Kaplan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Identifying Semantically Deviating Outlier Documents *</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2748" to="2757"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
					<note>2 Microsoft Research, Redmond 3 US Army Research Lab</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A document outlier is a document that substantially deviates in semantics from the majority ones in a corpus. Automatic identification of document outliers can be valuable in many applications, such as screening health records for medical mistakes. In this paper, we study the problem of mining semantically deviating document outliers in a given corpus. We develop a generative model to identify frequent and characteristic semantic regions in the word embedding space to represent the given corpus, and a robust outlierness measure which is resistant to noisy content in documents. Experiments conducted on two real-world textual data sets show that our method can achieve an up to 135% improvement over baselines in terms of recall at top-1% of the outlier ranking.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The technology today has made it unprecedent- edly easy to collect and store documents in an increasing number of domains. Automatic text analysis (e.g. document clustering, summariza- tion, topic modeling) becomes more useful and de- manded as the corpus size grows. Some trending * Research was sponsored in part by the U.S. Army Re- search Lab. under Cooperative Agreement No. W911NF- 09-2-0053 (NSCTA), National Science Foundation IIS- 1320617, IIS 16-18481, and NSF IIS 17-04532, and grant 1U54GM114838 awarded by NIGMS through funds pro- vided by the trans-NIH Big Data to Knowledge (BD2K) initiative (www.bd2k.nih.gov). The views and conclusions contained in this document are those of the author(s) and should not be interpreted as representing the official policies of the U.S. Army Research Laboratory or the U.S. Govern- ment. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon.</p><p>domains (e.g. health records) call for a new analyt- ical task, mining outlier documents: given a cor- pus, identify a small number of documents which substantially deviate from the semantic focuses of the given corpus. Outlier documents can provide valuable insights or imply potential errors. For example, an outlier health record from records of the same disease could indicate a new variation of the disease if it has an abnormal symptom descrip- tion, or a medical error if it has an abnormal treat- ment description. A previous study <ref type="bibr" target="#b7">(Hauskrecht et al., 2013</ref>) uses structured data in health records to show the importance of this application, and points out that further improvement should be achieved by leveraging text data.</p><p>Existing work has studied a related albeit different task, novel document detection <ref type="bibr" target="#b10">(Kasiviswanathan et al., 2012</ref><ref type="bibr" target="#b9">(Kasiviswanathan et al., , 2013</ref><ref type="bibr" target="#b16">Zhang et al., 2002</ref><ref type="bibr" target="#b15">Zhang et al., , 2004</ref>), where one aims to identify from a document stream if a newly arriving document is novel or redundant. In other words, this task assumes all the previous documents are known to be "normal", and only checks if a new docu- ment is novel. In our task, no document is known to be normal, and there could be multiple out- liers in the corpus. Outlier detection ( <ref type="bibr" target="#b2">Chandola et al., 2009;</ref><ref type="bibr" target="#b8">Hodge and Austin, 2004</ref>) is a popu- lar topic in data mining but few focus on text data. A study <ref type="bibr" target="#b6">(Guthrie, 2008)</ref> identifies anomalous text segments in a document, but mainly based on writ- ing styles. We focus on studying semantically de- viating documents.</p><p>The problem of detecting outlier documents has its unique challenges. First, different words or phrases may be used to indicate the same semantic meaning, which introduces lexical sparsity. Sec- ond, finding proper words or phrases to charac- terize the corpus is non-trivial. Semantically fre- quent words or phrases can still be too general or too vague. Third, a document can carry extremely rich and noisy signals, most of which are not help- ful to determine whether it is an outlier.</p><p>We tackle the problem of mining outlier doc- uments in the following steps. We leverage word embedding ( <ref type="bibr" target="#b13">Mikolov et al., 2013</ref>) to capture the semantic proximities between words and/or phrases, in order to solve the sparsity issue. Then we propose a generative model to identify seman- tic regions in the embedded space frequently men- tioned by documents in the corpus. The model represents each semantic region with a von Mises- Fisher distribution. We also learn a concentration parameter for each region with our model, and de- velop a selection method to identify semantically specific regions which can better represent the cor- pus, and filter regions with largely uninformative words.</p><p>As the final step, we design a robust outlierness measure emphasizing only the words or phrases in a document relatively close to the semantic fo- cuses identified, and eliminating the noises and re- dundant information.</p><p>The remaining of the paper is organized as fol- lows. Section 2 introduces the preprocessing of data sets and clarifies the notations. Section 3 proposes the methodology to mine outlier docu- ments. Section 4 describes the experiment setup, Section 5 presents the results and Section 6 con- cludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>In this section, we formalize the problem and then briefly describe the preprocessing step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notations</head><p>The notations used in this study are introduced here. A document is represented as a sequence</p><formula xml:id="formula_0">d i = (w i1 , w i2 , · · · , w in i )</formula><p>, where each w ij ∈ V represents a word or phrase from a given vocabu- lary V and n i denotes the length of the d i . We refer to a set of documents as a corpus, represented as</p><formula xml:id="formula_1">D = {d i } |D| i=1</formula><p>. Notice that w ij may refer to a unigram word or a multi-gram phrase. Although it is nontrivial to ap- propriately segment a document into a mixed se- quence of words and phrases, it is not the focus of our paper. A recently developed phrase min- ing technique ( <ref type="bibr" target="#b12">Liu et al., 2015</ref>) is used to extract quality phrases and segment the documents.</p><p>Word embedding provides vectorized represen- tations of words and phrases to capture their se- mantic proximity. We assume there is an effective word embedding technique (e.g. ( <ref type="bibr" target="#b13">Mikolov et al., 2013)</ref>), f : V → R ν , where f is the transform- ing function that takes a word or a phrase as input and projects it into a ν-dimensional vector as its distributed representation. The semantic proxim- ity between two words or phrases w and w can be preserved by the cosine similarity between their embedded vectors:</p><formula xml:id="formula_2">CosSim f (w), f (w ) = f (w) · f (w ) f (w) × f (w ) Problem definition.</formula><p>This work studies how to effectively rank documents in a corpus based on how much they deviate from the semantic fo- cuses of the corpus. Given a set of documents D, our objective is to design an outlierness measure Ω : D → R, such that documents with larger out- lierness Ω(d) semantically deviate more from the majority of D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Preprocessing</head><p>We perform several steps of preprocessing to de- rive the input representation of each document in a given corpus.</p><p>Phrase mining. SegPhrase, a recently developed phrase-mining method ( <ref type="bibr" target="#b12">Liu et al., 2015)</ref>, is uti- lized to automatically identify quality phrases in a corpus. After being trained in one corpus, Seg- Phrase is also capable of segmenting unseen docu- ments into chunks of phrases with mixed lengths. We train SegPhrase on an external corpus D e to obtain the list of quality phrases. Then for each corpus D given for outlier detection, we employ the trained SegPhrase to chunk each document into a sequence of words and quality phrases.</p><p>Word embedding. We adopt word embedding as a preprocessing step to capture the semantic proximity between words/phrases. Instead of us- ing the raw text, similar to ( <ref type="bibr" target="#b12">Liu et al., 2015)</ref>, we use the sequence derived from SegPhrase as in- put to the word embedding algorithm. In particu- lar, <ref type="bibr">word2vec (Mikolov et al., 2013</ref>) is utilized in our experiments, but can be seamlessly replaced by any other embedding results.</p><p>We run the embedding algorithm based on the external corpus D e , the same corpus used in phrase mining. As D e is sufficiently large, there are only few words or phrases in D which never appear in D e , and are simply discarded in the ex- periments.</p><p>Stop words removal. We remove stop words, as well as the words or phrases ranked high within a certain quantile in terms of document frequency 1 (DF) in the external corpus D e . Such words or phrases usually carry background noise, and ob- struct outlier detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Mining Outlier Documents</head><p>Our framework consists of the following steps. First, we leverage a generative model to identify semantic "regions" in the word embedding space frequently mentioned by documents in the given corpus. Second, we develop a selection method to further remove semantics regions that are too gen- eral to properly characterize the given corpus, and only keep regions both frequent and semantically specific, denoted as "semantic focuses". Finally, we calculate the outlierness measure for each doc- ument based on the mined semantic focuses. We design a robust outlierness measure which is less sensitive to noisy words or phrases in documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Embedded von Mises-Fisher Allocation</head><p>We start with a generative model to identify the frequent semantic regions in the word embedding space.</p><p>Since we use cosine similarity to capture the se- mantic proximities between two words or phrases, the magnitude of the embedding vector of each word can be omitted in this part. We use x ij = f (w ij )/f (w ij ) to represent the unit vector with the same direction as the embedded vector of w ij , and use X to represent the collection of all x ij where 1 ≤ i ≤ |D| and 1 ≤ j ≤ n i .</p><p>In order to characterize a semantic region in the embedded space, we introduce von Mises-Fisher (vMF) distribution. The von Mises-Fisher (vMF) distribution is prevalently adopted in directional statistics, which studies the distribution of normal- ized vectors on a spherical space. The probability density function of the vMF distribution is explic- itly instantiated by the cosine similarity. It is an ideal distribution for our task because we use co- sine similarity to measure the semantic proximity. Moreover, as we will see later, it empowers us to characterize how specific each semantic region is, which is helpful in further identification of seman- tic focuses for outlier detection.</p><p>We first introduce the formalization of the von Mises-Fisher distribution.</p><p>Von Mises-Fisher (vMF) distribution.</p><p>A ν- dimensional unit random vector x (i.e. x ∈ R ν and x = 1) follows a von Mises-Fisher distri- bution vMF(·|µ, κ) if the probability density func- tion follows:</p><formula xml:id="formula_3">p(x) = C ν (κ) exp κµ x where C ν (κ) = κ ν/2−1 (2π) ν/2 I ν/2−1 (κ); and I ν/2−1 (·)</formula><p>is the modified Bessel function of the first kind; (ν/2 − 1) is the order.</p><p>The two parameters in the vMF distribution are the mean direction µ and the concentration param- eter κ respectively, where µ ∈ R ν , µ = 1 and κ &gt; 0. The distribution concentrated around the mean direction µ, and is more concentrated if the concentration parameter κ is larger.</p><p>Embedded von Mises-Fisher allocation. We propose a generative model by regarding each doc- ument as a bag of normalized embedded vectors, analogous to the bag-of-word representation of documents utilized in typical topic model (e.g., LDA ( <ref type="bibr" target="#b1">Blei et al., 2001)</ref>). The major difference is that the data to be generated is now a bag-of- normalized-embedded-vectors for each document, and should be generated from a mixed vMF distri- bution instead of a mixed multinomial distribution.</p><p>A formalized description of the model is sum- marized as follows:</p><formula xml:id="formula_4">µ t ∼ vMF(·|µ 0 , C 0 ), t = 1, 2, · · · , T κ t ∼ logNormal(·|m 0 , σ 2 0 ), t = 1, 2, · · · , T π i ∼ Dirichlet(·|α), i = 1, 2, · · · , |D| z ij ∼ Categorical(·|π i ), j = 1, 2, · · · , |d i | x ij ∼ vMF(·|µ z ij , κ z ij ), j = 1, 2, · · · , |d i |</formula><p>where T &gt; 0 is an integer indicating the number of semantic regions, namely the number of vMF distributions in our mixture model. We regularize the vMF parameters by the fol- lowing prior distributions. We assume the mean direction µ t of each vMF distribution is gener- ated from a prior vMF distribution vMF(·|µ 0 , C 0 ), while the concentration parameter κ t is generated from a log-normal prior logNormal(·|m 0 , σ 2 0 ). A similar design is also adopted in ( <ref type="bibr" target="#b5">Gopal and Yang, 2014</ref>).</p><p>Parameter inference. We infer the parameters by Gibbs sampling. Because both the von Mises- Fisher distribution and the Dirichlet distribution have conjugate priors, we can integrate out pa- rameters µ t and π i and develop a collapsed Gibbs sampler of z ij :</p><formula xml:id="formula_5">P (z ij = t|Z −ij , X, κ; α, m 0 , σ 2 0 , µ 0 , C 0 ) ∝ n −ij it + 1 + α (t) C ν (κ t )C ν C 0 µ 0 + κ t x −ij ·t C ν C 0 µ 0 + κ t x −ij ·t + x ij where n −ij it = |d i | j δ(z ij = t) − δ(z ij = t)</formula><p>is the number of words in the i-th document be- ing assigned to the t-th von Mises-Fisher distri- bution without taking w ij into account;</p><formula xml:id="formula_6">x −ij ·t = |D| i |d i | j x i j δ(z i j = t) − δ(z ij = t)</formula><p>is the sum of word vectors assigned to semantic region t without counting w ij . Here δ(·) is the indicator function.</p><p>We can also derive a collapsed Gibbs sampler for concentration parameters κ t 's:</p><formula xml:id="formula_7">P (κ t |Z, X, κ −t ; α, m 0 , σ 2 0 , µ 0 , C 0 ) ∝ C n·t ν (κ t ) C ν C 0 µ 0 + κ t x ·t logNormal(κ t |m 0 , σ 2 0 )</formula><p>where n ·t is the number of words in semantic re- gion t.</p><p>While sampling z ij is relatively trivial, sam- pling κ t is not straightforward. Similar difficulty is also mentioned in ( <ref type="bibr" target="#b5">Gopal and Yang, 2014</ref>). We employ a Metropolis-Hasting algorithm with an- other log-normal distribution centered at the cur- rent κ t value as the proposal distribution.</p><p>After obtaining a sample from the posterior dis- tribution of z ij 's and κ t 's, we can easily obtain the MAP estimate of mean directions µ t 's and the mixing distribution of each documents π i :</p><formula xml:id="formula_8">ˆ µ t = C 0 µ 0 + κ t x ·t C 0 µ 0 + κ t x ·t , ˆ π i = n it + α (t) n i· + t α (t)</formula><p>Discussions. We notice that there are some topic models ( <ref type="bibr" target="#b4">Das et al., 2015;</ref><ref type="bibr" target="#b0">Batmanghelich et al., 2016)</ref> proposed for similar data, where words are represented as embedding vectors. Our model is proposed independently for the purpose of identi- fying semantic focuses, which serves the task of outlier detection. Existing models may lack sig- nals for the following outlier detection steps and hence cannot be directly plugged in. However, it is possible to adapt certain models to the outlier detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Identifying Semantic Focuses</head><p>The semantic regions learned from the Embedded vMF Allocation model provide a set of candidates frequently mentioned by documents in the corpus. However, not all of them are semantic focuses of the corpus -some are too general to distinguish outlier and normal document. We notice that uninformative semantic regions (e.g. a semantic region containing {"percent", "av- erage", "compare", ...}) tend to have more scat- tered distribution over embedded vectors, possi- bly because of the diverse context of their usage. In contrast, corpus-specific semantic regions are more concentrated, (e.g. a semantic region con- taining {"drugs", "antidepressant", "prescription", ...}). Modeling semantic regions by vMF distri- butions provides us with a parsimonious signal to characterize how concentrated a semantic region is, i.e. the concentration parameter κ t . This al- lows us to simply filter unqualified semantic re- gions with too small concentration parameters and obtain high-quality semantic focuses. Let a binary variable φ t (t = 1, 2, · · · , T ) indicate whether the t-th vMF distribution is a semantic focus. Sup- pose a user specifies a threshold parameter 0 ≤ β ≤ 1. We can determine φ t by estimating the log-normal distribution that generates all κ t 's, logN ormal( ˆ m, ˆ σ 2 ), wherê</p><formula xml:id="formula_9">wherê m = 1 T t log(κ t ), ˆ σ 2 = 1 T t log(κ t ) − ˆ m 2</formula><p>SetˆFSetˆ SetˆF κ (·) to be its cumulative distribution func- tion. We assign φ t = 1 for semantic regions with κ t ≥ F −1 κ (β), and filter all the other semantic re- gions as φ t = 0.</p><p>Although parameter β needs to be set manu- ally, our experiments suggest the performance is not quite sensitive to its value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Document Outlierness</head><p>In this subsection, we start with a straightforward definition of outlierness based on the mined se- mantic focuses. Then we present several refine- ments to improve its robustness.</p><p>Baseline outlierness measure.</p><p>A straightfor- ward intuition is to assume outlier documents av- eragely have fewer words or phrases drawn from semantic focuses. To estimate this, we first need to calculate the probability of each word being drawn from the semantic focuses.</p><formula xml:id="formula_10">P φ z ij = 1|x ij , π i = t φ t π (t) i vMF(x ij |µ t , κ t ) t π (t)</formula><p>i vMF(x ij |µ t , κ t ) It is then possible to estimate the expected percent- age of words not drawn from semantic focuses in each document as the outlierness:</p><formula xml:id="formula_11">Ω sf (d i ) = 1 − 1 |d i | |d i | j=1 P (φ z ij = 1|x ij , π i ) (1)</formula><p>However, due to the noisiness in text data, this assumption oversimplifies the characteriza- tion of outlier documents. In practice, we ob- serve the following two issues: lexically general words/phrases, and noisy content in documents.</p><p>Penalizing lexically general words and phrases. Not all words or phrases close to semantic focuses are strong indicators of normal documents. Gen- eral words (e.g. "science") can happen to be se- mantically close to a semantic focus, but are not as specific as most other words close to it (e.g. "medi- cal research"). Therefore, we utilize a background corpus D bg to calculate the specificity of the word. Assuming the actual mention of the word can be chosen from either the general background, or a corpus-specific vocabulary, we write down the probability that a word is corpus-specific to be:</p><formula xml:id="formula_12">P λ ij |w ij = nd(w ij )/|D| nd(w ij )/|D| + nd bg (w ij )/|D bg |</formula><p>where nd(w) = |{d i |w ∈ d i , d i ∈ D}| is the number of documents in D containing word w; nd bg (w) = |{d i |w ∈ d i , d i ∈ D bg }| is the num- ber of documents containing word w in the back- ground corpus D bg ; λ ij is a binary random vari- able indicating whether w ij is specific enough.</p><p>For each word, we define the word is orthodox if the word is not only semantically close to a se- mantic focus of the corpus, but also sufficiently specific. We then define the probability that a word or phrase w ij in document d i is orthodox as:</p><formula xml:id="formula_13">P (ϕ ij |x ij , π i w ij ) = P φ z ij |x ij , π i P λ ij |w ij )</formula><p>where ϕ ij = 1 indicates that w ij (or equivalently x ij ) is orthodox. Now, we can define a second outlierness mea- sure as the expected percentage of words that are not orthodox.  Noisy content in documents. We present the second issue of normal documents with an exam- ple. We compare a normal document in a corpus of New York Times news articles with tag "Health", to another document originally from another cor- pus, but with its outlierness calculated with regard to the semantic focuses of the "Health" corpus.</p><formula xml:id="formula_14">Ω e (d i ) = 1 − 1 |d i | |d i | j=1 P (ϕ ij |x ij , π i , w ij ) (2)</formula><p>In <ref type="figure" target="#fig_0">Figure 1</ref>(a), we show the distribution of in- ferred orthodox probability P (ϕ ij = 1|x ij , w ij ) by ranking the words or phrases according to their probability value. We can observe that the outlier document barely has any words or phrases surely orthodox, while the normal document has 5% of words or phrases with a probability no less than 0.8 to be orthodox. However, if we simply take the average, these two documents become indis- tinguishable as the average is substantially domi- nated by the "tail" where most words or phrases in either documents are clearly not orthodox. Let n ϕ i be a random variable indicating the true num- ber of orthodox words or phrases in document d i . Since n ϕ i follows a Poisson-Binomial distri- bution, we can plot the probability distribution of n ϕ i normalized by the length of the document, as shown in <ref type="figure" target="#fig_0">Figure 1(b)</ref>. It can be observed that the difference between the normalized expecta- tion E[n ϕ i ]/d i of two documents is insignificant. Therefore, the measure described in Equation (2) will be unable to tell the difference between these two documents.</p><p>This example illustrates why the strategy of tak- ing the average over the whole document can make mistakes, and also provides an important insight. As long as a document has a (potentially small) portion of words or phrases that are highly certain to be orthodox, it should not be considered as an outlier. Based on the above observation, we pro- pose a third outlierness measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Orthodox quantile outlierness.</head><p>We define a quantile-based outlierness definition to rank doc- ument outliers. Notice that the distribution of ran- dom variable n ϕ i follows a Poisson-Binomial dis- tribution, which is the total number of success tri- als when one tosses a coin for each word or phrase in the document to determine whether it is ortho- dox with probability P (ϕ ij |x ij , w ij ).</p><p>Moreover, we define the first 1 1−θ -quantile of the Poisson-Binomial distribution of n ϕ i as:</p><formula xml:id="formula_15">q θ (n ϕ i ) = sup q {q : P (n ϕ i ≥ q) ≥ θ} (3)</formula><p>where 0 &lt; θ &lt; 1 is a given parameter close to 1. Intuitively, it measures the maximum lower bound of n ϕ i we can guarantee with confidence θ. Based on Equation <ref type="formula">(3)</ref>, we can give a formal- ized definition of our proposed outlierness:</p><formula xml:id="formula_16">Ω θ-q (d i ) = 1 − q θ (n ϕ i ) + 1 |d i | + 1<label>(4)</label></formula><p>where the 1 1−θ -quantile is normalized by the doc- ument length with a smoothing constant. The cumulative probability distribution of a Poisson- Binomial distribution can be efficiently calculated by dynamic programming <ref type="bibr" target="#b3">(Chen and Liu, 1997)</ref>.</p><p>The advantage of the last proposed outlierness measure is that it emphasizes more on the highly orthodox words or phrases and eliminates the noise from a number of relatively uncertain ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Sets</head><p>New York Times News (NYT). We collected 41,959 news article published in 2013 from The New York Times API 2 . Each article is assigned with a unique label indicating in which section the article is published, such as Arts, Travel, Sports, and Health. There are totally 9 section labels in our collected data set. We treat papers in each sec- tion as a corpus D. Thereby we have a set of cor- pora D = {D s }, without overlapping documents. We also have an external news data set D e crawled from Google news, with 51,114 news article pub- lished in 2015 without any label information.</p><p>ArnetMiner Paper Abstracts (ARNET). We employ abstracts of papers published in the field We confine ω to be a small integer less than 1% of the size of |D s |. More concretely, ω is an integer uni- formly sampled from (0, 0.01|D s |].</p><p>For each data set, we randomly generate 10 out- lier detection benchmarks, and evaluate the overall performance by the average performance on all the benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Methods Evaluated</head><p>We compare the performances of the following methods.</p><p>Cosine similarity based. We characterize each document as a vector, and use the negative aver- age cosine similarity between each document and the corpus as outlierness. We use two different ways to vectorize documents: TF-IDF weighted, and paragraph2vec ( <ref type="bibr" target="#b11">Le and Mikolov, 2014</ref>). The two methods are denoted as TFIDF-COS and P2V- COS respectively.</p><p>KL divergence based. We represent each doc- ument as a probability distribution, and the entire corpus as another probability distribution. Then we use the KL-divergence between each document and the entire corpus as the outlierness. We also use two different ways to calculate the probability distribution. The first is to estimate the unigram distribution for each document and the entire cor- pus respectively, denoted as UNI-KL. The other is to first perform LDA on the entire corpus with 10 topics, and then infer topical allocation distribu- tion of each document and the entire corpus. This method is represented as TM-KL.</p><p>Our method Our quantile based method is de- noted as VMF-Q. We also provide two baselines derived from our own method as an ablation anal- ysis. One method abandons the quantile based outlierness but use the expected orthodox percent- age as Equation <ref type="formula">(2)</ref>, denoted as VMF-E. The other method further removes the penalty on lexical gen- eral words and phrases, using Equation (1), de- noted as VMF-SF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Measures</head><p>In most outlier detection applications, people are more concerned with recall. We measure the per- formance by recall at a certain percentage. More specifically, we compute the recall of outlier de- tection if the user checks a certain percentage r of the top-ranked documents in the output results. Since in our benchmark generation, the percent- age of outliers does not exceed 1%. Therefore, the perfect results for any r ≥ 1% should be 1.0.</p><p>We choose r to be 1%, 2%, and 5% respectively and evaluate different methods with recall at top-r (percentage). We also report the performance in terms of mean average precision (MAP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Parameter Configurations</head><p>All benchmark data sets are preprocessed as de- scribed in Section 2. In the NYT data set we re- move words or phrases within top 20% with re- spect to document frequency, while in the ARNET data set we remove the top 10%. The document frequency is calculated based on a background corpus D bg , which is the same as the external cor- pus of NYT. Word embedding are trained on the external data set D e using code of <ref type="bibr" target="#b13">Mikolov et al. (Mikolov et al., 2013</ref>) with default parameter configurations, where the embedded vector length is set to 200. For paragraph2vec, we learn the length-100 vectors for each document along with the external data set to guarantee sufficient train- ing data.</p><p>For the prior vMF distribution, we set C 0 = 0.1, a sufficiently small number so the prior distribu- tion is close to a uniform distribution. µ 0 is set as a normalized all-1 vector. We also set m 0 = log(100), and σ 2 = 0.01. The total number for Gibbs sampling is set to be 50 times of the total count of z ij 's (i.e. η = 50). The number of vMF distributions T is set to 20 in the NYT data set and 10 in the ARNET data set respectively, due to the smaller sizes of corpora in the ARNET data set.</p><p>To determine semantic focuses, we set threshold parameter β = 0.55 for both data sets. The confi- dence parameter θ in outlierness calculation is set to 0.95 in both data sets. Our experiments later will show the performance is relatively robust to different configurations of both parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We present the experimental results in this section.</p><p>Performance comparison. <ref type="table" target="#tab_2">Table 2</ref> shows per- formance of different outlier document detection methods. It can be observed that our method out- performs all the baselines in both data sets. In both data sets, VMF-Q can achieve a 45% to 135% in- crease from baselines in terms of recall by exam- ining the top 1% outliers. Generally, performances of most methods are lower in the ARNET data set comparing to NYT, potentially because the rela- tively short document lengths and more technical terminologies in ARNET.</p><p>Ablation analysis. Both refinements of the out- lierness measure benefits the performance. Specif- ically, by changing the average based outlierness to quantile based outlierness, the recall@1% can be improved by 50-75%, and the recall@5% can also be improved by more than 17%. Sensitivity studies of parameters.</p><p>We study if our proposed method is sensitive to the confi- dence parameter θ and filtering threshold param- eter β. We compare the performance of VMF-Q by varying each parameter on both data sets.  <ref type="bibr">2(d)</ref> show that the performance is relatively stable when β is between 0.5 and 0.7, but drops a little when β is set to larger value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human judgments.</head><p>We compare VMF-Q to VMF-E and P2V-COS respectively by crowd- sourcing, without artificially inserting "outliers". We conduct this experiments on two corpora in NYT data sets with topic "Health" and "Art" re- spectively. To compare two methods, we ran- domly select pairs of documents d i and d j such that both are ranked as top-10% outliers by at least one method, but their orders in the two rankings disagree. We conduct the experiments on Crowd- Flower. Online crowd workers are given d i and d j as well as other documents in the corpus, and are asked to judge which one of d i and d j deviates more from the corpus. For each corpus, we select 200 pairs of documents.</p><p>Before taking the questions, each crowd worker needs to go through at least 10 "test questions" which we know the correct answer. These ques- tions are constructed by taking one document from the corpus as d i and another document not from the corpus as d j . Therefore, the one not from the corpus should be the answer. A crowd worker needs to achieve no less than 80% of accuracy to be eligible to work on actual questions, and the accuracy needs to be maintained over 80% during the work, which is measured by "test questions" hidden in actual questions. Each question is an- swered by 3 workers. The final answer is deter- mined by majority voting. <ref type="figure">Figure 3</ref> presents the results. On both corpora, there are significantly more workers tend to agree with VMF-Q comparing to P2V-COS, with sig- nificance level α = 0.05. This further verifies that our method VMF-Q can achieve better perfor- mance than the P2V-COS baseline. On the other hand, on both data sets we can still observe more workers favoring VMF-Q than VMF-E, but the difference is not as large as the difference between VMF-Q and P2V-COS.</p><p>Case study. We also conduct a case study to show how our proposed method outperforms other baselines. <ref type="table">Table 3</ref> shows two pairs of documents in "Health" corpus of NYT data set. The left two columns show some comparing methods and their higher ranked outlier documents. The row of "Crowds" shows the outlier document chosen by human workers from the crowdsourcing platform, with a consensus of opinions from multiple work- ers.</p><p>In the first document pair, document A is about gun control policy and is substantially irrelevant to "Health" topic, while document B is about lung in- fection cases. Document A is a significant outlier, and VMF-Q and VMF-E also agree with our intu- ition. However, paragraph2vec (P2V) ranks doc- ument B higher, probably because it tries to sum- marize the entire document.</p><p>In the second document pair, document B is clearly not an outlier as the story is about a new book of AIDS. In comparison, document A dis- <ref type="table">Table 3</ref>: Case study of documents in "Health" corpus of NYT data set. We present several pairs of documents and how different methods rank the pair. The "Outlier" column indicates the document ranked higher in the outlier document ranking generated by the corresponding methods, and the row "Crowds" shows the ranking given by human evaluators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Outlier Document A Document B P2V-COS Doc B CHICAGO (AP) States with the most gun con- trol laws have the fewest gun-related deaths, ac- cording to a study that suggests sheer quantity of measures might make a difference ...</p><p>A prominent Scottish bagpiping school has warned pipers around to world to clean their instruments regularly after one of its longtime members nearly died of a lung infection ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VMF-E Doc A VMF-Q Doc A Crowds</head><p>Doc A P2V-COS Doc B ATLANTA There's more evidence that U.S. births may be leveling off after years of de- cline. The number of babies born last year only slipped a little, ...</p><p>Young men in a state prison for juveniles and professors of library science from the Univer- sity of South Carolina have joined forces to fight AIDS with a graphic novel ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VMF-E Doc B VMF-Q Doc A Crowds</head><p>Doc A cussing U.S. population is an outlier. However, a great part of document B is about the content of the book, which confuses baselines P2V and VMF- E, as both methods tend to summarize the entire document and highly relevant words like "AIDS" are overwhelmed by the majority of the document. The only method that agrees with human annota- tors is VMF-Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a novel task of detecting document outliers from a given corpus. We pro- pose a generative model to identify semantic fo- cuses of a corpus, each represented as a vMF dis- tribution in the embedded space. We also design a document outlierness measure. We experimen- tally verify the effectiveness of our methods. We hope this work provides insights for further stud- ies on outlier document texts in specific domains, and in more challenging settings such as detecting outliers from crowdsourced data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of a normal document and an outlier document in a news corpus ("Health" topic).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Performance of outlier document detection with different parameter configurations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig- ure 2(a) and 2(b) show that the performance is not very sensitive to different values of θ, as long as θ is sufficiently large (close to 1). Figure 2(c) and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Data set statistics.</head><label>1</label><figDesc></figDesc><table>Data set 
Corpus D 
External corpus De 
Avg. |D| Avg. |d| 
|De| 
Avg. |d| 
NYT 
4,662.11 
592.66 
52,114 
471.63 
ARNET 
2,930.60 
137.21 
11,463 
152.17 

of computer science up to 2013, collected by Ar-
netMiner (Tang et al., 2008), and assign each pa-
per into a field, according to Wikipedia 3 . We use 
papers from a set of domains to serve as an exter-
nal corpus D e , while papers in other domains form 
different corpora D = {D s }. Each domain (e.g., 
data mining, computational biology, and computer 
graphics) forms a corpus D s respectively. Again, 
notice that the corpora do not have overlapping 
documents with each other. 
A summary is presented in Table 1. 

Benchmark generation. Since we do not have 
true labels for outliers in a corpus, we use injec-
tion method to generate outlier detection bench-
mark. For each data set, we randomly select a cor-
pus D s ∈ D and mark all of its document as "nor-
mal documents". We then randomly select another 
corpus D 
s ∈ D, D 
s = D s , to inject ω documents 
from D 
s into D s and mark them as outliers. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Performance comparison of different out- lier document detection methods. All results are shown as percents.</head><label>2</label><figDesc></figDesc><table>Data set Method 
MAP Rcl@1%Rcl@2%Rcl@5% 

NYT 

TFIDF-COS 05.03 04.73 06.72 14.72 
P2V-COS 
22.07 23.45 44.64 66.18 
UNI-KL 
10.28 11.92 16.32 31.34 
TM-KL 
14.51 16.50 16.50 24.67 
VMF-SF 
33.70 31.03 44.45 62.60 
VMF-E 
36.57 35.91 49.41 67.56 
VMF-Q 
41.88 56.99 63.29 79.23 

ARNET 

TFIDF-COS 08.99 15.40 18.75 30.23 
P2V-COS 
07.39 10.51 14.78 24.14 
UNI-KL 
07.46 14.13 22.26 39.40 
TM-KL 
10.09 12.04 15.37 20.24 
VMF-SF 
10.69 12.05 22.58 44.51 
VMF-E 
10.51 12.67 25.92 45.37 
VMF-Q 
19.74 22.40 34.40 53.87 

</table></figure>

			<note place="foot" n="1"> Document frequency of a word (or phrase) is defined as number of documents where this word or phrase appears.</note>

			<note place="foot" n="2"> http://developer.nytimes.com/docs</note>

			<note place="foot" n="3"> https://en.wikipedia.org/wiki/List_ of_computer_science_conferences</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nonparametric spherical topic modeling with word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayhan</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ardavan</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="601" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">58</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Statistical applications of the poisson-binomial and conditional bernoulli distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica Sinica</title>
		<imprint>
			<biblScope unit="page" from="875" to="892" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gaussian LDA for topic models with word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="795" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Von misesfisher clustering models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="154" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unsupervised Detection of Anomalous Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Guthrie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>University of Sheffield</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Outlier detection for patient monitoring and alerting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milos</forename><surname>Hauskrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iyad</forename><surname>Batal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Visweswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clermont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="55" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey of outlier detection methodologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Victoria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Hodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="126" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Novel document detection for massive data streams using distributed dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Kasiviswanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Melville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM J. Res. Dev</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="9" to="10" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Online l1dictionary learning with application to novel document detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huahua</forename><surname>Shiva P Kasiviswanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Melville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2258" to="2266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mining quality phrases from massive text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1729" to="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Arnetminer: extraction and mining of academic social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A probabilistic model for online document clustering with application to novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1617" to="1624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Novelty and redundancy detection in adaptive filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
