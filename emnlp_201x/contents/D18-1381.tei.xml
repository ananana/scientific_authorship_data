<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attentive Gated Lexicon Reader with Contrastive Contextual Co-Attention for Sentiment Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Institute for Infocomm Research</orgName>
								<orgName type="institution">Nanyang Technological University ψ,δ A*Star</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Institute for Infocomm Research</orgName>
								<orgName type="institution">Nanyang Technological University ψ,δ A*Star</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Institute for Infocomm Research</orgName>
								<orgName type="institution">Nanyang Technological University ψ,δ A*Star</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu</forename><surname>Cheung Hui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Institute for Infocomm Research</orgName>
								<orgName type="institution">Nanyang Technological University ψ,δ A*Star</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Institute for Infocomm Research</orgName>
								<orgName type="institution">Nanyang Technological University ψ,δ A*Star</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attentive Gated Lexicon Reader with Contrastive Contextual Co-Attention for Sentiment Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3443" to="3453"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3443</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper proposes a new neural architecture that exploits readily available sentiment lexicon resources. The key idea is that that incorporating a word-level prior can aid in the representation learning process, eventually improving model performance. To this end, our model employs two distinctly unique components , i.e., (1) we introduce a lexicon-driven contextual attention mechanism to imbue lexicon words with long-range contextual information and (2), we introduce a contrastive co-attention mechanism that models contrasting polarities between all positive and negative words in a sentence. Via extensive experiments , we show that our approach outper-forms many other neural baselines on sentiment classification tasks on multiple benchmark datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Across the rich history of sentiment analysis re- search ( <ref type="bibr" target="#b10">Kim and Hovy, 2004;</ref><ref type="bibr" target="#b13">Liu, 2012;</ref><ref type="bibr" target="#b20">Pang et al., 2008)</ref>, sentiment lexicons have been exten- sively used as features for sentiment classification tasks. Lexicons, either handcrafted or algorithmi- cally generated, consist of words and their asso- ciated polarity score. For instance, lexicons as- sign a high positive score for the word 'excellent' but a negative score for the word 'terrible'. Tradi- tionally, the summation of lexicon scores has been treated as a reasonable heuristic estimate (or fea- ture) that is capable of supporting opinion mining applications. Throughout the years, plenty of lex- icon lists have been built for various specific do- mains or general purposes ( <ref type="bibr" target="#b9">Hu and Liu, 2004;</ref><ref type="bibr" target="#b17">Mohammad et al., 2013;</ref><ref type="bibr" target="#b39">Wilson et al., 2005</ref>). They are indeed valuable resources that should be ex- ploited. * Denotes equal contribution.</p><p>However, sentiment lexicons are in reality hardly useful without context. After all, the com- plexity and ambiguity of natural language pose great challenges for the crude bag-of-words gen- eralization of lexicons. Firstly, the concept of se- mantic compositionality is non-existent in simple lexicon approaches which raises problems when handling flipping negation (not happy), content word negation (ameliorates pain) or unbounded dependencies (no body passed the exam). Sec- ondly, lexicons also do not handle word sense, e.g., not being able to differentiate the meaning of hot in the phrases 'a hot, attractive person' and a 'a scorching hot day'. Thirdly, simple summa- tion over lexicon scores cannot deal with sentences with double contrasting polarities, e.g., the lexi- con polarity score of 'Thanks for making this un- comfortable situation more comfortable' becomes negative because uncomfortable has a higher neg- ative lexicon score over the positive score of the word comfortable. Lastly, strongly positive or negative words may occur in neutral context which forces an inclination of predictions towards a non- neutral polarity. As such, the exploitation of read- ily available lexicon lists is an inherently challeng- ing task.</p><p>Deep learning has demonstrated incredibly competitive performance in many NLP tasks ( <ref type="bibr" target="#b14">Liu et al., 2015;</ref><ref type="bibr" target="#b3">Bradbury et al., 2016;</ref><ref type="bibr" target="#b28">Tai et al., 2015</ref>). With no exception, the task of sentiment anal- ysis is recently also dominated by neural archi- tectures. It has been proven from the fact that the top systems from SemEval Sentiment analy- sis challenges (e.g., notably 2016 and 2017) have mainly leveraged the effectiveness of deep learn- ing models. The main advantage of deep learning approach is that it is effective in exploring both lin- guistic and semantic relations between words, thus can overcomes the problems of lexicon-based ap- proach. However, current deep learning approach for sentiment analysis usually faces with the ma- jor shortcoming, i.e., being limited by the quantity of high quality labeled data. Manual labeling of data, however, is costly and require domain expert knowledge which is not always available in prac- tice.</p><p>Given the pros and cons of previous two pre- vious approaches, we aim to combine the best of both worlds -the traditional sentiment lexicon and modern deep learning architectures. To the best of our knowledge, the only work that combines the two paradigms within end-to-end neural net- works is the Lexicon RNN model ( . In their approach, sentiment lexicons are extracted from the hidden states of a recurrent neural network and passed through a simple feed- forward neural network to produce a new polarity weight. This approach, however, has some limita- tions which will be illustrated using the following example:</p><p>"Thanks for making this horrible situation at work more bearable."</p><p>Firstly, the Lexicon RNN does not consider the interactions between positive or negative lexicon words, which makes it susceptible to misleading strong lexicon priors. In the above example, the word 'horrible' is a strongly negative word in most lexicons. As a result, the Lexicon RNN (and many other lexicon based approaches in general) will as- sign a negative polarity to the sentence. Clearly, modeling similarity between two contrasting po- larity words <ref type="bibr">('horrible' and 'bearable'</ref>) can help the model resolve this confusion. Secondly, the RNN encoder in the Lexicon RNN is restricted by the sequential nature of the recurrent model, re- sulting in a limited global view of the entire sen- tence. For example, the word pairs ('horrible', 'bearable') and ('thanks', 'bearable') are useful for detecting the polarity of the sentence but do not have any explicit interaction even with a se- quential RNN encoder. Moreover, the word pair ('thanks', 'bearable') is very far apart in the above example sentence, making it challenging for RNN encoders to capture interactions between them. Fi- nally, the Lexicon RNN faces difficulty dealing with more than two classes due to its design, i.e., linear combination of two scalar scores. In or- der to cope with this weakness, the authors define hardcoded dataset specific thresholds for 5-way classification. Adapting this to 3-way (positive, negative and neutral) is cumbersome as thresholds have to be found by either maximizing over the development set or defined heuristically.</p><p>In this paper, we introduce a new end to end paradigm that integrates lexicon information into neural network for the task of sentiment anal- ysis. More specifically, instead of learning a lexicon-based score, we propose to learn an aux- iliary embedding by exploiting lexicon informa- tion. The key motivation behind the auxiliary representation is that compositional learning with prior/global knowledge of positive and negative inclined words can lead to improved represen- tations. Next, a gating mechanism controls the additive blend between this lexicon-based repre- sentation and a standard attention-based recurrent model. In essence, this supporting network aims to learn a 'lexicon-based' view of the sentence and can be interpreted as 'learning to compose' by ex- ploiting lexicon information. Finally, instead of the combination of two scalar values (the base lex- icon score and sentence bias score) as in the Lex- icon RNN model, we propose to use the k-class softmax function at the final layer. Intuitively, it is a more natural solution for fine-grained senti- ment classification over the cumbersome tuning of ad-hoc threshold values. Our contributions can be summarized as follows:</p><p>• We propose to learn an auxiliary embedding by exploiting lexicon information rather than learning a lexicon-based score. Its design is a more natural and flexible solution for k-class sentiment classification.</p><p>• We propose a contextual attention (CA) mechanism that learns to attend to lexicon words based on the context. Unlike Lexi- con RNN which extracts the hidden repre- sentations from the recurrent model, contex- tual attention allows a wider, global and more complete view of the context (sentence) by matching against every single word in the sentence. In addition to semantic composi- tionality, our model also benefits from se- mantic similarity.</p><p>• We propose to model the interaction between the positive and negative lexicon words in- side the neural network. Positive and neg- ative lexicon words are modeled seperately and subsequently compared using contrasive co-attention (CC) which learns the relative importance of positive lexicons with respect to negative lexicons (and vice versa). Mod- eling such intricacies between positive and negative words allows our model to deal with scenarios such as contrasting polarities, neu- trality and also sarcasm. We also discover that our CC mechanism produces a neutraliz- ing effect which negates misleading attention on words with intense polarity even though the context is neutral.</p><p>Overall, we propose AGLR (Attentive Gated Lexicon Reader), a new attention-based neural architecture that exploits sentiment lexicons for learning to compose an auxiliary sentence embed- ding. Our model achieves state-of-the-art perfor- mance on several benchmark datasets. Finally, our AGLR, a single neural model, also achieves com- petitive performance with respect to top teams in SemEval runs which are mostly comprised of ex- tensively engineered ensembles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Sentiment lexicons have a rich traditional in sen- timent analysis research and have been exploited in many statistical methods across the years ( <ref type="bibr" target="#b9">Hu and Liu, 2004;</ref><ref type="bibr" target="#b10">Kim and Hovy, 2004;</ref><ref type="bibr" target="#b0">Agarwal et al., 2011;</ref><ref type="bibr" target="#b17">Mohammad et al., 2013;</ref><ref type="bibr">Tang et al., 2014b,a;</ref>. It is easy to see how sentiment lexicons are able to benefit opinion min- ing applications. More specifically, sentiment lex- icons form an integral role in the winning solutions of <ref type="bibr" target="#b19">SemEval 2013</ref><ref type="bibr" target="#b17">(Mohammad et al., 2013</ref><ref type="bibr" target="#b16">(Miura et al., 2014</ref>). In many of these these approaches, standard machine learning classifiers (such as Support Vector Machines) are trained on discrete features partly derived from resources such as sentiment lexicon.</p><p>In recent years, we see a shift of the state-of-the- art from discrete models to neural models <ref type="bibr" target="#b26">(Socher et al., 2013;</ref><ref type="bibr" target="#b11">Kim, 2014;</ref><ref type="bibr" target="#b5">Dong et al., 2014;</ref><ref type="bibr" target="#b29">Tang et al., 2016;</ref><ref type="bibr" target="#b28">Tai et al., 2015;</ref><ref type="bibr" target="#b22">Ren et al., 2016;</ref>. This ranges from learning sentiment-specific word em- beddings ( <ref type="bibr" target="#b31">Tang et al., 2014b;</ref><ref type="bibr" target="#b6">Faruqui et al., 2015)</ref> to end-to-end neural architectures ( <ref type="bibr" target="#b1">Angelidis and Lapata, 2017)</ref>. The win- ning solution of SemEval 2016 ( <ref type="bibr" target="#b4">Deriu et al., 2016</ref>) utilized ensembles of convolutional neural net- works (CNN). Recurrent-based models such as the bidirectional long short-term memory (BiLSTM) <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b7">Graves et al., 2013</ref>) are popular and standard strong baselines for many opinion mining tasks including senti- ment analysis <ref type="bibr" target="#b32">(Tay et al., 2017)</ref> and sarcasm detec- tion ( <ref type="bibr" target="#b35">Tay et al., 2018c)</ref>. These neural models such as the BiLSTM are capable of modeling seman- tic compositionality and produce a feature vector which can be used for classification.</p><p>To integrate the information of lexicon inside Lexicon RNN model,  pro- posed to use the hidden representations from a BiLSTM to influence the lexicon score, i.e., learn- ing context-sensitive lexicon features. However, our method can be considered as a vastly different paradigm and instead learns a d-dimensional em- bedding using neural attention ( <ref type="bibr" target="#b2">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b15">Luong et al., 2015</ref>) instead of a lexicon score. The key idea of neural attention is that it allows neural networks to look (or attend) to cer- tain words in a sequence. This concept has in- deed profoundly impacted the fields of NLP, giv- ing rise to many variant architectures including end-to-end memory networks ( <ref type="bibr" target="#b27">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b12">Li et al., 2017)</ref>.</p><p>Our approach draws inspiration from memory networks and co-attentive models for machine comprehension <ref type="bibr" target="#b25">Seo et al., 2016)</ref>. In fact, the auxiliary network can be inter- preted as a form of multi-layered attention which draws connection to vanilla memory networks. Attending over two sequences (or bidirectional at- tention) are intuitive approaches for NLP tasks such as information retrieval <ref type="bibr" target="#b34">(Tay et al., 2018b</ref>) and generic text matching ( <ref type="bibr" target="#b33">Tay et al., 2018a</ref>). In our work, we adapt this to model the similarities between (1) lexicon-context and (2) contrasting polarities which borrows inspiration from ( <ref type="bibr" target="#b23">Riloff et al., 2013</ref>). Since our matching problem is de- rived from the same sequence (identified by a lexicon prior), this work can be interpreted as a form of self-attention ( <ref type="bibr" target="#b38">Vaswani et al., 2017)</ref> which draws relations to the intra-attentive model for sar- casm detection ( <ref type="bibr" target="#b35">Tay et al., 2018c</ref>). co-attention layers. The latter is generated by a vanilla attention-based BiLSTM model. A gating mechanism then combines them for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Embedding Layer</head><p>Firstly, we extract all lexicon words from the in- put sequence and then separately 1 denote them as positive or negative words. Overall, our model ac- cepts three sequences as an input. (1) the origi- nal sentence, (2) a list of positive lexicon words found in the sentence and (3) a list of negative lex- icon words found in the sentence. The three se- quences are indexed into a word embedding layer W ∈ R |V |×d which outputs three matrices S ∈ R d×Ls (sentence embeddings), P ∈ R d×Gp (posi- tive lexicon embeddings) and N ∈ R d×Gn (nega- tive lexicon embeddings). d is the dimensionality of the word embeddings and L s , G p and G n are the maximum sequence lengths of sentence, posi- tive lexicon and negative lexicon respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning Sentence Representation</head><p>To learn sentence representations of the input se- quence, we pass S = (w 1 , w 2 · · · w Ls ) into a Bidirectional Long Short-Term Memory (LSTM) layer. As such, the output of the BiLSTM is de- scribed as follows:</p><formula xml:id="formula_0">h t = BiLST M (h t−1 , w t )<label>(1)</label></formula><p>where h t is the hidden representation at step t. Given a sequence of inputs w 1 , w 2 · · · w L , the out- put of the BiLSTM layer is a sequence of hidden states h 1 , h 2 · · · h L . Note that since we use a bidi- rectional LSTM, then h t ∈ R 2r where r is the di- mensionality of the BiLSTM layer. In our case r is set to d 2 such that the output vector has dimen- sionality d.</p><p>Sentence Attention To learn a final sentence representation of the sentence, we adopt an atten- tion mechanism. The attention mechanism is de- fined by the following equations:</p><formula xml:id="formula_1">Y = tanh(W y H)<label>(2)</label></formula><formula xml:id="formula_2">a c = sof tmax(w T y Y) and s = H a T c<label>(3)</label></formula><p>where s ∈ R d is the output sentence representa- tion, W y ∈ R d×d and w y ∈ R d are parameters of the attention layer. Intuitively, the attention layer learns to pay attention to important segments of the sentence, producing a weighted representation of the hidden states of the BiLSTM layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning Auxiliary Lexicon Embedding</head><p>This layer aims to learn a single d-dimensional lexicon-based representation of the sentence. In order to learn the lexicon embedding, our model adopts a two layer attention mechanism, namely the contextual attention (CA) and contrastive co- attention (CC).</p><p>Contextual Attention (CA) We utilize an atten- tion mechanism to learn the relative importance of each lexicon word based on the sentence represen- tation. This layer is applied to and is functionally identical for both P and N . As such, for notational convenience, we use Q to represent either positive (P ) or negative (N ), and G to represent the maxi- mum number of lexicon words. Let Q ∈ R G×d be a sequence of lexicon words and H ∈ R Ls×d be the intermediate hidden representations obtained from the contextual BiLSTM layer:</p><formula xml:id="formula_3">M = tanh(Q U H T )<label>(4)</label></formula><p>where U ∈ R d×d are the parameters of this layer. Next, we apply a column-wise max pooling of M .</p><p>The key idea is to generate an attention vector:</p><formula xml:id="formula_4">a = sm(max col (M )) ; c i = a i * q i<label>(5)</label></formula><p>where a ∈ R G . The softmax function normal- izes the values of the vector max col (M ) into a probability distribution. To learn the context- sensitive weight importance of each lexicon word, we then apply the attention vector on Q. C = {c 1 , c 2 · · · c G } is the context-sensitive lexicon rep- resentation of Q. Intuitively, the CA mechanism attends to each lexicon word based on its maxi- mum influence on each word of the main sentence. There are several advantages to our context at- tention mechanism. Unlike Lexicon RNN which simply extracts the hidden representation (gener- ated from BiLSTM) of the lexicon word, our ap- proach has a global view of the entire sentence which allows each lexicon word to benefit from wider contextual knowledge as opposed to being limited to the temporal compositionality provided by the BiLSTM layer. Overall, the outputs of this layer are two matrices (positive and negative lexicon embeddings) which are context-sensitive.  Note that these lexicon embeddings retain their di- mensionality passing through this layer.</p><p>Contrastive Co-Attention (CC) This layer aims to model the contrast between polarities. In- tuitively, this layer helps to model sentences with double or conflicting polarities. It also aims to negate strongly positive or negative words in the case of a neutral context. In order to do so, we em- ploy a contrastive co-attention model that learns to weight the relative importance of each positive lexicon word based on the negative lexicon (and vice versa). We accept the contextualized positive and negative lexicon embeddings from the previ- ous layer as an input. LetˆPLetˆ LetˆP ∈ R G×d be the contextualized positive lexicons andˆNandˆ andˆN ∈ R G×d be the contextualized negative lexicons, our co- attention layer learns a soft attention alignment be- tween positive and negative lexicon embeddings. Similar to our contextual attention layer, we first learn an affinity matrix Z that models the relation- ship between positive and negative lexicon embed- dings:</p><formula xml:id="formula_5">Z = tanh(P A N T )<label>(6)</label></formula><p>Next, we apply both column-wise and row-wise max-pooling on the affinity matrix Z to obtain two attention vectors. The two attention vectors are then normalized with the softmax function (de- noted as sm).</p><formula xml:id="formula_6">a p = sm(max col (Z)) ; a n = sm(max row (Z)) (7)</formula><p>a p is the attention vector for the positive lexicon embeddings and a n is the attention vector for the negative lexicon embeddings. The final vector rep- resentations are therefore:</p><formula xml:id="formula_7">p f = P a p ; n f = N a n<label>(8)</label></formula><p>where p f ∈ R d and n f ∈ R d are the vector rep- resentations for positive lexicon and negative lex- icon respectively. Note that this layer, unlike the contextual attention layer, is named 'co-attention' because both P and N are both 'attended over' concurrently. It is also good to note that attentions are applied over the original embeddings P, N and not the contextualized embeddingsˆPembeddingsˆ embeddingsˆP , ˆ N .</p><p>Fully-Connected Layer Next, we pass the concatenation of p and n through a fully- connected layer to learn the final representation for the auxiliary lexicon embedding, i.e., r = tanh(W h ([p; n]) + b h ) where W h ∈ R 2d×d are the parameters of the hidden layer and b h is the bias value. The output r ∈ R d is the final auxil- iary lexicon-based embedding.</p><p>Learning Final Representations To combine the lexicon-based representation with the sentence representation, we adopt a gating mechanism.</p><formula xml:id="formula_8">ˆ s = σ(w g r) r + (1 − σ(w g r)) s (9)</formula><p>where w g ∈ R d are the parameters of this layer, σ is the sigmoid function. ˆ s is the overall final representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Final Layer and Optimization</head><p>Finally, we passˆspassˆ passˆs the overall final representation into a softmax layer.</p><formula xml:id="formula_9">y = sof tmax(W f ˆ s + b f )<label>(10)</label></formula><p>where y ∈ R k , where k is the number of classes (2 for positive and negative and 3 including neu- tral). W f and b f are standard parameters of a lin- ear regression layer. For optimization, we adopt the standard cross entropy loss function with L2 regularization.</p><formula xml:id="formula_10">L = − N i=1 [y i log o i + (1 − y i ) log(1 − o i )] + R (11)</formula><p>where o is the output of the softmax layer and R = λ ψ 2 2 is the L2 regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical Evaluation</head><p>This section describes our empirical experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Procedure</head><p>In this section, we describe the datasets used, eval- uation metric and implementation details.</p><p>Datasets We conduct our experiments 2 on sub- sets of sentiment analysis benchmarks from Se- mEval 2013 <ref type="bibr" target="#b19">(Nakov et al., 2013)</ref>, <ref type="bibr" target="#b24">SemEval 2014</ref><ref type="bibr" target="#b24">(Rosenthal et al., 2014</ref>) and <ref type="bibr">SemEval 2016</ref><ref type="bibr" target="#b18">(Nakov et al., 2016</ref>). More specifically, we focus on the sentence level of sentiment analysis and evaluate on the datasets of SemEval 2013 task 2, SemEval 2014 task 9 and SemEval 2016 task 4, which we will name as SemEval13, SemEval14 and Se- mEval16 respectively in this section. For fair com- parison, we use the same setting of training, devel- opment and testing as in SemEval competitions.</p><p>To further evaluate the performance of methods when data is limited, for SemEval16, we experi- ment on two different training settings. The first, TRAIN, uses only the 2016 training set while the other, TRAIN-ALL, appends the 2013 training set to the 2016 training set, following the official set- ting of SemEval 2016 while TRAIN explores the setting where training data is limited.</p><p>Evaluation Metrics We evaluate on two set- tings, i.e., 3-way (positive, negative and neutral) and also binary (positive and negative) classifica- tion. We report the accuracy and macro-averaged F1 score for all settings.</p><p>Compared Baselines In this section, we list the neural baselines we use for comparisons.</p><p>• NBOW-MLP (Neural Bag-of-Words + Multi-layered Perceptron) is a simple sum of all word embeddings which is connected to a 2-layer MLP of 100 dimensions.</p><p>• CNN (Convolutional Neural Network) is an- other popular neural encoder for learning sentence representations. We use a filter size of 3 and 150 filters.</p><p>• BiLSTM (Bidirectional Long Short-Term Memory) is a standard strong neural baseline for many NLP tasks. The size of the LSTM is set to 150.</p><p>• AT-BiLSTM (Attention-based BiLSTM) is an extension of the BiLSTM model with neu- ral attention.</p><p>• Lexicon RNN (Lexicon Recurrent Neural Network) is the model of ( . The first neural model that incorporates sen- timent lexicon. The size of the BiLSTM in this model is also set to 150.</p><p>All models except Lexicon RNN optimize the softmax cross entropy loss. The authors use Lex- icon RNN for binary and 5-way classification. In order to adapt Lexicon RNN to 3-way classifica- tion (positive, negative, neutral), we adapt the 5- way formulation that minimizes the MSE (mean square error) loss to 3-way. The output is scaled 3 to s ∈ [−1, 1] where s &gt; 0.25 is treated as posi- tive, s &lt; −0.25 is treated as negative and every- thing in between is neutral.    <ref type="table">Table 1 and Table 2</ref> report the results of our ex- periments. The results on TRAIN-ALL are higher than TRAIN for SemEval16 in lieu of the larger dataset. Firstly, we observe that our proposed AGLR outperforms all neural baselines on 3-way classification. The overall performance of AGLR achieves state-of-the-art performance. On aver- age, AGLR outperforms Lexicon RNN and AT- BiLSTM by 1% − 3% in terms of F1 score. We also observe that AGLR always improves AT- BiLSTM which ascertains the effectiveness of learning auxiliary lexicon embeddings. The key idea here is that the auxiliary lexicon embeddings provide a different view of the sentence which sup- ports the network in making predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>We also observe that Lexicon RNN does not handle 3-way classification well. Even though it has achieved good performance on binary classi- fication, the performance on 3-way classification is lackluster (the performance of AGLR outper- forms Lexicon RNN by up to 8% on SemEval16 TRAIN). This could also be attributed to the MSE based loss function. Conversely, by learning an auxiliary embedding (instead of a scalar score), our model becomes more flexible at the final layer and can be adapted to using a k softmax func- tion. Finally, we observe that BiLSTM and AT- BiLSTM outperform Lexicon RNN on average with Lexicon RNN being slightly better on binary classification. <ref type="table" target="#tab_4">Table 3</ref> reports the results of our proposed ap- proach against the top team of each SemEval run, i.e., NRC-Canada ( <ref type="bibr" target="#b17">Mohammad et al., 2013</ref>) for 2013 <ref type="bibr">Task 2, Team-X (Miura et al., 2014</ref>) for 2014 <ref type="bibr">Task 9, SwissCheese (Deriu et al., 2016</ref>) for 2016 Task 4. We follow the exact training datasets al- lowed for each SemEval run. Following the com- petition setting, with the exception of accuracy for SemEval 2016, all metrics reported are the macro averaged F1 score of positive and negative classes.  We observe that AGLR achieves competitive performance relative to the top runs in <ref type="bibr" target="#b19">SemEval 2013</ref><ref type="bibr" target="#b24">SemEval , 2014</ref><ref type="bibr">SemEval and 2016</ref>. It is good to note that Se- mEval approaches are often heavily engineered containing ensembles and many handcrafted fea- tures which include extensive use of sentiment lexicons, POS tags and negation detectors. Re- cent SemEval runs gravitate towards neural en- sembles. For instance, the winning approach for SwissCheese (SemEval 2016) uses an ensemble of 6 CNN models along with a meta-classifier (ran- dom forest classifier). On the other hand, our pro- posed model is a single neural model. In addi- tion, SwissCheese also uses emoticon-based dis- tant supervision which exploits a huge corpus of sentences (millions) for training. Conversely, our approach only uses the 2013 and 2016 training sets which are significantly smaller. Given these condi- tions, we find it remarkable that our single model is able to achieve competitive performance relative to the extensively engineered approach of Swiss- Cheese. Moreover, we actually outperform sig- nificantly in terms of pure accuracy. AGLR per- forms competitively on SemEval 2013 and 2014 as well. The good performance on the sarcasm dataset could be attributed to our contrastive at- tention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons against Top SemEval Systems</head><p>Ablation Study In this section, we study the im- pacts and contribution of the different components of our model. Specifically, we tested 3 settings. The first, we removed CC only. In this case, pos- itive and negative lexicons are summed instead of a weighted summed using attention. In the next setting, we removed CA only. Similarly, embed- dings are summed instead of attentively summed. Finally, we removed both CA and CC. In this case, all lexicons are considered neural bag-of-words (NBOW) and passed to the MLP layer. <ref type="table" target="#tab_6">Table  4</ref> shows the results of this ablation study on Se- mEval16 using the TRAIN-ALL setting.  It is clear that both CC and CA are critical to the performance of AGLR. Removing either or both can cause performance to degrade. In partic- ular, we also observe that CA seems to be less im- portant than CC, i.e., performance drops more as compared to removing CA. We also note that re- moving both and a simple NBOW for lexicons can degrade performance since the base AT-BiLSTM is better than using NBOW lexicons as an auxil- iary support. As such, the design of the auxiliary embeddings must be treated with care.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Qualitative Analysis In order to study what are the specific roles of the contextual and con- trastive attention mechanism, we inspect the at- tention maps over the positive and negative lexi- cons. We use the following example in which the ground truth label is positive: "Very excited about Tuesday night @user free iced coffee and smooth- ies courtesy of Dunkin Donuts will be set up.". <ref type="figure" target="#fig_1">Figure 2a</ref> shows the attention maps for contex- tual attention. We observe that contextual atten- tion focuses more on the context, i.e., focusing on words such as 'night', 'iced coffee' and 'smooth- ies'. On the other hand, <ref type="figure" target="#fig_1">Figure 2b</ref> shows the atten- tion maps after contrastive attention. We observe that contrastive attention learns more polarity spe- cific attentions, i.e., shifting some focus to 'very excited'. We also observe that the contrastive at- tention tends to shift its attention weights to less meaningful words for the negative lexicon if the ground truth label is positive (and vice versa). We believe that this indicates that there is an absence of negative sentiment.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a novel method of incorporating lex- icons into neural models for the task of senti- ment analysis. More specifically, we learn an aux- iliary lexicon embedding using neural attention. Our proposed model AGLR achieves an overall state-of-the-art performance on multiple bench- mark datasets outperforming strong neural base- lines such as AT-BiLSTM and Lexicon RNN. The performance of AGLR is also competitive relative to top SemEval systems which utilized neural en- sembles or very extensive feature engineering.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of our proposed Attentive Gated Lexicon Reader model (best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 :</head><label>2</label><figDesc>Experimental results on Sem2016 with two training settings TRAIN and TRAIN-ALL. Implementation Details Text are lowercased, tokenized using NLTK's tweet tokenizer and padded to the maximum sequence length of the dataset. For Lexicon RNN and AGLR, we use 4 the ST140 (Sentiment140) lexicon which was created by distant supervision (Mohammad et al., 2013). The maximum numbers of positive lexicons and negative lexicons extracted per sample are tuned amongst {5, 8, 10}. Models are trained for a max- imum of 30 epochs with early stopping if the per- formance on the development set does not improve after 5 epochs. Results reported are the test scores from the model that performed best on the de- velopment set. The batch size is tuned amongst {50, 100, 300}. L2 Regularization tuned amongst {10 −6 , 10 −7 , 10 −8 }. Dropout is set to 0.5. We optimized all networks with the RMSprop opti- mizer and with initial learning rate tuned amongst {0.01, 0.005, 0.001}. Word embeddings are ini- tialized with Glove 27B (Pennington et al., 2014) (d = 200) trained on tweets and are trainable pa- rameters. The size of the BiLSTM is d = 100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc>a) Attention over positive and negative lexicons for contextual attention. (b) Attention over positive and negative lexicons for contrastive attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of Contextual Attention and Contrastive Co-Attention.</figDesc><graphic url="image-2.png" coords="9,72.00,256.18,226.77,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparisons against top SemEval sys-
tems. Results reported are the F P N metric scores 
used in the SemEval tasks. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Ablation study on SemEval16 (TRAIN-
ALL) 

</table></figure>

			<note place="foot" n="3"> Attentive Gated Lexicon Reader In this section, we describe our proposed deep learning model for sentiment classification. The key idea of our model is to generate two representations, i.e., a lexicon-based auxiliary embedding of the sentence and also a generic compositional representation of the sentence. The former is generated via a supporting network that consists of contextual attention and contrastive</note>

			<note place="foot" n="1"> For our experiments, we mainly use ST140 lexicon and therefore use score &gt; 0 to separate positive and negative words. Notably, about ≈ 85% of all words in the sentence has a lexicon assignment.</note>

			<note place="foot" n="2"> SemEval 2015 was omitted due to space in favor of SemEval 2016 since testing sets are significantly larger in the latter.</note>

			<note place="foot" n="3"> We experimented with other thresholds but found 0.25 to work the best.</note>

			<note place="foot" n="4"> We also used Bing Liu&apos;s opinion lexicon but found it to perform slightly worse.</note>

			<note place="foot">Meishan Zhang, Yue Zhang, and Duy-Tin Vo. 2016. Gated neural networks for targeted sentiment analysis.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sentiment analysis of twitter data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilia</forename><surname>Vovsha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Passonneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on languages in social media</title>
		<meeting>the workshop on languages in social media</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="30" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multiple instance learning networks for fine-grained sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Angelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09645</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Quasi-recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>CoRR abs/1611.01576</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Swisscheese at semeval-2016 task 4: Sentiment classification using an ensemble of convolutional neural networks with distant supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Deriu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Gonzenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Uzdilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valeria</forename><forename type="middle">De</forename><surname>Luca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adaptive recursive neural network for target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sujay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing (icassp), 2013 ieee international conference on</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="doi">10.1145/1014052.1014073</idno>
		<ptr target="https://doi.org/10.1145/1014052.1014073" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-08-22" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Determining the sentiment of opinions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Soo-</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">1367</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep memory networks for attitude identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, WSDM 2017</title>
		<meeting>the Tenth ACM International Conference on Web Search and Data Mining, WSDM 2017<address><addrLine>Cambridge, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-02-06" />
			<biblScope unit="page" from="671" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis lectures on human language technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="167" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fine-grained opinion mining with recurrent neural networks and word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shafiq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><forename type="middle">M</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meng</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D/D15/D15-1168.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="1433" to="1443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Teamx: A sentiment analyzer with enhanced lexicon mapping and weighting scheme for unbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhide</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeyuki</forename><surname>Sakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keigo</forename><surname>Hattori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Ohkuma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Nrc-canada: Building the stateof-the-art in sentiment analysis of tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.6242</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semeval2016 task 4: Sentiment analysis in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval@ NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semeval-2013 task 2: Sentiment analysis in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Semantic Evaluation</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="312" to="320" />
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (* SEM)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Opinion mining and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="135" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Improving twitter sentiment classification using topic-enriched multi-prototype word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Sarcasm as contrast between a positive sentiment and negative situation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashequl</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Surve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lalindra De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihong</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 9: Sentiment analysis in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S14-2009" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics and Dublin City University</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-712" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Effective lstms for target-dependent sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-11" />
			<biblScope unit="page" from="3298" to="3307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Building large-scale twitter-specific sentiment lexicon: A representation learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning sentimentspecific word embedding for twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning to attend via word-aspect associative fusion for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Hui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05403</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hermitian co-attention networks for text matching in asymmetrical domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4425" to="4431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Multi-cast attention networks for retrieval-based question answering and response prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00778</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02856</idno>
		<title level="m">Reasoning with sarcasm by reading inbetween</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Context-sensitive lexicon features for neural sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duy-Tin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="1629" to="1638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Bidirectional tree-structured lstm with head lexicalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06788</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recognizing contextual polarity in phrase-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT/EMNLP 2005, Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference</title>
		<meeting><address><addrLine>British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2005-06-08" />
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>CoRR abs/1611.01604</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
