<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
							<email>miguel.ballesteros@upf.edu,</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♠</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<email>nasmith@cs.washington.edu</email>
							<affiliation key="aff3">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">♦ NLP Group</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Pompeu Fabra University</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1744" to="1753"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce two first-order graph-based dependency parsers achieving a new state of the art. The first is a consensus parser built from an ensemble of independently trained greedy LSTM transition-based parsers with different random initializations. We cast this approach as minimum Bayes risk decoding (under the Hamming cost) and argue that weaker consensus within the ensemble is a useful signal of difficulty or ambiguity. The second parser is a &quot;distillation&quot; of the ensemble into a single model. We train the distillation parser using a structured hinge loss objective with a novel cost that incorporates ensemble uncertainty estimates for each possible attachment , thereby avoiding the intractable cross-entropy computations required by applying standard distillation objectives to problems with structured outputs. The first-order distillation parser matches or surpasses the state of the art on English, Chinese, and German.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural network dependency parsers achieve state of the art performance ( <ref type="bibr" target="#b39">Weiss et al., 2015;</ref><ref type="bibr" target="#b0">Andor et al., 2016)</ref>, but training them in- volves gradient descent on non-convex objectives, which is unstable with respect to initial parameter values. For some tasks, an ensemble of neural net- works from different random initializations has been found to improve performance over individual mod- els ( <ref type="bibr" target="#b31">Sutskever et al., 2014;</ref><ref type="bibr">Vinyals et al., 2015, inter alia)</ref>. In §3, we apply this idea to build a first- order graph-based (FOG) ensemble parser <ref type="bibr" target="#b28">(Sagae and Lavie, 2006</ref>) that seeks consensus among 20 randomly-initialized stack LSTM parsers , achieving nearly the best-reported per- formance on the standard Penn Treebank Stanford dependencies task (94.51 UAS, 92.70 LAS).</p><p>We give a probabilistic interpretation to the en- semble parser (with a minor modification), viewing it as an instance of minimum Bayes risk inference. We propose that disagreements among the ensem- ble's members may be taken as a signal that an at- tachment decision is difficult or ambiguous.</p><p>Ensemble parsing is not a practical solution, how- ever, since an ensemble of N parsers requires N times as much computation, plus the runtime of find- ing consensus. We address this issue in §5 by distill- ing the ensemble into a single FOG parser with dis- criminative training by defining a new cost function, inspired by the notion of "soft targets" ( . The essential idea is to derive the cost of each possible attachment from the ensemble's division of votes, and use this cost in discriminative learning. The application of distilliation to structured predic- tion is, to our knowledge, new, as is the idea of em- pirically estimating cost functions.</p><p>The distilled model performs almost as well as the ensemble consensus and much better than (i) a strong LSTM FOG parser trained using the con- ventional Hamming cost function, (ii) recently pub- lished strong LSTM FOG parsers <ref type="bibr" target="#b15">(Kiperwasser and Goldberg, 2016;</ref><ref type="bibr" target="#b38">Wang and Chang, 2016)</ref>, and (iii) many higher-order graph-based parsers ( <ref type="bibr" target="#b16">Koo and Collins, 2010;</ref><ref type="bibr" target="#b22">Martins et al., 2013;</ref><ref type="bibr" target="#b17">Le and Zuidema, 2014)</ref>. It represents a new state of the art for graph- based dependency parsing for English, Chinese, and German. The code to reproduce our results is pub- licly available. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Notation and Definitions</head><p>Let x = x 1 , . . . , x n denote an n-length sentence. A dependency parse for x, denoted y, is a set of tuples <ref type="bibr">(h, m, )</ref>, where h is the index of a head, m the index of a modifier, and a dependency label (or relation type). Most dependency parsers are con- strained to return y that form a directed tree.</p><p>A first-order graph-based (FOG; also known as "arc-factored") dependency parser exactly solvesˆy solvesˆ solvesˆy(x) = arg max</p><formula xml:id="formula_0">y∈T (x) (h,m)∈y s(h, m, x) S(y,x)</formula><p>, <ref type="formula">(1)</ref> where T (x) is the set of directed trees over x, and s is a local scoring function that considers only a single dependency arc at a time. (We suppress de- pendency labels; there are various ways to incorpo- rate them, discussed later.) To define s, <ref type="bibr" target="#b23">McDonald et al. (2005a)</ref> used hand-engineered features of the surrounding and in-between context of x h and x m ; more recently, Kiperwasser and Goldberg (2016) used a bidirectional LSTM followed by a single hid- den layer with non-linearity.</p><p>The exact solution to Eq. 1 can be found using a minimum (directed) spanning tree algorithm <ref type="bibr" target="#b24">(McDonald et al., 2005b</ref>) or, under a projectivity con- straint, a dynamic programming algorithm <ref type="bibr" target="#b9">(Eisner, 1996)</ref>, in O(n 2 ) or O(n 3 ) runtime, respectively. We refer to parsing with a minimum spanning tree algo- rithm as MST parsing.</p><p>An alternative that runs in linear time is transition-based parsing, which recasts parsing as a sequence of actions that manipulate auxiliary data structures to incrementally build a parse tree <ref type="bibr" target="#b26">(Nivre, 2003)</ref>. Such parsers can return a solution in a faster O(n) asymptotic runtime. Unlike FOG parsers, transition-based parsers allow the use of scoring functions with history-based features, so that attach- ment decisions can interact more freely; the best per- forming parser at the time of this writing employ neural networks <ref type="bibr" target="#b0">(Andor et al., 2016</ref>).</p><p>Let h y (m) denote the parent of x m in y (using a special null symbol when m is the root of the tree), and h y (m) denotes the parent of x m in the pre- dicted tree y . Given two dependency parses of the same sentence, y and y , the Hamming cost is</p><formula xml:id="formula_1">C H (y, y ) = n m=1 0 if h y (m) = h y (m) 1 otherwise</formula><p>This cost underlies the standard dependency pars- ing evaluation scores (unlabeled and labeled attach- ment scores, henceforth UAS and LAS). More gen- erally, a cost function C maps pairs of parses for the same sentence to non-negative values interpreted as the cost of mistaking one for the other, and a first- order cost function (FOC) is one that decomposes by attachments, like the Hamming cost. Given a cost function C and a probabilistic model that defines p(y | x), minimum Bayes risk (MBR) decoding is defined byˆy</p><formula xml:id="formula_2">byˆ byˆy MBR (x) = arg min y∈T (x) y ∈T (x) p(y | x) · C(y, y ) = arg min y∈T (x) E p(Y |x) [C(y, Y )].<label>(2)</label></formula><p>Under the Hamming cost, MBR parsing equates al- gorithmically to FOG parsing with s(h, m, x) = p((h, m) ∈ Y | x), the posterior marginal of the attachment under p. This is shown by linearity of expectation; see also <ref type="bibr" target="#b34">Titov and Henderson (2006)</ref>. Apart from MBR decoding, cost functions are also used for discriminative training of a parser. For example, suppose we seek to estimate the param- eters θ of scoring function S θ . One approach is to minimize the structured hinge loss of a training dataset D with respect to θ:</p><formula xml:id="formula_3">min θ (x,y)∈D [ − S θ (y, x) + max y ∈T (x) S θ (y , x) + C(y , y) ]<label>(3)</label></formula><p>Intuitively, this amounts to finding parameters that separate the model score of the correct parse from any wrong parse by a distance proportional to the cost of the wrong parse. With regularization, this is equivalent to the structured support vector machine <ref type="bibr" target="#b33">(Taskar et al., 2005;</ref><ref type="bibr" target="#b36">Tsochantaridis et al., 2005)</ref>, and if S θ is (sub)differentiable, many algorithms are available. Variants have been used extensively in training graph-based parsers <ref type="bibr" target="#b24">(McDonald et al., 2005b;</ref><ref type="bibr" target="#b21">Martins et al., 2009)</ref>, which typically make use of Hamming cost, so that the inner max can be solved efficiently using FOG parsing with a slightly revised local scoring function:</p><formula xml:id="formula_4">s (h, m, x) = s(h, m, x) + 0 if (h, m) ∈ y 1 otherwise (4)</formula><p>Plugging this into Eq. 1 is known as cost- augmented parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Consensus and Minimum Bayes Risk</head><p>Despite the recent success of neural network depen- dency parsers, most prior works exclusively report single-model performance. Ensembling neural net- work models trained from different random start- ing points is a standard technique in a variety of problems, such as machine translation <ref type="bibr" target="#b31">(Sutskever et al., 2014</ref>) and constituency parsing ( . We aim to investigate the benefit of en- sembling independently trained neural network de- pendency parsers by applying the parser ensembling method of <ref type="bibr" target="#b28">Sagae and Lavie (2006)</ref> to a collection of N strong neural network base parsers.</p><p>Here, each base parser is an instance of the greedy, transition-based parser of , known as the stack LSTM parser, trained from a different random initial estimate. Given a sen- tence x, the consensus FOG parser (Eq. 1) defines score s(h, m, x) as the number of base parsers that include the attachment (h, m), which we denote votes(h, m). <ref type="bibr">2</ref> An example of this scoring function with an ensemble of 20 models is shown in <ref type="figure" target="#fig_0">Figure 1</ref> We assign to dependency (h, m) the label most fre- quently selected by the base parsers that attach m to h.</p><p>Next, note that if we let s(h, m, x) = votes(h, m)/N , this has no effect on the parser (we have only scaled by a constant factor). We can there- fore view s as a posterior marginal, and the ensemble parser as an MBR parser (Eq. 2). <ref type="bibr">2</ref> An alternative to building an ensemble of stack LSTM parsers in this way would be to average the softmax decisions at each timestep (transition), similar to .</p><p>John saw the woman with a telescope   <ref type="table" target="#tab_0">Table 1</ref> shows that ensembles, even with small N , strongly outperform a single stack LSTM parser. Our ensembles of greedy, locally normalized parsers perform comparably to the best previously reported, due to <ref type="bibr" target="#b0">Andor et al. (2016)</ref>, which uses a beam (width 32) for training and decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">What is Ensemble Uncertainty?</head><p>While previous works have already demonstrated the merit of ensembling in dependency parsing ( <ref type="bibr" target="#b28">Sagae and Lavie, 2006;</ref><ref type="bibr" target="#b30">Surdeanu and Manning, 2010)</ref>, usually with diverse base parsers, we con- sider whether the posterior marginals estimated byˆp byˆ byˆp((h, m) ∈ Y | x) = votes(h, m)/N can be in- terpreted. We conjecture that disagreement among base parsers about where to attach x m (i.e., uncer- tainty in the posterior) is a sign of difficulty or am-Sentence: It will go for work ranging from refinery modification to changes in the distribution system, including the way service stations pump fuel into cars.  biguity. If this is true, then the ensemble provides information about which confusions are more or less reasonable-information we will exploit in our dis- tilled parser ( §5). A complete linguistic study is out of scope here; instead, we provide a motivating example before empirically validating our conjecture. <ref type="table" target="#tab_2">Table 2</ref> shows an example where there is considerable disagree- ment among base parsers over the attachment of a word (including). We invite the reader to attempt to select the correct attachment and gauge the difficulty of doing so, before reading on.</p><p>Regardless of whether our intuition that this is an inherently difficult and perhaps ambiguous case is correct, it is uncontroversial to say that the words in the sentence not listed, which received zero votes (e.g., both instances of the), are obviously implausi- ble attachments.</p><p>Our next idea is to transform ensemble uncer- tainty into a new estimate of cost-a replacement</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Distilling the Ensemble</head><p>Despite its state of the art performance, our ensem- ble requires N parsing calls to decode each sen- tence. To reduce the computational cost, we intro- duce a method for "distilling" the ensemble's knowl- edge into a single parser, making use of a novel cost function to communicate this knowledge from the ensemble to the distilled model. While models that combine the outputs of other parsing models have been proposed before ( <ref type="bibr" target="#b20">Martins et al., 2008;</ref><ref type="bibr" target="#b25">Nivre and McDonald, 2008;</ref><ref type="bibr" target="#b42">Zhang and Clark, 2008</ref>, in- ter alia), these works incorporated the scores or out- puts of the baseline parsers as features and as such require running the first-stage models at test-time. Creating a cost function from a data analysis proce- dure is, to our knowledge, a new idea.</p><p>The idea is attractive because cost functions are model-agnostic; they can be used with any parser amenable to discriminative training. Further, only the training procedure changes; parsing at test time does not require consulting the ensemble at all, avoiding the costly application of the N parsers to new data, unlike model combination techniques like stacking and beam search.</p><p>Distilling an ensemble of classifiers into one sim- pler classifer that behaves similarly is due to Bucilˇa <ref type="bibr" target="#b5">Bucilˇa et al. (2006)</ref> and ; they were likewise motivated by a desire to create a simpler model that was cheaper to run at test time. In their work, the ensemble provides a probability distribu- tion over labels for each input, and this predicted distribution serves as the training target for the dis- tilled model (a sum of two cross entropies objective is used, one targeting the empirical training distribu- tion and the other targeting the ensemble's posterior distribution). This can be contrasted with the super- vision provided by the training data alone, which conventionally provides a single correct label for each instance. These are respectively called "soft" and "hard" targets.</p><p>We propose a novel adaptation of the soft target idea to the structured output case. Since a sentence <ref type="table">Table 3</ref>: Example of soft targets (taken from our 20- model ensemble's uncertainty on the sentence) and hard targets (taken from the gold standard) for possible parents of with. The soft target corresponds with the posterior (second column) in <ref type="table" target="#tab_2">Table 2</ref>, but the hard target differs from the Hamming cost (last column of <ref type="table" target="#tab_2">Table 2</ref>) since the hard target assigns a value of 1 to the correct answer and 0 to all others (the reverse is true for Hamming cost).</p><note type="other">Sentence: John saw the woman with a telescope x h soft hard John 0.0 0 saw 0.95 1 the 0.0 0 woman 0.05 0 a 0.0 0 telescope 0.0 0</note><p>has an exponential (in its length) number of parses, representing the posterior distribution over parses predicted by the ensemble is nontrivial. We solve this problem by taking a single parse from each model, representing the N -sized ensemble's parse distribution using N samples. Second, rather than considering uncertainty at the level of complete parse trees (which would be anal- ogous to the classification case) or larger structures, we instead consider uncertainty about individual at- tachments, and seek to "soften" the attachment tar- gets used in training the parser. An illustration for the prepositional phrase attachment ambiguity in <ref type="figure" target="#fig_0">Fig. 1</ref>, taken from the ensemble output for the sen- tence, is shown in <ref type="table">Table 3</ref>. Soft targets allow us to encode the notion that mistaking woman as the par- ent of with is less bad than attaching with to John or telescope. Hard targets alone do not capture this information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Distillation Cost Function</head><p>The natural place to exploit this additional informa- tion when training a parser is in the cost function. When incorporated into discriminative training, the Hamming cost encodes hard targets: the correct at- tachment should receive a higher score than all in- correct ones, with the same margin. Our distillation cost function aims to reduce the cost of decisions that-based on the ensemble uncertainty-appear to be more difficult, or where there may be multiple plausible attachments.</p><p>Let</p><formula xml:id="formula_5">π(h, m) = 1 − ˆ p((h, m) ∈ Y | x) = N − votes(h, m) N .</formula><p>Our new cost function is defined by C D (y, y ) =</p><formula xml:id="formula_6">n m=1 max 0, π(h y (m), m) − π(h y (m), m) = n m=1 max 0, ˆ p(h y (m), m) − ˆ p(h y (m), m) .<label>(5)</label></formula><p>Recall that y denotes the correct parse, according to the training data, and y is a candidate parse.</p><p>This function has several attractive properties:</p><p>1. When a word x m has more than one plausi- ble (according to the ensemble) but incorrect (according to the annotations) attachment, each one has a diminished cost (relative to Hamming cost and all implausible attachments). 2. The correct attachment (according to the gold- standard training data) always has zero cost since h y (m) = h y (m) and Eq. 5 cancels out. 3. When the ensemble is confident, cost for its choice(s) is lower than it would be under Ham- ming cost-even when the ensemble is wrong. This means that we are largely training the dis- tilled parser to simulate the ensemble, includ- ing mistakes and correct predictions. This en- courages the model to replicate the state of the art ensemble performance. 4. Further, when the ensemble is perfectly con- fident and correct, every incorrect attachment has a cost of 1, just as in Hamming cost. 5. The cost of any attachment is bounded above by the proportion of votes assigned to the cor- rect attachment.</p><p>One way to understand this cost function is to imagine that it gives the parser more ways to achieve a zero-cost 5 attachment. The first is to correctly at- tach a word to its correct parent. The second is to predict a parent that the ensemble prefers to the correct parent, i.e., π(h y (m), m) &lt; π(h y (m), m). Any other decision will incur a non-zero cost that is proportional to the implausibility of the attachment, according to the ensemble. Hence the model is su- pervised both by the hard targets in the training data annotations and the soft targets from the ensemble.</p><p>While it may seem counter-intuitive to place zero cost on an incorrect attachment, recall that the cost is merely a margin that must separate the scores of parses containing correct and incorrect arcs. In contrast, the loss (in our case, the structured hinge loss) is the "penalty" the learner tries to minimize while training the graph-based parser, which de- pends on both the cost and model score as defined in Equation 3. When an incorrect arc is preferred by the ensemble over the gold arc (hence assigned a cost/margin of 0), the model will still incur a loss if s(h y (m), m, x) &lt; s(h y (m), m, x). In other words, the score of any incorrect arc (including those strongly preferred by the ensemble) cannot be higher than the score of the gold arc.</p><p>The learner only incurs 0 loss if s(h y (m), m, x) ≥ s(h y (m), m, x). This means that the gold score and the predicted score can have a margin of 0 (i.e., have the same score and incur no loss) when the ensemble is highly confident of that prediction, but the score of the correct parse cannot be lower regardless of how confident the ensemble is (hence the objective does not encourage incorrect trees at the expense of gold ones).</p><p>In the example in <ref type="table" target="#tab_2">Table 2</ref>, we show the (additive) contribution to the distillation cost by each attach- ment decision (column labeled "new cost"). Note that more plausible attachments according to the en- semble have a lower cost than less plausible ones (e.g., the cost for modification is less than system, though both are incorrect). While in the last line sta- tions received no votes in the ensemble (implausible attachment), its contribution to the cost is bounded by the proportion of votes for correct attachment. The intuition is that, when the ensemble is not cer- tain of the correct answer, it should not assign a large cost to implausible attachments. In contrast, Ham- ming cost would assign a cost of 1 (column labeled "Hamming") in all incorrect cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Distilled Parser</head><p>Our distilled parser is trained discriminatively with the structured hinge loss <ref type="bibr">(Eq. 3)</ref>. This is a natural choice because it makes the cost function explicit and central to learning. <ref type="bibr">6</ref> Further, because our en- semble's posterior gives us information about each attachment individually, the cost function we con- struct can be first-order, which simplifies training with exact inference.</p><p>This approach to training a model is well- studied for a FOG parser, but not for a transition- based parser, which is comprised of a collection of classifiers trained to choose good sequences of transitions-not to score whole trees for good at- tachment accuracy. Transition-based approaches are therefore unsuitable for our proposed distillation cost function, even though they are asymptotically faster. We proceed with a FOG parser (with Eis- ner's algorithm for English and Chinese, and MST for German since it contains a considerable number of non-projective trees) as the distilled model.</p><p>Concretely, we use a bidirectional LSTM fol- lowed by a hidden layer of non-linearity to calculate the scoring function s(h, m, x), following <ref type="bibr" target="#b15">Kiperwasser and Goldberg (2016)</ref> with minor modifica- tions. The bidirectional LSTM maps each word x i to a vector ¯ x i that embeds the word in context (i.e., x 1:i−1 and x i+1:n ). Local attachment scores are given by:</p><formula xml:id="formula_7">s(h, m, x) = v tanh (W[¯ x h ; ¯ x m ] + b)<label>(6)</label></formula><p>where the model parameters are v, W, and b, plus the bidirectional LSTM parameters. We will refer to this parsing model as neural FOG. Our model architecture is nearly identical to that of <ref type="bibr" target="#b15">Kiperwasser and Goldberg (2016)</ref>, with two pri- mary differences. The first difference is that we fix the pretrained word embeddings and compose them with learned embeddings and POS tag embeddings ( ), allowing the model to simulta- neously leverage pretrained vectors and learn a task- specific representation. <ref type="bibr">7</ref> Unlike Kiperwasser and Goldberg (2016), we did not observe any degrada- tion by incorporating the pretrained vectors. Second, we apply a per-epoch learning rate decay of 0.05 to the Adam optimizer. While the Adam optimizer au- tomatically adjusts the global learning rate accord- ing to past gradient magnitudes, we find that this ad- ditional per-epoch decay consistently improves per- formance across all settings and languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We ran experiments on the English PTB-SD version 3.3.0, Penn Chinese Treebank ( <ref type="bibr" target="#b40">Xue et al., 2002</ref>), and German CoNLL 2009 <ref type="figure">(Hajič et al., 2009)</ref> tasks.</p><p>Experimental settings. We used the standard splits for all languages. Like Chen and Manning (2014) and , we use predicted tags with the Stanford tagger ( <ref type="bibr" target="#b35">Toutanova et al., 2003)</ref> for English and gold tags for Chinese. For German we use the predicted tags provided by the CoNLL 2009 shared task organizers. All models were aug- mented with pretrained structured-skipgram (  embeddings; for English we used the Gi- gaword corpus and 100 dimensions, for Chinese Gi- gaword and 80, and for German WMT 2010 mono- lingual data and 64.</p><p>Hyperparameters. The hyperparameters for neural FOG are summarized in <ref type="table">Table 4</ref>. For the Adam optimizer we use the default settings in the CNN neural network library. 8 Since the ensemble is used to obtain the uncertainty on the training set, it is imperative that the stack LSTMs do not overfit the training set. To address this issue, we performed five-way jackknifing of the training data for each stack LSTM model to obtain the training data uncer- tainty under the ensemble. To obtain the ensemble uncertainty on each language, we use 21 base mod- els for English (see footnote 4), 17 for Chinese, and 11 for German.</p><p>Speed. One potential drawback of using a quadratic or cubic time parser to distill an ensemble of linear-time transition-based models is speed. Our FOG model is implemented using the same CNN li- brary as the stack LSTM transition-based parser. On the same single-thread CPU hardware, the distilled MST parser 9 parses 20 sentences per second with- out any pruning, while a single stack LSTM model <ref type="table" target="#tab_0">Bi-LSTM dimension  100  Bi-LSTM layers  2  POS tag embedding  12  Learned word embedding  32  Hidden Layer Units  100  Labeler Hiden Layer Units 100  Optimizer</ref> Adam Learning rate decay 0.05 <ref type="table">Table 4</ref>: Hyperparameters for the distilled FOG parser. Both the model architecture and the hyperparameters are nearly identical with <ref type="bibr" target="#b15">Kiperwasser and Goldberg (2016)</ref>. We apply a per-epoch learning rate decay to the Adam op- timizer, which consistently improves performance across all datasets.</p><p>is only three times faster at 60 sentences per second.</p><p>Running an ensemble of 20 stack LSTMs is at least 20 times slower (without multi-threading), not in- cluding consensus parsing. In the end, the distilled parser is more than ten times faster than the ensem- ble pipeline.</p><p>Accuracy. All scores are shown in <ref type="table">Table 5</ref>. First, consider the neural FOG parser trained with Ham- ming cost (C H in the second-to-last row). This is a very strong benchmark, outperforming many higher- order graph-based and neural network models on all three datasets. Nonetheless, training the same model with distillation cost gives consistent improvements for all languages. For English, we see that this model comes close to the slower ensemble it was trained to simulate. For Chinese, it achieves the best published scores, for German the best published UAS scores, and just after Bohnet and Nivre <ref type="formula" target="#formula_2">(2012)</ref> for LAS.</p><p>Effects of Pre-trained Word Embedding. As an ablation study, we ran experiments on English without pre-trained word embedding, both with the Hamming and distillation costs. The model trained with Hamming cost achieved 93.1 UAS and 90.9 LAS, compared to 93.6 UAS and 91.1 LAS for the model with distillation cost. This result further showcases the consistent improvements from using the distillation cost across different settings and lan- guages.</p><p>We conclude that "soft targets" derived from en- semble uncertainty offer useful guidance, through the distillation cost function and discriminative training of a graph-based parser. Here we consid-  <ref type="table">Table 5</ref>: Dependency parsing performance on English, Chinese, and German tasks. The "P?" column indicates the use of pretrained word embeddings. Reranking/blend indicates that the reranker score is interpolated with the base model's score. Note that previous works might use different predicted tags for English. We report accuracy without punctuation for English and Chinese, and with punctuation for German, using the standard evaluation script in each case. We only consider systems that do not use additional training data. The best overall results are indicated with bold (this was achieved by the ensemble of greedy stack LSTMs in Chinese and German), while the best non-ensemble model is denoted with an underline. The † sign indicates the use of predicted tags for Chinese in the original publication, although we report accuracy using gold Chinese tags based on private correspondence with the authors.</p><p>ered a FOG parser, though future work might inves- tigate any parser amenable to training to minimize a cost-aware loss like the structured hinge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Our work on ensembling dependency parsers is based on <ref type="bibr" target="#b28">Sagae and Lavie (2006)</ref> and <ref type="bibr" target="#b30">Surdeanu and Manning (2010)</ref>; an additional contribution of this work is to show that the normalized ensemble votes correspond to MBR parsing. Petrov (2010) pro- posed a similar model combination with random ini- tializations for phrase-structure parsing, using prod- ucts of constituent marginals. The local optima in his base model's training objective arise from latent variables instead of neural networks (in our case). Model distillation was proposed by Bucilˇa <ref type="bibr" target="#b5">Bucilˇa et al. (2006)</ref>, who used a single neural network to simu- late a large ensemble of classifiers. More recently, <ref type="bibr" target="#b1">Ba and Caruana (2014)</ref> showed that a single shal- low neural network can closely replicate the per- formance of an ensemble of deep neural networks in phoneme recognition and object detection. Our work is closer to , in the sense that we do not simply compress the ensemble and hit the "soft target," but also the "hard target" at the same time <ref type="bibr">10</ref> . These previous works only used model compression and distillation for classification; we extend the work to a structured prediction problem (dependency parsing). <ref type="bibr">Täckström et al. (2013)</ref> similarly used an ensem- ble of other parsers to guide the prediction of a seed model, though in a different context of "ambiguity- aware" ensemble training to re-lexicalize a trans- fer model for a target language. We similarly use an ensemble of models as a supervision for a sin-gle model. By incorporating the ensemble uncer- tainty estimates in the cost function, our approach is cheaper, not requiring any marginalization during training. An additional difference is that we learn from the gold labels ("hard targets") rather than only ensemble estimates on unlabeled data. <ref type="bibr" target="#b14">Kim and Rush (2016)</ref> proposed a distillation model at the sequence level, with application in sequence-to-sequence neural machine translation. There are two primary differences with this work. First, we use a global model to distill the ensemble, instead of a sequential one. Second, <ref type="bibr" target="#b14">Kim and Rush (2016)</ref> aim to distill a larger model into a smaller one, while we propose to distill an ensemble instead of a single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>We demonstrate that an ensemble of 20 greedy stack LSTMs (  can achieve state of the art accuracy on English dependency parsing. This approach corresponds to minimum Bayes risk de- coding, and we conjecture that the arc attachment posterior marginals quantify a notion of uncertainty that may indicate difficulty or ambiguity. Since run- ning an ensemble is computationally expensive, we proposed discriminative training of a graph-based model with a novel cost function that distills the en- semble uncertainty. Deriving a cost function from a statistical model and extending distillation to struc- tured prediction are new contributions. This dis- tilled model, trained to simulate the slower ensemble parser, improves over the state of the art on Chinese and German.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our ensemble's votes (20 models) on an ambiguous PP attachment of with. The ensemble is nearly but not perfectly unanimous in selecting saw as the head. Model UAS LAS UEM Andor et al. (2016) 94.61 92.79N = 1 (stack LSTM) 93.10 90.90 47.60 ensemble, N = 5, MST 93.91 91.94 50.12 ensemble, N = 10, MST 94.34 92.47 52.07 ensemble, N = 15, MST 94.40 92.57 51.86 ensemble, N = 20, MST 94.51 92.70 52.44</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>PTB-SD task: ensembles improve over a strong greedy baseline. UEM indicates unlabeled exact match.</figDesc><table>Experiment. We consider this approach on the 
Stanford dependencies version 3.3.0 (De Marneffe 
and Manning, 2008) Penn Treebank task. As noted, 
the base parsers instantiate the greedy stack LSTM 
parser (Dyer et al., 2015). 3 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>An ambiguous sentence from the training set and 
the posteriors 4 of various possible parents for including. 
The last two columns are, respectively, the contributions 
to the distillation cost C D (explained in  §5.1, Eq. 5) and 
the standard Hamming cost C H . The most probable head 
under the ensemble is changes, which is also the correct 
answer. 

</table></figure>

			<note place="foot" n="1"> https://github.com/adhigunasurya/ distillation_parser.git</note>

			<note place="foot" n="3"> We use the standard data split (02-21 for training, 22 for development, 23 for test), automatically predicted partof-speech tags, same pretrained word embedding as Dyer et al. (2015), and recommended hyperparameters; https:// github.com/clab/lstm-parser, each with a different random initialization; this differs from past work on ensembles, which often uses different base model architectures.</note>

			<note place="foot" n="4"> In §3, we used 20 models. Since those 20 models were trained on the whole training set, they cannot be used to obtain the uncertainty estimates on the training set, where the example sentence in Table 2 comes from. Therefore we trained a new ensemble of 21 models from scratch with five-way jackknifing. The same jackknifing setting is used in the distillation parser ( §6). for the Hamming cost-and use it in discriminative training of a single FOG parser. This allows us to distill what has been learned by the ensemble into a single model.</note>

			<note place="foot" n="5"> It is important to note the difference between cost (Eq. 5) and loss (Eq. 3).</note>

			<note place="foot" n="6"> Alternatives that do not use cost functions include probabilistic parsers, whether locally normalized like the stack LSTM parser used within our ensemble, or globally normalized, as in Andor et al. (2016); cost functions can be incorporated in such cases with minimum risk training (Smith and Eisner, 2006) or softmax margin (Gimpel and Smith, 2010). 7 To our understanding, Kiperwasser and Goldberg (2016) initialized with pretrained vectors and backpropagated during training.</note>

			<note place="foot" n="8"> https://github.com/clab/cnn.git 9 The runtime of the Hamming-cost bidirectional LSTM FOG parser is the same as the distilled parser.</note>

			<note place="foot" n="10"> Our cost is zero when the correct arc is predicted, regardless of what the soft target thinks, something a compression model without gold supervision cannot do.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Swabha Swayamdipta, Sam Thomson, Jesse Dodge, Dallas Card, Yuichiro Sawai, Gra-ham Neubig, and the anonymous reviewers for use-ful feedback. We also thank Juntao Yu and Bernd Bohnet for re-running the parser of Bohnet and Nivre (2012) on Chinese with gold tags. This work was sponsored in part by the Defense Advanced Re-search Projects Agency (DARPA) Information Inno-vation Office (I2O) under the Low Resource Lan-guages for Emergent Incidents (LORELEI) program issued by DARPA/I2O under Contract No. HR0011-15-C-0114; it was also supported in part by Contract No. W911NF-15-1-0543 with the DARPA and the Army Research Office (ARO). Approved for public release, distribution unlimited. The views expressed are those of the authors and do not reflect the of-ficial policy or position of the Department of De-fense or the U.S. Government. Miguel Ballesteros was supported by the European Commission un-der the contract numbers FP7-ICT-610411 (project MULTISENSOR) and H2020-RIA-645012 (project KRISTINA).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL<address><addrLine>Slav Petrov, and Michael Collins</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improved transition-based parsing by modeling characters instead of words with LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Training with exploration improves a greedy stack-LSTM parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP-CoNLL</title>
		<meeting>of EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bucilˇabucilˇa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescumizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of KDD</title>
		<meeting>of KDD</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine De</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Stanford typed dependencies manual</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Three new probabilistic models for dependency parsing: An exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">M</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Softmaxmargin CRFs: Training log-linear models with cost functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<imprint>
			<pubPlace>Maria Antònia Martí, Lluís M` arquez, Adam Meyers, Joakim Nivre, Sebastian</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The CoNLL2009 Shared Task: Syntactic and semantic dependencies in multiple languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaň</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Stěpánek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Straňák</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CONLL 2009 Shared Task</title>
		<meeting>of CONLL 2009 Shared Task</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sequencelevel knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Simple and accurate dependency parsing using bidirectional LSTM feature representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient thirdorder dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The insideoutside recursive neural network model for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Two/too simple adaptations of word2vec for syntax problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fourth-order dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stacking dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Polyhedral outer approximations with application to natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Turning on the turbo: Fast third-order non-projective turbo parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana</forename><forename type="middle">S C</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Online large-margin training of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Non-projective dependency parsing using spanning tree algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiril</forename><surname>Ribarov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Integrating graph-based and transition-based dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An efficient algorithm for projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IWPT</title>
		<meeting>of IWPT</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Products of random latent variable grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc.of NAACL</title>
		<meeting>.of NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Parser combination by reparsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Minimum risk annealing for training log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ensemble models for dependency parsing: Cheap and good?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Target language adaptation of discriminative transfer parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">T</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning structured prediction models: A large margin approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Chatalbashev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Bayes risk minimization in natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>University of Geneva</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Feature-rich part-of-speech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph-based dependency parsing with bidirectional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Structured training for neural network transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Building a large-scale annotated chinese corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Incremental recurrent neural network dependency parser with search-based discriminative training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing using beamsearch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with rich non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A re-ranking model for dependency parser with recursive convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-IJCNLP</title>
		<meeting>of ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
