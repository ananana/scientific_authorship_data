<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Relieving the Computational Bottleneck: Joint Inference for Event Extraction with High-Dimensional Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Venugopal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Human Language Technology Research Institute University of Texas at Dallas Richardson</orgName>
								<address>
									<postCode>75083-0688</postCode>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Human Language Technology Research Institute University of Texas at Dallas Richardson</orgName>
								<address>
									<postCode>75083-0688</postCode>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Gogate</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Human Language Technology Research Institute University of Texas at Dallas Richardson</orgName>
								<address>
									<postCode>75083-0688</postCode>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Human Language Technology Research Institute University of Texas at Dallas Richardson</orgName>
								<address>
									<postCode>75083-0688</postCode>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Relieving the Computational Bottleneck: Joint Inference for Event Extraction with High-Dimensional Features</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="831" to="843"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Several state-of-the-art event extraction systems employ models based on Support Vector Machines (SVMs) in a pipeline architecture , which fails to exploit the joint dependencies that typically exist among events and arguments. While there have been attempts to overcome this limitation using Markov Logic Networks (MLNs), it remains challenging to perform joint inference in MLNs when the model encodes many high-dimensional sophisticated features such as those essential for event extraction. In this paper, we propose a new model for event extraction that combines the power of MLNs and SVMs, dwarfing their limitations. The key idea is to reliably learn and process high-dimensional features using SVMs; encode the output of SVMs as low-dimensional, soft formulas in MLNs; and use the superior joint in-ferencing power of MLNs to enforce joint consistency constraints over the soft formulas. We evaluate our approach for the task of extracting biomedical events on the BioNLP 2013, 2011 and 2009 Genia shared task datasets. Our approach yields the best F1 score to date on the BioNLP&apos;13 (53.61) and BioNLP&apos;11 (58.07) datasets and the second-best F1 score to date on the BioNLP&apos;09 dataset (58.16).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Event extraction is the task of extracting and la- beling all instances in a text document that corre- spond to a pre-defined event type. This task is quite challenging for a multitude of reasons: events are often nested, recursive and have several arguments; there is no clear distinction between arguments and events; etc. For instance, consider the BioNLP Ge- nia event extraction shared task <ref type="bibr">(Nédellec et al., 2013)</ref>. In this task, participants are asked to extract instances of a pre-defined set of biomedical events from text. An event is identified by a keyword called the trigger and can have an arbitrary number of arguments that correspond to pre-defined argu- ment types. The task is complicated by the fact that an event may serve as an argument of another event (nested events). An example of the task is shown in <ref type="figure">Figure 1</ref>. As we can see, event E13 takes as arguments two events, E14 and E12, which in turn has E11 as one of its arguments.</p><p>A standard method that has been frequently em- ployed to perform this shared task uses a pipeline architecture with three steps: (1) detect if a token is a trigger and assign a trigger type label to it; (2) for every detected trigger, determine all its argu- ments and assign types to each detected argument; and (3) combine the extracted triggers and argu- ments to obtain events. Though adopted by the top-performing systems such as the highest scoring system on the BioNLP'13 Genia shared task , this approach is problematic for at least two reasons. First, as is typical in pipeline architectures, errors may propagate from one stage to the next. Second, since each event/argument is identified and assigned a type independently of the others, it fails to capture the relationship between a trigger and its neighboring triggers, an argument and its neighboring arguments, etc.</p><p>More recently, researchers have investigated joint inference techniques for event extraction us- ing Markov Logic Networks (MLNs) (e.g., <ref type="bibr" target="#b43">Poon and Domingos (2007)</ref>, <ref type="bibr" target="#b44">Poon and Vanderwende (2010)</ref>, <ref type="bibr" target="#b47">Riedel and McCallum (2011a)</ref>), a statis- tical relational model that enables us to model the dependencies between different instances of a data sample. However, it is extremely challenging to make joint inference using MLNs work well in practice <ref type="bibr" target="#b43">(Poon and Domingos, 2007)</ref>. One reason is that it is generally difficult to model sophisti- cated linguistic features using MLNs. The diffi-. . . demonstrated that HOIL-1L interacting protein (HOIP), a ubiquitin ligase that can catalyze the assembly of linear polyubiquitin chains, is recruited to DC40 in a TRAF2-dependent manner following engagement of CD40 . . .   <ref type="figure">Figure 1</ref>: Example of event extraction in the BioNLP Genia task. The table in (b) shows all the events extracted from sentence (a). Note that successful extraction of E13 depends on E12 and E14.</p><p>culty stems from the fact that some of these fea- tures are extremely high dimensional (e.g., <ref type="bibr" target="#b8">Chen and Ng (2012)</ref>, <ref type="bibr" target="#b20">Huang and Riloff (2012b)</ref>, <ref type="bibr">Li et al. (2012)</ref>, <ref type="bibr">Li et al. (2013b)</ref>, <ref type="bibr">Li et al. (2013c)</ref>), and to reliably learn weights of formulas that encode such features, one would require an enormous number of data samples. Moreover, even the complexity of approximate inference on such models is quite high, often prohibitively so. For example, a trigram can be encoded as an MLN formula, Word(</p><formula xml:id="formula_0">w 1 , p−1) ∧ Word(w 2 , p) ∧ Word(w 3 , p + 1) ⇒ Type(p, T ).</formula><p>For any given position (p), this formula has W 3 groundings, where W is the number of possible words, making it too large for learning/inference. Therefore, current MLN-based systems tend to in- clude a highly simplified model ignoring powerful linguistic features. This is problematic because such features are essential for event extraction.</p><p>Our contributions in this paper are two-fold. First, we propose a novel model for biomedical event extraction based on MLNs that addresses the aforementioned limitations by leveraging the power of Support Vector Machines (SVMs) <ref type="bibr" target="#b56">(Vapnik, 1995;</ref><ref type="bibr" target="#b22">Joachims, 1999</ref>) to handle high-dimensional features. Specifically, we (1) learn SVM models us- ing rich linguistic features for trigger and argument detection and type labeling; (2) design an MLN composed of soft formulas (each of which encodes a soft constraint whose associated weight indicates how important it is to satisfy the constraint) and hard formulas (constraints that always need to be satisfied, thus having a weight of ∞) to capture the relational dependencies between triggers and arguments; and (3) encode the SVM output as prior knowledge in the MLN in the form of soft formulas, whose weights are computed using the confidence values generated by the SVMs. This formulation naturally allows SVMs and MLNs to complement each other's strengths and weaknesses: learning in a large and sparse feature space is much easier with SVMs than with MLNs, whereas modeling relational dependencies is much easier with MLNs than with SVMs.</p><p>Our second contribution concerns making infer- ence with this MLN feasible. Recall that inference involves detecting and assigning the type label to all the triggers and arguments. We show that exist- ing Maximum-a-posteriori (MAP) inference meth- ods, even the most advanced approximate ones (e.g., <ref type="bibr" target="#b52">Selman et al. (1996)</ref>, <ref type="bibr" target="#b33">Marinescu and Dechter (2009)</ref>, <ref type="bibr" target="#b54">Sontag and Globerson (2011)</ref> ), are infea- sible on our proposed MLN because of their high memory cost. Consequently, we identify decompo- sitions of the MLN into disconnected components and solve each independently, thereby drastically reducing the memory requirements.</p><p>We evaluate our approach on the BioNLP 2009, 2011 and 2013 Genia shared task datasets. On the BioNLP'13 dataset, our model significantly outperforms state-of-the-art pipeline approaches and achieves the best F1 score to date. On the BioNLP'11 and BioNLP'09 datasets, our scores are slightly better and slightly worse respectively than the best reported results. However, they are significantly better than state-of-the-art MLN- based systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Related Work</head><p>As a core task in information extraction, event ex- traction has received significant attention in the nat- ural language processing (NLP) community. The development and evaluation of large-scale learning- based event extraction systems was propelled in part by the availability of annotated corpora pro- duced as part of the Message Understanding Con- ferences (MUCs), the Automatic Content Extrac- tion (ACE) evaluations, and the BioNLP shared tasks on event extraction. Previous work on event extraction can be broadly divided into two cate- gories, one focusing on the development of fea- tures (henceforth feature-based approaches) and the other focusing on the development of models (henceforth model-based approaches).</p><p>Feature-based approaches. Early work on feature-based approaches has primarily focused on designing local sentence-level features such as token and syntactic features ( <ref type="bibr" target="#b15">Grishman et al., 2005;</ref><ref type="bibr" target="#b3">Ahn, 2006</ref>). Later, it was realized that local features were insufficient to reliably and accurately perform event extraction in complex domains and therefore several researchers proposed using high-level fea- tures. For instance, <ref type="bibr" target="#b21">Ji and Grishman (2008)</ref> used global information from related documents; Gupta and Ji (2009) extracted implicit time information; Patwardhan and <ref type="bibr" target="#b42">Riloff (2009)</ref> used broader sen- tential context; <ref type="bibr">Liao and Grishman (2010;</ref> leveraged document-level cross-event information and topic-based features; and Huang and Riloff (2012b) explored discourse properties.</p><p>Model-based approaches. The model-based ap- proaches developed to date have focused on mod- eling global properties and seldom use rich, high- dimensional features. To capture global event struc- ture properties, <ref type="bibr" target="#b34">McClosky et al. (2011a)</ref> proposed a dependency parsing model. To extract event ar- guments, <ref type="bibr">Li et al. (2013b)</ref> proposed an Integer Linear Programming (ILP) model to encode the relationship between event mentions. To overcome the error propagation problem associated with the pipeline architecture, several joint models have been proposed, including those that are based on MLNs (e.g., <ref type="bibr" target="#b43">Poon and Domingos (2007)</ref>, <ref type="bibr" target="#b49">Riedel et al. (2009)</ref>, <ref type="bibr" target="#b44">Poon and Vanderwende (2010)</ref>), struc- tured perceptrons (e.g., <ref type="bibr">Li et al. (2013c)</ref>), and dual decomposition with minimal domain adaptation (e.g., <ref type="bibr" target="#b47">Riedel and McCallum (2011a;</ref>).</p><p>In light of the high annotation cost required by supervised learning-based event extraction systems, several semi-supervised, unsupervised, and rule- based systems have been proposed. For instance, Huang and Riloff (2012a) proposed a bootstrap- ping method to extract event arguments using only a small amount of annotated data; Lu and Roth (2012) developed a novel unsupervised sequence labeling model; <ref type="bibr" target="#b7">Bui et al. (2013)</ref> implemented a rule-based approach to extract biomedical events; and <ref type="bibr" target="#b51">Ritter et al. (2012)</ref> used unsupervised learning to extract events from Twitter data.</p><p>Our work extends prior work by developing a rich framework that leverages sophisticated feature- based approaches as well as joint inference using MLNs. This combination gives us the best of both worlds because on one hand, it is challenging to model sophisticated linguistic features using MLNs while on the other hand, feature-based approaches employing sophisticated high-dimensional features suffer from error propagation as the model is gen- erally not rich enough for joint inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Genia Event Extraction Task</head><p>The BioNLP Shared Task (BioNLP-ST) series ( <ref type="bibr" target="#b24">Kim et al. (2009)</ref>, <ref type="bibr" target="#b25">Kim et al. (2011a)</ref> and <ref type="bibr">Nédellec et al. (2013)</ref>) is designed to tackle the problem of extracting structured information from the biomedi- cal literature. The Genia Event Extraction task is ar- guably the most important of all the tasks proposed in BioNLP-ST and is also the only task organized in all three events in the series.</p><p>The 2009 edition of the Genia task ( <ref type="bibr" target="#b24">Kim et al., 2009</ref>) was conducted on the Genia event corpus ( <ref type="bibr" target="#b23">Kim et al., 2008)</ref>, which only contains abstracts of the articles that represent domain knowledge around NFκB proteins. The 2011 edi- tion ( <ref type="bibr" target="#b26">Kim et al., 2011b</ref>) augmented the dataset to include full text articles, resulting in two collec- tions, the abstract collection and the full text col- lection. <ref type="bibr">edition (Kim et al., 2013</ref>) further augmented the dataset with recent full text articles but removed the abstract collection entirely.</p><p>The targeted event types have also changed slightly over the years. Both the 2009 and 2011 editions are concerned with nine fine-grained event sub-types that can be categorized into three main types, namely simple, binding and regulation events. These three main event types can be dis- tinguished by the kinds of arguments they take. A simple event can take exactly one protein as its T heme argument. A binding event can take one or more proteins as its T heme arguments, and is therefore slightly more difficult to extract than a simple event. A regulation event takes exactly one protein or event as its T heme argument and option- ally one protein or event as its Cause argument. If a regulation event takes another event as its T heme or Cause argument, it will lead to a nested event.</p><p>Regulation events are considered the most difficult- to-extract among the three event types owing in part to the presence of an optional Cause argument and their recursive structure. The 2013 edition intro-duced a new event type, protein-mod, and its three sub-types. Theoretically, a protein-mod event takes exactly one protein as its T heme argument and optionally one protein or event as its Cause argu- ment. In practice, however, it rarely occurs: there are only six protein-mod events having Cause ar- guments in the training data for the 2013 edition. Consequently, our model makes the simplifying assumption that a protein-mod event can only take one T heme argument, meaning that we are effec- tively processing protein-mod events in the same way as simple events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Markov Logic Networks</head><p>Statistical relational learning (SRL) <ref type="bibr">(Getoor and Taskar, 2007</ref>) is an emerging field that seeks to unify logic and probability, and since most NLP techniques are grounded either in logic or proba- bility or both, NLP serves as an ideal application domain for SRL. In this paper, we will employ a popular SRL approach called Markov logic net- works (MLNs) <ref type="bibr" target="#b11">(Domingos and Lowd, 2009)</ref>. At a high level, an MLN is a set of weighted first-order logic formulas (f i , w i ), where w i is the weight associated with formula f i . Given a set of con- stants that model objects in the domain, it defines a Markov network or a log-linear model <ref type="bibr" target="#b28">(Koller and Friedman, 2009</ref>) in which we have one node per ground first-order atom and a propositional feature corresponding to each grounding of each first-order formula. The weight of the feature is the weight of the corresponding first-order formula.</p><p>Formally, the probability of a world ω, which represents an assignment of values to all ground atoms in the Markov network, is given by:</p><formula xml:id="formula_1">Pr(ω) = 1 Z exp i w i N (f i , ω)</formula><p>where N (f i , ω) is the number of groundings of f i that evaluate to True in ω and Z is a normalization constant called the partition function.</p><p>The key inference tasks over MLNs are com- puting the partition function (Z) and the most- probable explanation given evidence (the MAP task). Most queries, including those required by event extraction, can be reduced to these inference tasks. Formally, the partition function and the MAP tasks are given by:</p><formula xml:id="formula_2">Z = ω exp i w i N (f i , ω) (1) arg max ω P (ω) = arg max ω i w i N (f i , ω) (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pipeline Model</head><p>We implement a pipeline event extraction system using SVMs. This pipeline model serves two im- portant functions: (1) providing a baseline for eval- uation and (2) producing prior knowledge for the joint model.</p><p>Our pipeline model consists of two steps: trig- ger labeling and argument labeling. In the trigger labeling step, we determine whether a candidate trigger is a true trigger and label each true trigger with its trigger type. Then, in the argument label- ing step, we identify the arguments for each true trigger discovered in the trigger labeling step and assign a role to each argument.</p><p>We recast each of the two steps as a classification task and employ SVM multiclass ( <ref type="bibr" target="#b55">Tsochantaridis et al., 2004</ref>) to train the two classifiers. We describe each step in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Trigger Labeling</head><p>A preliminary study of the BioNLP'13 training data suggests that 98.7% of the true triggers' head words 1 are either verbs, nouns or adjectives. There- fore, we consider only those words whose part-of- speech tags belong to the above three categories as candidate triggers. To train the trigger classifier, we create one training instance for each candidate trigger in the training data. If the candidate trigger is not a trigger, the class label of the corresponding instance is N one; otherwise, the label is the type of the trigger. Thus, the number of class labels equals the number of trigger types plus one. Each training instance is represented by the features de- scribed in <ref type="table">Table 1</ref>(a). These features closely mirror those used in state-of-the-art trigger labeling sys- tems such as <ref type="bibr" target="#b38">Miwa et al. (2010b)</ref> and <ref type="bibr" target="#b5">Björne and Salakoski (2013)</ref>.</p><p>After training, we apply the resulting trigger clas- sifier to classify the test instances, which are cre- ated in the same way as the training instances. If a test instance is predicted as N one by the classifier, the corresponding candidate trigger is labeled as a non-trigger; otherwise, the corresponding candi- date trigger is posited as a true trigger whose type is the class value assigned by the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) Features for trigger labeling Token features</head><p>The basic token features (see <ref type="table">Table 1</ref>(c)) computed from (1) the candidate trigger word and (2) the surrounding tokens in a window of two; character bigrams and trigrams of the candidate trigger word; word n-grams (n=1,2,3) of the candidate trigger word and its context words in a window of three; whether the candidate trigger word contains a digit; whether the candidate trigger word contains an upper case letter; whether the candidate trigger word contains a symbol. Dependency features</p><p>The basic dependency path features (see <ref type="table">Table 1</ref>(c)) computed using the shortest paths from the candidate trigger to (1) the nearest protein word, (2) the nearest protein word to its left, and (3) the nearest protein word to its right. Other features</p><p>The distances from the candidate trigger word to (1) the nearest protein word, (2) the nearest protein word to its left, and (3) the nearest protein word to its right; the number of protein words in the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) Features for argument labeling Token features</head><p>Word n-grams (n=1,2,3) of (1) the candidate trigger word and its context in a window of three and (2) the candidate argument word and its context in a window of three; the basic token features (see <ref type="table">Table 1</ref>(c)) computed from (1) the candidate trigger word and (2) the candidate argument word; the trigger type of the candidate trigger word. Dependency features</p><p>The basic dependency features (see <ref type="table">Table 1</ref>(c)) computed using the shortest path from the candidate trigger word to the candidate argument word. Other features</p><p>The distance between the candidate trigger word and the candidate argument word; the number of proteins between the candidate trigger word and the candidate argument word; the concatenation of the candidate trigger word and the candidate argument word; the concatenation of the candidate trigger type and the candidate argument word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(c) Basic token and dependency features Basic token fea- tures</head><p>Six features are computed given a token t, including: (a) the lexical string of t, (b) the lemma of t, (c) the stem of t obtained using the Porter stemmer <ref type="bibr" target="#b45">(Porter, 1980)</ref>, (d) the part-of-speech tag of t, (e) whether t appears as a true trigger in the training data, and (f) whether t is a protein name. Basic dependency features</p><p>Six features are computed given a dependency path p, including: (a) the vertex walk in p, (b) the edge walk in p, (c) the n-grams (n=2,3,4) of the (stemmed) words associated with the vertices in p, (d) the n-grams (n=2,3,4) of the part-of-speech tags of the words associated with the vertices in p, (e) the n-grams (n=2,3,4) of the dependency types associated with the edges in p, and (f) the length of p. <ref type="table">Table 1</ref>: Features for trigger labeling and argument labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Argument Labeling</head><p>The argument classifier is trained as follows. Each training instance corresponds to a candidate trigger and one of its candidate arguments. <ref type="bibr">2</ref> A candidate argument for a candidate trigger ct is either a pro- tein or a candidate trigger that appears in the same sentence as ct. If ct is not a true trigger, the label of the associated instance is set to N one. On the other hand, if ct is a true trigger, we check whether the candidate argument in the associated instance is in- deed one of ct's arguments. If so, the class label of the instance is the argument's role; otherwise, the class label is N one. The features used for repre- senting each training instance, which are modeled after those used in <ref type="bibr" target="#b38">Miwa et al. (2010b)</ref> and <ref type="bibr" target="#b5">Björne and Salakoski (2013)</ref>, are shown in <ref type="table" target="#tab_3">Table 1(b).</ref> After training, we can apply the resulting clas- sifier to classify the test instances, which are cre- ated in the same way as the training instances. If a test instance is assigned the class N one by the classifier, the corresponding candidate argument is classified as not an argument of the trigger. Other- 2 Following the definition of the GENIA event extraction task, the protein names are provided as part of the input. wise, the candidate argument is a true argument of the trigger whose role is the class value assigned by the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Joint Model</head><p>In this section, we describe our Markov logic model that encodes the relational dependencies in the shared task and uses the output of the pipeline model as prior knowledge (soft evidence). We be- gin by describing the structure of our Markov logic model, and then describe the parameter learning and inference algorithms for it. <ref type="figure">Figure 2</ref> shows our proposed MLN for BioNLP event extraction, which we refer to as BioMLN. The MLN contains six predicates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MLN Structure</head><p>The query predicates in <ref type="figure">Figure 2</ref>(a) are those whose assignments are not given during infer- ence and thus need to be predicted. Predicate TriggerType(sid,tid,ttype!) is true when the token located in sentence sid at position tid has type ttype. ∆ ttype , which denotes the set of con- stants (or objects) that the logical variable ttype TriggerType(sid,tid,ttype!) ArgumentRole(sid,aid,tid,arole!) (a) Query</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simple(sid,tid) Regulation(sid,tid) (b) Hidden</head><p>Word(sid,tid,word) DepType(sid,aid,tid,dtype) (c) Evidence</p><formula xml:id="formula_3">1. ∃t TriggerType(i,j,t).</formula><p>2. ∃a ArgumentRole(i,k,j,a).</p><p>3. ¬TriggerType(i,j,N one) ⇒ ∃k ArgumentRole(i,k,j,T heme).</p><p>4. Simple(i,j) ⇒ ¬ ∃k ArgumentRole(i,k,j,Cause).</p><p>5. TriggerType(i,j,N one) ⇔ ArgumentRole(i,k,j,N one).</p><p>6. ¬ArgumentRole(i,k,j,N one) ∧ ¬TriggerType(i,k,N one) ⇒ Regulation(i,j). can be instantiated to, includes all possible trigger types in the dataset plus N one (which indicates that the token is not a trigger). The "!" symbol mod- els commonsense knowledge that only one of the types in the domain ∆ ttype of ttype is true for every unique combination of sid and tid. Similarly, pred- icate ArgumentRole(sid,aid,tid,arole!) as- serts that a token in sentence sid at position aid plays exactly one argument role, denoted by arole, with respect to the token at position tid. ∆ arole includes the two argument types, namely, T heme and Cause plus the additional N one that indicates that the token is not an argument.</p><p>The hidden predicates in <ref type="figure">Figure 2</ref>(b) are "clus- ters" of trigger types. Predicate Simple(sid,tid) is true when the token in sentence sid at posi- tion tid corresponds to one of the Simple event trigger types (BioNLP'13 has 9 simple events, BioNLP'09/'11 have 5) or a binding event trig- ger type. Similarly, Regulation(sid,tid) asserts that the token in sentence sid at position tid corre- sponds to any of the three regulation event trigger types.</p><p>The evidence predicates in <ref type="figure">Figure 2</ref>(c) are those that are always assumed to be known during in- ference. We define two evidence predicates based on dependency structures. Word(sid,tid,word) is true when the word in sentence sid at position tid is equal to word. DepType(sid,aid,tid,dtype) asserts that dtype is the dependency type in the de- pendency parse tree that connects the token at posi- tion tid to the token at position aid in sentence sid.</p><p>If the word at tid and the word at aid are directly connected in the dependency tree, then dtype is the label of dependency edge with direction; otherwise dtype is N one.</p><p>The MLN formulas, expressing commonsense, prior knowledge in the domain (Poon and Van- derwende, 2010; <ref type="bibr" target="#b47">Riedel and McCallum, 2011a</ref>), are shown in <ref type="figure">Fig. 2(d)</ref>. All formulas, except For- mula (9), are hard formulas, meaning that they have infinite weights. Note that during weight learning, we only learn the weights of soft formulas. Formulas (1) and (2) along with the "!" con- straint in the predicate definition ensure that the token types are mutually exclusive and exhaustive. Formula (3) asserts that every trigger should have an argument of type T heme, since a T heme argu- ment is mandatory for any event. Formula (4) mod- els the constraint that a Simple or Binding trigger has no arguments of type Cause since only regu- lation events have a Cause. Formula (5) asserts that non-triggers have no arguments and vice-versa. Formula (6) models the constraint that if a token is both an argument of t and a trigger by itself, then t must belong to one of the three regulation trigger types. This formula captures the recursive relationship between triggers. Formulas (7) and (8) connect the hidden predicates with the query predicates. Formula (9) is a soft formula encoding the relationship between triggers and arguments in a dependency parse tree. It joins a word and the dependency type label that connects the word token to the argument token in the dependency parse tree with the trigger types and argument types of the two tokens. The "+" symbol indicates that each grounding of Formula (9) may have a different weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Weight Learning</head><p>We can learn BioMLN from data either discrimina- tively or generatively. Since discriminative learning is much faster than generative learning, we use the former. In discriminative training, we maximize the conditional log-likelihood (CLL) of the query and the hidden variables given an assignment to the evidence variables. In principle, we can use the standard gradient descent algorithm for maximiz- ing the CLL. In each iteration of gradient descent, we update the weights using the following equation (cf. <ref type="bibr" target="#b53">Singla and Domingos (2005)</ref> and <ref type="bibr" target="#b11">Domingos and Lowd (2009)</ref>):</p><formula xml:id="formula_4">w t+1 j = w t j − α(E w (n j ) − n j )<label>(3)</label></formula><p>where w t j represents the weight of the j th formula in the t th iteration, n j is the number of groundings in which the j th formula is satisfied in the training data, E w (n j ) is the expected number of ground- ings in which the j th formula is satisfied given the current weight vector w, and α is the learning rate.</p><p>As such, the update rule given in Equation (3) is likely to yield poor accuracy because the num- ber of training examples of some types (e.g., N one) far outnumber other types. To rectify this ill-conditioning problem <ref type="bibr" target="#b53">(Singla and Domingos, 2005;</ref><ref type="bibr" target="#b31">Lowd and Domingos, 2007)</ref>, we divide the gradient with the number of true groundings in the data, namely, we compute the gradient using</p><formula xml:id="formula_5">(Ew(n j )−n j ) n j .</formula><p>Another key issue with using Equation <ref type="formula" target="#formula_4">(3)</ref> is that computing E w (n j ) requires performing inference over the MLN. This step is intractable, #P-complete in the worst case. To circumvent this problem and for fast, scalable training, we instead propose to use the voted perceptron algorithm <ref type="bibr" target="#b10">(Collins, 2002;</ref><ref type="bibr" target="#b53">Singla and Domingos, 2005</ref>). This algorithm ap- proximates E w (n j ) by counting the number of satisfied groundings of each formula in the MAP assignment. Computing the MAP assignment is much easier (although still NP-hard in the worst case) than computing E w (n j ), and as a result the voted perceptron algorithm is more scalable than the standard gradient descent algorithm. In addi- tion, it converges much faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Testing</head><p>In the testing phase, we combine BioMLN with the output of the pipeline model (see Section 3) to ob- tain a new MLN, which we refer to as BioMLN + . For every candidate trigger, the SVM trigger clas- sifier outputs a vector of signed confidence val- ues (which is proportional to the distance from the separating hyperplane) of dimension ∆ ttype with one entry for each trigger type. Similarly, for every candidate argument, the SVM argu- ment classifier outputs a vector of signed confi- dence values of dimension ∆ arole with one en- try for each argument role. In BioMLN + , we model the SVM output as soft evidence, using two soft unit clauses, TriggerType(i,+j,+t) and ArgumentRole(i,+k,+j,+a). We use the con- fidence values to determine the weights of these clauses. Intuitively, higher (smaller) the confidence, higher (smaller) the weight.</p><p>Specifically, the weights of the soft unit clauses are set as follows. If the SVM trigger classifier determines that the trigger in sentence i at po- sition j belongs to type t with confidence C i,j , then we attach a weight of</p><formula xml:id="formula_6">C i,j αn i</formula><p>to the clause TriggerType(i,j,t). Here, n i denotes the num- ber of trigger candidates in sentence i. Similarly, if the SVM argument classifier determines that the token at position k in sentence i belongs to the ar- gument role a with respect to the token at position j, with confidence C i,k,j , then we attach a weight of</p><formula xml:id="formula_7">C i,k,j β n i j=1 m ij</formula><p>to the clause ArgumentRole(i, k, j,a). Here, m ij denotes the number of argument candidates for the j th trigger candidate in sentence i. α and β act as scale parameters for the confi- dence values ensuring that the weights don't get too large (or too small).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Inference</head><p>As we need to perform MAP inference, both at training time and at test time, in this subsection we will describe how to do it efficiently by exploiting unique properties of our proposed BioMLN. Naively, we can perform MAP inference by grounding BioMLN to a Markov network and then reducing the Markov network by removing from it all (grounded propositional) formulas that are inconsistent with the evidence. On the re-duced Markov network, we can then compute the MAP solution using standard MAP solvers such as MaxWalkSAT (a state-of-the-art local search based MAP solver) <ref type="bibr" target="#b52">(Selman et al., 1996)</ref> and Gurobi 3 (a state-of-the-art, parallelized ILP solver).</p><p>The problem with the above approach is that grounding the MLN is infeasible in practice; even the reduced Markov network is just too large. For example, assuming a total of |∆ sid | sentences and a maximum of N tokens in a sentence, Formula (3) alone has O <ref type="figure">(|∆ sid |N 3 )</ref> groundings. Concretely, at training time, assuming 1000 sentences with 10 tokens per sentence, Formula (3) itself yields one million groundings. Clearly, this approach is not scalable. It turns out, however, that the (ground) Markov network can be decomposed into several disconnected components, each of which can be solved independently. This greatly reduces the memory requirement of the inference step. Specif- ically, for every grounding of sid, we get a set of nodes in the Markov network that are disconnected from the rest of the Markov network and therefore independent of the rest of the network. Formally, Proposition 1. For any world ω of the BioMLN,</p><formula xml:id="formula_8">P M (ω) = P M i (ω i )P M\M i (ω \ ω i )<label>(4)</label></formula><p>where ω i is the world ω projected on the ground- ings of sentence i and M i is BioMLN grounded only using sentence i.</p><p>Using Equation (4), it is easy to see that the MLN M can be decomposed into |∆ sid | disjoint MLNs,</p><formula xml:id="formula_9">{M k } |∆ sid | k=1 .</formula><p>The MAP assignment to M can be computed using,</p><formula xml:id="formula_10">|∆ sid | i=1 arg max ω i P M i (ω i )</formula><p>. This result ensures that to approximate the expected counts E w (n j ), it is sufficient to keep exactly one sentence's groundings in memory. Specifically, E w (n j ) can be written as</p><formula xml:id="formula_11">|∆ sid | k=1 E w (n k j )</formula><p>, where E w (n k j ) indicates the expected number of satisfied groundings of the j th formula in the k th sentence. Since the MAP computation is decomposable, we can estimate E w (n k j ) using MAP inference on just the k th sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>We evaluate our system on the BioNLP'13 ( ), '11 ( <ref type="bibr" target="#b25">Kim et al., 2011a</ref>) and '09 (Kim <ref type="table">Table 2</ref>: Statistics on the BioNLP datasets, which consist of annotated papers/abstracts from PubMed. (x, y, z): x in training, y in development and z in test. #TT indicates the total number of trigger types. The total number of argument types is 2.</p><p>et al., 2009) Genia datasets for the main event ex- traction shared task. Note that this task is the most important one for Genia and therefore has the most active participation. Statistics on the datasets are shown in <ref type="table">Table 2</ref>. All our evaluations use the on- line tool provided by the shared task organizers. We report scores obtained using the approximate span, recursive evaluation.</p><p>To generate features, we employ the support- ing resources provided by the organizers. Specif- ically, sentence split and tokenization are done using the GENIA tools, while part-of-speech in- formation is provided by the BLLIP parser that uses the self-trained biomedical model <ref type="bibr" target="#b36">(McClosky, 2010)</ref>. Also, we create dependency features from the parse trees provided by two dependency parsers, the Enju parser ( <ref type="bibr" target="#b40">Miyao and Tsujii, 2008)</ref> and the aforementioned BLLIP parser that uses the self- trained biomedical model, which results in two sets of dependency features.</p><p>For MAP inference, we use Gurobi, a par- allelized ILP solver. After inference, a post- processing step is required to generate biomedi- cal events from the extracted triggers and argu- ments. Specifically, for binding events, we em- ploy a learning-based method similar to <ref type="bibr" target="#b4">Björne and Salakoski (2011)</ref>, while for the other events, we employ a rule-based approach similar to <ref type="bibr" target="#b6">Björne et al. (2009)</ref>. Both the SVM baseline system and the combined MLN+SVM system employ the same post-processing strategy.</p><p>During weight learning, in order to combat the problem of different initializations yielding radi- cally different parameter estimates, we start at sev- eral different initialization points and average the weights obtained after 100 iterations of gradient descent. However, we noticed that if we simply choose random initialization points, the variance of the weights was quite high and some initialization points were much worse than others. To counter this, we use the following method to systematically</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rec. Prec. F1 Our System</head><p>48.95 59.24 53.61 EVEX ( <ref type="bibr">Hakala et al., 2013)</ref> 45.44 58.03 50.97 TEES-2.1 <ref type="bibr" target="#b5">(Björne and Salakoski, 2013)</ref> 46.17 56.32 50.74 BIOSEM ( <ref type="bibr" target="#b7">Bui et al., 2013)</ref> 42.47 62.83 50.68 NCBI ( <ref type="bibr" target="#b30">Liu et al., 2013)</ref> 40.53 61.72 48.93 DLUTNLP ( <ref type="bibr">Li et al., 2013a)</ref> 40.81 57.00 47.56 <ref type="table">Table 3</ref>: Recall (Rec.), Precision (Prec.) and F1 score on the BioNLP'13 test data.</p><p>initialize the weights. Let n i be the number of sat- isfied groundings of formula f i in the training data and m i be the total number of possible groundings of f i . We use a threshold γ to determine whether we wish to make the initial weight positive or neg- ative. If n i m i ≤ γ, then we choose the initial weight uniformly at random from the range <ref type="bibr">[−0.1, 0]</ref>. Oth- erwise, we chose it from the range [0, 0.1]. These steps ensure that the weights generated from dif- ferent initialization points have smaller variance. Also, in the testing phase, we set the scale parame- ters for the soft evidence as α = β = max c∈C |c|, where C is the set of SVM confidence values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results on the BioNLP'13 Dataset</head><p>Among the three datasets, the BioNLP'13 dataset is most "realistic" one because it is the only one that contains full papers and no abstracts. As a re- sult, it is also the most challenging dataset among the three. <ref type="table">Table 3</ref> shows the results of our system along with the results of other top systems pub- lished in the official evaluation of BioNLP'13. Our system achieves the best F1-score (an improvement of 2.64 points over the top-performing system) and has a much higher recall (mainly because our sys- tem detects more regulation events which outnum- ber other event types in the dataset) and a slightly higher precision than the winning system. Of the top five teams, NCBI is the only other joint infer- ence system, which adopts joint pattern matching to predict triggers and arguments at the same time. These results illustrate the challenge in using joint inference effectively. NCBI performed much worse than the SVM-based pipeline systems, EVEX and TEES2.1. It was also worse than BIOSEM, a rule- based system that uses considerable domain exper- tise. Nevertheless, it was better than DLUTNLP, another SVM-based system. <ref type="figure">Figure 3</ref> compares our baseline pipeline model with our combined model. We can clearly see that the combined model has a significantly better F1 score than the pipeline model on most event types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Rec. Prec. F1 Our System 53.42 63.61 58.07 Miwa12 ( <ref type="bibr" target="#b39">Miwa et al., 2012)</ref> 53.35 63.48 57.98 <ref type="bibr">Riedel11 (Riedel et al., 2011</ref>) − − 56 UTurku ( <ref type="bibr">Salakoski, 2011) 49.56 57.65 53.30 MSR-NLP (Quirk et al., 2011)</ref> 48.64 54.71 51.50 <ref type="table">Table 4</ref>: Results on the BioNLP'11 test data.</p><p>The regulation events are considered the most com- plex events to detect because they have a recursive structure. At the same time, this structure yields a large number of joint dependencies. The advantage of using a rich model such as MLNs can be clearly seen in this case; the combined model yields a 10 point and 6 point increase in F1-score on the test data and development data respectively compared to the pipeline model. <ref type="table">Table 4</ref> shows the results on the BioNLP'11 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results on the BioNLP'11 Dataset</head><p>We can see that our system is marginally better than Miwa12, which is a pipeline-based system. It is also more than two points better than Riedel11, a state-of-the-art structured prediction-based joint inference system. Reidel11 incorporates the Stan- ford predictions <ref type="bibr" target="#b35">(McClosky et al., 2011b</ref>) as fea- tures in the model. On the two hardest, most complex tasks, detecting regulation events (which have recursive structures and more joint dependen- cies than other event types) and detecting bind- ing events (which may have multiple arguments), our system performs better than both Miwa12 and Riedel11. 4 Specifically, our system's F1 score for regulation events is 46.84, while those of Miwa12 and Riedel11 are 45.46 and 44.94 respectively. Our system's F1 score for the binding event is 58.79, while those of Miwa12 and Riedel11 are 56.64 and 48.49 respectively. These results clearly demon- strate the effectiveness of enforcing joint dependen- cies along with high-dimensional features. <ref type="table" target="#tab_2">Table 5</ref> shows the results on the BioNLP'09 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results on the BioNLP'09 Dataset</head><p>Our system has a marginally lower score (by 0.11 points) than Miwa12, which is the best performing system on this dataset. Specifically, our system achieves a higher recall but a lower precision than Miwa12. However, note that Miwa12 used co- reference features while we are able to achieve  similar accuracy without the use of co-reference data. The F1 score of Miwa10, which does not use co-reference features, is nearly 2 points lower than that of our system. Our system also has a higher F1 score than Reidel11, which is the best joint inference-based system for this task. On the regulation events, our system (47.55) out- performs both Miwa12 (45.99) and <ref type="bibr">Riedel11 (46.9)</ref>, while on the binding event, our system (59.88) is marginally worse than Miwa12 (59.91) and signifi- cantly better than Riedel11 (52.6). As mentioned earlier, these are the hardest events to extract. Also, existing MLN-based joint inference systems such as RiedelMLN and PoonMLN do not achieve state- of-the-art results because they do not leverage com- plex, high-dimensional features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary and Future Work</head><p>Markov logic networks (MLNs) are a powerful representation that can compactly encode rich rela- tional structures and ambiguities (uncertainty). As a result, they are an ideal representation for com- plex NLP tasks that require joint inference, such as event extraction. Unfortunately, the superior representational power greatly complicates infer- ence and learning over MLN models. Even the most advanced methods for inference and learning in MLNs (Gogate and Domingos, 2011) are un- able to handle complex, high-dimensional features, and therefore existing MLN systems primarily use low-dimensional features. This limitation severely affects the accuracy of MLN-based NLP systems, and as a result, in some cases their performance is inferior to pipeline methods that do not employ joint inference.</p><p>In this paper, we presented a general approach for exploiting the power of high-dimensional lin- guistic features in MLNs. Our approach involves reliably processing and learning high-dimensional features using SVMs and encoding their output as low-dimensional features in MLNs. We showed that we could achieve scalable learning and in- ference in our proposed MLN model by exploit- ing decomposition. Our results on the BioNLP shared tasks from '13, '11, and '09 clearly show that our proposed combination is extremely effec- tive, achieving the best or second best score on all three datasets.</p><p>In future work, we plan to (1) improve our joint model by incorporating co-reference information and developing model ensembles; (2) transfer the results of this investigation to other complex NLP tasks that can potentially benefit from joint infer- ence; and (3) develop scalable inference and learn- ing algorithms <ref type="bibr" target="#b2">(Ahmadi et al., 2013</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>7 .Figure 2 :</head><label>72</label><figDesc>Figure 2: The BioMLN structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Results on the BioNLP'09 test data. "−" 
indicates that the corresponding values are not 
known. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Peifeng Li, Guodong Zhou, Qiaoming Zhu, and Libin 
Hou. 2012. Employing compositional semantics 
and discourse consistency in Chinese event extrac-
tion. In Proceedings of the 2012 Joint Conference 
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning, 
pages 1006-1016. 

Lishuang Li, Yiwen Wang, and Degen Huang. 2013a. 
Improving feature-based biomedical event extrac-
tion system by integrating argument information. In 
Proceedings of the BioNLP Shared Task 2013 Work-
shop, pages 109-115. 

Peifeng Li, Qiaoming Zhu, and Guodong Zhou. 2013b. 
Argument inference from relevant event mentions in 
Chinese argument extraction. In Proceedings of the 
51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 1477-1487. 

Qi Li, Heng Ji, and Liang Huang. 2013c. Joint event 
extraction via structured prediction with global fea-
tures. In Proceedings of the 51st Annual Meeting 
of the Association for Computational Linguistics, 
pages 73-82. 

Shasha Liao and Ralph Grishman. 2010. Using doc-
ument level cross-event inference to improve event 
extraction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics, 
pages 789-797. </table></figure>

			<note place="foot" n="1"> Head words are found using Collins&apos; (1999) rules.</note>

			<note place="foot" n="3"> http://www.gurobi.com/</note>

			<note place="foot" n="4"> Detailed results are not shown for any of these three datasets due to space limitations.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by the AFRL un-der contract number FA8750-14-C-0021, by the ARO MURI grant W911NF-08-1-0242, and by the DARPA Probabilistic Programming for Advanced-Machine Learning Program under AFRL prime contract number FA8750-14-C-0005. Any opin-ions, findings, conclusions, or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views or official policies, either expressed or implied, of DARPA, AFRL, ARO or the US government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">Dataset #Papers #Abstracts #TT #Events BioNLP&apos;13</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">3348</biblScope>
		</imprint>
	</monogr>
	<note>0,0,0) 13 (2817</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">BioNLP&apos;</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3182</biblScope>
			<date type="published" when="10310" />
		</imprint>
	</monogr>
	<note>BioNLP&apos;09 (0,0,0) (800</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting symmetries for scaling loopy belief propagation and relational training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Mladenov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriraam</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="91" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The stages of event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Annotating and Reasoning About Time and Events</title>
		<meeting>the Workshop on Annotating and Reasoning About Time and Events</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generalizing biomedical event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jari</forename><surname>Björne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapio</forename><surname>Salakoski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BioNLP Shared Task 2011 Workshop</title>
		<meeting>the BioNLP Shared Task 2011 Workshop</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="183" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">TEES 2.1: Automated annotation scheme learning in the bionlp 2013 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jari</forename><surname>Björne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapio</forename><surname>Salakoski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BioNLP Shared Task 2013 Workshop</title>
		<meeting>the BioNLP Shared Task 2013 Workshop</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="16" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extracting complex biological events with rich graphbased feature sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jari</forename><surname>Björne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Heimonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Airola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapio</forename><surname>Pahikkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapio</forename><surname>Salakoski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task</title>
		<meeting>the BioNLP 2009 Workshop Companion Volume for Shared Task</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A fast rule-based approach for biomedical event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc-Chinh</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Van Mulligen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BioNLP Shared Task 2013 Workshop</title>
		<meeting>the BioNLP Shared Task 2013 Workshop</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="104" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint modeling for Chinese event extraction with rich linguistic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Computational Linguistics</title>
		<meeting>the 24th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="529" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Head-Driven Statistical Models for Natural Language Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<pubPlace>Philadelphia, PA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Markov Logic: An Interface Layer for Artificial Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lowd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morgan</surname></persName>
		</author>
		<imprint>
			<pubPlace>San Rafael, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Introduction to Statistical Relational Learning</title>
		<editor>Lise Getoor and Ben Taskar</editor>
		<imprint>
			<date type="published" when="2007" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Probabilistic theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Gogate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 27th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="256" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">NYU&apos;s English ACE 2005 system description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Westbrook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACE 2005 Evaluation Workshop</title>
		<meeting>the ACE 2005 Evaluation Workshop<address><addrLine>Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Predicting unknown time arguments based on cross-event propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashant</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-IJCNLP</title>
		<meeting>the ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Conference Short Papers</title>
		<imprint>
			<biblScope unit="page" from="369" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Yves Van de Peer, and Filip Ginter. 2013. EVEX in ST&apos;13: Application of a large-scale text mining resource to event extraction and network construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hakala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofie</forename><surname>Van Landeghem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapio</forename><surname>Salakoski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BioNLP Shared Task 2013 Workshop</title>
		<meeting>the BioNLP Shared Task 2013 Workshop</meeting>
		<imprint>
			<biblScope unit="page" from="26" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bootstrapped training of event extraction classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="286" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling textual cohesion for event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 26th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Refining event extraction through cross-document inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="254" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Making large-scale SVM learning practical</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel MethodsSupport Vector Learning</title>
		<editor>B. Schlkopf, C. Burges, and A. Smola</editor>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Corpus annotation for mining biomedical events from literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Overview of BioNLP&apos;09 shared task on event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshinobu</forename><surname>Kano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task</title>
		<meeting>the BioNLP 2009 Workshop Companion Volume for Shared Task</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Overview of BioNLP shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bossy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BioNLP Shared Task 2011 Workshop</title>
		<meeting>the BioNLP Shared Task 2011 Workshop</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Overview of Genia event task in BioNLP shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihisa</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Yonezawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BioNLP Shared Task 2011 Workshop</title>
		<meeting>the BioNLP Shared Task 2011 Workshop</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="7" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The Genia event extraction shared task, 2013 edition-overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yamamoto</forename><surname>Yasunori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BioNLP Shared Task 2013 Workshop</title>
		<meeting>the BioNLP Shared Task 2013 Workshop</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Probabilistic Graphical Models: Principles and Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Acquiring topic features to improve event extraction: in preselected and balanced collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shasha</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference Recent Advances in Natural Language Processing</title>
		<meeting>the International Conference Recent Advances in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generalizing an approximate subgraph matching-based system to extract events in molecular biology and cancer genetics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">C</forename><surname>Comeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W John</forename><surname>Wilbur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BioNLP Shared Task 2013 Workshop</title>
		<meeting>the BioNLP Shared Task 2013 Workshop</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient weight learning for markov logic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th European Conference on Principles and Practice of Knowledge Discovery in Databases</title>
		<meeting>the 11th European Conference on Principles and Practice of Knowledge Discovery in Databases</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="200" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automatic event extraction with structured preference modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="835" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">AND/OR branch-and-bound search for combinatorial optimization in graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Marinescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rina</forename><surname>Dechter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="1457" to="1491" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Event extraction as dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1626" to="1635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Event extraction as dependency parsing for BioNLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BioNLP Shared Task 2011 Workshop</title>
		<meeting>the BioNLP Shared Task 2011 Workshop</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="41" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Any domain parsing: Automatic domain adaptation for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Providence, RI</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Brown University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Evaluating dependency representation for event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadayoshi</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="779" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Event extraction with complex event classification using rich features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rune</forename><surname>Saetre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Bioinformatics and Computational Biology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="131" to="146" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Boosting automatic event extraction from the literature using domain adaptation and coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1759" to="1765" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Feature forest models for probabilistic HPSG parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Overview of BioNLP shared task 2013</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Nédellec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bossy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Zweigenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BioNLP Shared Task 2013 Workshop</title>
		<meeting>the BioNLP Shared Task 2013 Workshop</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A unified model of phrasal and sentential evidence for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Joint inference in information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd National Conference on Artificial Intelligence</title>
		<meeting>the 22nd National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="913" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Joint inference for knowledge extraction from biomedical literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="813" to="821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">An algorithm for suffix stripping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
			<publisher>Program</publisher>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="130" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">MSR-NLP entry in BioNLP shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BioNLP Shared Task 2011 Workshop</title>
		<meeting>the BioNLP Shared Task 2011 Workshop</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="155" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fast and robust joint models for biomedical event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Robust biomedical event extraction with dual decomposition and minimal domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BioNLP Shared Task 2011 Workshop</title>
		<meeting>the BioNLP Shared Task 2011 Workshop</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="46" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A Markov logic approach to bio-molecular event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Woo</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihisa</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task</title>
		<meeting>the BioNLP 2009 Workshop Companion Volume for Shared Task</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Model combination for event extraction in bionlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BioNLP Shared Task 2011 Workshop</title>
		<meeting>the BioNLP Shared Task 2011 Workshop</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="51" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Open domain event extraction from Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1104" to="1112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Local Search Strategies for Satisfiability Testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bram</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cliques, Coloring, and Satisfiability: Second DIMACS Implementation Challenge</title>
		<editor>D. S. Johnson and M. A. Trick</editor>
		<meeting><address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<publisher>American Mathematical Society</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="521" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Discriminative training of Markov logic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parag</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th National Conference on Artificial Intelligence</title>
		<meeting>the 20th National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="868" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Introduction to Dual Decomposition for Inference. Optimization for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Support vector machine learning for interdependent and structured output spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Machine Learning</title>
		<meeting>the 21st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="104" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">The Nature of Statistical Learning Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Springer</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
