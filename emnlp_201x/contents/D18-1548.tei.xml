<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Linguistically-Informed Self-Attention for Semantic Role Labeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google AI Language New York</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google AI Language New York</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Linguistically-Informed Self-Attention for Semantic Role Labeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="5027" to="5038"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>5027</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Current state-of-the-art semantic role labeling (SRL) uses a deep neural network with no explicit linguistic features. However, prior work has shown that gold syntax trees can dramatically improve SRL decoding, suggesting the possibility of increased accuracy from explicit modeling of syntax. In this work, we present linguistically-informed self-attention (LISA): a neural network model that combines multi-head self-attention with multi-task learning across dependency parsing, part-of-speech tagging, predicate detection and SRL. Unlike previous models which require significant pre-processing to prepare linguistic features, LISA can incorporate syntax using merely raw tokens as input, encoding the sequence only once to simultaneously perform parsing, predicate detection and role labeling for all predicates. Syntax is incorporated by training one attention head to attend to syntactic parents for each token. Moreover , if a high-quality syntactic parse is already available, it can be beneficially injected at test time without retraining our SRL model. In experiments on CoNLL-2005 SRL, LISA achieves new state-of-the-art performance for a model using predicted predicates and standard word embeddings, attaining 2.5 F1 absolute higher than the previous state-of-the-art on newswire and more than 3.5 F1 on out-of-domain data, nearly 10% reduction in error. On ConLL-2012 English SRL we also show an improvement of more than 2.5 F1. LISA also out-performs the state-of-the-art with contextually-encoded (ELMo) word representations , by nearly 1.0 F1 on news and more than 2.0 F1 on out-of-domain text.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic role labeling (SRL) extracts a high-level representation of meaning from a sentence, label- ing e.g. who did what to whom. Explicit repre- sentations of such semantic information have been shown to improve results in challenging down- stream tasks such as dialog systems <ref type="bibr" target="#b32">(Tur et al., 2005;</ref><ref type="bibr">Chen et al., 2013)</ref>, machine reading ( <ref type="bibr">Berant et al., 2014;</ref><ref type="bibr" target="#b34">Wang et al., 2015</ref>) and translation ( <ref type="bibr" target="#b5">Liu and Gildea, 2010;</ref><ref type="bibr">Bazrafshan and Gildea, 2013)</ref>.</p><p>Though syntax was long considered an obvious prerequisite for SRL systems <ref type="bibr" target="#b3">(Levin, 1993;</ref><ref type="bibr" target="#b20">Punyakanok et al., 2008)</ref>, recently deep neural net- work architectures have surpassed syntactically- informed models ( <ref type="bibr" target="#b38">Zhou and Xu, 2015;</ref><ref type="bibr" target="#b2">He et al., 2017;</ref><ref type="bibr" target="#b29">Tan et al., 2018;</ref><ref type="bibr">He et al., 2018)</ref>, achieving state-of-the art SRL performance with no explicit modeling of syntax. An additional benefit of these end-to-end models is that they require just raw tokens and (usually) detected predicates as input, whereas richer lin- guistic features typically require extraction by an auxiliary pipeline of models.</p><p>Still, recent work ( <ref type="bibr" target="#b22">Roth and Lapata, 2016;</ref><ref type="bibr" target="#b2">He et al., 2017;</ref> indi- cates that neural network models could see even higher accuracy gains by leveraging syntactic in- formation rather than ignoring it. <ref type="bibr" target="#b2">He et al. (2017)</ref> indicate that many of the errors made by a syntax- free neural network on SRL are tied to certain syntactic confusions such as prepositional phrase attachment, and show that while constrained in- ference using a relatively low-accuracy predicted parse can provide small improvements in SRL ac- curacy, providing a gold-quality parse leads to substantial gains.  incorporate syntax from a high-quality parser <ref type="bibr">(Kiperwasser and Goldberg, 2016</ref>) using graph convolutional neural networks <ref type="bibr" target="#b1">(Kipf and Welling, 2017</ref>), but like <ref type="bibr" target="#b2">He et al. (2017)</ref> they attain only small increases over a model with no syntactic parse, and even perform worse than a syntax-free model on out-of-domain data. These works sug- gest that though syntax has the potential to im- prove neural network SRL models, <ref type="bibr">we</ref> have not yet designed an architecture which maximizes the benefits of auxiliary syntactic information.</p><p>In response, we propose linguistically-informed self-attention (LISA): a model that combines multi-task learning <ref type="bibr">(Caruana, 1993)</ref> with stacked layers of multi-head self-attention ( <ref type="bibr" target="#b33">Vaswani et al., 2017)</ref>; the model is trained to: (1) jointly pre- dict parts of speech and predicates; (2) perform parsing; and (3) attend to syntactic parse parents, while (4) assigning semantic role labels. Whereas prior work typically requires separate models to provide linguistic analysis, including most syntax- free neural models which still rely on external predicate detection, our model is truly end-to-end: earlier layers are trained to predict prerequisite parts-of-speech and predicates, the latter of which are supplied to later layers for scoring. Though prior work re-encodes each sentence to predict each desired task and again with respect to each predicate to perform SRL, we more efficiently en- code each sentence only once, predict its pred- icates, part-of-speech tags and labeled syntactic parse, then predict the semantic roles for all pred- icates in the sentence in parallel. The model is trained such that, as syntactic parsing models im- prove, providing high-quality parses at test time will improve its performance, allowing the model to leverage updated parsing models without re- quiring re-training.</p><p>In experiments on the CoNLL-2005 and CoNLL-2012 datasets we show that our linguistically-informed models out-perform the syntax-free state-of-the-art. On <ref type="bibr">CoNLL-2005</ref> with predicted predicates and standard word embeddings, our single model out-performs the previous state-of-the-art model on the WSJ test set by 2.5 F1 points absolute. On the challenging out-of-domain Brown test set, our model improves substantially over the previous state-of-the-art by more than 3.5 F1, a nearly 10% reduction in error. On CoNLL-2012, our model gains more than 2.5 F1 absolute over the previous state-of-the-art. Our models also show improvements when using contextually-encoded word representations ( <ref type="bibr" target="#b17">Peters et al., 2018)</ref>, obtaining nearly 1.0 F1 higher than the state-of-the-art on <ref type="bibr">CoNLL-2005</ref> news and more than 2.0 F1 improvement on out-of-domain text. 1 <ref type="bibr">1</ref> Our implementation in TensorFlow ( <ref type="bibr" target="#b0">Abadi et al., 2015</ref> saw climbing B-ARG0 B-V B-ARG1 I-ARG1 I-ARG1 O O B-ARG0 I-ARG0 B-V <ref type="figure">Figure 1</ref>: Word embeddings are input to J layers of multi-head self-attention. In layer p one attention head is trained to attend to parse parents ( <ref type="figure" target="#fig_1">Figure  2</ref>). Layer r is input for a joint predicate/POS clas- sifier. Representations from layer r correspond- ing to predicted predicates are passed to a bilinear operation scoring distinct predicate and role rep- resentations to produce per-token SRL predictions with respect to each predicted predicate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>Our goal is to design an efficient neural network model which makes use of linguistic information as effectively as possible in order to perform end- to-end SRL. LISA achieves this by combining: (1) A new technique of supervising neural attention to predict syntactic dependencies with (2) multi-task learning across four related tasks. <ref type="figure">Figure 1</ref> depicts the overall architecture of our model. The basis for our model is the Trans- former encoder introduced by <ref type="bibr" target="#b33">Vaswani et al. (2017)</ref>: we transform word embeddings into contextually-encoded token representations us- ing stacked multi-head self-attention and feed- forward layers ( §2.1).</p><p>To incorporate syntax, one self-attention head is trained to attend to each token's syntactic par- ent, allowing the model to use this attention head as an oracle for syntactic dependencies. We in- troduce this syntactically-informed self-attention ( <ref type="figure" target="#fig_1">Figure 2</ref>) in more detail in §2.2.</p><p>Our model is designed for the more realistic set- ting in which gold predicates are not provided at test-time. Our model predicts predicates and inte- grates part-of-speech (POS) information into ear- lier layers by re-purposing representations closer to the input to predict predicate and POS tags us- MatMul: ing hard parameter sharing ( §2.3). We simplify optimization and benefit from shared statistical strength derived from highly correlated POS and predicates by treating tagging and predicate detec- tion as a single task, performing multi-class clas- sification into the joint Cartesian product space of POS and predicate labels. Though typical models, which re-encode the sentence for each predicate, can simplify SRL to token-wise tagging, our joint model requires a different approach to classify roles with respect to each predicate. Contextually encoded tokens are projected to distinct predicate and role em- beddings ( §2.4), and each predicted predicate is scored with the sequence's role representations us- ing a bilinear model (Eqn. 6), producing per-label scores for BIO-encoded semantic role labels for each token and each semantic frame.</p><formula xml:id="formula_0">Concat + FF sloth (i+1) + A[t] parse M [t] parse A i h V i h A[t] parse A[t] parse M i 0 [t] M i 1 [t] M i 2 [t] A i 2 [t] A i 1 [t] A i 0 [t]</formula><p>The model is trained end-to-end by maximum likelihood using stochastic gradient descent ( §2.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Self-attention token encoder</head><p>The basis for our model is a multi-head self- attention token encoder, recently shown to achieve state-of-the-art performance on SRL ( <ref type="bibr" target="#b29">Tan et al., 2018)</ref>, and which provides a natural mechanism for incorporating syntax, as described in §2.2. Our implementation replicates <ref type="bibr" target="#b33">Vaswani et al. (2017)</ref>.</p><p>The input to the network is a sequence X of T token representations x t . In the standard setting these token representations are initialized to pre- trained word embeddings, but we also experiment with supplying pre-trained ELMo representations combined with task-specific learned parameters, which have been shown to substantially improve performance of other SRL models ( <ref type="bibr" target="#b17">Peters et al., 2018)</ref>. For experiments with gold predicates, we concatenate a predicate indicator embedding p t following previous work <ref type="bibr" target="#b2">(He et al., 2017)</ref>.</p><p>We project 2 these input embeddings to a rep- resentation that is the same size as the output of the self-attention layers. We then add a positional encoding vector computed as a deterministic sinu- soidal function of t, since the self-attention has no innate notion of token position.</p><p>We feed this token representation as input to a series of J residual multi-head self-attention lay- ers with feed-forward connections. Denoting the jth self-attention layer as T (j) (·), the output of that layer s </p><formula xml:id="formula_1">s (j) t = LN (s (j1) t + T (j) (s (j1) t ))<label>(1)</label></formula><p>gives our final token representations s (j)</p><p>t . Each T (j) (·) consists of: (a) multi-head self-attention and (b) a feed-forward projection.</p><p>The multi-head self attention consists of H at- tention heads, each of which learns a distinct at- tention function to attend to all of the tokens in the sequence. This self-attention is performed for each token for each head, and the results of the H self-attentions are concatenated to form the final self-attended representation for each token.</p><p>Specifically, consider the matrix S (j1) of T to- ken representations at layer j 1. For each atten- tion head h, we project this matrix into distinct key, value and query representations K</p><formula xml:id="formula_2">(j) h , V (j) h and Q (j)</formula><p>h of dimensions T ⇥d k , T ⇥d q , and T ⇥d v , respectively. We can then multiply Q  <ref type="formula" target="#formula_1">(2017)</ref> we perform scaled dot-product attention: We scale the weights by the inverse square root of their embedding dimension and normalize with the softmax function to pro- duce a distinct distribution for each token over all the tokens in the sentence:</p><formula xml:id="formula_3">A (j) h = softmax(d 0.5 k Q (j) h K (j) h T )<label>(2)</label></formula><p>These attention weights are then multiplied by</p><formula xml:id="formula_4">V (j)</formula><p>h for each token to obtain the self-attended to- ken representations M (j) h :</p><formula xml:id="formula_5">M (j) h = A (j) h V (j) h (3) Row t of M (j)</formula><p>h , the self-attended representation for token t at layer j, is thus the weighted sum with respect to t (with weights given by A</p><formula xml:id="formula_6">(j) h ) over the token representations in V (j)</formula><p>h . The outputs of all attention heads for each token are concatenated, and this representation is passed to the feed-forward layer, which consists of two linear projections each followed by leaky ReLU activations ( <ref type="bibr" target="#b7">Maas et al., 2013)</ref>. We add the out- put of the feed-forward to the initial representa- tion and apply layer normalization to give the final output of self-attention layer j, as in Eqn. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Syntactically-informed self-attention</head><p>Typically, neural attention mechanisms are left on their own to learn to attend to relevant inputs. In- stead, we propose training the self-attention to at- tend to specific tokens corresponding to the syn- tactic structure of the sentence as a mechanism for passing linguistic knowledge to later layers.</p><p>Specifically, we replace one attention head with the deep bi-affine model of <ref type="bibr">Dozat and Manning (2017)</ref>, trained to predict syntactic dependencies. Let A parse be the parse attention weights, at layer i. Its input is the matrix of token representations S (i1) . As with the other attention heads, we project S (i1) into key, value and query represen- tations, denoted K parse , Q parse , V parse . Here the key and query projections correspond to parent and dependent representations of the tokens, and we allow their dimensions to differ from the rest of the attention heads to more closely follow the im- plementation of <ref type="bibr">Dozat and Manning (2017)</ref>. Un- like the other attention heads which use a dot prod- uct to score key-query pairs, we score the compati- bility between K parse and Q parse using a bi-affine operator U heads to obtain attention weights:</p><formula xml:id="formula_7">A parse = softmax(Q parse U heads K T parse ) (4)</formula><p>These attention weights are used to compose a weighted average of the value representations V parse as in the other attention heads. We apply auxiliary supervision at this attention head to encourage it to attend to each token's par- ent in a syntactic dependency tree, and to encode information about the token's dependency label. Denoting the attention weight from token t to a candidate head q as A parse <ref type="bibr">[t, q]</ref>, we model the probability of token t having parent q as:</p><formula xml:id="formula_8">P (q = head(t) | X ) = A parse [t, q]<label>(5)</label></formula><p>using the attention weights A parse <ref type="bibr">[t]</ref> as the distri- bution over possible heads for token t. We define the root token as having a self-loop. This atten- tion head thus emits a directed graph 3 where each token's parent is the token to which the attention A parse assigns the highest weight.</p><p>We also predict dependency labels using per- class bi-affine operations between parent and de- pendent representations Q parse and K parse to pro- duce per-label scores, with locally normalized probabilities over dependency labels y dep t given by the softmax function. We refer the reader to <ref type="bibr">Dozat and Manning (2017)</ref> for more details.</p><p>This attention head now becomes an oracle for syntax, denoted P, providing a dependency parse to downstream layers. This model not only pre- dicts its own dependency arcs, but allows for the injection of auxiliary parse information at test time by simply setting A parse to the parse parents pro- duced by e.g. a state-of-the-art parser. In this way, our model can benefit from improved, external parsing models without re-training. Unlike typi- cal multi-task models, ours maintains the ability to leverage external syntactic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-task learning</head><p>We also share the parameters of lower layers in our model to predict POS tags and predicates. Fol- lowing <ref type="bibr" target="#b2">He et al. (2017)</ref>, we focus on the end-to- end setting, where predicates must be predicted on-the-fly. Since we also train our model to predict syntactic dependencies, it is beneficial to give the model knowledge of POS information. While much previous work employs a pipelined approach to both POS tagging for dependency parsing and predicate detection for SRL, we take a multi-task learning (MTL) approach <ref type="bibr">(Caruana, 1993)</ref>, sharing the parameters of earlier layers in our SRL model with a joint POS and predicate de- tection objective. Since POS is a strong predic- tor of predicates 4 and the complexity of training a multi-task model increases with the number of tasks, we combine POS tagging and predicate de- tection into a joint label space: For each POS tag TAG which is observed co-occurring with a predi- cate, we add a label of the form TAG:PREDICATE.</p><p>Specifically, we feed the representation s (r) t from a layer r preceding the syntactically- informed layer p to a linear classifier to pro- duce per-class scores r t for token t. We compute locally-normalized probabilities using the softmax function: P (y prp t | X ) / exp(r t ), where y prp t is a label in the joint space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Predicting semantic roles</head><p>Our final goal is to predict semantic roles for each predicate in the sequence. We score each predicate against each token in the sequence using a bilinear operation, producing per-label scores for each to- ken for each predicate, with predicates and syntax determined by oracles V and P.</p><p>First, we project each token representation s</p><formula xml:id="formula_9">(J) t</formula><p>to a predicate-specific representation s pred t and a role-specific representation s role t . We then provide these representations to a bilinear transformation U for scoring. So, the role label scores s ft for the token at index t with respect to the predicate at index f (i.e. token t and frame f ) are given by:</p><formula xml:id="formula_10">s ft = (s pred f ) T Us role t<label>(6)</label></formula><p>which can be computed in parallel across all se- mantic frames in an entire minibatch. We calculate a locally normalized distribution over role labels for token t in frame f using the softmax function: P (y role ft | P, V, X ) / exp(s ft ). At test time, we perform constrained decoding using the Viterbi algorithm to emit valid sequences of BIO tags, using unary scores s ft and the transi- tion probabilities given by the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Training</head><p>We maximize the sum of the likelihoods of the in- dividual tasks. In order to maximize our model's ability to leverage syntax, during training we clamp P to the gold parse (P G ) and V to gold predicates V G when passing parse and predicate representations to later layers, whereas syntactic head prediction and joint predicate/POS prediction are conditioned only on the input sequence X . The overall objective is thus:</p><formula xml:id="formula_11">1 T T X t=1 h F X f =1 log P (y role ft | P G , V G , X )</formula><p>+ log P (y prp t | X ) + 1 log P (head(t) | X )</p><formula xml:id="formula_12">+ 2 log P (y dep t | P G , X ) i<label>(7)</label></formula><p>where 1 and 2 are penalties on the syntactic at- tention loss. We train the model using Nadam (Dozat, 2016) SGD combined with the learning rate schedule in <ref type="bibr" target="#b33">Vaswani et al. (2017)</ref>. In addition to MTL, we reg- ularize our model using dropout <ref type="bibr" target="#b24">(Srivastava et al., 2014</ref>). We use gradient clipping to avoid explod- ing gradients <ref type="bibr">(Bengio et al., 1994;</ref><ref type="bibr" target="#b14">Pascanu et al., 2013)</ref>. Additional details on optimization and hy- perparameters are included in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related work</head><p>Early approaches to SRL <ref type="bibr" target="#b19">(Pradhan et al., 2005;</ref><ref type="bibr" target="#b25">Surdeanu et al., 2007;</ref><ref type="bibr">Johansson and Nugues, 2008;</ref><ref type="bibr" target="#b30">Toutanova et al., 2008</ref>) focused on devel- oping rich sets of linguistic features as input to a linear model, often combined with complex con- strained inference e.g. with an ILP ( <ref type="bibr" target="#b20">Punyakanok et al., 2008)</ref>. <ref type="bibr">Täckström et al. (2015)</ref> showed that constraints could be enforced more efficiently us- ing a clever dynamic program for exact inference. <ref type="bibr" target="#b26">Sutton and McCallum (2005)</ref> modeled syntactic parsing and SRL jointly, and <ref type="bibr" target="#b4">Lewis et al. (2015)</ref> jointly modeled SRL and CCG parsing.  2018) jointly predict SRL spans and predicates in a model based on that of <ref type="bibr" target="#b2">Lee et al. (2017)</ref>, obtain- ing state-of-the-art predicted predicate SRL. Con- current to this work, <ref type="bibr" target="#b17">Peters et al. (2018)</ref> and <ref type="bibr">He et al. (2018)</ref> report significant gains on PropBank SRL by training a wide LSTM language model and using a task-specific transformation of its hid- den representations (ELMo) as a deep, and com- putationally expensive, alternative to typical word embeddings. We find that LISA obtains further ac- curacy increases when provided with ELMo word representations, especially on out-of-domain data.</p><p>Some work has incorporated syntax into neu- ral models for SRL. <ref type="bibr" target="#b22">Roth and Lapata (2016)</ref> in- corporate syntax by embedding dependency paths, and similarly  en- code syntax using a graph CNN over a pre- dicted syntax tree, out-performing models with- out syntax on CoNLL-2009. These works are limited to incorporating partial dependency paths between tokens whereas our technique incorpo- rates the entire parse. Additionally,  report that their model does not out-perform syntax-free models on out-of-domain data, a setting in which our technique excels.</p><p>MTL <ref type="bibr">(Caruana, 1993</ref>) is popular in NLP, and others have proposed MTL models which incor- porate subsets of the tasks we do <ref type="bibr">(Collobert et al., 2011;</ref><ref type="bibr" target="#b37">Zhang and Weiss, 2016;</ref><ref type="bibr">Hashimoto et al., 2017;</ref><ref type="bibr" target="#b15">Peng et al., 2017;</ref><ref type="bibr" target="#b27">Swayamdipta et al., 2017)</ref>, and we build off work that investigates where and when to combine different tasks to achieve the best results <ref type="bibr" target="#b23">(Søgaard and Goldberg, 2016;</ref><ref type="bibr">Bingel and Søgaard, 2017;</ref><ref type="bibr">Alonso and Plank, 2017)</ref>. Our specific method of incorporating supervision into self-attention is most similar to the concur- rent work of <ref type="bibr" target="#b6">Liu and Lapata (2018)</ref>, who use edge marginals produced by the matrix-tree algorithm as attention weights for document classification and natural language inference.</p><p>The question of training on gold versus pre- dicted labels is closely related to learning to search <ref type="bibr">(Daumé III et al., 2009;</ref><ref type="bibr" target="#b21">Ross et al., 2011;</ref><ref type="bibr">Chang et al., 2015</ref>) and scheduled sampling ( <ref type="bibr">Bengio et al., 2015)</ref>, with applications in NLP to sequence labeling and transition-based parsing ( <ref type="bibr">Choi and Palmer, 2011;</ref><ref type="bibr">Goldberg and Nivre, 2012;</ref><ref type="bibr">Ballesteros et al., 2016)</ref>. Our approach may be inter- preted as an extension of teacher forcing <ref type="bibr" target="#b35">(Williams and Zipser, 1989)</ref> to MTL. We leave exploration of more advanced scheduled sampling techniques to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental results</head><p>We present results on the CoNLL-2005 shared task ( <ref type="bibr">Carreras and M` arquez, 2005</ref>) and the CoNLL-2012 English subset of OntoNotes 5.0 ( <ref type="bibr" target="#b18">Pradhan et al., 2006</ref>), achieving state-of-the-art results for a single model with predicted predicates on both corpora. We experiment with both stan- dard pre-trained GloVe word embeddings <ref type="bibr" target="#b16">(Pennington et al., 2014</ref>) and pre-trained ELMo rep- resentations with fine-tuned task-specific parame- ters ( <ref type="bibr" target="#b17">Peters et al., 2018</ref>) in order to best compare to prior work. Hyperparameters that resulted in the best performance on the validation set were selected via a small grid search, and models were trained for a maximum of 4 days on one TitanX GPU using early stopping on the validation set. We convert constituencies to dependencies using the Stanford head rules v3. <ref type="bibr">5 (de Marneffe and Manning, 2008)</ref>. A detailed description of hyper- parameter settings and data pre-processing can be found in Appendix A.</p><p>We compare our LISA models to four strong baselines: For experiments using predicted predi- cates, we compare to <ref type="bibr">He et al. (2018)</ref> and the en- semble model (PoE) from <ref type="bibr" target="#b2">He et al. (2017)</ref>, as well as a version of our own self-attention model which does not incorporate syntactic information (SA). To compare to more prior work, we present addi- tional results on CoNLL-2005 with models given gold predicates at test time. In these experiments we also compare to <ref type="bibr" target="#b29">Tan et al. (2018)</ref>, the previous state-of-the art SRL model using gold predicates and standard embeddings.</p><p>We demonstrate that our models benefit from injecting state-of-the-art predicted parses at test time (+D&amp;M) by fixing the attention to parses predicted by <ref type="bibr">Dozat and Manning (2017)</ref>, the win- ner of the 2017 CoNLL shared task ( <ref type="bibr" target="#b36">Zeman et al., 2017</ref>) which we re-train using ELMo embeddings. In all cases, using these parses at test time im- proves performance.</p><p>We also evaluate our model using the gold syn- tactic parse at test time (+Gold), to provide an up- per bound for the benefit that syntax could have for SRL using LISA. These experiments show that despite LISA's strong performance, there remains substantial room for improvement. In §4.3 we per- form further analysis comparing SRL models us- ing gold and predicted parses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dev</head><p>WSJ <ref type="table">Test  Brown Test  GloVe  P  R  F1  P  R  F1  P  R  F1  He et</ref>    <ref type="table">Table 1</ref> lists precision, recall and F1 on the CoNLL-2005 development and test sets using pre- dicted predicates. For models using GloVe embed- dings, our syntax-free SA model already achieves a new state-of-the-art by jointly predicting pred- icates, POS and SRL. LISA with its own parses performs comparably to SA, but when supplied with D&amp;M parses LISA out-performs the previous state-of-the-art by 2.5 F1 points. On the out-of- domain Brown test set, LISA also performs com- parably to its syntax-free counterpart with its own parses, but with D&amp;M parses LISA performs ex- ceptionally well, more than 3.5 F1 points higher than <ref type="bibr">He et al. (2018)</ref>. Incorporating ELMo em- beddings improves all scores. The gap in SRL F1 between models using LISA and D&amp;M parses is smaller due to LISA's improved parsing ac- curacy (see §4.2), but LISA with D&amp;M parses still achieves the highest F1: nearly 1.0 abso- lute F1 higher than the previous state-of-the art on WSJ, and more than 2.0 F1 higher on Brown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Semantic role labeling</head><p>In both settings LISA leverages domain-agnostic syntactic information rather than over-fitting to the newswire training data which leads to high perfor- mance even on out-of-domain text.</p><p>To compare to more prior work we also evalu- ate our models in the artificial setting where gold predicates are provided at test time. For fair com- parison we use GloVe embeddings, provide pred- icate indicator embeddings on the input and re- encode the sequence relative to each gold predi- cate. Here LISA still excels: with D&amp;M parses, LISA out-performs the previous state-of-the-art by more than 2 F1 on both WSJ and Brown. <ref type="table" target="#tab_4">Table 3</ref> reports precision, recall and F1 on the CoNLL-2012 test set. We observe perfor- mance similar to that observed on ConLL-2005: Using GloVe embeddings our SA baseline al- ready out-performs <ref type="bibr">He et al. (2018)</ref> by nearly 1.5 F1. With its own parses, LISA slightly under-performs our syntax-free model, but when provided with stronger D&amp;M parses LISA out- performs the state-of-the-art by more than 2.5 F1. Like CoNLL-2005, ELMo representations im- prove all models and close the F1 gap between models supplied with LISA and D&amp;M parses. On this dataset ELMo also substantially narrows the  difference between models with-and without syn- tactic information. This suggests that for this chal- lenging dataset, ELMo already encodes much of the information available in the D&amp;M parses. Yet, higher accuracy parses could still yield improve- ments since providing gold parses increases F1 by 4 points even with ELMo embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Parsing, POS and predicate detection</head><p>We first report the labeled and unlabeled attach- ment scores (LAS, UAS) of our parsing models on the CoNLL-2005 and 2012 test sets <ref type="table" target="#tab_6">(Table 4)</ref> with GloVe (G) and ELMo (E) embeddings. D&amp;M achieves the best scores. Still, LISA's GloVe UAS is comparable to popular off-the-shelf de- pendency parsers such as spaCy, <ref type="bibr">5</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis</head><p>First we assess SRL F1 on sentences divided by parse accuracy. <ref type="table" target="#tab_9">Table 6</ref> lists average SRL F1 (across sentences) for the four conditions of LISA and D&amp;M parses being correct or not (L±, D±). Both parsers are correct on 26% of sentences.   Here there is little difference between any of the models, with LISA models tending to perform slightly better than SA. Both parsers make mis- takes on the majority of sentences (57%), diffi- cult sentences where SA also performs the worst. These examples are likely where gold and D&amp;M parses improve the most over other models in overall F1: Though both parsers fail to correctly parse the entire sentence, the D&amp;M parser is less wrong (87.5 vs. 85.7 average LAS), leading to higher SRL F1 by about 1.5 average F1. Following <ref type="bibr" target="#b2">He et al. (2017)</ref>, we next apply a series of corrections to model predictions in or- der to understand which error types the gold parse resolves: e.g. Fix Labels fixes labels on spans matching gold boundaries, and Merge Spans merges adjacent predicted spans into a gold span. <ref type="bibr">6</ref> In <ref type="figure" target="#fig_6">Figure 3</ref> we see that much of the performance gap between the gold and predicted parses is due to span boundary errors (Merge Spans, Split Spans and Fix Span Boundary), which supports the hy- pothesis proposed by <ref type="bibr" target="#b2">He et al. (2017)</ref> that incorpo- rating syntax could be particularly helpful for re- solving these errors. <ref type="bibr" target="#b2">He et al. (2017)</ref>  LISA +D&amp;M +Gold <ref type="figure">Figure 4</ref>: Percent and count of split/merge correc- tions performed in <ref type="figure" target="#fig_6">Figure 3</ref>, by phrase type.</p><formula xml:id="formula_13">facts-figures L+/D+ L-/D+ L+/D-L-/D- Proportion</formula><p>that these errors are due mainly to prepositional phrase (PP) attachment mistakes. We also find this to be the case: <ref type="figure">Figure 4</ref> shows a breakdown of split/merge corrections by phrase type. Though the number of corrections decreases substantially across phrase types, the proportion of corrections attributed to PPs remains the same (approx. 50%) even after providing the correct PP attachment to the model, indicating that PP span boundary mis- takes are a fundamental difficulty for SRL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present linguistically-informed self-attention: a multi-task neural network model that effectively incorporates rich linguistic information for seman- tic role labeling. LISA out-performs the state-of- the-art on two benchmark SRL datasets, includ- ing out-of-domain. Future work will explore im- proving LISA's parsing accuracy, developing bet- ter training techniques and adapting to more tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Syntactically-informed self-attention for the query word sloth. Attention weights A parse heavily weight the token's syntactic governor, saw, in a weighted average over the token values V parse. The other attention heads act as usual, and the attended representations from all heads are concatenated and projected through a feed-forward layer to produce the syntacticallyinformed representation for sloth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>t</head><label></label><figDesc>, and LN (·) layer normalization, the following recurrence applied to initial input c (p) t :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>to obtain a T ⇥ T matrix of attention weights A (j) h between each pair of tokens in the sentence. Fol- lowing Vaswani et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Collobert et al.</head><label></label><figDesc>(2011) were among the first to use a neural network model for SRL, a CNN over word embeddings which failed to out-perform non-neural models. FitzGerald et al. (2015) suc- cessfully employed neural networks by embed- ding lexicalized features and providing them as factors in the model of Täckström et al. (2015). More recent neural models are syntax-free. Zhou and Xu (2015), Marcheggiani et al. (2017) and He et al. (2017) all use variants of deep LSTMs with constrained decoding, while Tan et al. (2018) apply self-attention to obtain state-of- the-art SRL with gold predicates. Like this work, He et al. (2017) present end-to-end experiments, predicting predicates using an LSTM, and He et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance of CoNLL-2005 models after performing corrections from He et al. (2017).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Multi-head self-attention + FF Feed Forward J Feed Forward</head><label></label><figDesc></figDesc><table>) 
is available at : http://github.com/strubell/ 
LISA 

Bilinear 

I 
saw 
the 
sloth climbing 

PRP VBP:PRED DT NN VBG:PRED 

Syntactically-informed self-attention + FF 

Multi-head self-attention + FF 
r 

... 

p 

... 

s pred 
s role 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Precision, recall and F1 on CoNLL-2005 
with gold predicates. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Precision, recall and F1 on the CoNLL-
2012 development and test sets. Italics indicate 
a synthetic upper bound obtained by providing a 
gold parse at test time. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Parsing (labeled and unlabeled attach-
ment) and POS accuracies attained by the models 
used in SRL experiments on test datasets. Sub-
script G denotes GloVe and E ELMo embeddings. 

Model 
P 
R 
F1 

WSJ 
He et al. (2017) 94.5 98.5 96.4 
LISA 
98.9 97.9 98.4 

Brown 
He et al. (2017) 89.3 95.7 92.4 
LISA 
95.5 91.9 93.7 
CoNLL-12 LISA 
99.8 94.7 97.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Predicate detection precision, recall and 
F1 on CoNLL-2005 and CoNLL-2012 test sets. 

embeddings comparable to the standalone D&amp;M 
parser. The difference in parse accuracy between 
LISA G and D&amp;M likely explains the large in-
crease in SRL performance we see from decoding 
with D&amp;M parses in that setting. 
In Table 5 we present predicate detection pre-
cision, recall and F1 on the CoNLL-2005 and 
2012 test sets. SA and LISA with and without 
ELMo attain comparable scores so we report only 
LISA+GloVe. We compare to He et al. (2017) on 
CoNLL-2005, the only cited work reporting com-
parable predicate detection F1. LISA attains high 
predicate detection scores, above 97 F1, on both 
in-domain datasets, and out-performs He et al. 
(2017) by 1.5-2 F1 points even on the out-of-
domain Brown test set, suggesting that multi-task 
learning works well for SRL predicate detection. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Average SRL F1 on CoNLL-2005 for sen-
tences where LISA (L) and D&amp;M (D) parses were 
completely correct (+) or incorrect (-). 

Orig. 
Fix 
Labels 
Move 
Core 
Arg. 

Merge 
Spans 
Split 
Spans 
Fix 
Span 
Boundary 

Drop 
Arg. 
Add 
Arg. 

85.0 

87.5 

90.0 

92.5 

95.0 

97.5 

100.0 

F1 
SA 
LISA 
+D&amp;M 
+Gold 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head></head><label></label><figDesc>also point out</figDesc><table>PP 
NP 
VP SBAR ADVP PRN Other 
0 

10 

20 

30 

40 

50 

60 

% split/merge labels 

99 

27 
20 
12 
8 
4 
2 

78 

20 
15 
11 
7 
3 
2 

44 

11 
13 

3 
5 
5 
4 

</table></figure>

			<note place="foot" n="2"> All linear projections include bias terms, which we omit in this exposition for the sake of clarity.</note>

			<note place="foot" n="3"> Usually the head emits a tree, but we do not enforce it here.</note>

			<note place="foot" n="4"> All predicates in CoNLL-2005 are verbs; CoNLL-2012 includes some nominal predicates.</note>

			<note place="foot" n="6"> Refer to He et al. (2017) for a detailed explanation of the different error types.</note>

			<note place="foot">Eliyahu Kiperwasser and Yoav Goldberg. 2016. Simple and accurate dependency parsing using bidirectional LSTM feature representations. Transactions of the Association for Computational Linguistics, 4:313-327.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to Luheng He for helpful discus-sions and code, Timothy Dozat for sharing his code, and to the NLP reading groups at Google and UMass and the anonymous reviewers for feed-back on drafts of this work. This work was sup-ported in part by an IBM PhD Fellowship Award to E.S., in part by the Center for Intelligent Infor-mation Retrieval, and in part by the National Sci-ence Foundation under Grant Nos. DMR-1534431 and IIS-1514053. Any opinions, findings, conclu-sions or recommendations expressed in this mate-rial are those of the authors and do not necessarily reflect those of the sponsor.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martın</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">English verb classes and alternations: A preliminary investigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beth</forename><surname>Levin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>University of Chicago press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint A* CCG Parsing and Semantic Role Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic role features for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 23rd International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning structured text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="63" to="75" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A simple and accurate syntax-agnostic neural model for dependency-based semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Frolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn TreeBank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics-Special issue on using large corpora</title>
		<imprint>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The stanford typed dependencies representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2008 Workshop on Crossframework and Cross-domain Parser Evaluation</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A method of solving a convex programming problem with convergence rate o(1/k 2 )</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurii</forename><surname>Nesterov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="372" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The proposition bank: An annotated corpus of semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30 th International Conference on Machine Learning</title>
		<meeting>the 30 th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep multitask learning for semantic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards robust linguistic analysis using ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic role labeling using different syntactic views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kadri</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hacioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics 43rd annual meeting (ACL)</title>
		<meeting>the Association for Computational Linguistics 43rd annual meeting (ACL)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The importance of syntactic parsing and inference in semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasin</forename><surname>Punyakanok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="287" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the 14th International Conference on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural semantic role labeling with dependency path embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1192" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep multi-task learning with low level tasks supervised at lower layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="231" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Combination strategies for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pere</forename><forename type="middle">R</forename><surname>Comas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="105" to="151" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Joint parsing and semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Frame-semantic parsing with softmax-margin segmental rnns and a syntactic scaffold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09528</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient inference and structured learning for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="29" to="41" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep semantic role labeling with self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yidong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A global joint model for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="191" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semi-supervised learning for spoken language understanding using semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananlada</forename><surname>Chotimongkol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASRU</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Machine comprehension with syntax, frames, and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Conll 2017 shared task: Multilingual parsing from raw text to universal dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhani</forename><surname>Luotolahti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stackpropagation: Improved representation learning for syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1557" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">End-to-end learning of semantic role labeling using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>of the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
