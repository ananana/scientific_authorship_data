<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bilingual Correspondence Recursive Autoencoders for Statistical Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Yao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bilingual Correspondence Recursive Autoencoders for Statistical Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Learning semantic representations and tree structures of bilingual phrases is beneficial for statistical machine translation. In this paper, we propose a new neu-ral network model called Bilingual Correspondence Recursive Autoencoder (BCor-rRAE) to model bilingual phrases in translation. We incorporate word alignments into BCorrRAE to allow it freely access bilingual constraints at different levels. BCorrRAE minimizes a joint objective on the combination of a recursive au-toencoder reconstruction error, a structural alignment consistency error and a cross-lingual reconstruction error so as to not only generate alignment-consistent phrase structures, but also capture different levels of semantic relations within bilingual phrases. In order to examine the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently a variety of "deep architecture" ap- proaches, including autoencoders, have been suc- cessfully used in statistical machine translation (SMT) ( <ref type="bibr" target="#b39">Zou et al., 2013;</ref><ref type="bibr" target="#b5">Devlin et al., 2014;</ref><ref type="bibr" target="#b30">Tamura et al., 2014;</ref><ref type="bibr" target="#b29">Sundermeyer et al., 2014;</ref><ref type="bibr" target="#b14">Kočisk´Kočisk´y et al., 2014</ref>). Typically, these approaches represent words as dense, low-dimensional and * Corresponding author. real-valued vectors, i.e., word embeddings. How- ever, translation units in machine translation have long since shifted from words to phrases (se- quence of words), of which syntactic and se- mantic information cannot be adequately captured and represented by word embeddings. There- fore, learning compact vector representations for phrases or even longer expressions is more crucial for successful "deep" SMT.</p><p>To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the suc- cess of work on monolingual phrase embeddings <ref type="bibr" target="#b23">(Socher et al., 2010;</ref><ref type="bibr" target="#b24">Socher et al., 2011a;</ref><ref type="bibr" target="#b28">Socher et al., 2013b;</ref><ref type="bibr" target="#b2">Chen and Manning, 2014;</ref><ref type="bibr" target="#b11">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b12">Kim, 2014</ref>). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models ( ), translation models ( <ref type="bibr" target="#b4">Cui et al., 2014;</ref><ref type="bibr" target="#b6">Gao et al., 2014</ref>), or both language and translation models ( ). In spite of their success, these approaches center around cap- turing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases. The neglect of these important clues may be due to the big challenge imposed by the integration of them into the learning process of bilingual phrase rep- resentations. However, we believe such internal structures and correspondences can help us learn better phrase representations since they provide multi-level syntactic and semantic constraints.</p><p>In this paper, we propose a Bilingual Corre- spondence Recursive Autoencoder (BCorrRAE) to learn bilingual phrase embeddings. BCorrRAE substantially extends the Bilingually-constrained Recursive Auto-encoder (BRAE) ( ) to exploit both inner structures and corre-resolution 0 adopted <ref type="bibr">1</ref>  n ¯ e <ref type="figure">Figure 1</ref>: BRAE vs BCorrRAE models for generating of a bilingual phrase ("䘸坕 嚤 埏䍌 䛻懆", "resolution adopted today") with word alignments ("0-2 2-1 3-0"). The subscript number of each word indicates its position within phrase. Solid lines depict the generation procedure of phrase struc- tures, while dash lines illustrate the reconstruction procedure from one language to the other. In this paper, the dimension- ality of vector d in all figures is set to 3 for better illustration.</p><p>spondences within bilingual phrases. The intu- itions behind BCorrRAE are twofold: 1) bilingual phrase structure generation should satisfy word alignment constraints as much as possible; and 2) corresponding sub-phrases on the source and target side of bilingual phrases should be able to reconstruct each other as they are semantic equivalents. In order to model the first intuition, BCorrRAE punishes bilingual structures that vio- late word alignment constraints and rewards those in consistent with word alignments. This en- ables BCorrRAE to produce desirable bilingual phrase structures from the perspective of word alignments. With regard to the second intuition, BCorrRAE reconstructs structures of sub-phrases of one language according to aligned nodes in the other language and minimizes the gap between original and reconstructed structures. In doing so, BCorrRAE is capable of capturing semantic rela- tions at different levels.</p><p>To better illustrate our model, let us consider the example in <ref type="figure">Figure 1</ref>. Similar to the conven- tional recursive antoencoder (RAE), BRAE ne- glects bilingual correspondences of sub-phrases. Thus, it may combine "adopted" and "today" to- gether to generate an undesirable target tree struc- ture which violates word alignments. In contrast, BCorrRAE aligns source-side nodes (e.g. ("埏 䍌", "䛻 懆")) to their corresponding target-side nodes (accordingly ("resolution", "adopted")) ac- cording to word alignments. Furthermore, in BCorrRAE, each subtree on the target side can be reconstructed from the corresponding source node that aligns to the target-side node dominating the subtree and vice versa. These advantages allow us to obtain improved bilingual phrase embeddings with better inner correspondences of sub-phrases and word alignment consistency.</p><p>We conduct experiments with a state-of-the-art SMT system on large-scale data to evaluate the ef- fectiveness of BCorrRAE model. Results on the NIST 2006 and 2008 datasets show that our sys- tem achieves significant improvements over base- line methods. The main contributions of our work lie in the following three aspects:</p><p>• We learn both embeddings and tree struc- tures for bilingual phrases using cross-lingual RAE reconstruction that minimizes semantic distances between original and reconstructed subtrees. To the best of our knowledge, this has not been investigated before.</p><p>• We incorporate word alignment information to guide phrase structure generation and es- tablish internal semantic associations of sub- phrases within bilingual phrases.</p><p>• We integrate two similarity features based on BCorrRAE to enhance translation candidate selection, and achieve an improvement of 1.55 BLEU points on Chinese-English trans- lation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RAE and BRAE</head><p>In this section, we briefly introduce the RAE and its bilingual variation BRAE. This will provide background knowledge on our proposed BCor- rRAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">RAE</head><p>The component in the dash box of <ref type="figure" target="#fig_0">Figure 2</ref> illus- trates an instance of an RAE applied to a three- word phrase. The input to the RAE is x = (x 1 , x 2 , x 3 ), which are the d-dimensional vector representations of the ordered words in a phrase.</p><p>For two children c 1 = x 1 and c 2 = x 2 , the parent vector y 1 can be computed in the following way:</p><formula xml:id="formula_0">p = f (W (1) [c 1 ; c 2 ] + b (1) )<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">[c 1 ; c 2 ] ∈ R 2d×1 is the concatenation of c 1 and c 2 , W (1) ∈ R d×2d is a parameter matrix, b (1) ∈ R d×1</formula><p>is a bias term, and f is an element-</p><formula xml:id="formula_2">y 2 x 1 x 2 x 3 y 1 W (1) W (1) W (2) W (2) W (3)</formula><p>Reconstruction Error</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reconstruction Error</head><p>Max-Semantic- Margin Error wise activation function such as tanh(·), which is used for all activation functions in BRAE and our model. The learned parent vector p is also a d- dimensional vector. In order to measure how well p represents its children, we reconstruct the origi- nal children nodes in a reconstruction layer:</p><formula xml:id="formula_3">[c 1 ; c 2 ] = f (W (2) p + b (2) )<label>(2)</label></formula><p>where c 1 and c 2 are reconstructed children vectors,</p><formula xml:id="formula_4">W (2) ∈ R 2d×d and b (2) ∈ R 2d×1 .</formula><p>We can set y 1 = p and then further use Eq. (1) again to compute y 2 by setting [c 1 ;</p><formula xml:id="formula_5">c 2 ] = [y 1 ; x 3 ].</formula><p>This combination and reconstruction process of auto-encoder repeats at each node until the vec- tor of the entire phrase is generated. To obtain the optimal binary tree and phrase representation for x, we employ a greedy algorithm <ref type="bibr" target="#b26">(Socher et al., 2011c</ref>) to minimize the sum of reconstruction er- ror at each node in the binary tree T (x):</p><formula xml:id="formula_6">E rec (x; θ) = n∈T (x) 1 2 [c 1 ; c 2 ] n −[c 1 ; c 2 ] n 2 (3)</formula><p>where θ denotes model parameters and n repre- sents a node in T (x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">BRAE</head><p>BRAE jointly learns two RAEs for source and tar- get phrase embeddings as shown in <ref type="figure">Figure 1</ref>(a). The core idea behind BRAE is that a source phrase and its target correct translation should share the same semantic representations, while non-equivalent pairs should have different seman- tic representations.  use this intuition to constrain semantic pharse embedding learning. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, in addition to the above- mentioned reconstruction error, BRAE introduces a max-semantic-margin error to minimize the se- mantic distance between translation equivalents and maximize the semantic distance between non- equivalent pairs simultaneously. Formally, the max-semantic-margin error of a bilingual phrase (f, e) is defined as</p><formula xml:id="formula_7">E sem (f, e; θ) = E * sem (f |e, θ)+E * sem (e|f, θ) (4) where E * sem (f |e, θ)</formula><p>is used to ensure that the se- mantic error for an equivalent pair is much smaller than that for a non-equivalent pair (the source phrase f and a bad translation e ):</p><formula xml:id="formula_8">E * sem (f |e, θ) = max{0, E sem (f |e, θ) − E sem (f |e , θ) + 1} (5)</formula><p>where E sem (f |e, θ) is defined as the semantic dis- tance between the learned vector representations of f and e, denoted by p f and p e , respectively. Since phrase embeddings for the source and target language are learned separately in different vec- tor spaces, a transformation matrix W</p><formula xml:id="formula_9">(3) f</formula><p>∈ R d×d is introduced to capture this semantic transfor- mation in the source-to-target direction. Thus,</p><formula xml:id="formula_10">E sem (f |e, θ) is calculated as E sem (f |e, θ) = 1 2 p e − f (W (3) f p f + b (3) f ) 2 (6)</formula><p>where b</p><formula xml:id="formula_11">(3)</formula><p>f ∈ R d×1 is a bias term. E * sem (e|f, θ) and E sem (e|f, θ) can be computed in a similar way. The joint error of (f, e) is therefore defined as fol- lows:</p><formula xml:id="formula_12">E(f, e; θ) = α(E rec (f, θ) + E rec (e, θ)) +(1 − α)(E * sem (f |e, θ) + E * sem (e|f, θ))<label>(7)</label></formula><p>The final BRAE objective function over the train- ing instance set D becomes:</p><formula xml:id="formula_13">J BRAE = (f,e)∈D E(f, e; θ) + λ 2 θ 2 (8)</formula><p>Model parameters can be optimized over the total errors on training bilingual phrases in a co-training style algorithm ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The BCorrRAE Model</head><p>As depicted above, the learned embeddings us- ing BRAE may be unreasonable due to the ne- glect of bilingual constraints at different levels.</p><p>To address this drawback, we propose the BCor- rRAE for bilingual phrase embeddings, which in- corporates bilingual correspondence information into the learning process of structures and embed- dings via word alignments. In our model, we ex- plore word alignments in two ways: (1) ensuring that a learned bilingual phrase structure is con- sistent with word alignments as much as possi-ble; (2) identifying corresponding sub-phrases in the source language for reconstructing sub-phrases in the target language, and vice versa. More specifically, the former is to encourage alignment- consistent generation of sub-structures, while the latter is to minimize semantic distances between bilingual sub-phrases. In this section, we first formally introduce a concept of structural alignment consistency en- coded in bilingual phrase structure learning, which is the basis of our model. Then, we describe the objective function which is composed of three types of errors. Finally, we provide details on the training of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Structural Alignment Consistency</head><p>We adapt word alignment to structural alignment and introduce some related concepts. Given a bilingual phrase (f, e) with its binary tree struc- tures (T f , T e ), if the source node n ¯ f ∈ T f cov- ers a source-side sub-phrase ¯ f , and there exists a target-side sub-phrase ¯ e such that ( ¯ f , ¯ e) are consistent with word alignments <ref type="bibr" target="#b20">(Och and Ney, 2003)</ref>, we say n ¯ f satisfies the structural alignment consistency, and it is referred to as a structural- alignment-consistent (SAC) node. Further, if ¯ e is covered by a target node n ¯ e ∈ T e , we say n ¯ e is the aligned node of n ¯ f . In this way, several dif- ferent target nodes may be all aligned to the same source node because of null alignments. For this, we choose the target node with the smallest span as the aligned one for the considered source node. This is because a smaller span reflects a stronger semantic relevance in most situations.</p><p>Likewise, we have similar definitions for tar- get nodes. Note that alignment relations between source-and target-side nodes may not be symmet- ric. For example, in <ref type="figure">Figure 1</ref></p><formula xml:id="formula_14">(b), node n ¯ e is the aligned node of node n ¯ f 1 , while node n ¯ f 2 rather than n ¯ f 1</formula><p>is the aligned node of n ¯ e .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Objective Function</head><p>We elaborate the three types of errors defined for a bilingual phrase (f, e) with its binary tree struc- tures (T f , T e ) on both sides below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Reconstruction Error</head><p>Similar to RAE, the first error function is used to estimate how well learned phrase embeddings rep- resent corresponding phrases. The reconstruction error E rec (f, e; θ) of (f, e) is defined as follows:</p><formula xml:id="formula_15">E rec (f, e; θ) = E rec (f ; θ) + E rec (e; θ)<label>(9)</label></formula><p>where both E rec (f ; θ) and E rec (e; θ) can be calcu- lated according to Eq. (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Consistency Error</head><p>This metric corresponds to the first way in which we exploit word alignments mentioned before, which enables our model to generate as many SAC nodes as possible to respect word alignments. Formally, the consistency error E con (f, e; θ) of (f, e) is defined in the following way:</p><formula xml:id="formula_16">E con (f, e; θ) = E con (T f ; θ) + E con (T e ; θ) (10)</formula><p>where E con (T f ; θ) and E con (T e ; θ) denote the con- sistency error score for T f and T e , given word alignments. Here we only describe the calculation of the former while the latter can be calculated in exactly the same way.</p><p>To calculate E con (T f ; θ), we first judge whether a source node n ¯ f is an SAC node according to word alignments. Let p n ¯ f be the vector repre- sentation of n ¯ f . Following <ref type="bibr" target="#b23">Socher et al. (2010)</ref>, who use a simple inner product to measure how well the two words are combined into a phrase, we use inner product to calculate the consis- tency/inconsistency score for n ¯ f :</p><formula xml:id="formula_17">s(n ¯ f ) = W score p n ¯ f<label>(11)</label></formula><p>where W score ∈ R 1×d is the score parameter. We calculate W score by distinguishing SAC from non- SAC nodes defined as follows:</p><formula xml:id="formula_18">W score = W score cns if n ¯ f is an SAC node W score inc otherwise</formula><p>where the subscript cns and inc represent consis- tency and inconsistency, respectively. For exam- ple, in <ref type="figure">Figure 3</ref>, as n ¯ f 3 is a non-SAC node, we calculate the inconsistency score using W score inc for it.</p><p>We expect T f to satisfy structural alignment consistency as much as possible. Therefore we en- courage the consistency score for T f to be larger than its inconsistency score using a max-margin consistency error function:</p><formula xml:id="formula_19">E con (T f ; θ) =max{0, 1 − s(T f ) cns + s(T f ) ins } (12)</formula><p>where s(T f ) cns denotes the sum of consistency scores over all SAC nodes and s(T f ) ins the sum of inconsistency scores over all non-SAC nodes in T f . Minimizing this error function will maximize the sum of consistency scores of SAC nodes and minimize (up to a margin) the sum of inconsis-</p><formula xml:id="formula_20">resolution 0 adopted 1 所 1 通过 2 决议 3 n ¯ e W u e W (3) f n ¯ f 1 n ¯ f 2 n ¯ f 3</formula><p>Figure 3: The structure generation procedure of the source sub-phrase "嚤 埏䍌 䛻懆" and the structure reconstruction procedure of the target sub-phrase "resolution adopted". Ac- cording to word alignments ("2-1 3-0"), the node n ¯ f 1 and n ¯ f 2 are SAC ones while the node n ¯ f 3 is a non-SAC node.</p><p>tency scores of non-SAC nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Cross-Lingual Reconstruction Error</head><p>This metric corresponds to the second way in which we exploit word alignments. The assump- tion behind this is that a source/target node should be able to reconstruct the entire subtree rooted at its target/source aligned node as they are seman- tically equivalent. Based on this, for the consid- ered node, we calculate the cross-lingual recon- struction error along the entire subtree rooted at its aligned node in the other language and use the error to measure how well the learned vector rep- resents this node. Similarly, the cross-lingual reconstruction error E clrec (f, e; θ) of (f, e) can be decomposed into two parts as follows:</p><formula xml:id="formula_21">E clrec (f, e; θ) = E f 2e·rec (T f , T e ; θ) + E e2f ·rec (T f , T e ; θ)<label>(13)</label></formula><p>where E f 2e·rec (T f , T e ; θ) denotes the error score using T f to reconstruct T e . Note that in this pro- cess, the structure and the original node vector rep- resentations of T e have been already generated. E e2f ·rec (T f , T e ; θ) denotes the reconstruction er- ror score using T e to reconstruct T f . Here we still only describe the method of computing the former, which also applies to the latter.</p><p>To calculate E f 2e·rec (T f , T e ; θ), we first collect all source nodes (n ¯ f ) in T f and their aligned nodes (n ¯ e ) in T e to form a set of aligned node pairs S = {{n ¯ f , n ¯ e } according to word alignments. We then calculate E f 2e·rec (T f , T e ; θ) as the sum of error scores over all node pairs in S. Given a source node n ¯ f with its aligned node n ¯ e on the target side, we use n ¯ f to reconstruct the sub-tree structure T ¯ e rooted at n ¯ e and compute the error score based on the semantic distance between the original and reconstructed vector representations of nodes in T ¯ e . As source and target phrase em- beddings are separately learned, we first introduce a transformation matrix W</p><p>f and a bias term b</p><formula xml:id="formula_23">(3) f</formula><p>to transform source phrase embeddings into the target-side semantic space, following  and <ref type="bibr" target="#b14">Hermann and Blunsom (2014)</ref>:</p><formula xml:id="formula_24">p n¯ e = f (W (3) f p n ¯ f + b (3) f )<label>(14)</label></formula><p>here p n¯ e denotes the reconstructed vector represen- tation of n ¯ e , which is transformed from the vec- tor representation p n ¯ f of n ¯ f . Then, we repeat the reconstruction procedure in a top-down manner along the corresponding target tree structure un- til leaf nodes are reached, following <ref type="bibr" target="#b24">Socher et al. (2011a)</ref>. Specifically, given the vector representa- tion p n¯ e , we reconstruct vector representations of its two children nodes:</p><formula xml:id="formula_25">[c u e1 ; c u e2 ] = f (W u e p n¯ e + b u e )<label>(15)</label></formula><p>where c u e1 and c u e2 are the reconstructed vector rep- resentations of the children nodes, W u e ∈ R 2d×d , and b u e ∈R 2d×1 . Eventually, given the original and reconstructed target phrase representations, we calculate E f 2e·rec (T f , T e ; θ) as follows:</p><formula xml:id="formula_26">E f 2e·rec (T f , T e ; θ) = 1 2 n ¯ f ,n¯ e∈S n∈T¯ e p n −p n 2</formula><p>(16) where p n and p n are the original and reconstructed vector representations of node n in the sub-tree structure T ¯ e rooted at n ¯ e . This error function will be minimized so that semantic differences between original and reconstructed structures are minimal. <ref type="figure">Figure 3</ref> demonstrates the structure reconstruc- tion from a generated source sub-tree to its target counterpart. In this way, BCorrRAE propagates semantic information along dash lines sequentially until leaf nodes in the generated structure of the target phrase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">The Final Objective</head><p>Similar to Eq. (8), we define the final objective function of our model based on the three types of errors described above</p><formula xml:id="formula_27">J BCorrRAE = (f,e)∈D {α (E rec (f ; θ) + E rec (e; θ)) + β (E con (T f ; θ) + E con (T e ; θ)) + γ (E f 2e·rec (T f , T e ; θ) + E e2f ·rec (T f , T e ; θ))} + R(θ)<label>(17)</label></formula><p>where weights α, β, γ (s.t. α+β +γ = 1) are used to balance the preference among the three errors, and R(θ) is the regularization term. Parameters θ are divided into four sets 1 :</p><p>1. For regularization, we assign each parameter set a unique weight:</p><formula xml:id="formula_28">R(θ) = λ L 2 θ L 2 + λ rec 2 θ rec 2 + λ con 2 θ con 2 + λ lcrec 2 θ lcrec 2<label>(18)</label></formula><p>Additionally, in order to prevent the hidden layer from being very small, we normalize all output vectors of the hidden layer to have length 1, p = </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Training</head><p>Similar to , we adopt a co- training style algorithm to train model parameters in the following two steps: First, we use a normal distribution (µ = 0, σ = 0.01) to randomly initialize all model parameters, and adopt the standard RAE to pre-train source- and target-side phrase embeddings and tree struc- tures (Section 2.1).</p><p>Second, for each bilingual phrase, we update its source-side parameters to obtain the fine-tuned vector representation and binary tree of the source phrase, given the target-side phrase structure and node representations, and vice versa. In this pro- cess, we apply L-BFGS to tune parameters based on gradients over the joint error, as implemented in <ref type="bibr" target="#b26">(Socher et al., 2011c</ref>).</p><p>We repeat the procedure of the second step until either the joint error (shown in Eq. <ref type="formula" target="#formula_0">(17)</ref>) reaches a local minima or the number of iterations is larger than a pre-defined number (25 is used in experi- ments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Decoding with BCorrRAE</head><p>Once the model training is completed, we in- corporate two different phrasal similarity features built on the trained BCorrRAE into the standard log-linear framework of SMT. Given a bilingual phrase (f, e), we first obtain their semantic phrase representations (p f , p e ). Then we transform p f into p e in the target semantic space and p e into p f in the source semantic space via transforma- tion matrixes. Finally, we reconstruct sub-trees of p f along the source structure T f learned by BCor- rRAE, sub-trees of p e along the target structure T e . We exploit two kinds of phrasal similarity fea- tures based on the learned phrase representations and their tree structures as follows:</p><p>• Semantic Similarity measures the similarity between original and transformed phrase rep- resentations of (f, e):</p><formula xml:id="formula_29">Sim SM (p f , p f ) = 1 2 p f − p f 2 Sim SM (p e , p e ) = 1 2 p e − p e 2<label>(19)</label></formula><p>• Structural Similarity calculates the similarity between original and reconstructed tree struc- tures learned by BCorrRAE for (f, e):</p><formula xml:id="formula_30">Sim ST (p f , p f ) = 1 2C f n∈T f p n − p n 2 Sim ST (p e , p e ) = 1 2C e n∈Te p n − p n 2<label>(20)</label></formula><p>where p n and p n represent vector representations of original and reconstructed node n, and C f and C e count the number of nodes in the source and target tree structure respectively. Note that if we only compute the similarity for root nodes in the bilingual tree of (f, e), the structural similarity equals to the semantic similarity in Eq. <ref type="bibr">(19)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We conducted experiments on NIST Chinese- English translation task to validate the effective- ness of BCorrRAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">System Overview</head><p>Our baseline decoder is a state-of-the-art phrase- based translation system equipped with a maxi- mum entropy based reordering model (MEBTG). It adopts three bracketing transduction grammar rules <ref type="bibr" target="#b34">(Wu, 1997;</ref><ref type="bibr" target="#b36">Xiong et al., 2006</ref>): merging rules A → [A 1 , A 2 ]||A 1 , A 2 which are used to merge two neighboring blocks 2 A 1 and A 2 in a straight|inverted order, and lexical rule A → f /e used to translate a source phrase f into a target phrase e.</p><p>The MEBTG system features a maximal en- tropy classifier based reordering model that pre- dicts orientations of neighboring blocks. During training, we extract bilingual phrases containing up to 7 words on the source side from the training corpus. With the collected reordering examples, we adopt the maximal entropy toolkit 3 developed by Zhang to train the reordering model with the following parameters: iteration number iter=200 and gaussian prior g=1.0. Following <ref type="bibr" target="#b36">Xiong et al. (2006)</ref>, we use only boundary words of blocks to trigger the reordering model. The whole translation model is organized in a log-linear framework <ref type="bibr" target="#b19">(Och and Ney, 2002</ref>). The adopted sub-models mainly include: (1) rule trans- lation probabilities in two directions, (2) lexical weights in two directions, (3) targets-side word number, (4) phrase number, (5) language model score, and (6) the score of maximal entropy based reordering model. We perform minimum error rate training <ref type="bibr" target="#b21">(Och, 2003)</ref> to tune various fea- ture weights. During decoding, we set ttable- limit=20 for translation candidates kept for each source phrase, stack-size=100 for hypotheses in each span, and swap-span=15 for the length of the maximal reordering span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Setup</head><p>Our bilingual data is the combination of the FBIS corpus and Hansards part of LDC2004T07 corpus, which contains 1.0M parallel sentences (25.2M Chinese words and 29M English words). Follow- ing , we collected 1.44M bilin- gual phrases using forced decoding <ref type="bibr" target="#b35">(Wuebker et al., 2010</ref>) to train BCorrRAE from the training data. We used a 5-gram language model trained on the Xinhua portion of Gigaword corpus using SRILM Toolkits <ref type="bibr">4</ref> . Translation quality is evaluated by case-insensitive BLEU-4 metric ( <ref type="bibr" target="#b22">Papineni et al., 2002</ref>). We performed paired bootstrap sam- pling <ref type="bibr" target="#b13">(Koehn, 2004</ref>) to test the significance in BLEU score differences. In our experiments, we used NIST MT05 and MT06/MT08 data set as the</p><formula xml:id="formula_31">Parameter BRAE BCorrRAE α 0.119 0.121 β - 0.6331 γ - 0.2459 λL 4.95 ×10 −5 3.13 ×10 −5 λrec</formula><p>2.64 ×10 −7 2.05 ×10 −5 λcon - 7.32 ×10 −6 λ lcrec 9.31 ×10 −5 5.25 ×10 −6  <ref type="table">Table 2</ref>: Experiment results for different dimensions (d).</p><p>BCorrRAESM and BCorrRAEST are our systems that are enhanced with the semantic and structural similarity features learned by BCorrRAE, respectively. ↓/⇓: significantly worse than the BCorrRAEST with the same dimensionality (p &lt; 0.05/p &lt; 0.01).</p><p>development and test set, respectively. In addition to the baseline described below, we also compare our method against the BRAE model, which focuses on modeling relations of source and target phrases as a whole unit. Word embeddings in BRAE are pre-trained with toolkit Word2Vec 5 ( <ref type="bibr" target="#b18">Mikolov et al., 2013</ref>) on large-scale monolingual data that contains 0.83B words for Chinese and 0.11B words for English.</p><p>Hyper-parameters in all neural models are op- timized by random search <ref type="bibr" target="#b1">(Bergstra and Bengio, 2012</ref>) based on related joint errors. We ran- domly extracted 250, 000 bilingual phrases from the above-mentioned training data as training set, 5, 000 as development set and another 5, 000 as test set. We drew α, β, γ uniformly from 0.10 to 0.50, and λ L , λ rec , λ con and λ lcrec exponentially from 10 −8 to 10 −2 . Final parameters are shown in <ref type="table" target="#tab_2">Table 1</ref> for both BRAE and BCorrRAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Dimensionality of Embeddings</head><p>To investigate the impact of embedding dimen- sionality on our BCorrRAE, we tried four differ- ent dimensions from 25 to 100 with an increment of 25 each time. The results are displayed in Ta- ble 2. We can observe that the performance of our model is not consistently improved with the incre- ment of dimensionality. This may be because a larger dimension brings in much more parameters, and therefore makes parameter tuning more diffi- cult. In practice, setting the dimension d to 50, we can get satisfactory results without much compu- tation effort, which has also been found by .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Structural Similarity vs. Semantic Similarity</head><p>Table 2 also shows that the performance of BCorrRAE ST , the system with the structural sim- ilarity feature in Eq. <ref type="formula" target="#formula_3">(20)</ref>, is always superior to that of BCorrRAE SM with the semantic similar- ity feature in Eq. <ref type="bibr">(19)</ref>. BCorrRAE ST is bet- ter than BCorrRAE SM by 0.483 BLEU points on average. In most cases, differences between BCorrRAE ST and BCorrRAE SM with the same dimensionality are statistically significant. This suggests that digging into structures of bilingual phrases (BCorrRAE ST ) can obtain further im- provements over only modeling bilingual phrases as whole units (BCorrRAE SM ). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Overall Performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Analysis</head><p>We compute a ratio of aligned nodes (Section 3.1) over all nodes to estimate how well tree struc- tures of bilingual phrases generated by BRAE and BCorrRAE are consistent with word alignments. We consider two factors when computing the ra- tio: the length of the source side of a bilingual phrase l s and the length of a span covered by an aligned node l a . The result is illustrated in <ref type="table" target="#tab_6">Table  4</ref>. <ref type="bibr">6</ref> We find that BCorrRAE significantly outper-   forms BRAE model by 7.22% on average in terms of the aligned node ratio. This strongly demon- strates that the proposed BCorrRAE is able to gen- erate tree structures that are more consistent with word alignments than those generated by BRAE.</p><p>We further show example source phrases in Ta- ble 5 with their most semantically similar trans- lations learned by BRAE and BCorrRAE in the training corpus. Both models can select correct translations for content words. However, they are different in dealing with function words. Com- pared to our model, the BRAE model prefers longer target phrases surrounded with function words. Take the source phrase "惮䜃 坝揔" as an example, the BRAE model learns both "a serious challenge to" and "a serious challenge from" as its semantically similar target phrases. Although the content words "惮䜃" and "坝揔" are trans- lated correctly into "serious" and "challenge", the function words "to" and "from" express exactly the opposite meanings. In contrast, our model, es- pecially the BCorrRAE ST model, tends to choose shorter translations that are consistent with word alignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>A variety of efforts have been devoted to learn- ing vector representations for words/phrases with deep neural networks. According to the differ- ence of learning contexts, previous work mainly include the following two strands.</p><p>(1) Monolingual Word/Phrase Embeddings. The straightforward approach to represent word/phrases is to learn their hidden represen- tations with traditional feature vectors, which requires manual and task-dependent feature engineering ( <ref type="bibr" target="#b4">Cui et al., 2014;</ref><ref type="bibr" target="#b33">Wu et al., 2014</ref>  Chen and Manning, 2014). To avoid exploiting manually input features, <ref type="bibr" target="#b0">Bengio et al. (2003)</ref> convert words to dense, real-valued vectors by learning probability distributions of n-grams. <ref type="bibr" target="#b18">Mikolov et al. (2013)</ref> generate word vectors by predicting their limited context words. Instead of exploiting outside context information, recursive auto-encoder is usually adopted to learn the com- position of internal words <ref type="bibr" target="#b23">(Socher et al., 2010;</ref><ref type="bibr" target="#b25">Socher et al., 2011b;</ref><ref type="bibr" target="#b28">Socher et al., 2013b;</ref><ref type="bibr" target="#b27">Socher et al., 2013a)</ref>. Recently, convolution architecture has drawn more and more attention due to its ability to explicitly capture short and long-range relations <ref type="bibr" target="#b3">(Collobert et al., 2011;</ref><ref type="bibr" target="#b10">Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b11">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b12">Kim, 2014)</ref>.</p><p>(2) Bilingual Word/Phrase Embeddings. In the field of machine translation and cross-lingual in- formation processing, bilingual embedding learn- ing has become an increasingly important study. The bilingual embedding research origins in the word embedding learning, upon which <ref type="bibr" target="#b39">Zou et al. (2013)</ref> utilize word alignments to constrain translational equivalence. Kočisk´ <ref type="bibr" target="#b14">Kočisk´y et al. (2014)</ref> propose a probability model to capture more se- mantic information by marginalizing over word alignments. More specifically to SMT, its main components have been exploited to learn better bilingual phrase embeddings in different aspects: language models ( <ref type="bibr" target="#b7">Garmash and Monz, 2014</ref>), reordering models ( ) and translation models ( <ref type="bibr" target="#b31">Tran et al., 2014;</ref>). Instead of exploiting a single model,  combine the recursive and recur- rent neural network to incorporate the language and translation model. Different from the methods mentioned above, our model considers both the cross-language con- sistency of phrase structures and internal corre- spondence relations inside bilingual phrases. The most related works include  and <ref type="bibr" target="#b24">Socher et al. (2011a)</ref>. Compared with these works, our model exploits different levels of cor- respondence relations inside bilingual phrases in- stead of only the top level of entire phrases, and reconstructs tree structures of sub-phrases in one language according to aligned nodes in the other language, which, to the best of our knowledge, has never been investigated before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>In this paper, we have presented the BCorrRAE to learn phrase embeddings and tree structures of bilingual phrases for SMT. Punishing structural- alignment-inconsistent sub-structures and mini- mizing the gap between original and reconstructed structures, our approach is able to not only gen- erate alignment-consistent phrase structures, but also capture different levels of semantic rela- tions within bilingual phrases. Experiment results demonstrate the effectiveness of our model.</p><p>In the future, we would like to derive more features from BCorrRAE, e.g., consis- tency/inconsistency scores of bilingual phrases, to further enhance SMT. Additionally, we also want to apply our model to other bilingual tasks, e.g., learning bilingual terminology or paraphrases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of the BRAE architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Hyper-parameters for BCorrRAE and BRAE model. 

Method 
d 
MT06 
MT08 
AVG 

BCorrRAESM 

25 
30.81 
22.68 ⇓ 26.75 
50 
30.58 ↓ 
22.72 ⇓ 26.65 
75 
30.50 
22.53 ⇓ 26.52 
100 30.34 ⇓ 22.61 ⇓ 26.48 

BCorrRAEST 

25 
30.56 
23.28 
26.92 
50 
30.94 
23.33 
27.14 
75 
30.73 
23.40 
27.07 
100 30.90 
23.50 
27.20 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 summarizes</head><label>3</label><figDesc></figDesc><table>the comparison results of dif-
ferent models on the test sets. The BCorrRAE SM 
outperforms the baseline and BRAE by 1.06 and 
0.25 BLEU points on average respectively, while 
BCorrRAE ST gains 1.55 and 0.74 BLEU points 
on average over the baseline and BRAE. The im-
provements of BCorrRAE ST over the baseline, 
BRAE and BCorrRAE SM are statistically signif-
icant at different levels. This demonstrates the 
advantage of our BCorrRAE over BRAE in that 
BCorrRAE is able to explore sub-structures of 
bilingual phrases. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 : Experiment results on the test sets. AVG = average BLEU scores for test sets. For both BRAE and BCorrRAE</head><label>3</label><figDesc></figDesc><table>, 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Aligned node ratio for source phrases of different lengths.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>;</head><label></label><figDesc></figDesc><table>Source Phrase 

BRAE 
BCorrRAESM 
BCorrRAEST 

䌓 䌓 䌓㥎 㥎 㥎 
to advocate the 
out to advocate 
encouraging 
in preaching the 
been encouraging 
claimed 
(advocate) 
the promotion of 
an advocate 
advocate 

惮 惮 惮䜃 䜃 䜃 坝 坝 坝揔 揔 揔 
as well as severe challenges 
of rigorous challenges 
rigorous challenge 
a serious challenge to 
as well as severe challenges 
enormous challenge 
(serious challenge) 
a serious challenge from 
of severe challenges 
severe challenge 

䋺 䋺 䋺㟙 㟙 㟙 䀛 䀛 䀛 嗪 嗪 嗪䛢 䛢 䛢 
by the figures published by the to the estimates announced 
published data 
the statistics released by 
at the figures published 
released figures 
(data released) 
data published by the 
the statistics released by 
the estimates announced 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Semantically similar target phrases in the training set for example source phrases. 

</table></figure>

			<note place="foot" n="1"> Note that the source and target languages have different four sets of parameters.</note>

			<note place="foot" n="2"> A block is a bilingual phrase without maximum length limitation. 3 http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html 4 http://www.speech.sri.com/projects/srilm/download.html</note>

			<note place="foot" n="5"> https://code.google.com/p/word2vec/</note>

			<note place="foot" n="6"> We only give ratios for bilingual phrases with sourceside length from 3 to 4 words because 1) ratios of BRAE and BCorrRAE in the case of la &lt; 3 are very close and 2) phrases with length &gt; 4 are rarely used during decoding (accounting for &lt; 0.5%).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors were supported by National Nat-ural Science Foundation of China (Grant Nos 61303082 and 61403269), Natural Science </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1139" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning topic representation for smt with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muyun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL 2014</title>
		<meeting>of ACL 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="133" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL 2014</title>
		<meeting>of ACL 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1370" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning continuous phrase representations for translation modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="699" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Garmash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dependency-based bilingual language models for reordering in statistical machine translation</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP 2014</title>
		<meeting>of EMNLP 2014</meeting>
		<imprint>
			<biblScope unit="page" from="1689" to="1700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multilingual models for compositional distributed semantics</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP 2013</title>
		<meeting>of EMNLP 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP 2014</title>
		<meeting>of EMNLP 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning bilingual word representations by marginalizing alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk´kočisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="224" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recursive autoencoders for ITG-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP 2013</title>
		<meeting>of EMNLP 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="567" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Additive neural networks for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL 2013</title>
		<meeting>of ACL 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="791" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A recursive recurrent neural network for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1491" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discriminative training and maximum entropy models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL 2002</title>
		<meeting>of ACL 2002</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="295" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL 2003</title>
		<meeting>of ACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL 2002</title>
		<meeting>of ACL 2002</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning continuous phrase representations and syntactic parsing with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff Chiung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP 2011</title>
		<meeting>of EMNLP 2011</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ng</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL 2013</title>
		<meeting>of ACL 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP 2013</title>
		<meeting>of EMNLP 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Translation modeling with bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamer</forename><surname>Alkhouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="14" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for word alignment model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1470" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Word translation prediction for morphologically rich languages with bilingual neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP 2014</title>
		<meeting>of EMNLP 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1676" to="1688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural network based bilingual language model growing for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP 2014</title>
		<meeting>of EMNLP 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="189" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improve statistical machine translation with context-sensitive bilingual semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP 2014</title>
		<meeting>of EMNLP 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="142" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stochastic inversion transduction grammars and bilingual parsing of parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="377" to="403" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Training phrase translation models with leaving-one-out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Mauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL 2010</title>
		<meeting>of ACL 2010</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="475" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Maximum entropy based phrase reordering model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="521" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Word alignment modeling with context dependent deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL 2013</title>
		<meeting>of ACL 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="166" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bilingually-constrained phrase embeddings for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL 2014</title>
		<meeting>of ACL 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="111" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP 2013</title>
		<meeting>of EMNLP 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
