<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AD3: Attentive Deep Document Dater</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swayambhu</forename><surname>Nath</surname></persName>
							<email>swayambhunath93@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Indian Institute of Science Bangalore</orgName>
								<orgName type="department" key="dep2">Indian Institute of Science Bangalore</orgName>
								<orgName type="institution">Indian Institute of Science Bangalore</orgName>
								<address>
									<country>India, India, India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Indian Institute of Science Bangalore</orgName>
								<orgName type="department" key="dep2">Indian Institute of Science Bangalore</orgName>
								<orgName type="institution">Indian Institute of Science Bangalore</orgName>
								<address>
									<country>India, India, India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shib</forename><forename type="middle">Sankar</forename><surname>Dasgupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Indian Institute of Science Bangalore</orgName>
								<orgName type="department" key="dep2">Indian Institute of Science Bangalore</orgName>
								<orgName type="institution">Indian Institute of Science Bangalore</orgName>
								<address>
									<country>India, India, India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Indian Institute of Science Bangalore</orgName>
								<orgName type="department" key="dep2">Indian Institute of Science Bangalore</orgName>
								<orgName type="institution">Indian Institute of Science Bangalore</orgName>
								<address>
									<country>India, India, India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AD3: Attentive Deep Document Dater</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1871" to="1880"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1871</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Knowledge of the creation date of documents facilitates several tasks such as summarization, event extraction, temporally focused information extraction etc. Unfortunately, for most of the documents on the Web, the time-stamp metadata is either missing or can&apos;t be trusted. Thus, predicting creation time from document content itself is an important task. In this paper, we propose Attentive Deep Document Dater (AD3), an attention-based neural document dating system which utilizes both context and temporal information in documents in a flexible and principled manner. We perform extensive experimentation on multiple real-world datasets to demonstrate the effectiveness of AD3 over neural and non-neural baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many natural language processing tasks require document creation time (DCT) information as a useful additional metadata. Tasks such as infor- mation retrieval ( <ref type="bibr" target="#b13">Li and Croft, 2003;</ref><ref type="bibr" target="#b4">Dakka et al., 2008)</ref>, temporal scoping of events and facts <ref type="bibr" target="#b0">(Allan et al., 1998;</ref><ref type="bibr" target="#b21">Talukdar et al., 2012b</ref>), document summarization ( <ref type="bibr" target="#b27">Wan, 2007</ref>) and analysis <ref type="bibr" target="#b5">(de Jong et al., 2005a</ref>) require precise and validated cre- ation time of the documents. Most of the docu- ments obtained from the Web either contain DCT that cannot be trusted or contain no DCT informa- tion at all ( <ref type="bibr" target="#b8">Kanhabua and Nørvåg, 2008)</ref>. Thus, predicting the time of these documents based on their content is an important task, often referred to as Document Dating.</p><p>A few generative approaches (de <ref type="bibr" target="#b6">Jong et al., 2005b;</ref><ref type="bibr" target="#b8">Kanhabua and Nørvåg, 2008)</ref> as well as a discriminative model <ref type="bibr" target="#b3">(Chambers, 2012</ref>) have been previously proposed for this task. <ref type="bibr" target="#b11">Kotsakos et al. (2014)</ref> employs term-burstiness resulting in im- proved precision on this task.</p><p>Recently proposed NeuralDater ( <ref type="bibr" target="#b23">Vashishth et al., 2018</ref>) uses a graph convolution network (GCN) based approach for document dating, out- performing all previous models by a significant margin. NeuralDater extensively uses the syntac- tic and temporal graph structure present within the document itself. Motivated by NeuralDater, we explicitly develop two different methods: a) Atten- tive Context Model, and b) Ordered Event Model. The first component tries to accumulate knowl- edge across documents, whereas the latter uses the temporal structure of the document for predicting its DCT.</p><p>Motivated by the effectiveness of attention based models in different NLP tasks <ref type="bibr" target="#b28">(Yang et al., 2016a</ref>; <ref type="bibr" target="#b1">Bahdanau et al., 2014)</ref>, we incorporate at- tention in our method in a principled fashion. We use attention not only to capture context but also for feature aggregation in the graph convolution network ( <ref type="bibr" target="#b7">Hamilton et al., 2017)</ref>. Our contributions are as follows.</p><p>• We propose Attentive Deep Document Dater (AD3), the first attention-based neural model for time-stamping documents.</p><p>• We devise a novel method for label based attentive graph convolution over directed graphs and use it for the document dating task.</p><p>•   <ref type="bibr">[1, n]</ref>) denotes attention over the words of document and αa, α b and αs denote attention over nodes connected with edge labels AFTER, BEFORE and SIMULTANEOUS, respectively. OE-GCN provides the probability scores over the years given the encoded DCT, while AC-GCN provides the probability scores given the context of the document. Both the models are trained separately.</p><p>made for document time-stamping task include statistical language models proposed by de <ref type="bibr" target="#b6">Jong et al. (2005b)</ref> and <ref type="bibr" target="#b8">Kanhabua and Nørvåg (2008)</ref>. <ref type="bibr" target="#b3">(Chambers, 2012</ref>) use temporal and hand-crafted features extracted from documents to predict DCT. They propose two models, one of which learns the probabilistic constraints between year mentions and the actual creation time, whereas the other one is a discriminative model trained on hand-crafted features. <ref type="bibr" target="#b11">Kotsakos et al. (2014)</ref> propose a term- burstiness ( <ref type="bibr" target="#b12">Lappas et al., 2009</ref>) based statistical method for the task. <ref type="bibr" target="#b23">Vashishth et al. (2018)</ref> pro- pose a deep learning based model which exploits the temporal and syntactic structure in documents using graph convolutional networks (GCN).</p><p>Event Ordering System: The task of extract- ing temporally rich events and time expressions and ordering between them is introduced in the TempEval challenge <ref type="bibr" target="#b22">(UzZaman et al., 2013;</ref><ref type="bibr" target="#b26">Verhagen et al., 2010)</ref>. Various approaches <ref type="bibr" target="#b16">(McDowell et al., 2017;</ref><ref type="bibr" target="#b17">Mirza and Tonelli, 2016</ref>) made for solving the task use sieve-based archi- tectures, where multiple classifiers are ranked ac- cording to their precision and their predictions are weighted accordingly resulting in a temporal graph structure. A method to extract temporal ordering among relational facts was proposed in ( <ref type="bibr" target="#b20">Talukdar et al., 2012a</ref>).</p><p>Graph Convolutional Network (GCN): GCN ( <ref type="bibr" target="#b10">Kipf and Welling, 2016</ref>) is the extension of con- volutional networks over graphs. In different NLP tasks such as semantic-role labeling , neural machine transla- tion ( <ref type="bibr" target="#b2">Bastings et al., 2017)</ref>, and event detection <ref type="bibr" target="#b18">(Nguyen and Grishman, 2018)</ref>, GCNs have proved to be effective. We extensively use GCN for cap- turing both syntactic and temporal aspect of the document.</p><p>Attention Network: Attention networks have been well exploited for various tasks such as doc- ument classification ( <ref type="bibr" target="#b29">Yang et al., 2016b</ref>), question answering ( <ref type="bibr" target="#b28">Yang et al., 2016a</ref>), machine transla- tion ( <ref type="bibr" target="#b1">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b24">Vaswani et al., 2017)</ref>. Recently, attention over graph structure has been shown to work <ref type="bibr">well by Veličkovi´Veličkovi´c et al. (2018)</ref>. Taking motivation from them, we deploy an atten- tive convolutional network on temporal graph for the document dating problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background: GCN &amp; NeuralDater</head><p>The task of document dating can be modeled as a multi-class classification problem. Following prior work, we shall focus on DCT prediction at the year-granularity in this paper. In this section, we summarize the previous state-of-the-art model NeuralDater ( <ref type="bibr" target="#b23">Vashishth et al., 2018)</ref>, before mov- ing onto our method. An overview of graph convo- lutional network (GCN) <ref type="bibr" target="#b10">(Kipf and Welling, 2016)</ref> is also necessary as it is used in NeuralDater as well as in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph Convolutional Network</head><p>GCN for Undirected Graph: Consider an undi- rected graph, G = (V, E), where V and E are the set of n vertices and set of edges respectively. Ma- trix X ∈ R n×m , whose rows are input represen- tation of node u, where x u ∈ R m , ∀ u ∈ V, is the input feature matrix. The output hidden repre- sentation h v ∈ R d of a node v after a single layer of graph convolution operation can be obtained by considering only the immediate neighbours of v, as formulated in <ref type="bibr" target="#b10">(Kipf and Welling, 2016)</ref>. In or- der to capture information at multi-hop distance, one can stack layers of GCN, one over another. GCN for Directed Graph: Consider a labelled edge from node u to v with label l(u, v), denoted collectively as (u, v, l(u, v)). Based on the as- sumption that information in a directed edge need not only propagate along its direction, Marcheg- giani and Titov (2017) added opposite edges viz., for each (u, v, l(u, v)), (v, u, l(u, v) −1 ) is added to the edge list. Self loops are also added for passing the current embedding information. When GCN is applied over this modified directed graph, the embedding of the node v after k th layer will be,</p><formula xml:id="formula_0">h k+1 v = f   u∈N (v) W k l(u,v) h k u + b k l(u,v)   .</formula><p>We note that the parameters W k l(u,v) and b k l <ref type="bibr">(u,v)</ref> in this case are edge label specific. h k u is the input to the k th layer. Here, N (v) refers to the set of neighbours of v, according to the updated edge list and f is any non-linear activation function (e.g., ReLU: f (x) = max(0, x)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NeuralDater</head><p>In this sub-section, we provide a brief overview of the components of the NeuralDater ( <ref type="bibr" target="#b23">Vashishth et al., 2018)</ref>. Given a document D with n tokens w 1 , w 2 , · · · w n , NeuralDater extracts a temporally rich embedding of the document in a principled way as explained below:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Context Embedding</head><p>Bi-directional LSTM is employed for embedding each word with its context. The GloVe represen- tation of the words X ∈ R n×k is transformed to a context aware representation H cntx ∈ R n×k to get the context embedding. This is essentially shown as the Bi-LSTM in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Syntactic Embedding</head><p>In this step, the context embeddings are further processed using GCN over the dependency parse tree of the sentences in the document, in order to capture long range connection among words. The syntactic dependency structure is extracted by Stanford CoreNLP's dependency parser ( <ref type="bibr" target="#b14">Manning et al., 2014</ref>). NeuralDater follows the same for- mulation of GCN for directed graph as described in Section 3.1, where additional edges are added to the graph to model the information flow. Again following , Neu- ralDater does not allocate separate weight matri- ces for different types of dependency edge labels, rather it considers only three type of edges: a) edges that exist originally, b) the reverse edges that are added explicitly, and c) self loops. The S-GCN portion of <ref type="figure" target="#fig_0">Figure 1</ref> represents this component.</p><p>More formally, H cntx ∈ R n×k is transformed to H syn ∈ R n×ksyn by applying S-GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Temporal Embedding</head><p>In this layer, NeuralDater exploits the Event-Time graph structure present in the document. CATENA <ref type="bibr" target="#b17">(Mirza and Tonelli, 2016)</ref>, current state-of-the-art temporal and causal relation extraction algorithm, produces the temporal graph from the event time annotation of the document. GCN applied over this Event-Time graph, namely T-GCN, chooses n T number of tokens out of total n tokens from the document for further revision in their embeddings. Note that T is the total number of events and time mentions present in the document. A special node DCT is added to the graph and its embedding is jointly learned. Note that this layer learns both label and direction specific parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Classifier</head><p>Finally, the DCT embedding concatenated with the average pooled syntactic embedding is fed to a softmax layer for classification. This whole pro- cedure is trained jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Attentive Deep Document Dater (AD3): Proposed Method</head><p>In this section, we describe Attentive Deep Doc- ument Dater (AD3), our proposed method. AD3 is inspired by NeuralDater, and shares many of its components. Just like in NeuralDater, AD3 also leverages two main types of signals from the doc- ument -syntactic and event-time -to predict the document's timestamp. However, there are crucial differences between the two systems. Firstly, in- stead of concatenating embeddings learned from these two sources as in NeuralDater, AD3 treats these two models completely separate and com- bines them at a later stage. Secondly, unlike Neu- ralDater, AD3 employs attention mechanisms in each of these two models. We call the result- ing models Attentive Context Model (AC-GCN) and Ordered Event Model (OE-GCN). These two models are described in Section 4.1 and Section 4.2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Attentive Context Model (AC-GCN)</head><p>Recent success of attention-based deep learning models for classification ( <ref type="bibr" target="#b29">Yang et al., 2016b</ref>), question answering ( <ref type="bibr" target="#b28">Yang et al., 2016a)</ref>, and ma- chine translation ( <ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>) have mo- tivated us to use attention during document dating. We extend the syntactic embedding model of Neu- ralDater (Section 3.2.2) by incorporating an atten- tive pooling layer. We call the resulting model AC- GCN. This model (right side in <ref type="figure" target="#fig_0">Figure 1</ref>) has two major components.</p><p>• Context Embedding and Syntactic Em- bedding: Following NeuralDater, we used Bi-LSTM and S-GCN to capture context and long-range syntactic dependencies in the doc- ument (Please refer to Section 3.2.1, Section 3.2.2 for brief description). The syntactic embedding, H syn ∈ R n×ksyn is then fed to an Attention Network for further processing.</p><p>Note that, k syn is the dimension of the out- put of Syntactic-GCN and n is the number of tokens in the document.</p><p>• Attentive Embedding: In this layer, we learn the representation for the whole docu- ment through word level attention network. We learn a context vector, u s ∈ R s with re- spect to which we calculate attention for each token. Finally, we aggregate the token fea- tures with respect to their attention weights in order to represent the document. More for- mally, let h syn t ∈ R ksyn be the syntactic rep- resentation of the t th token in the document. We take non-linear projection of it in R s with W s ∈ R s×ksyn . Attention weight α t for t th token is calculated with respect to the context vector u T t as follows.</p><formula xml:id="formula_1">u t = tanh(W s h syn t ), α t = exp(u T t u s ) t exp(u T t u s )</formula><p>.</p><p>Finally, the document representation for the AC-GCN is computed as shown below.</p><formula xml:id="formula_2">d AC−GCN = t α t h syn t</formula><p>This representation is fed to a softmax layer for the final classification.</p><p>The final probability distribution over years pre- dicted by the AC-GCN is given below.</p><formula xml:id="formula_3">P AC−GCN (y|D) = Softmax(W · d AC−GCN + b).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ordered Event Model (OE-GCN)</head><p>The OE-GCN model is shown on the left side of <ref type="figure" target="#fig_0">Figure 1</ref>. Just like in AC-GCN, context and syn- tactic embedding is also part of OE-GCN. The syntactic embedding is fed to the Attentive Graph Convolution Network (AT-GCN) where the graph is obtained from the time-event ordering algorithm CATENA ( <ref type="bibr" target="#b17">Mirza and Tonelli, 2016)</ref>. We describe these components in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Temporal Graph</head><p>We use the same process used in NeuralDater is constructed from the partial ordering between event verbs and time expressions. Let E T be the edge list of the Temporal Graph. Similar to <ref type="bibr" target="#b23">Vashishth et al., 2018)</ref>, we also add reverse edges for each of the existing edge and self loops for passing current node information as explained in Section 3.1. The new edge list E T is shown below.</p><formula xml:id="formula_4">E T = E T ∪ {(j, i, l(i, j) −1 ) | (i, j, l(i, j)) ∈ E T } ∪ {(i, i, self) | i ∈ V)}.</formula><p>The reverse edges are added with reverse labels like AFTER −1 , BEFORE −1 etc . Finally, we get 10 labels for our temporal graph and we denote the set of edge labels by L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Attentive Graph Convolution</head><p>(AT-GCN) Since the temporal graph is automatically gener- ated, it is likely to have incorrect edges. Ide- ally, we would like to minimize the influence of such noisy edges while computing temporal em- bedding. In order to suppress the noisy edges in the Temporal Graph and detect important edges for reasoning, we use attentive graph convolu- tion ( <ref type="bibr" target="#b7">Hamilton et al., 2017</ref>) over the Event-Time graph. The attention mechanism learns the aggre- gation function jointly during training. Here, the main objective is to calculate the attention over the neighbouring nodes with respect to the current node for a given label. Then the embedding of the current node is updated by mixing neighbour- ing node embedding according to their attention scores. In this respect, we propose a label-specific attentive graph convolution over directed graphs.</p><p>Let us consider an edge in the temporal graph from node i to node j with type l, where l ∈ L and L is the label set. The label set L can be divided broadly into two coarse labels as done in Section 3.2.2. The attention weights are specific to only these two type of edges to reduce parameter and prevent overfitting. For illustration, if there exists an edge from node i to j then the edge types will be,</p><p>• L(i, j) = → if (i, j, l(i, j)) ∈ E T , i.e., if the edge is an original event-time edge.</p><p>• L(i, j) = ← if (i, j, l(i, j) −1 ) ∈ E T , i.e., if the edge is added later. both of them in the same direction-specific space.</p><formula xml:id="formula_5">The concatenated vector [W atten L(i,j) × h i ; W atten L(i,j) × h j ]</formula><p>, signifies the importance of the node j w.r.t. node i. A non linear transformation of this con- catenation can be treated as the importance feature vector between i and j.</p><formula xml:id="formula_6">e ij = tanh[W atten L(i,j) × h i ; W atten L(i,j) × h j ].</formula><p>Now, we compute the attention weight of node j for node i with respect to a direction-specific con- text vector a L(i,j) ∈ R 2F , as follows.</p><formula xml:id="formula_7">α l(i,j) ij = exp a T L(i,j) e ij k∈N l(i,·) i exp a T L(i,j) e ik ,</formula><p>where, α l(i,j) ij = 0 if node i and j is not con- nected through label l. N l(i,·) denotes the sub- set of the neighbourhood of node i with label l only. Please note that, although the linear trans- form weight (W atten L(i,j) ∈ R F ×ksyn ) is specific to the coarse labels L, but for each finer label l ∈ L we get these convex weights of attentions. <ref type="figure" target="#fig_1">Figure  2</ref> illustrates the above description w.r.t. edge type BEFORE. Finally, the feature aggregation is done accord- ing to the attention weights. Prior to that, another label specific linear transformation is taken to per- form the convolution operation. Then, the updated feature for node i is calculated as follows.</p><formula xml:id="formula_8">h k+1 i = f l∈L j∈N l(i,·) i α l(i,j) ij W l(i,j) h j + b l(i,j) .</formula><p>where, α ii = 1, N l(i,·) denotes the subset of the neighbourhood of node i with label l only. Note that, α l(i,j) ij = 0 when j / ∈ N l(i,·) . To illustrate for- mally, from <ref type="figure" target="#fig_1">Figure 2</ref>, we see that weight α 1 and α 2 is calculated specific to label type BEFORE and the neighbours which are connected through BE- FORE is being multiplied with W bef ore prior to aggregation in the ReLU block. Now, after applying attentive graph convolu- tion network, we only consider the representa- tion of Document Creation Time (DCT), h DCT , as the document representation itself. h DCT is now passed through a fully connected layer prior to softmax. Prediction of the OE-GCN for the doc- ument D will be given as</p><formula xml:id="formula_9">P OE−GCN (y|D) = Softmax(W · d DCT + b).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">AD3: Attentive Deep Document Dater</head><p>In this section, we propose an unified model by mixing both AC-GCN and OE-GCN. Even on val- idation data, we see that performance of both the models differ to a large extent. This significant difference (McNemar test p &lt; 0.000001) moti- vated the unification. We take convex combina- tion of the output probabilities of the two models as shown below.</p><note type="other">Datasets # Docs Start Year End Year APW 675k 1995 2010 NYT 647k 1987 1996</note><formula xml:id="formula_10">P joint (y|D) = λP AC−GCN (y|D) + (1 − λ)P OE−GCN (y|D).</formula><p>The combination hyper-parameter λ is tuned on the validation data. We obtain the value of λ to be 0.52 <ref type="figure" target="#fig_2">(Figure 3</ref>) and 0.54 for APW and NYT datasets, respectively. This depicts that the two models are capturing significantly different aspects of documents, resulting in a substantial improvement in performance when combined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>Dataset: Experiments are carried out on the Asso- ciated Press Worldstream (APW) and New York Times (NYT) sections of the Gigaword corpus <ref type="bibr" target="#b19">(Parker et al., 2011</ref>). We have used the same 8:1:1 split as <ref type="bibr" target="#b23">Vashishth et al. (2018)</ref> for all the models. For quantitative details please refer to <ref type="table" target="#tab_2">Table 1</ref>.</p><p>Evaluation Criteria: In accordance with prior work <ref type="bibr" target="#b3">(Chambers, 2012;</ref><ref type="bibr" target="#b11">Kotsakos et al., 2014;</ref><ref type="bibr" target="#b23">Vashishth et al., 2018</ref>) the final task is to predict the publication year of the document. We give a brief description of the baselines below.</p><p>Baseline Methods:</p><p>• MaxEnt-Joint (Chambers, 2012): This method engineers several hand-crafted tem- porally influenced features to classify the document using MaxEnt Classifier.</p><p>• BurstySimDater ( <ref type="bibr" target="#b11">Kotsakos et al., 2014</ref>): This is a purely statistical method which uses lexical similarity and term burstiness ( <ref type="bibr" target="#b12">Lappas et al., 2009</ref>) for dating documents in arbitrary length time frame. For our experiments, we used a time frame length of 1 year.</p><p>• NeuralDater ( <ref type="bibr" target="#b23">Vashishth et al., 2018)</ref>: This is the first deep neural network based approach for the document dating task. Details are pro- vided in Section 3.2.   Israel's consumer price index increased by 1.2 percent in December, bringing the overall inflation rate for 1995 to 8.1 percent, well within the government's target rate for the year, officials said Friday. Israel radio said that it was the lowest annual inflation rate in twenty years. Hyperparameters: We use 300-dimensional GloVe embeddings and 128-dimensional hidden state for both GCNs and BiLSTM with 0.8 dropout. We use Adam ( <ref type="bibr" target="#b9">Kingma and Ba, 2014</ref>) with 0.001 learning rate for training. For OE-GCN we use 2-layers of AT-GCN. 1-layer of S-GCN is used for both the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Performance Analysis</head><p>In this section, we compare the effectiveness of our method with that of prior work. The deep network based NeuralDater model in <ref type="bibr" target="#b23">(Vashishth et al., 2018</ref>   neered <ref type="bibr" target="#b3">(Chambers, 2012)</ref> and statistical methods ( <ref type="bibr" target="#b11">Kotsakos et al., 2014</ref>) by a large margin. We ob- serve a similar trend in our case. Compared to the state-of-the-art model NeuralDater, we gain, on an average, a 3.7% boost in accuracy on both the datasets <ref type="table" target="#tab_4">(Table 2)</ref>.</p><p>Among individual models, OE-GCN performs at par with NeuralDater, while AC-GCN outper- forms it. The empirical results imply that AC- GCN by itself is effective for this task. The rela- tively worse performance of OE-GCN can be at- tributed to the fact that it only focuses on the Event-Time information and leaves out most of the contextual information. However, it captures various different (p &lt; 0.000001, McNemar's test, 2-tailed) aspects of the document for classifica- tion, which motivated us to propose an ensemble of the two models. This explains the significant boost in performance of AD3 over NeuralDater as well as the individual models. It is worth mention- ing that although AC-GCN and OE-GCN do not provide significant boosts in accuracy, their pre- dictions have considerably lower mean-absolute-deviation as shown in <ref type="figure" target="#fig_3">Figure 4</ref>.</p><p>We concatenated the DCT embedding provided by OE-GCN with the document embedding pro- vided by AC-GCN and trained in an end to end joint fashion like NeuralDater. We see that even with a similar training method, the Attentive Neu- ralDater model on an average, performs 1.6% bet- ter in terms of accuracy, once again proving the ef- ficacy of attention based models over normal mod- els.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Effectiveness of Attention</head><p>Attentive Graph Convolution (Section 4.2.2) proves to be effective for OE-GCN, giving a 2% accuracy improvement over non-attentive T-GCN of NeuralDater <ref type="table" target="#tab_6">(Table 3)</ref>. Similarly the efficacy of word level attention is also prominent from <ref type="table" target="#tab_6">Table  3</ref>. We have also analyzed our models by visualiz- ing attentions over words and attention over graph nodes. <ref type="figure" target="#fig_4">Figure 5</ref> shows that AC-GCN focuses on temporally informative words such as "said" (for tense) or time mentions like "1995", alongside im- portant contextual words like "inflation", "Israel" etc. For OE-GCN, from <ref type="figure" target="#fig_5">Figure 6</ref> we observe that "DCT" and time-mention '1995' grabs the high- est attention. Attention between "DCT" and other event verbs indicating past tense are quite promi- nent, which helps the model to infer 1996 (which is correct) as the most likely time-stamp of the document. These analyses provide us with a good justification for the performance of our attentive models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Apart from empirical improvements over previ- ous models, we also perform a qualitative analy- sis of the individual models. <ref type="figure" target="#fig_6">Figure 7</ref> shows that the performance of AC-GCN improves with the length of documents, thus indicating that richer context leads to better model prediction. <ref type="figure">Figure  8</ref> shows how the performance of OE-GCN im- proves with the number of event-time mentions in the document, thus further reinforcing our claim that more temporal information improves model performance. <ref type="bibr" target="#b23">Vashishth et al. (2018)</ref> reported that their model got confused by the presence of multiple mislead- ing time mentions. AD3 overcomes this limitation using attentive graph convolution, which success- fully filters out noisy time mentions as is evident </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We propose AD3, an ensemble model which ex- ploits both syntactic and temporal information in a document explicitly to predict its creation time <ref type="bibr">(DCT)</ref>. To the best of our knowledge, this is the first application of attention based deep models for dating documents. Our experimental results demonstrate the effectiveness of our model over all previous models. We also visualize the attention weights to show that the model is able to choose what is important for the task and filter out noise inherent in language. As part of future work, we would like to incorporate external knowledge as a side information for improved time-stamping of documents.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Two proposed models a) Ordered Event Model (left) and b) Attentive Context Model (right), where wi are the words of a document (D), ei are the words signifying events and ti are the temporal tokens as detected in the document. Both models use Bi-LSTM and S-GCN (Syntactic-GCN, see Section 3.2.2) in the initial part of their pipeline. Ordered Event Model (OE-GCN) uses a label based attentive graph convolutional network for encoding the DCT, whereas Attentive Context Model (AC-GCN) uses a word attention based model to encode the document. αi(∀ i ∈ [1, n]) denotes attention over the words of document and αa, α b and αs denote attention over nodes connected with edge labels AFTER, BEFORE and SIMULTANEOUS, respectively. OE-GCN provides the probability scores over the years given the encoded DCT, while AC-GCN provides the probability scores given the context of the document. Both the models are trained separately.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FirstFigure 2 :</head><label>2</label><figDesc>Figure 2: Attentive Graph Convolution (AT-GCN). In this layer, we learn attention weights for every edge based on label and direction. The attention weights are learnt using a context vector. The final representation of every node is a summation of weighted convolution over neighboring nodes based on labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Variation of validation accuracy with λ (for APW dataset). We observe that AC-GCN and OE-GCN are both important for the task as we get optimal λ = 0.52.</figDesc><graphic url="image-1.png" coords="6,62.39,62.81,268.60,166.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Mean absolute deviation (in years; lower is better) between a model's top prediction and the true year in the APW dataset. We find that all of our proposed methods outperform the previous state-of-the-art NeuralDater. Please refer to Section 6.1 for details.</figDesc><graphic url="image-2.png" coords="7,57.74,62.81,295.20,115.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualization of the attention of AC-GCN. ACGCN captures the intuitive tokens as seen in the figure. Darker shade implies higher attention. The correct DCT is 1996.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualization of the average edge attention of the temporal graph as learnt by OE-GCN for the document shown in Figure 5. Darker color implies higher attention. The correct DCT is 1996.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Variation of validation accuracy (%) with respect to length of documents (for APW dataset) for AC-GCN. Documents having more than 100 tokens are selected for this analysis. Please see Section 7.</figDesc><graphic url="image-3.png" coords="8,322.92,57.97,232.72,143.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Details of datasets used. Please refer Section 5 for 

details. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Accuracy (%) of different methods on the APW 

and NYT datasets for the document dating problem (higher 
is better). The unified model significantly outperforms all 
previous models. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Accuracy (%) comparisons of component models 

with and without Attention. This results show the effective-
ness of both word attention and Graph Attention for this task. 
Please see Section 6.2 for more details. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the Ministry of Human Resource Development (MHRD), Government of India.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On-line new event detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Papka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;98</title>
		<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;98<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Graph convolutional encoders for syntax-aware neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima</surname></persName>
		</author>
		<idno>abs/1704.04675</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Labeling documents with timestamps: Learning from their time expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="98" to="106" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Answering general time sensitive queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wisam</forename><surname>Dakka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Gravano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Panagiotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM &apos;08</title>
		<meeting>the 17th ACM Conference on Information and Knowledge Management, CIKM &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1437" to="1438" />
		</imprint>
	</monogr>
	<note>Ipeirotis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Temporal Language Models for the Disclosure of Historical Text. KNAW. Imported from EWI/DB PMS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Franciska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Jong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djoerd</forename><surname>Rode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hiemstra</surname></persName>
		</author>
		<idno>db-utwente:inpr:0000003683</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Temporal Language Models for the Disclosure of Historical Text. KNAW. Imported from EWI/DB PMS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Franciska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Jong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djoerd</forename><surname>Rode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hiemstra</surname></persName>
		</author>
		<idno>db-utwente:inpr:0000003683</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>abs/1706.02216</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving temporal language models for determining time of non-timestamped documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nattiya</forename><surname>Kanhabua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kjetil</forename><surname>Nørvåg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th European Conference on Research and Advanced Technology for Digital Libraries, ECDL &apos;08</title>
		<meeting>the 12th European Conference on Research and Advanced Technology for Digital Libraries, ECDL &apos;08<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>SpringerVerlag</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="358" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A burstiness-aware approach for document dating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kotsakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodoros</forename><surname>Lappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kotzias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Gunopulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nattiya</forename><surname>Kanhabua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kjetil</forename><surname>Nørvåg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International ACM SIGIR Conference on Research &amp;#38; Development in Information Retrieval, SIGIR &apos;14</title>
		<meeting>the 37th International ACM SIGIR Conference on Research &amp;#38; Development in Information Retrieval, SIGIR &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1003" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On burstiness-aware search for document sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodoros</forename><surname>Lappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Arai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Platakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kotsakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Gunopulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;09</title>
		<meeting>the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="477" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Time-based language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W. Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Information and Knowledge Management, CIKM &apos;03</title>
		<meeting>the Twelfth International Conference on Information and Knowledge Management, CIKM &apos;03<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="469" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno>abs/1703.04826</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Event ordering with a generalized model for sieve prediction ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Mcdowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ororbia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Reitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="843" to="853" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Catena: Causal and temporal relation extraction from natural language texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paramita</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Tonelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="64" to="75" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Graph convolutional networks with argument-aware pooling for event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">English gigaword fifth edition ldc2011t07. dvd. Philadelphia: Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Acquiring temporal constraints between relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derry</forename><surname>Wijaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Information and knowledge management</title>
		<meeting>the 21st ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="992" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Coupled temporal scoping of relational facts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derry</forename><surname>Wijaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth ACM international conference on Web search and data mining</title>
		<meeting>the fifth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naushad</forename><surname>Uzzaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><surname>Llorens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Verhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Pustejovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Semantic Evaluation</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (* SEM)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dating documents using graph convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swayambhu</forename><surname>Shib Sankar Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Nath Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">56th Annual Meeting of the Association for Computational Linguistics (ACL 2018)</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličkovi´veličkovi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietrolì</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 13: Tempeval-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Verhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roser</forename><surname>Sauri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Pustejovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th international workshop on semantic evaluation</title>
		<meeting>the 5th international workshop on semantic evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="57" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Timedtextrank: Adding the temporal dimension to multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;07</title>
		<meeting>the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="867" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1217" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
