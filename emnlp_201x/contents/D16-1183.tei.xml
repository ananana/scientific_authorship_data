<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Shift-Reduce CCG Semantic Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipendra</forename><forename type="middle">K</forename><surname>Misra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell Tech Cornell University New York</orgName>
								<address>
									<postCode>10011</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell Tech Cornell University New York</orgName>
								<address>
									<postCode>10011</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Shift-Reduce CCG Semantic Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1775" to="1786"/>
							<date type="published">November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a shift-reduce CCG semantic parser. Our parser uses a neural network architecture that balances model capacity and computational cost. We train by transferring a model from a computationally expensive log-linear CKY parser. Our learner addresses two challenges: selecting the best parse for learning when the CKY parser generates multiple correct trees, and learning from partial derivations when the CKY parser fails to parse. We evaluate on AMR parsing. Our parser performs comparably to the CKY parser, while doing significantly fewer operations. We also present results for greedy semantic parsing with a relatively small drop in performance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Shift-reduce parsing is a class of parsing methods that guarantees a linear number of operations in sen- tence length. This is a desired property for practical applications that require processing large amounts of text or real-time response. Recently, such techniques were used to build state-of-the-art syntactic parsers, and have demonstrated the effectiveness of deep neural architectures for decision making in linear- time dependency parsing <ref type="bibr" target="#b11">(Chen and Manning, 2014;</ref><ref type="bibr" target="#b14">Dyer et al., 2015;</ref><ref type="bibr" target="#b1">Andor et al., 2016;</ref><ref type="bibr" target="#b22">Kiperwasser and Goldberg, 2016)</ref>. In contrast, semantic parsing often relies on algorithms with polynomial number of operations, which results in slow parsing times unsuitable for practical applications. In this paper, we apply shift-reduce parsing to semantic parsing. Specifically, we study transferring a learned Combi- natory Categorial <ref type="bibr">Grammar (CCG;</ref><ref type="bibr" target="#b35">Steedman, 1996</ref><ref type="bibr" target="#b36">Steedman, , 2000</ref>) from a dynamic-programming CKY model to a shift-reduce neural network architecture.</p><p>We focus on the feed-forward architecture of <ref type="bibr" target="#b11">Chen and Manning (2014)</ref>, where each parsing step is a multi-class classification problem. The state of the parser is represented using simple feature em- beddings that are passed through a multilayer per- ceptron to select the next action. While simple, the capacity of this model to capture interactions be- tween primitive features, instead of relying on sparse complex features, has led to new state-of-the-art per- formance ( <ref type="bibr" target="#b1">Andor et al., 2016)</ref>. However, applying this architecture to semantic parsing presents learn- ing and inference challenges.</p><p>In contrast to dependency parsing, semantic pars- ing corpora include sentences labeled with the sys- tem response or the target formal representation, and omit derivation information. CCG induction from such data relies on latent-variable techniques and re- quires careful initialization (e.g., <ref type="bibr">Collins, 2005, 2007)</ref>. Such feature initialization does not directly transfer to a neural network archi- tecture with dense embeddings, and the use of hid- den layers further complicates learning by adding a large number of latent variables. We focus on data that includes sentence-representation pairs, and learn from a previously induced log-linear CKY parser. This drastically simplifies learning, and can be viewed as bootstrapping a fast parser from a slow one. While this dramatically narrows down the num- ber of parses per sentence, it does not eliminate am- biguity. In our experiments, we often get multiple correct parses, up to 49K in some cases. We also observe that the CKY parser generates no parses for Some old networks remain inoperable N P <ref type="bibr">[x]</ref> /N <ref type="bibr">[x]</ref> N <ref type="bibr">[x]</ref> /N <ref type="bibr">[x]</ref> N <ref type="bibr">[pl]</ref> S\N P <ref type="bibr">[pl]</ref> /(N <ref type="bibr">[pl]</ref> /N <ref type="bibr">[pl]</ref> ) N <ref type="bibr">[x]</ref> /N <ref type="bibr">[x]</ref> λf.A(λx.f (x) ∧ quant(x, λf.λx.f (x)∧ λn.network(n) λf.λx.f (λr.remain-01(r)∧ λf.λx.f (x) ∧ ARG3(x, A(λs.some(s)))) mod(x, A(λo.old(o))) ARG1(r, x)) A(λp.possible(p) ∧ polarity(p, −)∧ domain(p, A(λo.operate-01(o)))))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&gt; &gt;</head><p>N <ref type="bibr">[pl]</ref> S\N P <ref type="bibr">[pl]</ref> λn.network(n)∧ λx.λr.remain-01(r) ∧ ARG1(r, x) ∧ ARG3(r, A(λp.possible(p) mod(n, A(λo.old(o))) ∧polarity(p, −) ∧ domain(p, A(λo.operate-01(o))))) &gt; N P <ref type="bibr">[pl]</ref> A(λn.network(n) ∧ mod(n, A(λo.old(o))) ∧ quant(n, A(λs.some(s)))) &lt; S λr.remain-01(r) ∧ ARG1(r, A(λn.network(n) ∧ mod(n, A(λo.old(o))) ∧ quant(n, A(λs.some(s)))))∧ ARG3(r, A(λp.possible(p) ∧ polarity(p, −) ∧ domain(p, A(λo.operate-01(o)))))</p><p>Figure 1: Example CCG tree with five lexical entries, three forward applications (&gt;) and a backward application (&lt;). a significant number of training sentences. There- fore, we propose an iterative algorithm that automat- ically selects the best parses for training at each iter- ation, and identifies partial derivations for best-effort learning, if no parses are available.</p><p>CCG parsing largely relies on two types of ac- tions: using a lexicon to map words to their cate- gories, and combining categories to acquire the cat- egories of larger phrases. In most semantic pars- ing approaches, the number of operations is dom- inated by the large number of categories available for each word in the lexicon. For example, the lex- icon in our experiments includes 1.7M entries, re- sulting in an average of 146, and up to 2K, ap- plicable actions. Additionally, both operations and parser state have complex structures, for example including both syntactic and semantic information. Therefore, unlike in dependency parsing <ref type="bibr" target="#b11">(Chen and Manning, 2014</ref>), we can not treat action selection as multi-class classification, and must design an archi- tecture that can accommodate a varying number of actions. We present a network architecture that con- siders a variable number of actions, and emphasizes low computational overhead per action, instead fo- cusing computation on representing the parser state.</p><p>We evaluate on Abstract Meaning Representa- tion (AMR; <ref type="bibr" target="#b8">Banarescu et al., 2013</ref>) parsing. We demonstrate that our modeling and learning contri- butions are crucial to effectively commit to early de- cisions during parsing. Somewhat surprisingly, our shift-reduce parser provides equivalent performance to the CKY parser used to generate the training data, despite requiring significantly fewer operations, on average two orders of magnitude less. Similar to previous work, we use beam search, but also, for the first time, report greedy CCG semantic parsing results at a relatively modest 9% decrease in perfor- mance, while the source CKY parser with a beam of one demonstrates a 71% decrease. While we focus on semantic parsing, our learning approach makes no task-specific assumptions and has potential for learning efficient models for structured prediction from the output of more expensive ones. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task and Background</head><p>Our goal is to learn a function that, given a sentence x, maps it to a formal representation of its meaning z with a linear number of operations in the length of x. We assume access to a training set of N examples</p><formula xml:id="formula_0">D = {(x (i) , z (i) )} N i=1</formula><p>, each containing a sentence x (i) and a logical form z (i) . Since D does not con- tain complete derivations, we instead assume access to a CKY parser learned from the same data. We evaluate performance on a test set {(</p><formula xml:id="formula_1">x (i) , z (i) )} M i=1</formula><p>of M sentences x (i) labeled with logical forms z (i) . While we describe our approach in general terms, we apply our approach to AMR parsing and evalu- ate on a common benchmark (Section 6).</p><p>To map sentences to logical forms, we use CCG, a linguistically-motivated grammar formalism for modeling a wide-range of syntactic and seman- tic phenomena <ref type="bibr" target="#b35">(Steedman, 1996</ref><ref type="bibr" target="#b36">(Steedman, , 2000</ref>. A CCG is defined by a lexicon Λ and sets of unary R u and binary R b rules. In CCG parse trees, each node is a category. <ref type="figure">Figure 1</ref> shows a CCG tree for the sentence Some old networks remain inop- erable. For example,</p><formula xml:id="formula_2">S\N P [pl] /(N [pl] /N [pl] ) :</formula><p>λf.λx.f (λr.remain-01(r)∧ARG1(r, x)) is the cat- egory of the verb remain. The syntactic type <ref type="bibr">[pl]</ref> ) indicates that two argu- ments are expected: first an adjective N <ref type="bibr">[pl]</ref> /N <ref type="bibr">[pl]</ref> and then a plural noun phrase N P <ref type="bibr">[pl]</ref> . The final syntac- tic type will be S. The forward slash / indicates the argument is expected on the right, and the back- ward slash \ indicates it is expected on the left. The syntactic attribute pl is used to express the plural-ity constraint of the verb. The simply-typed lambda calculus logical form in the category represents se- mantic meaning. The typing system includes atomic types (e.g., entity e, truth value t) and functional types (e.g., e, t is the type of a function from e to t). In the example category above, the expression on the right of the colon is a e, t, e, t, e, e, t- typed function expecting first an adjectival modi- fier and then an ARG1 modifier. The conjunction ∧ specifies the roles of remain-01. The lexicon Λ maps words to CCG categories. For example, the lexical entry remain S\N P <ref type="bibr">[pl]</ref> </p><formula xml:id="formula_3">S\N P [pl] /(N [pl] /N</formula><formula xml:id="formula_4">/(N [pl] /N [pl] ) :</formula><p>λf.λx.f (λr.remain-01(r) ∧ ARG1(r, x)) pairs the example category with remain. The parse tree in the figure includes four binary operations: three forward applications (&gt;) and a backward application (&lt;).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Shift Reduce Semantic Parsing</head><p>Given a sentence x = x 1 , . . . , x m with m tokens x i and a CCG lexicon Λ, let GEN(x; Λ) be a function that generates CCG parse trees. We design GEN as a shift-reduce parser, and score decisions using em- beddings of parser states and candidate actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Shift-Reduce Parsing for CCG</head><p>Shift-reduce parsers perform a single pass of the sentence from left to right to construct a parse tree. The parser configuration 2 is defined with a stack and a buffer. The stack contains partial parse trees, and the buffer the remainder of the sentence to be pro- cessed. Formally, a parser configuration c is a tu- ple σ, β, where the stack σ is a list of CCG trees [s l · · · s 1 ], and the buffer β is a list of tokens from x to be processed [x i · · · x m ]. 3 For example, the top- left of <ref type="figure" target="#fig_0">Figure 2</ref> shows a parsing configuration with two partial trees on the stack and two words on the buffer (remain and inoperable).</p><p>Parsing starts with the configuration</p><formula xml:id="formula_5">[], [x 1 · · · x m ]</formula><p>, where the stack is empty and the buffer is initialized with x. In each parsing step, the parser either consumes a word from the buffer and pushes a new tree to the stack, or applies a parsing rule to the trees at the top of the stack. For simplicity, we apply CCG rules to trees, where a rule is applied to the root categories of the argument trees to create a new tree with the argu- ments as children. We treat lexical entries as trees with a single node. There are three types of actions: 4</p><formula xml:id="formula_6">SHIFT(l, σ, x i | · · · |x j |β) = σ|g, β BINARY(b, σ|s 2 |s 1 , β) = σ|b(s 2 , s 1 ), β UNARY(u, σ|s 1 , β) = σ|u(s 1 ), β .</formula><p>Where b ∈ R b is a binary rule, u ∈ R u is a unary rule, and l is a lexical entry x i , . . . x j g for the to- kens x i ,. . . ,x j and CCG category g. SHIFT creates a tree given a lexical entry for the words at the top of the buffer, BINARY applies a binary rule to the two trees at the head of the stack, and UNARY applies a unary rule to the tree at head of the stack. A config- uration is terminal when no action is applicable. Given a sentence x, a derivation is a sequence of action-configuration pairs c 1 , a 1 , . . . , c k , a k , where action a i is applied to configuration c i to gen- erate configuration c i+1 . The result configuration c k+1 is of the form <ref type="bibr">[s]</ref>, [], where s represents a complete parse tree, and the logical form z at the root category represents the meaning of the com- plete sentence. Following previous work with CKY parsing <ref type="bibr" target="#b40">(Zettlemoyer and Collins, 2005</ref>), we disal- low consecutive unary actions. We denote the set of actions allowed from configuration c as A(c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model</head><p>Our goal is to balance computation and model ca- pacity. To recover a rich representation of the con- figuration, we use a multilayer perceptron (MLP) to create expressive interactions between a small num- ber of simple features. However, since we con- sider many possible actions in each step, comput- ing activations for multiple hidden layers for each action is prohibitively expensive. Instead, we opt for a computationally-inexpensive action represen- tation computed by concatenating feature embed- dings. <ref type="figure" target="#fig_0">Figure 2</ref> illustrates our architecture.</p><p>Given a configuration c, the probability of an ac- tion a is:  where φ(a, c) is the action embedding, ξ(c) is the configuration embedding, and F is an MLP. W b is a bilinear transformation matrix. Given a sen- tence x and a sequence of action-configuration pairs c 1 , a 1 , . . . , c k , a k , the probability of a CCG tree y is</p><formula xml:id="formula_7">p(a | c) = exp {φ(a, c)W b F(ξ(c))} a ∈A(c) exp {φ(a , c)W b F(ξ(c))} , Stack Buffer h 2 = max{0, W 2 h 1 + b 2 } h 1 = max{0, W 1 h 0 + b 1 } h 3 = W 3 h 2 + b</formula><formula xml:id="formula_8">(a 1 , c) (a 2 , c) c Configuration A(c) Actions F MLP ⇠(c) s 2 s 1 b 2 ome old networks remain inoperable [x] /N [x] N [x] /N [x] N [pl] S\NP [pl] /(N [pl] /N [pl] ) N [x] /N [x] (x) ^ quant(x, f.x.f (x)^ n.network(n) f.x.f (r.remain-01(r)^ f.x.f (x) ^ ARG3(x, A(p.possible(p)^ some(s)))) MOD(x, A(o.old(o))) ARG1(r, x)) polarity(p, ) ^ domain(p, A(o.operate-01(o))))) &gt; &gt; N [pl] S\NP [pl] n.network(n)^ x.r.remain-01(r) ^ ARG1(r, x) ^ ARG3(r, A(p.possible(p) MOD(n, A(o.old(o))) ^polarity(p, ) ^ domain(p, A(o.operate-01(o))))) &gt; NP [pl] ork(n) ^ MOD(n, A(o.old(o))) ^ quant(n, A(s.some(s)))) &lt; S r.remain-01(r) ^ ARG1(r, A(n.network(n) ^ MOD(n, A(o.old(o))) ^ quant(n, A(s.some(s)))))^ ARG3(r, A(p.possible(p) ^ polarity(p, ) ^ domain(p, A(o.operate-01(o))))) 1 Some old networks remain inoperable NP [x] /N [x] N [x] /N [x] N [pl] S\NP [pl] /(N [pl] /N [pl] ) N [x] /N [x] f.A(x.f (x) ^ quant(x, f.x.f (x)^ n.network(n) f.x.f (r.remain-01(r)^ f.x.f (x) ^ ARG3(x, A(p.possible(p)^ A(s.some(s)))) MOD(x, A(o.old(o))) ARG1(r, x)) polarity(p, ) ^ domain(p, A(o.operate-01(o))))) &gt; &gt; N [pl] S\NP [pl] n.network(n)^ x.r.remain-01(r) ^ ARG1(r, x) ^ ARG3(r, A(p.possible(p) MOD(n, A(o.old(o))) ^polarity(p, ) ^ domain(p, A(o.operate-01(o))))) &gt; NP [pl] A(n.network(n) ^ MOD(n, A(o.old(o))) ^ quant(n, A(s.some(s)))) &lt; S r.remain-01(r) ^ ARG1(r, A(n.network(n) ^ MOD(n, A(o.old(o))) ^ quant(n, A(s.some(s)))))^ ARG3(r, A(p.possible(p) ^ polarity(p, ) ^ domain(p, A(o.operate-01(o))))) 1 A b 1 pl x NP/N N Some old networks remain inoperable NP [x] /N [x] N [x] /N [x] N [pl] S\NP [pl] /(N [pl] /N [pl] ) N [x] /N [x] f.A(x.f (x) ^ quant(x, f.x.f (x)^ n.network(n) f.x.f (r.remain-01(r)^ f.x.f (x) ^ ARG3(x, A(p.possible(p)^ A(s.some(s)))) MOD(x, A(o.old(o))) ARG1(r, x)) polarity(p, ) ^ domain(p, A(o.operate-01(o))))) &gt; &gt; N [pl] S\NP [pl] n.network(n)^ x.r.remain-01(r) ^ ARG1(r, x) ^ ARG3(r, A(p.possible(p) MOD(n, A(o.old(o))) ^polarity(p, ) ^ domain(p, A(o.operate-01(o))))) &gt; NP [pl] A(n.network(n) ^ MOD(n, A(o.old(o))) ^ quant(n, A(s.some(s)))) &lt; S r.remain-01(r) ^ ARG1(r, A(n.network(n) ^ MOD(n, A(o.old(o))) ^ quant(n, A(s.some(s)))))^ ARG3(r, A(p.possible(p) ^ polarity(p, ) ^ domain(p, A(o.operate-01(o))))) 1 Some old networks remain inoperable NP [x] /N [x] N [x] /N [x] N [pl] S\NP [pl] /(N [pl] /N [pl] ) N [x] /N [x] f.A(x.f (x) ^ quant(x, f.x.f (x)^ n.network(n) f.x.f (r.remain-01(r)^ f.x.f (x) ^ ARG3(x, A(p.possible(p)^ A(s.some(s)))) MOD(x, A(o.old(o))) ARG1(r, x)) polarity(p, ) ^ domain(p, A(o.operate-01(o))))) &gt; &gt; N [pl] S\NP [pl] n.network(n)^ x.r.remain-01(r) ^ ARG1(r, x) ^ ARG3(r, A(p.possible(p) MOD(n, A(o.old(o))) ^polarity(p, ) ^ domain(p, A(o.operate-01(o))))) &gt; NP [pl] A(n.network(n) ^ MOD(n, A(o.old(o))) ^ quant(n, A(s.some(s)))) &lt; S r.remain-01(r) ^ ARG1(r, A(n.network(n) ^ MOD(n, A(o.old(o))) ^ quant(n, A(s.some(s)))))^ ARG3(r, A(p.possible(p) ^ polarity(p, ) ^ domain(p, A(o.operate-01(o))))) 1 a 1 = Binary(forward-apply, c) a 2 = Unary(bare-plural, c) Some networks remain inoperable NP [x] /N [x] N [pl] S\NP [pl] /(N [pl] /N [pl] ) N [x] /N [x] f.A1(x.f (x) ^ REL(x, n.network(n) f.x.f (r.remain-01(r)^ f.x.f (x) ^ REL(x, A3(p.possi A2(s.some(s)))) ARG1(r, x)) REL(p, A4(o.operate &gt; NP [pl] S\NP [pl] A1(n.network(n) ^ REL(n, A2(s.some(s)))) x.r.remain-01(r) ^ ARG1(r, x) ^ REL(r, A3(p.possible( REL(p, A4(o.operate-01(o))))) S r.remain-01(r) ^ ARG1(r, A1(n.network(n) ^ REL(n, A2(s.some(s)))))^ REL(r, A3(p.possible(p) ^ REL(p, ) ^ REL(p, A4(o.operate-01(o))))) 1 a |A(c)| = Shift , c ! P (a i ) / exp((a i , c)W b h 3 ) 8i = 1 . . . |A(c)| (a |A(c)| , c)</formula><formula xml:id="formula_9">p(y | x) = i=1...k p(ai | ci) .</formula><p>The probability of a logical form z is then</p><formula xml:id="formula_10">p(z | x) = y∈Y(z) p(y | x) ,</formula><p>where Y(z) is the set of CCG trees with the logical form z at the root. </p><formula xml:id="formula_11">b . Configuration Embedding ξ(c) Given a config- uration c = [s l · · · s 1 ], [x i · · · x m ]</formula><p>, the input to F is a concatenation of syntactic and semantic embed- dings, as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. We concatenate em- beddings from the top three trees in the stack s 1 , s 2 , s 3 . 5 When a feature is not present, for example when the stack or buffer are too small, we use a tunable null embedding. Given a tree on the stack s j , we define two syn- tactic features: attribute set and stripped syntax. The attribute feature is created by extracting all the syntactic attributes of the root category of s j . The stripped syntax feature is the syntax of the root cat- egory without the syntactic attributes. For example, in <ref type="figure" target="#fig_0">Figure 2</ref>, we embed the stripped category N and attribute pl for s 1 , and N P/N and x for s 2 . The at- tributes are separated from the syntax to reduce spar- sity, and the interaction between them is computed by F. The sparse features are converted to dense embeddings using a lookup table and concatenated. In addition, we also embed the logical form at the root of s j . <ref type="figure" target="#fig_1">Figure 3</ref> illustrates the recursive embed- ding function ψ. <ref type="bibr">6</ref> Using a recursive function to em- bed logical forms is computationally intensive. Due to strong correlation between sentence length and logical form complexity, this computation increases the cost of configuration embedding by a factor lin- ear in sentence length. In Section 6, we experiment with including this option, balancing between poten- tial expressivity and speed.</p><note type="other">{Wr, r} {Wr, r} {Wr, r} JOHN arg0 e x x x.arg0(x, JOHN)</note><p>Action Embedding φ(a, c) Given an action a ∈ A(c), and the configuration c, we generate the action representation by computing sparse features, con- verting them to dense embeddings via table lookup, and concatenating. If more than one feature of the same type is triggered, we average their embed- dings. When no features of a given type are trig- gered, we use a tunable placeholder embedding in- stead. The features include all the features used by <ref type="bibr" target="#b4">Artzi et al. (2015)</ref>, including all conjunctive fea- tures, as well as properties of the action and configu- ration, such as the POS tags of tokens on the buffer. 7</p><p>Discussion Our use of an MLP is inspired by <ref type="bibr" target="#b11">Chen and Manning (2014)</ref>. However, their architecture is designed to handle only a fixed number of actions, while we observe varying number of actions. There- fore, we adopt a probabilistic model similar to <ref type="bibr" target="#b14">Dyer et al. (2015)</ref> to effectively combine the benefits of <ref type="bibr">7</ref> See the supplementary material for feature details. the two approaches. <ref type="bibr">8</ref> We factorize the exponent in our objective into action φ(a, c) and configuration F(ξ(c)) embeddings. While every parse step in- volves a single configuration, the number of actions is significantly higher. With the goal of minimizing the amount of computation per action, we use simple concatenation only for action embedding. However, this requires retaining sparse conjunctive action fea- tures since they are never combined through hidden layers similar to configuration features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference</head><p>To compute the set of parse trees GEN(x; Λ), we perform beam search to recover the top-k parses. The beam contains configurations. At each step, we expand all configurations with all actions, and keep only the top-k new configurations. To promote di- versity in the beam, given two configurations with the same signature, we keep only the highest scor- ing one. The signature includes the previous config- uration in the derivation, the state of the buffer, and the root categories of all stack elements. Since all features are computed from these elements, this op- timization does not affect the max-scoring tree. Ad- ditionally, since words are assigned structured cat- egories, a key problem is unknown words or word uses. Following <ref type="bibr">Zettlemoyer and Collins (2007)</ref>, we use a two-pass parsing strategy, and allow skipping words controlled by the term γ in the second pass. The term γ is added to the exponent of the action probability when words are skipped. See the sup- plementary material for the exact form. Complexity Analysis The shift-reduce parser pro- cesses the sentence from left to right with a linear number of operations in sentence length. We define an operation as applying an action to a configuration. Formally, the number of operations for a sentence of length m is bounded by O(4mk(|λ|+|R b |+|R u |)), where |λ| is the number of lexical entries per to- ken, k is the beam size, R b is the set of binary rules, and R u the set of unary rules. In compari- son, the number of operations for the CKY parser, where an operation is applying a rule to a single cell or two adjacent cells in the chart, is bounded by O(m|λ| + m 3 k 2 |R b | + m 2 b|R u |). For sentence length 25, the mean in our experiments, the shift- reduce parser performs 100 time fewer operations. See the supplementary material for the full analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning</head><p>We assume access to a training set of N examples</p><formula xml:id="formula_12">D = {(x (i) , z (i) )} N i=1</formula><p>, each containing a sentence x (i) and a logical form z (i) . The data does not in- clude information about the lexical entries and CCG parsing operations required to construct the correct derivations. We bootstrap this information from a learned parser. In our experiments we use a learned dynamic-programming CKY parser. We transfer the lexicon Λ directly from the input parser, and focus on estimating the parameters θ, which include fea- ture embeddings, hidden layer matrices, and bias terms. The main challenge is learning from the noisy supervision provided by the input parser. In our ex- periments, the CKY parser fails to correctly parse 40% of the training data, and returns on average 147 max-scoring correct derivations for the rest. We pro- pose an iterative algorithm that treats the choice be- tween multiple parse trees as latent, and effectively learns from partial analysis when no correct deriva- tion is available.</p><p>The learning algorithm (Algorithm 1) starts by processing the data using the CKY parser (lines 3 - 4). For each sentence x (i) , we collect the max- scoring CCG trees with z (i) at the root. The CKY parser often contains many correct parses with iden- tical scores, up to 49K parses per sentence. There- fore, we randomly sample and keep up to 1K trees. This process is done once, and the algorithm then runs for T iterations. At each iteration, given the sets of parses from the CKY parser Y, we select the max- probability parse according to our current parame- ters θ (line 10) and add all the shift-reduce decisions from this parse to D A (line 12), the action data set that we use to estimate the parameters. We approxi- mate the arg max with beam search using an oracle computed from the CKY parses. 9 CONFGEN aggre- gates the configuration-action pairs from the highest scoring derivation. Parse selection depends on θ and this choice will gradually converge as the parame- ters improve. The action data set is used to compute the 2 -regularized negative log-likelihood objective Algorithm 1 The learning algorithm.</p><formula xml:id="formula_13">Input: Training set D = {(x (i) , z (i) )} N i=1</formula><p>, learning rate µ, regularization parameter 2, and number of iterations T . Definitions: GENMAXCKY(x, z) returns the set of max- scoring CKY parses for x with z at the root. SCORE(y, θ) scores a tree y according to the parameters θ (Section 3.2). CONFGEN(x, y) is the sequence of action-configuration pairs that generates y given x (Section 3.1). BP(∆J ) takes the objective J and back-propagates the error J through the computation graph for the sample used to com- pute the objective. ADAGRAD(∆) applies a per-feature learning rate to the gradient ∆ ( <ref type="bibr" target="#b13">Duchi et al., 2011</ref>). Output: Model parameters θ.</p><p>1: » Get trees from CKY parser.</p><formula xml:id="formula_14">2: Y ← [] 3: for i = 1 to N do 4: Y[i] = GENMAXCKY(x (i) , z (i) ) 5: for t = 1 to T do 6:</formula><p>» Pick max-scoring trees and create action dataset. 7: DA = ∅ 8:</p><formula xml:id="formula_15">for i = 1 to N do 9: if Y[i] = ∅ then 10: A ← CONFGEN(x (i) , 11: arg max y∈Y[i] SCORE(y, θ)) 12:</formula><p>for c, a ∈ A do 13:</p><p>DA ← DA ∪ {{c, a} 14:</p><p>» Back-propagate the loss through the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15:</head><p>for c, a ∈ DA do 16:</p><formula xml:id="formula_16">J def = − log p(a | c) + 2 2 θ T θ 17: ∆ ← BP(J ) 18:</formula><p>θ ← θ − µADAGRAD(∆) 19: return θ J (line 16) and back-propagate the error to compute the gradient (line 17). We use AdaGrad ( <ref type="bibr" target="#b13">Duchi et al., 2011</ref>) to update the parameters θ (line 18).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learning from Partial Derivations</head><p>The input parser often fails to generate correct parses. In our experiments, this occurs for 40% of the training data. In such cases, we can obtain a forest of partial parse trees Y p . Each partial tree y ∈ Y p corresponds to a span of tokens in the sen- tence and is scored by the input parser. In practice, the spans are often overlapping. Our goal is to gen- erate high quality configuration-action pairs c, a from Y p . These pairs will be added to D A for train- ing. While extracting actions a is straightforward, generating configurations c requires reconstructing the stack σ from an incomplete forest of partial trees Y p . <ref type="figure">Figure 4</ref> illustrates our proposed process. Let CKYSCORE(y) be the CKY score of the partial tree y. To reconstruct σ, we select non-overlapping par-x1 x2 x3 x16 x 1:6</p><p>x 11:14</p><p>Figure 4: Partial derivation selection for learning (Sec- tion 4.1). The dotted triangles represent skipped spans in the sentence, where no high quality partial trees were found. Dark triangles represent the selected partial trees. We identify two contiguous spans, 1-6 and 11-14, and generate two synthetic sentences for training: the tokens are treated as complete sentences and actions and stack state are generated from the partial trees.</p><p>tial trees Y that correspond to the entire sentence by solving arg max Y ⊆Yp CKYSCORE(y) under two constraints: (a) no two trees from Y correspond to overlapping tokens, and (b) for each token in x, there exists y ∈ Y that corresponds to it. We solve the arg max using dynamic programming. The gener- ated set Y approximates an intermediate state of a shift-reduce derivation. However, Y p often does not contain high quality partial derivation for all spans.</p><p>To skip low quality partial trees and spans that have no trees, we generate empty trees y e for every span, where CKYSCORE(y e ) = 0, and add them to Y p . If the set of selected partial trees Y includes empty trees, we divide the sentence to separate examples and ignore these parts. This results in partial and approximate stack reconstruction. Finally, since Y P is noisy, we prune from it partial trees with a root that does not match the syntactic type for this span from an automatically generated CCGBank <ref type="bibr" target="#b20">(Hockenmaier and Steedman, 2007</ref>) syntactic parse. Our complete learning algorithm alternates be- tween epochs of learning with complete parse trees and learning with partial derivations. In epochs where we use partial derivations, we use a modified version of Algorithm 1, where lines 9-10 are updated to use the above process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>Our approach is inspired by recent results in de- pendency parsing, specifically by the architecture of <ref type="bibr" target="#b11">Chen and Manning (2014)</ref>, which was further developed by <ref type="bibr" target="#b39">Weiss et al. (2015)</ref> and <ref type="bibr" target="#b1">Andor et al. (2016)</ref>. <ref type="bibr" target="#b14">Dyer et al. (2015)</ref> proposed to encode the parser state using an LSTM recurrent architec- ture, which has been shown generalize well between languages ( <ref type="bibr" target="#b7">Ballesteros et al., 2015;</ref><ref type="bibr" target="#b0">Ammar et al., 2016)</ref>. Our network architecture combines ideas from the two threads: we use feature embeddings and a simple MLP to score actions, while our prob- ability distribution is similar to the LSTM parser.</p><p>The majority of CCG approaches for semantic parsing rely on CKY parsing with beam search (e.g., <ref type="bibr">Collins, 2005, 2007;</ref><ref type="bibr" target="#b24">Kwiatkowski et al., 2010</ref><ref type="bibr" target="#b25">Kwiatkowski et al., , 2011</ref><ref type="bibr" target="#b2">Artzi and</ref><ref type="bibr">Zettlemoyer, 2011, 2013;</ref><ref type="bibr" target="#b3">Artzi et al., 2014;</ref><ref type="bibr" target="#b30">Matuszek et al., 2012;</ref><ref type="bibr" target="#b23">Kushman and Barzilay, 2013)</ref>. Semantic parsing with other formalisms also often relied on CKY- style algorithms (e.g., <ref type="bibr" target="#b27">Liang et al., 2009;</ref><ref type="bibr" target="#b21">Kim and Mooney, 2012)</ref>. With a similar goal to ours, Berant and Liang (2015) designed an agenda-based parser. In contrast, we focus on a method with linear num- ber of operations guarantee.</p><p>Following the work of Collins and Roark (2004) on learning for syntactic parsers, <ref type="bibr" target="#b4">Artzi et al. (2015)</ref> proposed an early update procedure for inducing CCG grammars with a CKY parser. Our partial derivations learning method generalizes this method to parsers with global features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Setup</head><p>Task and Data We evaluate on AMR parsing with CCG. AMR is a general-purpose meaning represen- tation, which has been used in multiple tasks ( <ref type="bibr" target="#b31">Pan et al., 2015;</ref><ref type="bibr" target="#b28">Liu et al., 2015;</ref><ref type="bibr" target="#b34">Sawai et al., 2015;</ref><ref type="bibr" target="#b16">Garg et al., 2016)</ref>, We use the newswire portion of AMR Bank 1.0 release (LDC2014T12), which displays some of the fundamental challenges in se- mantic parsing, including long newswire sentences with a broad array of syntactic and semantic phe- nomena. We follow the standard train/dev/test split of 6603/826/823 sentences. We evaluate with the SMATCH metric . Our parser is incorporated into the two-stage approach of <ref type="bibr" target="#b4">Artzi et al. (2015)</ref>. The approach includes a bi-directional and deterministic conversion between AMR and lambda calculus. Distant references, for example such as introduced by pronouns, are represented using Skolem IDs, globally-scoped existentially- quantified unique IDs. A derivation includes a CCG tree, which maps the sentence to an underspecified logical form, and a constant mapping, which maps underspecified elements to their fully specified form.</p><p>The key to the approach is the underspecified logi- cal forms, where distant references and most rela- tions are not fully specified, but instead represented AMR Underspecified Logical Form Logical Form (c/conclude-02 :ARG0 (l/lawyer) :ARG1 (a/argument :poss l) :time (l2/late)) A1(λc.conclude-02(c) ∧ ARG0(c, A2(λl.lawyer(l))) ∧ ARG1(c, A3(λa.argument(a) ∧ poss(a, R(ID ID ID)))) ∧ REL REL REL(c, A4(λl2.late(l2)))) A1(λc.conclude-02(c) ∧ ARG0(c, A2(λl.lawyer(l))) ∧ ARG1(c, A3(λa.argument(a) ∧ poss(a, R(2)))) ∧ time(c, A4(λl2.late(l2))))</p><p>Figure 5: AMR for the sentence the lawyer concluded his arguments late. In <ref type="bibr" target="#b4">Artzi et al. (2015)</ref>, The AMR (left) is deterministically converted to the logical form (right). The underspecified logical form is the result of the first stage, CCG parsing, and contains two placeholders (bolded): ID for a reference, and REL for a relation. To generate the final logical form, the second stage resolves ID to the identifier of the lawyer (2), and REL to the relation time. We focus on a model for the first stage and use an existing model for the second stage.</p><p>as placeholders. <ref type="figure">Figure 5</ref> shows an example AMR, its lambda calculus conversion, and its underspec- ified logical form. ( <ref type="bibr" target="#b4">Artzi et al., 2015</ref>) use a CKY parser to identify the best CCG tree, and a factor graph for the second stage. We integrate our shift- reduce parser into the two-stage setup by replacing the CKY parser. We use the same CCG configura- tion and integrate our parser into the join probabilis- tic model. Formally, given a sentence x, the proba- bility of an AMR logical form z is</p><formula xml:id="formula_17">p(z | x) = u p(z | u, x) y∈Y(u) p(y | x) ,</formula><p>where u is an underspecified logical form, Y(u) is the set of CCG trees with u at the root. We use our shift-reduce parser to compute p(y | x) and use the pre-trained model from <ref type="bibr" target="#b4">Artzi et al. (2015)</ref> for p(z | u, x). Following <ref type="bibr" target="#b4">Artzi et al. (2015)</ref>, we disallow configurations that will not result in a valid AMR, and design a heuristic post-processing technique to recover a single logical form from terminal config- urations that include multiple disconnected partial trees on the stack. We use the recovery technique when no complete parses are available.</p><p>Tools We evaluate with the SMATCH metric . We use EasyCCG ( <ref type="bibr" target="#b26">Lewis and Steedman, 2014</ref>) for CCGBank categories (Sec- tion 4.1). We implement our system using Cornell SPF <ref type="bibr" target="#b2">(Artzi, 2016)</ref>, and the deeplearning4j library. <ref type="bibr">10</ref> The setup of <ref type="bibr" target="#b4">Artzi et al. (2015)</ref> also includes the Illi- nois NER <ref type="bibr" target="#b33">(Ratinov and Roth, 2009)</ref> and Stanford CoreNLP POS Tagger ( <ref type="bibr" target="#b29">Manning et al., 2014</ref>).</p><p>Parameters and Initialization We minimize our loss on a held-out 10% of the training data to tune our parameters, and train the final model on the full data. We set the number of epochs T = 3, regularization coefficient 2 = 10 −6 , learning rate 10 http://deeplearning4j.org/   <ref type="table">Table 1</ref> shows development results. We trained each model three times and report the best performance. We observed a variance of roughly 0.5 in these runs.</p><formula xml:id="formula_18">p(z | x, θ, Λ) = 1 |M | m∈M p(z | m, x, θ, Λ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>We experimented with different features for con- figuration embedding and with removing learning with partial derivations (Section 4.1). The com-plete model gives the best single-model performance of 65.3 F1 SMATCH, and we observe the benefits for semantic embeddings and learning from partial derivations. Using partial derivations allowed us to learn 370K more features, 22% of observed em- beddings. We also evaluate ensemble performance. We observe an overall improvement in performance. However, with multiple models, the benefit of us- ing semantic embeddings vanishes. This result is encouraging since semantic embeddings can be ex- pensive to compute if the logical form grows with sentence length. We also provide results for run- ning a shift-reduce log-linear parser p(a | c) ∝ exp{w T φ CKY (a, c)} using the input CKY model. We observe a significant drop in performance, which demonstrates the overall benefit of our architecture. <ref type="figure">Figure 6</ref> shows the development performance of our best performing ensemble model for different beam sizes. The performance decays slowly with decreasing beam size. Surprisingly, our greedy parser achieves 59.77 SMATCH F1, while the CKY parser with a beam of 1 achieves only 19.2 SMATCH F1 <ref type="table">(Table 1)</ref>. This allows our parser to trade-off a modest drop in accuracy for a significant improve- ment in runtime. <ref type="table" target="#tab_3">Table 2</ref> shows the test results using our best per- forming model (ensemble with syntax features). We compare our approach to the CKY parser of <ref type="bibr" target="#b4">Artzi et al. (2015)</ref> and JAMR ( <ref type="bibr" target="#b15">Flanigan et al., 2014</ref>). 11,12 We also list the results of <ref type="bibr" target="#b38">Wang et al. (2015b)</ref>, who demonstrated the benefit of auxiliary analyzers and is the current state of the art. <ref type="bibr">13</ref> Our performance is comparable to the CKY parser of ( <ref type="bibr" target="#b4">Artzi et al., 2015)</ref>, which we use to bootstrap our system. This demon- strates the ability of our parser to match the perfor- mance of a dynamic-programming parser, which ex- ecutes significantly more operations per sentence.</p><p>Finally, <ref type="figure">Figure 7</ref> shows our parser runtime rel- ative to sentence length. In this analysis, we fo- cus on runtime, and therefore use a single model. We compare two versions of our system, including and excluding semantic embeddings, and the CKY parser of <ref type="bibr" target="#b4">Artzi et al. (2015)</ref>. We run both parsers with 16 cores and 122GB memory. The shift-reduce parser is three times faster on average, and up to ten times faster on long sentences. Since our parser is currently using CPUs, future work focused on GPU porting is likely to see further improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>Our parser design emphasizes a balance between model capacity and the ability to combine atomic features against the computational cost of scor- ing actions. We also design a learning algorithm to transfer learned models and learn neural net- work models from ambiguous and partial supervi- sion. Our model shares many commonalities with transition-based dependency parsers. This makes it a good starting point to study the effectiveness of other dependency parsing techniques for semantic parsing, for example global normalization <ref type="bibr" target="#b1">(Andor et al., 2016)</ref> and bidirectional LSTM feature repre- sentations <ref type="bibr" target="#b22">(Kiperwasser and Goldberg, 2016)</ref>. sification with probabilistic categorial grammars. In Proceedings of the Conference on Uncertainty in Artificial Intelligence.</p><p>Zettlemoyer, L. S. and <ref type="bibr">Collins, M. (2007)</ref>. Online learning of relaxed CCG grammars for parsing to logical form. In Proceedings of the Joint Confer- ence on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of scoring the next action given the configuration c when parsing the sentence Some old networks remain inoperable. Embeddings of the same feature type are colored the same. The configuration embedding ξ(c) is a concatenation of syntax embeddings (green) and the logical form embedding (blue; computed by ψ) for the top entries in the stack. We then pass ξ(c) through the MLP F. Given the actions A(c), we compute the embeddings φ(a i , c), i = 1. .. |A(c)|. The actions and MLP representation are combined with a bilinear softmax layer. The number of concatenated vectors and stack elements used is for illustration. The details are described in Section 3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of embedding the logical form λx.arg0(x, JOHN) with the recursive embedding function ψ. In each level in ψ, the children nodes are combined with a single-layer neural network parameterized by W r , δ r , and the tanh activation function. Computed embeddings are in dark gray, and embeddings from lookup tables are in light gray. Constants are embedded by combining name and type embeddings, literals are unrolled to binary recursive structures, and lambda terms are combinations of variable type and body embeddings. For example, JOHN is embedded by combining the embeddings of its name and type, the literal arg0(x, JOHN) is recursively embedded by first embedding the arguments (x, JOHN) and then combining the predicate, and the lambda term is embedded to create the embedding of the entire logical form.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: The effect of beam size on model performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>3</head><label>3</label><figDesc></figDesc><table>Embedding Layer 

Hidden 
Layers 

Dimensionality 
Reduction Layer 

Embedding Layer 

Embedding Layer 

Bilinear Softmax Layer 

Configuration 
Embedding 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Test SMATCH results. 12 

µ = 0.05, skipping term γ = 1.0. We set the di-
mensionality of feature embeddings based on the vo-
cabulary size of the feature type. The exact dimen-
sions are listed in the supplementary material. We 
use 65 ReLU units for h 1 and h 2 , and 50 units for 
h 3 . We initialize θ with the initialization scheme of 
Glorot and Bengio (2010), except the bias term for 
ReLu layers, which we initialize to 0.1 to increase 
the number of active units on initialization. During 
test, we use the vector 0 as embedding for unseen 
features. We use a beam of 512 for testing and 2 for 
CONFGEN (Section 4). 
Model Ensemble For our final results, we 
marginalize the output over three models M using 
</table></figure>

			<note place="foot" n="1"> The source code and pre-trained models are available at http://www.cs.cornell.edu/~dkm/ccgparser.</note>

			<note place="foot" n="2"> We use the terms parser configuration and parser state interchangeably. 3 The head of the stack σ is the right-most entry, and the head of the buffer β is the left-most entry.</note>

			<note place="foot" n="4"> We follow the standard notation of L|x indicating a list with all the entries from L and x as the right-most element.</note>

			<note place="foot" n="5"> For simplicity, the figure shows only the top two trees. 6 The algorithm is provided in the supplementary material.</note>

			<note place="foot" n="8"> We experimented with an LSTM parser similar to Dyer et al. (2015). However, performance was not competitive. This direction remains an important avenue for future work.</note>

			<note place="foot" n="9"> Our oracle is non-deterministic and incomplete (Goldberg and Nivre, 2013).</note>

			<note place="foot" n="11"> JAMR results are taken from Artzi et al. (2015). 12 Pust et al. (2015), Flanigan et al. (2014), and Wang et al. (2015b) report results on different sections of the corpus. These results are not comparable to ours. 13 Our goal is to study the effectiveness of our model transfer approach and architecture. Therefore, we avoid using any resources used in (Wang et al., 2015b) that are not used in the CKY parser we compare to.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported in part by gifts from Google and Amazon. The authors thank Kenton Lee for technical advice, and Adam Gibson and Alex Black of Skymind for help with Deeplearning4j. We also thank Tom Kwiatkowski, Arzoo Katiyar, Tianze Shi, Vlad Niculae, the Cornell NLP Group, and the reviewers for helpful advice.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<title level="m">Many languages, one parser. Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collins</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Cornell SPF: Cornell semantic parsing framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning compact lexicons for CCG semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petrov</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Broad-coverage CCG semantic parsing with AMR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bootstrapping semantic parsers from conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of semantic parsers for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improved transition-based parsing by modeling characters instead of words with LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Linguistic Annotation Workshop</title>
		<meeting>the Linguistic Annotation Workshop</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imitation learning of agenda-based semantic parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Smatch: an evaluation metric for semantic feature structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the Association of Computational Linguistics</title>
		<meeting>the Conference of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Incremental parsing with the perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with stack long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A discriminative graphbased parser for the Abstract Meaning Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the Association of Computational Linguistics</title>
		<meeting>the Conference of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Extracting biomolecular interactions using semantic parsing of biomedical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Artificial Intelligence</title>
		<meeting>the Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Training deterministic parsers with non-deterministic oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">CCGBank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised PCFG induction for grounded language learning with highly ambiguous supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Simple and accurate dependency parsing using bidirectional LSTM feature representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics, 4</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Using semantic unification to generate regular expressions from natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Association for Computational Linguistics</title>
		<meeting>the Human Language Technology Conference of the North American Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Inducing probabilistic CCG grammars from logical form with higherorder unification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lexical generalization in CCG grammar induction for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A* CCG parsing with a supertag-factored model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning semantic correspondences with less supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the Association for Computational Linguistics the International Joint Conference on Natural Language Processing</title>
		<meeting>the Joint Conference of the Association for Computational Linguistics the International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Toward abstractive summarization using semantic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Association for Computational Linguistics</title>
		<meeting>the North American Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A joint model of language and perception for grounded attribute learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Matuszek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fox</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised entity linking with Abstract Meaning Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Association for Computational Linguistics</title>
		<meeting>the North American Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Parsing english into abstract meaning representation using syntax-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">May</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computational Natural Language Learning</title>
		<meeting>the Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic structure analysis of noun phrases using abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sawai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the annual meeting on Association for Computational Linguistics</title>
		<meeting>the annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Surface Structure and Interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The Syntactic Process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Boosting transition-based amr parsing with refined actions and auxiliary analyzers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradhan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A transition-based algorithm for AMR parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradhan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Association for Computational Linguistics</title>
		<meeting>the North American Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Structured training for neural network transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petrov</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the annual meeting on Association for Computational Linguistics</title>
		<meeting>the annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning to map sentences to logical form: Structured clas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
