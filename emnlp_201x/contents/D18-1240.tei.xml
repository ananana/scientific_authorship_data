<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Pair Text Representations for Answer Sentence Selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 2162</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kateryna</forename><surname>Tymoshenko</surname></persName>
							<email>kateryna.tymoshenko@unitn.it</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DISI</orgName>
								<orgName type="institution" key="instit2">University of Trento (Adeptmind scholar</orgName>
								<address>
									<postCode>38123, 90266</postCode>
									<settlement>Povo (TN)</settlement>
									<region>CA</region>
									<country>Italy, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DISI</orgName>
								<orgName type="institution" key="instit2">University of Trento (Adeptmind scholar</orgName>
								<address>
									<postCode>38123, 90266</postCode>
									<settlement>Povo (TN)</settlement>
									<region>CA</region>
									<country>Italy, USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-Pair Text Representations for Answer Sentence Selection</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2162" to="2173"/>
							<date type="published">October 31-November 4, 2018. 2018. 2162</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>High-level semantics tasks, e.g., paraphrasing , textual entailment or question answering , involve modeling of text pairs. Before the emergence of neural networks, this has been mostly performed using intra-pair features , which incorporate similarity scores or rewrite rules computed between the members within the same pair. In this paper, we compute scalar products between vectors representing similarity between members of different pairs, in place of simply using a single vector for each pair. This allows us to obtain a representation specific to any pair of pairs, which delivers the state of the art in answer sentence selection. Most importantly, our approach can outperform much more complex algorithms based on neural networks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Answer sentence selection (AS) is an impor- tant subtask of open-domain Question Answering (QA). Its input are a question Q and a set of can- didate answer passages A = {A 1 , A 2 , ..., A N }, which may, for example, be the output of a search engine. The objective consists in selecting A i , i âˆˆ {1, ..., N } that contain correct answers.</p><p>Pre-deep learning renaissance approaches to AS typically addressed the task by modeling Q-to-A (intra-pair) similarities <ref type="bibr" target="#b36">(Yih et al., 2013;</ref><ref type="bibr" target="#b29">Wang et al., 2007;</ref><ref type="bibr" target="#b9">Heilman and Smith, 2010;</ref><ref type="bibr" target="#b28">Wang and Manning, 2010)</ref>. Q-to-A similarity and align- ment are indeed crucial, but, in practice, it is very difficult to automatically extract meaningful rela- tions between Q and A. For example, consider two positive Q/A pairs in <ref type="table">Table 1</ref>. If we want to learn a model based only on the intra-pair Q- to-A matches, simple lexical matching (marked with italics) will not be enough. One would need to conduct more complex processing and identify * Professor at the University of <ref type="bibr">Trento.</ref> that movie and film are synonyms, and that the n- gram play in the movie or be in the movie can be paraphrased as star. While the former can be easily detected using an external lexical resource, e.g., WordNet <ref type="bibr" target="#b6">(Fellbaum, 1998)</ref>, the latter would require more complex inference.</p><p>On the other hand, Q 1 and Q 2 contain the same pattern who ... in the movie ..., and their respec- tive answers contain film ... starring .... If we know that P 1 = (Q 1 , A 1 ) is a positive AS example and want to classify P 2 = (Q 2 , A 2 ), then high Q 2 - to-Q 1 and A 2 -to-A 1 cross-pair similarities can suggest that P 1 and P 2 are likely to have the same label. This idea, for example, was exploited by <ref type="bibr">Severyn and Moschitti (2012)</ref>, whose system mea- sures syntactic-semantic similarities directly be- tween structural syntactic tree representations of Q 1 /Q 2 and A 1 /A 2 . This model still exhibited state-of-the-art performance in 2016 ( <ref type="bibr" target="#b22">Tymoshenko et al., 2016a</ref>).</p><p>Deep neural networks (DNNs) also naturally use such cross-pair similarity when modeling two input texts, and then further combine it with intra- pair similarity, for example, by means of atten- tion mechanisms <ref type="bibr" target="#b20">(Shen et al., 2017)</ref>, compare- aggregate architectures <ref type="bibr" target="#b3">(Bian et al., 2017;</ref><ref type="bibr" target="#b30">Wang and Jiang, 2017)</ref>, or fully-connected layers <ref type="bibr" target="#b25">(Severyn and</ref><ref type="bibr">Moschitti, 2015, 2016;</ref><ref type="bibr" target="#b19">Rao et al., 2016)</ref>.</p><p>In this work, we observe that: (i) the high accu- racy of the kernel model by <ref type="bibr">Severyn and Moschitti (2012)</ref> was due not only to the use of syntactic structures, but also to the use of cross-pair simi- larities; and (ii) the success of DNNs for QA can be partially attributed to an implicit combination of cross-and intra-pair similarity.</p><p>More specifically, we investigate, whether sim- ple similarity metrics, e.g., cosine similarity be- tween standard vector representations, can per- form competitively to the state-of-the-art neural models when employed as cross-pair kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>Answer Label Q1 who plays mary poppins in the movie?</p><p>A1 Mary Poppins is a 1964 musical film starring Julie Andrews, Dick Van Dyke, David Tomlinson, and Glynis Johns, produced by Walt Disney, and based on the Mary Poppins books series by P. L. Travers.  <ref type="table">Table 1</ref>: Question/Answer Sentence pairs from WikiQA corpus. We use italic font to mark intra-pair lexical matches between Q1 and A1, Q2 and A2, and bold font to mark the cross-pair matches between Q1 and Q2, A1 and A2.</p><p>To this end, we apply linear and cosine kernels to Q i /Q j and A i /A j pairs (i, j = 1, ..., N ) repre- sented as a bag-of-words (BoW) or an averaged sum of their pretrained word embeddings. Then, we combine them with the cross-pair Tree Kernels (TKs) and kernels applied to the traditional Q/A intra-pair similarity feature vector representations in a composite kernel and use it in an SVM model.</p><p>We experiment with three reference datasets, WikiQA ( <ref type="bibr" target="#b34">Yang et al., 2015</ref>), TREC13 ( <ref type="bibr" target="#b35">Yao et al., 2013;</ref><ref type="bibr" target="#b29">Wang et al., 2007)</ref> and <ref type="bibr">SemEval-2016</ref><ref type="bibr">, Task 3.A (Nakov et al., 2016</ref>, using a number of lexical-overlap/syntactic kernels. The latter chal- lenge refers to a community question answering (cQA) task. It consists in reranking the responses to user questions from online forums. It is the same setting as AS, but the text of questions and answer sentences can be ungrammatical due to the nature of the online forum language.</p><p>We obtain competitive results on WikiQA and SemEval tasks, showing that: (i) simple BoW rep- resentations, when used in cross-pair kernels, per- form comparably to and even outperform hand- crafted intra-pair features. (ii) In cQA, simple cross-pair embedding-and BoW-based similar- ity features outperform domain-specific similar- ity features, which are hand-crafted from intra- pair members. The simple features also perform comparably to syntactic TKs. (iii) We show that a combination of simple cosine-intra-and cross- pair kernels with TKs can outperform the most re- cent state-of-the-art DNN architectures.</p><p>Assuming the conjecture of our paper correct, cross-pair modeling is the major neural network contribution, the last point above is not surpris- ing as on relatively small datasets kernels-based models can exploit syntactic information very ef- fectively while neural models cannot.</p><p>The paper is structured as follows. We de- scribe the kernels incorporating intra-and cross- pair matches in Sec. 3.2, list the simple cross- and intra-pair features in Sec. 3.3, describe strong hand-crafted baseline features in Sec. 4, and report the experimental results in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Early approaches to AS typically focused on mod- eling intra-pair Q-to-A alignment similarities. For example, <ref type="bibr" target="#b36">Yih et al. (2013)</ref> proposed a latent alignment model that employed lexical-semantical Q-to-A alignments, <ref type="bibr" target="#b29">Wang et al. (2007)</ref> mod- eled syntactic alignments with probabilistic quasi- synchronous grammar, and <ref type="bibr" target="#b9">Heilman and Smith (2010)</ref>; <ref type="bibr" target="#b35">Yao et al. (2013)</ref>; <ref type="bibr" target="#b28">Wang and Manning (2010)</ref> employed Tree Edit Distance-based Q-to- A alignments.</p><p>Originally, the idea of cross-pair similarity was proposed by <ref type="bibr" target="#b39">Zanzotto and Moschitti (2006)</ref> and applied to the recognizing textual entailment task, which consists in detecting whether a text T en- tails a hypothesis H. They assumed that if two H/T pairs H 1 , T 1 and H 2 , T 2 share the same T-to-H "rewrite rules", they are likely to share the same label. Based on this idea, they proposed an al- gorithm applying TKs to (H 1 , H 2 ) and (T 1 , T 2 ) syntactic tree representations, enriched with H-to- T intra-pair rewrite rule information. More con- cretely, such algorithm aligns the constituents of H with T and then marks them with symbols di- rectly in the trees. This way the alignment infor- mation can be matched by tree kernels applied to cross-pair members.</p><p>Then, a line of work on AS, started by Sev- eryn and Moschitti <ref type="bibr">(2012,</ref><ref type="bibr">2013)</ref>; <ref type="bibr">Severyn et al. (2013)</ref>, was inspired by a similar idea of incor- porating "rewrite rules" directly into the tree rep- resentations of Q 1 /A 1 and Q 2 /A 2 . They represent Q and A as syntactic trees enhanced with Q-to-A relational information, and apply TKs <ref type="bibr" target="#b14">(Moschitti, 2006</ref>) to (Q 1 , Q 2 ) and (A 1 , A 2 ). Thus they model cross-pair similarity, and learn important patterns occurring in Q and A separately. As shown in <ref type="bibr" target="#b22">(Tymoshenko et al., 2016a</ref>), this approach is compet- itive with convolutional neural networks (CNNs).</p><p>In our approach, instead of using only one TK, we employ a number of different word-based kernels, most of which can be computed more efficiently than TKs.</p><p>Most recent AS models are based on Deep Neu- ral Networks (DNNs), which learn distributed rep- resentations of the input data. DNNs are trained to apply series of non-linear transformations to the input Q and A, represented as compositions of word or character embeddings. DNN architectures learn AS-relevant patterns using intra-pair similar- ities as well as cross-pair, Q-to-Q and A-to-A, sim- ilarities, when modeling the input texts. For exam- ple, the CNN network by <ref type="bibr">(Severyn and Moschitti, 2015</ref>) has two separate embedding layers for Q and A, which are followed by the respective con- volution layers, whose output is concatenated and then passed through the final fully-connected joint layer. The weights in the Q and A convolution layers are learned by means of the backpropaga- tion algorithm on the training Q/A pairs. Thus, obviously, classifying a new Q/A pair is partially equivalent to performing the implicit cross-pair Q- to-Q and A-to-A comparison.</p><p>Additionally, the DNN approaches model the Q-to-A relatedness explicitly in a variety of ways, e.g., by: (i) using a Q-to-A transformation matrix and simple Q-to-A similarity features ( <ref type="bibr" target="#b38">Yu et al., 2014;</ref><ref type="bibr">Severyn and Moschitti, 2015)</ref>, (ii) relying on RNN and LSTM architectures ( <ref type="bibr" target="#b27">Wang and Nyberg, 2015;</ref><ref type="bibr" target="#b20">Shen et al., 2017)</ref>, (iii) employing at- tention components <ref type="bibr" target="#b37">(Yin et al., 2016;</ref><ref type="bibr" target="#b20">Shen et al., 2017;</ref><ref type="bibr" target="#b26">Wang et al., 2016a</ref>), (iv) decomposing input into similarity and dissimilarity matches ( <ref type="bibr" target="#b32">Wang et al., 2016b</ref>) or (v) using the compare-aggregate method ( <ref type="bibr" target="#b30">Wang and Jiang, 2017;</ref><ref type="bibr" target="#b3">Bian et al., 2017</ref>).</p><p>We believe that the ability of DNNs to implic- itly capture cross-pair relational matching, i.e., the capacity of learning from (Q 1 , Q 2 ) and (A 1 , A 2 ), is a very important factor to their high perfor- mance. This is of course paired with their abil- ity to learn non-linear patterns and capture Q-to- A relatedness by means of attention mechanisms. It should be noted that the latter are typically hard-coded in kernel models as lexical match- ing/similarity ( <ref type="bibr">Severyn and Moschitti, 2012</ref>). This is effective as much as the attention approach, at least with standard-size dataset, also in neural models <ref type="bibr">(Severyn and Moschitti, 2016)</ref>.</p><p>In our work, we model Q-to-A, Q-to-Q and A- to-A similarities with intra-and cross-pair ker- nels and show that such combination also exhibits state-of-the-art performance on the reference cor- pora. In addition, our approach can be applied to smaller datasets as it utilizes less parameters, and can provide insights on future DNN design.</p><p>3 Cross-pair similarity kernels for text</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background on Kernel Machines</head><p>Kernel Machines (KMs) allow for replacing the dot product with kernel functions directly applied to examples, i.e., they avoid mapping examples into vectors. The main advantage of KMs is a much lower computational complexity than the dot product as the kernel computation does not de- pend on the size of the feature space.</p><p>KMs are linear classifiers: given a labeled train- ing dataset S = {(x i , y i ) : i = 1, . . . , n}, their classification function can be defined as:</p><formula xml:id="formula_0">f (x) = w Â· x + b = n i=1 Î± i y i x i Â· x + b.</formula><p>where x is a classification example, w is the gra- dient of the separating hyperplane, and b its bias. The equation shows that the gradient is a linear combination of the training points x i âˆˆ R n mul- tiplied by their labels y i âˆˆ {âˆ’1, 1} and their weights Î± i âˆˆ R + . Note that the latter are differ- ent from zero only for the support vectors: this reduces the classification complexity, which will be lower than O(n) for each example.</p><p>We can replace the scalar product with a ker- nel function directly defined over a pair of ob-</p><formula xml:id="formula_1">jects, K(o i , o) = Ï†(o i )Ï†(o)</formula><p>, where Ï† : O â†’ R n maps from objects to vectors of the final feature space. The new classification function becomes:</p><formula xml:id="formula_2">f (o) = n i=1 Î± i y i K(o i , o) + b,</formula><p>which only needs the initial input objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inter-and intra-pair match kernel</head><p>We cast AS as a text pair classification task: given a pair, P = (Q, A), constituted by a question (Q) and a candidate answer sentence (A), we classify it as either correct or incorrect. We used KMs, where K (Â·, Â·) operates on two pairs, P 1 = (Q 1 , A 1 ) and P 2 = (Q 2 , A 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Intra-pair similarity</head><p>A traditional baseline approach would (i) repre- sent Q/A pairs as feature vectors, where the com- ponents are similarity metrics applied to Q and A, e.g., a world overlap-based similarity; and (ii) train a classification model, e.g., an SVM using the following kernel:</p><formula xml:id="formula_3">K IP (P 1 , P 2 ) = K v V Q 1 ,A 1 , V Q 2 ,A 2 , (1)</formula><p>where K v can be any kernel operating on the fea- ture vectors, e.g., the polynomial or linear (as in our work) kernel.</p><formula xml:id="formula_4">V T 1 ,T 2 is a vector built on N similarity features, f 1 (Â·, Â·), f 2 (Â·, Â·), ..., f N (Â·, Â·),</formula><p>extracted by applying similarity metrics to two texts, T 1 and T 2 (see Sec. 3.3 for the list of the sim- ilarity metrics we used). K IP merely uses intra- pair similarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Cross-pair similarity</head><p>We incorporate the intuition, similar questions are likely to demand similar answer patterns, by means of a cross-pair kernel, which measures sim- ilarity between questions and answers from P 1 and P 2 as follows:</p><formula xml:id="formula_5">K CP (P 1 , P 2 ) = V Q 1 ,Q 2 Â· V A 1 ,A 2 = N i=1 f i (Q 1 , Q 2 ) Â· f i (A 1 , A 2 )<label>(2)</label></formula><p>K CP measures P 1 -to-P 2 similarity in terms of a sum of the products of Q 1 -to-Q 2 and A 1 -to-A 2 similarities. Note, that within</p><formula xml:id="formula_6">K IP , f i (Q i , A i )</formula><p>is merely an i-th feature in the V Q i ,A i feature vector. At the same time, within K CP , f i (Â·, Â·) becomes a kernel, which takes the (Q 1 , Q 2 ) or (A 1 , A 2 ) pairs as input. In other words, V Q 1 ,Q 2 Â· V A 1 ,A 2 is a sum of products of f i (Â·, Â·) kernels ap- plied to the (Q 1 , Q 2 ) and (A 1 , A 2 ) pairs. K CP is a valid kernel if the similarity metrics used to compute the f i (Â·, Â·) are valid kernel functions. Finally, combining K IP and K CP enables learning of two different kinds of valuable cross- and intra-pair AS patterns. We combine various K IP and K CP by summing them or by training a meta-classifier on their outputs. See Section 5.4 for more details. <ref type="figure">Figure 1</ref> summarizes the dif- ferences between the K IP and K CP computation processes described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Similarity features</head><p>We employ three similarity feature types as f i (Â·, Â·). Two of them are computed using the co- sine similarity metrics and differ only in terms of the input texts, T 1 and T 2 , representations. The other type is constituted by TKs applied to the structural representations of T 1 and T 2 . Note that, since cosine similarity and TKs are valid kernels,  K CP (P 1 ,P 2 )=V Q1,Q2 ! V A1,A2 (cross-pair kernel)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K IP (P 1 ,P 2 ) = K V (V Q1,A1 ,V Q2,A2 ) (intra-pair kernel)</head><p>Figure 1: Feature extraction schema for two Q/A pairs, P1</p><p>and P2. VT 1 ,T 2 is a vector of similarity features extracted for a pair of texts T1, T2, the respective dashed boxes show from which pair of input texts they are extracted.</p><p>K CP is also guaranteed to be a valid kernel when computed using these similarity features.</p><p>3.3.1 Bag-of-n-grams overlap (B)</p><formula xml:id="formula_7">f t,l,s B (T 1 , T 2 )</formula><p>is a cosine similarity metric applied to the bag-of-n-grams vector representations of T i , BOW {t,l,s} (T i ), i = 1, 2. The {t, l, s} in- dex describes an n-gram representation configura- tion: t denotes whether the n-grams are assembled of word lemmas (L), or their part-of-speech tags (P OS), or lemmas concatenated with their respec- tive POS-tags (L P OS ); l is a (n 1 , n 2 ) tuple, with n 1 and n 2 being the minimal and maximal length of n-grams considered, respectively; and s is Y ES if the representation discards the stopwords and N O, otherwise.</p><p>We used {t, l, s} configurations from the following set:</p><formula xml:id="formula_8">C = ({L, L P OS } Ã— {(1, 2), (1, 3), (1, 4), (2, 4), (2, 3)} Ã— {Y ES, N O}) âˆª ({P OS} Ã— {(1, 4), (2, 4)} Ã— {Y ES}).</formula><p>It follows that |C| = 23, which means we have 23 similarity features, f t,l,s B (T 1 , T 2 ), in total, in the intra-pair setting. The respective cross-pair ker- nels are a composite kernel summing 23 products of cosine kernels applied to 23 different (Q 1 , Q 2 ) and (A 1 , A 2 ) bag-of-ngram representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Embedding-based similarities (E).</head><p>We represent an input text as an average of em- beddings of its lemmas from pre-trained word embedding models. Then, the embedding fea- ture f E model (T 1 , T 2 ) is the cosine kernel ap- plied to the embedding-based representations of T 1 and T 2 . We use two pretrained embed- dings: <ref type="bibr">Word2Vec (Mikolov et al., 2013</ref>) and <ref type="bibr">GloVe (Pennington et al., 2014</ref>), resulting in three 1 embedding-based features (see Sec. 5 for more technical details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Tree-kernel based similarities</head><p>Following the framework defined in ( <ref type="bibr">Severyn et al., 2013;</ref><ref type="bibr" target="#b22">Tymoshenko et al., 2016a</ref>), we rep- resent T 1 and T 2 as syntactico-semantic structures and use TKs as semantic similarity metrics. When computing K CP with TK as similarities, in Eq.2, we employ summation instead of multiplication 2 .</p><p>More specifically, we represent T 1 and T 2 as (i) constituency trees and apply subset TK (SST); or (ii) shallow chunk-based trees, similar to the one presented in <ref type="figure" target="#fig_1">Figure 2</ref>, and apply partial tree (PTK) kernel. In the shallow trees, lemmas are leaves and POS tags are pre-terminals. POS nodes are grouped under chunk nodes, and then under the sentences nodes. These representations en- code also some intra-pair similarity information, e.g., prefix REL denotes the lexical Q-to-A match. In a structural representation, we prepend it to the parent and grand-parent nodes of lemmas which occur both in Q and A, e.g., "Mary" in the first example of <ref type="table">Table 1</ref>.</p><p>Then, for factoid QA 3 , we mark focus words in Q and entities in A, if the answer contains any named entities of types matching the question expected answer type (EAT) <ref type="bibr">4</ref> . More specifically, we mark the semantic Q-to-A match by prepend- ing the REL-FOCUS-&lt;EAT&gt; label to the answer chunk nodes that contain such named entities and also to the question focus word. Here, &lt;EAT&gt; stands for the EAT label. For example, in the Q 1 /A 1 pair in <ref type="table">Table 1</ref>, the Q 1 EAT is HUMan, and the matching named entities include "Julie Andrews", "David Tomlinson" and others. <ref type="figure" target="#fig_1">Fig- ure 2</ref> depicts Q 1 annotated both with REL-and REL-FOCUS links. We detect both question focus and EAT automatically. Due to the space limita- <ref type="bibr">1</ref> We use Word2Vec embeddings trained on two different corpora, which result in two features, and GloVe trained on one corpus. <ref type="bibr">2</ref> We have opted to use summation in this case to follow the earlier work.</p><p>3 WikiQA and TREC13 are the factoid AS datasets, as their questions ask for a specific fact, e.g. date or a name. <ref type="bibr">4</ref> For example, the PERson named entity type matches the HUMan EAT. More specifically, we employ the follow- ing NER-to-EAT matching rules: PERson, ORGanization â†’ HUMan; LOCation â†’ LOCation; DATE, TIME, MONEY, PERCENTAGE, DURATION, NUMBER, SET â†’ NUM; ORGanization, PERson, MISCellanious â†’ ENTiTY. We em- ploy the ( <ref type="bibr" target="#b10">Li and Roth, 2002</ref>) coarse-grained EAT taxonomy and Stanford CoreNLP ( ) entity types.  <ref type="table">Table 1</ref> (i) cosine similarity applied to the BoW representa- tions of T1 and T2 in terms of word lemmas, bi-, three-, four-grams (computed twice with and with- out stopwords); POS-tags; dependency triplets; (ii) longest common string subsequence measure w. and w/out stopwords; (iii) Jaccard similarity metric applied to one-, two-, four, three-grams w. and w/out stopwords; (iv) word n-gram containment measure on uni-and bi-grams w. and w/out stopwords (Broder, 1997); (v) greedy string tiling (Wise, 1996) with minimum matching length of 3 ; (vi) string kernel similarity ( <ref type="bibr" target="#b11">Lodhi et al., 2002</ref>); (vii) expected answer type match: percentage of named entities (NE) in the answer passage compat- ible with the question class 5 ; (viii) WordNet-based similarity. WordNet T1/T2 com- mon lemma/synonym/hypernym overlap ratio; (ix) PTK <ref type="bibr" target="#b14">(Moschitti, 2006</ref>) similarity between con- stituency or dependency tree representations of in- put texts; tions, we do not describe the structural representa- tions and matching algorithms in more detail, but refer the reader to the works above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Strong baseline feature vector</head><p>As a strong baseline, we use similarity feature vec- tors and intra-pair K IP kernel.</p><p>For the factoid answer sentence selection task, we use 47 strong features listed in <ref type="table" target="#tab_2">Table 2</ref>. This is a compilation of features used in the top- performing system at SemEval-2012 Semantic Text Similarity workshop <ref type="bibr">(BÃ¤r et al., 2012</ref>) and earlier factoid QA work <ref type="bibr">(Severyn and Moschitti, 2012)</ref>, extended with few additional features.</p><p>For the community question answering (cQA) task, we employ instead a combination of similarity-based and thread-level features shown to be very effective for the cQA task ( <ref type="bibr" target="#b16">Nicosia et al., 2015;</ref><ref type="bibr" target="#b2">BarrÃ³n-CedeÃ±o et al., 2016</ref>). We use the exact feature combination from <ref type="bibr">(BarrÃ³nCedeÃ±o et al., 2016)</ref>, which includes both lexical and syntactic similarity measures (cosine similar- ity of bag-of-words, PTK similarity over syntactic tree representations of the input texts) and thread-  <ref type="table" target="#tab_2">DEV  TEST  Q  A  Q A Q A  raw 2118 20360 296 2733 633 6165  no all âˆ’ 873 8672 126 1130 243 2351  clean 857 8651 121 1126 237 2341   Table 3</ref>: WikiQA corpus statistics level domain specific features (are the question and comment authored by the same person?, does the comment contain any questions?, and so on.).</p><p>We cannot directly use these feature vectors in the K CP kernels, as not all functions used to com- pute features are valid kernels, e.g., the longest common string subsequence is not a kernel func- tion. Moreover, some of them can be computed only on the (Q, A) pairs, e.g., the expected type match feature (vii) in Tab. 2, or many of the cQA domain-specific features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We conduct experiments on three corpora, namely TREC13, WikiQA and SemEval, and evaluate the results in terms of Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR). Our code is available at https://github.com/ iKernels/RelTextRank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>WikiQA dataset. WikiQA ( <ref type="bibr" target="#b34">Yang et al., 2015</ref>) is a factoid answer sentence selection dataset with Bing query logs as questions. Candidate answer sentences are extracted from Wikipedia and la- beled manually. Some of the questions have no correct answer sentence (all âˆ’ ) or have only correct answer sentences (all + ). <ref type="table">Table 3</ref> reports the statis- tics of the WikiQA corpus as distributed (raw), without all âˆ’ questions, and without both all âˆ’ and all + questions (clean). We train in the "no all âˆ’ " mode using 10 answer sentences per question <ref type="bibr">6</ref> and test in the "clean" mode.</p><p>TREC13 dataset. A factoid answer sen- tence selection dataset originally presented in ( <ref type="bibr" target="#b29">Wang et al., 2007)</ref>  <ref type="bibr">7</ref> , also frequently called QASent ( <ref type="bibr" target="#b34">Yang et al., 2015</ref>). We train on 1,229 automatically labeled TREC8-12 questions. We use only 10 candidate answer sentences per question. We test in the "clean" setting defined in ( <ref type="bibr" target="#b19">Rao et al., 2016)</ref>, i.e., we discard the all + and all âˆ’ questions, resulting in 65 DEV and 68 TEST <ref type="bibr">6</ref> The 10 answer sentences per question limit speeds up training time without loss in performance <ref type="bibr">7</ref> We use the version distributed by <ref type="bibr" target="#b35">(Yao et al., 2013</ref>) in https://code.google.com/p/jacana/ questions, respectively. DEV and TEST contain 1117 and 1442 candidate associated answer sentences, respectively. SemEval-2016, Task 3.A dataset. SemEval cQA dataset is a benchmark dataset in the Se- mEval 2016 Task 3. A question-to-comment sim- ilarity competition. It is a collection of user ques- tions and the respective answer comment threads from Qatar Living forum, where the user com- ments to questions were manually labeled as cor- rect or incorrect. Each question has 10 respective candidate answers. The training, dev and test sets have 1790, 244 and 327 questions, respectively. The AS task consists in reranking comments with respect to the question: most questions are non- factoid and the text is often noisy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Models</head><p>We used the following notation: B, E. Intra-pair K IP kernels (see Sec. 3.2) using the eponymous similarity features from Sec. 3.3. V. Linear kernel applied to the strong intra-pair feature vector representation defined in Section 4. Note that, as already mentioned in Sec. 4, due to the slightly different nature of the factoid and com- munity question answering tasks, we used differ- ent strong feature groups for WikiQA, TREC13 <ref type="table" target="#tab_2">(Table 2)</ref> and SemEval-2016. B cr , E cr . Cross-pair K CP kernel applied to B and E similarity feature vectors respectively. More specifically, B cr and E cr are a sum of 23 and 3 cross-pair kernel products, respectively (see Eq. 2 and Sec. 3.2). PTK, SST are the cross-pair PTK and SST tree kernels applied to the shallow chunk-and constituency-based representations (see Sec. 3.3). "+" denotes kernel summation. We use this sym- bol to denote that we sum the gram-matrices for the distinct standalone kernels, and use the result- ing kernel matrix as input to SVMs. META BASE;P T K , META BASE;SST . Logistic regression metaclassifiers trained on the outputs of two standalone systems, namely (i) V+B cr +E cr +E (we denote it as BASE to simplify the notation), and (ii) PTK or SST, respectively. We ran 10-fold cross-validation on the training set and used the resulting predictions as training data for the en- semble classifier. We did not use the development or training sets for any parameter tuning, thus we report the results both on the DEV and TEST sets. SUM BASE;P T K , SUM BASE;SST . Simple meta- classifiers, summing the output of the BASE and PTK or SST systems, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Toolkits</head><p>We trained the models using scikit-learn 8 by <ref type="bibr" target="#b17">Pedregosa et al. (2012)</ref> using the SVC version of SVM with precomputed K IP and K CP kernel ma- trices and default parameters. We trained the en- semble model using the scikit LogisticRegression classifier implementation with the default param- eters. We used spaCy library 9 and scikit to obtain bag-of-n-gram representations for the B similar- ity features, and to compute B-and E-base gram matrices.</p><p>We used the RelTextRank framework 10 (Ty- moshenko et al., 2017b) to generate the structural representations for the TK similarity features and to extract the strong baseline feature vectors from Sec. 4. We used <ref type="bibr">KeLP (Filice et al.)</ref> to compute the TK gram matrices.</p><p>Regarding the Embedding-based similarities (E), we obtain three similarity features by us- ing three word embedding models to generate the representations of the input texts, T 1 and T 2 , namely GloVe vectors trained on common crawl data 11 , Word2Vec vectors pre-trained on Google News 12 , and another Word2Vec vectors model 13 pre-trained on Aquaint 14 plus Wikipedia. <ref type="table" target="#tab_4">Table 4</ref> reports the results obtained with the intra- and cross-pair kernels K IP , K CP and their com- binations. In the following, we describe the results according to the model categories above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results and discussion</head><p>Intra-pair kernels. Taking into account intra- pair similarity is the standard approach in the ma- jority of the previous non-DNN work. In our ex- periments, we implement this approach as K IP using B, E, V groups of similarity features. K IP performs worse than the state-of-art (SoA) DNN systems on all the datasets (see tables 5, 6 and 7, for the SoA systems).</p><p>The results on WikiQA are particularly low even when the best K IP system, B+V+E, is used, which scores up to 15 points less than the state of the art. This confirms the <ref type="bibr" target="#b34">Yang et al. (2015)</ref> observation on WikiQA, according to which, sim- ple word matching methods are likely to under- perform on its data, considering how it was built. Nevertheless, despite its simplicity, B+V+E per- forms comparably to the <ref type="bibr" target="#b34">Yang et al. (2015)</ref> reim- plementation of LCLR, the complex latent struc- tured approach employing rich lexical and seman- tic intra-pair similarity features <ref type="figure" target="#fig_1">(Yih et al., 2013)</ref>. <ref type="bibr" target="#b34">Yang et al. (2015)</ref> report that on WikiQA LCLR obtains MRR of 60.86 and MAP of 59.93.</p><p>Then, on TREC13 and SemEval-2016, the intra-pair V, V+E and B+V+E kernels exhibit rather high performance, however, they are still significantly below the state of the art, thus con- firming our hypothesis that intra-pair similarity alone does not guarantee top results.</p><p>Cross-pair kernels. B cr and E cr obtain rather high results on WikiQA and SemEval. On Wik- iQA, both B cr and B cr + E cr outperform all the intra-pair kernels by a large margin, while, on Se- mEval, they perform comparably to the manually engineered domain-specific V features of <ref type="bibr" target="#b16">Nicosia et al. (2015)</ref>. On the contrary, on TREC13, V outperforms both B cr and E cr , thus showing that TREC13 is indeed biased towards intra-pair relat- edness features by construction.</p><p>More complex PTK and SST cross-pair kernels, both alone and combined with B cr , E cr , typically outperform the standalone B cr and E cr on all the corpora (PTK on TREC13 and WikiQA, and SST and B cr +E cr +PTK on SemEval). This can be ex- plained by the fact that PTK and SST are able to learn complex syntactic patterns and also con- tain some information about intra-pair relations, namely REL-labels described in Sec. 3.2. Thus, it is natural that they outperform simpler cross-pair kernels. Nevertheless, on WikiQA-DEV, B cr +E cr performs very close to PTK. Moreover, on Se- mEval, B cr +E cr outperforms PTK and is behind SST for less than 1 point in terms of MAP. This can be explained by the fact that Q and A, in Se- mEval, are frequently ungrammatical as the cQA corpus is collected from online forums.</p><p>Finally, note that the B cr +E cr +PTK system, which does not use any cQA domain-specific fea- tures, is only 0.56 MAP points behind KeLP, the best-performing system in the SemEval competi- tion (see Line 1 of <ref type="table" target="#tab_8">Table 7)</ref>.</p><p>Kernels combining the intra-and cross-pair similarities. The V+B cr +E cr +E combination (we</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TREC13</head><p>WikiQA <ref type="table" target="#tab_2">Semeval-2016  DEV  TEST  DEV  TEST  DEV  TEST  MRR MAP MRR MAP MRR MAP MRR MAP MRR MAP MRR</ref>   will refer to it as BASE), outperforms the stan- dalone domain-specific handcrafted cQA features, V, and both PTK and SST on SemEval 2016 TEST and DEV by at least 2.3 points in all metrics. Moreover, V+B cr +E cr +E is only less than 0.5 points behind the #1 system of the SemEval- 2016 competition (see Tab. 7). We recall that V+B cr +E cr +E only uses basic n-gram overlap- based cross-and intra-similarity features and embedding-based cosine similarities.</p><p>Finally, when we add tree kernel models to the combination, i.e., V+B cr +E cr +E+PTK or V+B cr +E cr +E+SST, we note improvement for Se- mEval and TREC13 tasks.</p><p>Ensemble models. We ensemble cross-and intra-pair kernels-based models by summing the predictions of the standalone SVM classifiers (SUM models) or by training a logistic regression meta-classifier on them (META models). We build the meta-classifiers on the outputs of the stan- dalone system BASE and TKs, namely PTK and SST. The "Ensemble" section of <ref type="table" target="#tab_4">Table 4</ref> shows that meta-system combinations mostly outperform the standalone kernels.</p><p>In general, combining cross-pair and intra-pair similarities (with kernel sum or meta-classifiers) provides state-of-the-art results without using deep learning. Additionally, the outcome is de- terministic, while the DNN accuracy may vary de- pending on the type of the hardware used or the random initialization parameters <ref type="bibr" target="#b5">(Crane, 2018)</ref>. <ref type="table" target="#tab_6">Tables 5, 6</ref> and 7 report the performance of the most recent state-of-the-art systems on WikiQA, TREC13 and SemEval in comparison with our best results. We discuss them with respect to the different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Comparison with the state of the art</head><p>WikiQA. As already mentioned earlier, Wik- iQA contains many questions without correct an- swer (see <ref type="table">Tab.</ref> 3). When evaluated on the full data, even the oracle system will achieve at most 38.38 points of MAP. Moreover, as originally ob- served in ( <ref type="bibr" target="#b29">Wang et al., 2007)</ref>, the questions that do not have either correct answers or incorrect answers are not useful for comparing the perfor- mance of different answer sentence selection sys- tems. Therefore, they are typically removed from WikiQA and TREC13 before the evaluation.</p><p>There has been some discrepancy in the com- munity when evaluating on WikiQA. The original baselines proposed for the corpus in ( <ref type="bibr" target="#b34">Yang et al., 2015)</ref> were evaluated in the "clean" setting 15 . We no all âˆ’ clean MRR MAP MRR MAP LCLR ( <ref type="bibr" target="#b34">Yang et al., 2015)</ref> impl. of <ref type="bibr" target="#b36">(Yih et al., 2013)</ref> 61.83 60.92 60. <ref type="bibr">86</ref>    n/a 79.5 AI-CNN ( <ref type="bibr" target="#b40">Zhang et al., 2017)</ref> n/a 80.14 Our model <ref type="bibr">(V+Bcr+Ecr+E+SST)</ref> 86.52 79.79 also evaluate in the "clean" setting. However, the performance of the most recent state-of-the- art systems listed in the Tab. 5 is reported in the "no all âˆ’ " setting, in the respective papers, i.e., they keep the all + questions 16 . Thus, they have 6 extra questions always answered correctly by de- fault. To account for this discrepancy, in Tab. 5, we report the results in both settings. It is trivial to convert the performance figures from one setting to another. In the table, we mark the conversion results with italic.</p><p>Our SUM BASE;P T K system (i) outperforms all the state-of-the-art systems, including the sophisti- cated architectures with attention, such as IWAN- att ( <ref type="bibr" target="#b20">Shen et al., 2017)</ref>, and compare-aggregate (C/A) frameworks ( <ref type="bibr" target="#b30">Wang and Jiang, 2017;</ref><ref type="bibr" target="#b3">Bian et al., 2017</ref>) in terms of MRR; and (ii) has the same MAP as ( <ref type="bibr" target="#b3">Bian et al., 2017)</ref>. Obviously, this improvement is not statistically significant with re- <ref type="bibr">16</ref> We deduced that from the corpus statistics reported by the authors of the papers. They all report having 243 test questions, which corresponds to the "no all âˆ’ " setting spect to C/A systems by <ref type="bibr" target="#b3">Bian et al. (2017)</ref>. Nev- ertheless, ours is a very promising result, consid- ering that we only use linear models with simple kernels and do not tune any learning parameter of such models.</p><p>TREC13. As shown in Tab. 6, our models do not outperform the state of the art on TREC13, but they still perform comparably to the recent DNN HyperQA model ( <ref type="bibr" target="#b21">Tay et al., 2018)</ref>. In general, our model is behind the state-of-the-art IWAN-att sys- tem by 4.55 points in terms of MAP. Note, how- ever, that TREC13 test set contains only 68 ques- tions, therefore this difference in performance is not likely to be statistically significant <ref type="bibr">17</ref> .</p><p>Semeval. <ref type="table" target="#tab_8">Table 7</ref> compares performance of B cr + E cr + V + E + SST system on Semeval to that of KeLP and ConvKN, the two top systems in the SemEval 2016 competition, and also to the per- formance of the recent DNN-based HyperQA and AI-CNN systems. In the Semeval 2016 competi- tion, our model would have been the first 18 , with #1 KeLP system being 0.6 MAP points behind. Then, it would have outperformed the state-of-the- art AI-CNN system by 0.35 MAP points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This work proposes a simple, yet effective ap- proach to the task of answer sentence selection based on the intuition that similar patterns in ques- tions are likely to demand similar patterns in an- swers. We showed that this hypothesis provides an improvement on three benchmark datasets, Wik- iQA, TREC13, Semeval-2016, and, moreover, it enables simple features to achieve the state of the art on WikiQA and Semeval-2016, outperform- ing many of state-of-the-art DNN-based systems. There is significant room for further elaboration of this approach, for example, by expanding feature spaces with more syntactic and semantic features, employing new types of kernels for measuring the inter-question/answer pair similarity or trying to implement the same idea in DNN architectures. <ref type="bibr">Aliaksei Severyn and Alessandro Moschitti. 2012</ref>.</p><p>Structural relationships for large-scale learning of answer re-ranking. Proceedings of the 35th inter- national ACM SIGIR conference on Research and development in information retrieval -SIGIR '12, pages 741-750. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Q</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Shallow syntactic representation of A1 from the running example in Table 1</figDesc><graphic url="image-2.png" coords="5,315.83,62.81,198.42,90.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>TRUE Q2 WHO WAS IN THE MOVIE I CONFESS WITH MONTGOMERY CLIFT A2 I Confess is a 1953 drama film directed by Alfred Hitchcock, and starring Montgomery Clift as Fr. Michael William Lo- gan, a Catholic priest, Anne Baxter as Ruth Grandfort, and</head><label></label><figDesc></figDesc><table>Karl 
Malden as Inspector Larrue . 

TRUE 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Strong baseline fatures</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results on TREC13, WikiQA and SemEval-2016 datasets. Best results in each feature category are 
highlighted with bold, overall best results are underlined. TK is SST for Semeval and PTK for WikiQA and 
TREC13. BASE refers to the V+B cr +E cr +E configuration. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 5 : Comparison to the SoA on WikiQA</head><label>5</label><figDesc></figDesc><table>MRR MAP 
Noise-contrastive estim. (Rao et al., 2016) 87.7 80.1 
IWAN-att (Shen et al., 2017) 
88.9 82.2 
BIMPM (Wang et al., 2017) 
87.5 80.2 
C/A, k-threshold (Bian et al., 2017) 
89.9 82.1 
C/A-listwise (Bian et al., 2017) 
88.9 81.0 
HyperQA (Tay et al., 2018) 
86.5 78.4 
Our model (Bcr+Ecr+PTK) 
85.07 77.18 
Our model (V+Ecr) 
85.65 76.72 
Our model (SUMBASE;P T K ) 
86.89 77.65 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Comparison to the SoA on TREC13 

MRR MAP 
Kelp [#1] (Filice et al., 2016) 
86.42 79.19 
Conv-KN [#2] (BarrÃ³n-CedeÃ±o et al., 2016) 84.93 77.6 
CTKC +VQF (Tymoshenko et al., 2016b) 
86.26 78.78 
HyperQA (Tay et al., 2018) 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 7 : Comparison to the SoA on SemEval-2016</head><label>7</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="8"> http://scikit-learn.org/ 9 https://spacy.io/ 10 This tool employs Stanford CoreNLP 3.6.0 (Manning et al., 2014) for text processing; and DKProSimilarity (BÃ¤r et al., 2013) to extract features (ii)-(v). 11 http://nlp.stanford.edu/data/glove.42B.300d.zip 12 https://code.google.com/archive/p/word2vec/ 13 https://github.com/aseveryn/deep-qa 14 https://catalog.ldc.upenn.edu/LDC2002T31</note>

			<note place="foot" n="15"> According to the WikiQA gold reference files at https://www.microsoft.com/en-us/download/details. aspx?id=52419</note>

			<note place="foot" n="17"> We cannot conduct the test since we do not have the outputs of such systems. However, 4 points more correspond to just a couple of correct questions more. 18 Ranking available in (Nakov et al., 2016)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The first author is supported by Adeptmind. Many thanks to the Area chairs, PC chairs and anony-mous reviewers for their professional work and valuable suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ukp: Computing semantic textual similarity by combining multiple content similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>BÃ¤r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zesch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Joint Conference on Lexical and Computational Semantics (*SEM 2012)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="435" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">DKPro Similarity: An Open Source Framework for Text Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>BÃ¤r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="121" to="126" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ConvKN at SemEval-2016 task 3: Answer and question selection for question answering on Arabic and English fora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>BarrÃ³n-CedeÃ±o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bonadiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Obaidli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Romeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tymoshenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval 2016-10th International Workshop on Semantic Evaluation, Proceedings</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Compare-Aggregate Model with Dynamic-Clip Attention for Answer Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM &apos;17</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management, CIKM &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1987" to="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the resemblance and containment of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Broder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Compression and Complexity of Sequences 1997. Proceedings</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Questionable Answers in Question Answering Research: Reproducibility and Variability of Published Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Crane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="241" to="252" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">71</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">KeLP: a Kernel-based Learning Platform in Java, year = 2015</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Filice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Castellucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The workshop on Machine Learning Open Source Software (MLOSS): Open Ecosystems</title>
		<meeting><address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>International Conference of Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">KeLP at SemEval-2016 Task 3: Learning Semantic Relations between Questions and Answers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Filice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1116" to="1123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tree edit models for recognizing textual entailments, paraphrases, and answers to questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2010 Annual Conference of the North American Chapter of the ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1011" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning question classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2002: The 19th International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Text Classification using String Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huma</forename><surname>Lodhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Watkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="419" to="444" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP Natural Language Processing Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Christopher D Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning ECML</title>
		<imprint>
			<biblScope unit="volume">4212</biblScope>
			<biblScope unit="page" from="318" to="329" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hamdy Mubarak, abed Alhakim Freihat, Jim Glass, and Bilal Randeree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>LluÃ­s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Magdy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval2016 Task 3: Community Question Answering. Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="525" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">QCRI: Answer selection for community question answering-experiments for Arabic and English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Nicosia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Filice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>BarrÃ³ncedeno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iman</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamdy</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kareem</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Others</forename><surname>Darwish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="203" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">GaÃ«l</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">NoiseContrastive Estimation for Answer Selection with Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management-CIKM &apos;16</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management-CIKM &apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1913" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inter-Weighted Alignment Network for Sentence Pair Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gehui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1179" to="1189" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, WSDM &apos;18</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining, WSDM &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="583" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional neural networks vs. convolution kernels: Feature engineering for answer sentence reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tymoshenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bonadiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016-Proceedings of the Conference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to rank non-factoid answers: Comment selection in Web forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tymoshenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bonadiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ranking Kernels for Structures and Embeddings: A Hybrid Preference and Classification Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kateryna</forename><surname>Tymoshenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Bonadiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">RelTextRank: An Open Source Framework for Building Relational Syntactic-Semantic Text Pair Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kateryna</forename><surname>Tymoshenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Nicosia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017, System Demonstrations</title>
		<meeting>ACL 2017, System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="79" to="84" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Inner Attention based Recurrent Neural Networks for Answer Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1288" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A Long Short-Term Memory Model for Answer Sentence Selection in Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="707" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Probabilistic Tree-Edit Models with Structured Latent Variables for Textual Entailment and Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010-08" />
			<biblScope unit="page" from="1164" to="1172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
		<title level="m">What is the Jeopardy Model? A QuasiSynchronous Grammar for QA. Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A compareaggregate model for matching text sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4144" to="4150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sentence Similarity Learning by Lexical Decomposition and Composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1340" to="1349" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">YAP3: Improved detection of similarities in computer program and other texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCSE Bulletin</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">WIKIQA: A Challenge Dataset for Open-Domain Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2015</title>
		<meeting>EMNLP 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2013" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Answer Extraction as Sequence Tagging with Tree Edit Distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callisonburch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Question Answering Using Enhanced Lexical Semantic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pastusiak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1744" to="1753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Wenpeng Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>SchÃ¼tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="259" to="272" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
		<title level="m">Deep Learning for Answer Sentence Selection. NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Automatic Learning of Textual Entailments with Cross-Pair Similarities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Massimo</forename><surname>Zanzotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Attentive Interactive Neural Networks for Answer Selection in Community Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="505" to="509" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
