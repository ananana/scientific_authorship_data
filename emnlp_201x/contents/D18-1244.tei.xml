<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Convolution over Pruned Dependency Trees Improves Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Convolution over Pruned Dependency Trees Improves Relation Extraction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2205" to="2215"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2205</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Dependency trees help relation extraction models capture long-range relations between words. However, existing dependency-based models either neglect crucial information (e.g., negation) by pruning the dependency trees too aggressively, or are computationally inefficient because it is difficult to parallelize over different tree structures. We propose an extension of graph convolutional networks that is tailored for relation extraction, which pools information over arbitrary dependency structures efficiently in parallel. To incorporate relevant information while maximally removing irrelevant content, we further apply a novel pruning strategy to the input trees by keeping words immediately around the shortest path between the two entities among which a relation might hold. The resulting model achieves state-of-the-art performance on the large-scale TACRED dataset, outperforming existing sequence and dependency-based neural models. We also show through detailed analysis that this model has complementary strengths to sequence models, and combining them further improves the state of the art.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction involves discerning whether a relation exists between two entities in a sentence (often termed subject and object, respectively). Successful relation extraction is the cornerstone of applications requiring relational understanding of unstructured text on a large scale, such as ques- tion answering ( <ref type="bibr" target="#b13">Yu et al., 2017)</ref>, knowledge base population ( <ref type="bibr" target="#b16">Zhang et al., 2017)</ref>, and biomedical knowledge discovery <ref type="bibr">(Quirk and Poon, 2017)</ref>.</p><p>Models making use of dependency parses of the input sentences, or dependency-based models, ⇤ Equal contribution. The order of authorship was decided by a tossed coin. A subtree of the original UD de- pendency tree between the subject ("he") and object ("Mike Cane") is also shown, where the shortest depen- dency path between the entities is highlighted in bold.</p><p>Note that negation ("not") is off the dependency path.</p><p>have proven to be very effective in relation ex- traction, because they capture long-range syntac- tic relations that are obscure from the surface form alone (e.g., when long clauses or complex scop- ing are present). Traditional feature-based models are able to represent dependency information by featurizing dependency trees as overlapping paths along the trees <ref type="bibr" target="#b3">(Kambhatla, 2004</ref>). However, these models face the challenge of sparse feature spaces and are brittle to lexical variations. More re- cent neural models address this problem with dis- tributed representations built from their computa- tion graphs formed along parse trees. One com- mon approach to leverage dependency information is to perform bottom-up or top-down computation along the parse tree or the subtree below the low- est common ancestor (LCA) of the entities <ref type="bibr">(Miwa and Bansal, 2016)</ref>. Another popular approach, in- spired by <ref type="bibr" target="#b1">Bunescu and Mooney (2005)</ref>, is to re- duce the parse tree to the shortest dependency path between the entities ( <ref type="bibr">Xu et al., 2015a,b)</ref>. However, these models suffer from several drawbacks. Neural models operating directly on parse trees are usually difficult to parallelize and thus computationally inefficient, because aligning trees for efficient batch training is usually non- trivial. Models based on the shortest dependency path between the subject and object are compu- tationally more efficient, but this simplifying as- sumption has major limitations as well. <ref type="figure">Figure 1</ref> shows a real-world example where crucial infor- mation (i.e., negation) would be excluded when the model is restricted to only considering the de- pendency path.</p><p>In this work, we propose a novel extension of the graph convolutional network <ref type="bibr" target="#b4">(Kipf and Welling, 2017;</ref><ref type="bibr" target="#b9">Marcheggiani and Titov, 2017</ref>) that is tailored for relation extraction. Our model encodes the dependency structure over the input sentence with efficient graph convolution opera- tions, then extracts entity-centric representations to make robust relation predictions. We also ap- ply a novel path-centric pruning technique to re- move irrelevant information from the tree while maximally keeping relevant content, which further improves the performance of several dependency- based models including ours.</p><p>We test our model on the popular SemEval 2010 Task 8 dataset and the more recent, larger TAC- RED dataset. On both datasets, our model not only outperforms existing dependency-based neu- ral models by a significant margin when combined with the new pruning technique, but also achieves a 10-100x speedup over existing tree-based mod- els. On TACRED, our model further achieves the state-of-the-art performance, surpassing a compet- itive neural sequence model baseline. This model also exhibits complementary strengths to sequence models on TACRED, and combining these two model types through simple prediction interpola- tion further improves the state of the art.</p><p>To recap, our main contributions are: (i) we pro- pose a neural model for relation extraction based on graph convolutional networks, which allows it to efficiently pool information over arbitrary de- pendency structures; (ii) we present a new path- centric pruning technique to help dependency- based models maximally remove irrelevant infor- mation without damaging crucial content to im- prove their robustness; (iii) we present detailed analysis on the model and the pruning technique, and show that dependency-based models have complementary strengths with sequence models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Models</head><p>In this section, we first describe graph convo- lutional networks (GCNs) over dependency tree structures, and then we introduce an architecture that uses GCNs at its core for relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Convolutional Networks over Dependency Trees</head><p>The graph convolutional network <ref type="bibr" target="#b4">(Kipf and Welling, 2017</ref>) is an adaptation of the convolu- tional neural network ( <ref type="bibr" target="#b5">LeCun et al., 1998</ref>) for en- coding graphs. Given a graph with n nodes, we can represent the graph structure with an n ⇥ n adjacency matrix A where A ij = 1 if there is an edge going from node i to node j. In an L-layer GCN, if we denote by h (l1) i the input vector and</p><formula xml:id="formula_0">h (l)</formula><p>i the output vector of node i at the l-th layer, a graph convolution operation can be written as</p><formula xml:id="formula_1">h (l) i = n X j=1 A ij W (l) h (l1) j + b (l) ,<label>(1)</label></formula><p>where W (l) is a linear transformation, b (l) a bias term, and a nonlinear function (e.g., ReLU). Intuitively, during each graph convolution, each node gathers and summarizes information from its neighboring nodes in the graph. We adapt the graph convolution operation to model dependency trees by converting each tree into its corresponding adjacency matrix A, where A ij = 1 if there is a dependency edge between to- kens i and j. However, naively applying the graph convolution operation in Equation (1) could lead to node representations with drastically different magnitudes, since the degree of a token varies a lot. This could bias our sentence representation towards favoring high-degree nodes regardless of the information carried in the node (see details in Section 2.2). Furthermore, the information in</p><formula xml:id="formula_2">h (l1) i</formula><p>is never carried over to h (l) i , since nodes never connect to themselves in a dependency tree.</p><p>We resolve these issues by normalizing the acti- vations in the graph convolution before feeding it through the nonlinearity, and adding self-loops to each node in the graph:</p><formula xml:id="formula_3">h (l) i = n X j=1˜A j=1˜ j=1˜A ij W (l) h (l1) j /d i + b (l) ,<label>(2)</label></formula><p>where˜Awhere˜ where˜A = A + I with I being the n ⇥ n identity matrix, and d i = P n j=1˜A j=1˜ j=1˜A ij is the degree of token i in the resulting graph. </p><formula xml:id="formula_4">h sent h s h o h (l1) h (l) h (0) h (L)</formula><p>Figure 2: Relation extraction with a graph convolutional network. The left side shows the overall architecture, while on the right side, we only show the detailed graph convolution computation for the word "relative" for clarity. A full unlabeled dependency parse of the sentence is also provided for reference.</p><p>Stacking this operation over L layers gives us a deep GCN network, where we set h <ref type="formula">(0)</ref> 1 , . . . , h (0) n to be input word vectors, and use h</p><formula xml:id="formula_5">(L) 1 , . . . , h (L)</formula><p>n as output word representations. All operations in this network can be efficiently implemented with ma- trix multiplications, making it ideal for batching computation over examples and running on GPUs. Moreover, the propagation of information between tokens occurs in parallel, and the runtime does not depend on the depth of the dependency tree.</p><p>Note that the GCN model presented above uses the same parameters for all edges in the depen- dency graph. We also experimented with: (1) us- ing different transformation matrices W for top- down, bottom-up, and self-loop edges; and (2) adding dependency relation-specific parameters for edge-wise gating, similar to <ref type="bibr" target="#b9">(Marcheggiani and Titov, 2017)</ref>. We found that modeling directions does not lead to improvement, 1 and adding edge- wise gating further hurts performance. We hypoth- esize that this is because the presented GCN model is usually already able to capture dependency edge patterns that are informative for classifying rela- tions, and modeling edge directions and types does not offer additional discriminative power to the network before it leads to overfitting. For exam- ple, the relations entailed by "A's son, B" and "B's son, A" can be readily distinguished with "'s" at- tached to different entities, even when edge direc- tionality is not considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Encoding Relations with GCN</head><p>We now formally define the task of relation ex- traction. Let X = [x 1 , ..., x n ] denote a sentence, where x i is the i th token. A subject entity and an object entity are identified and correspond to two spans in the sentence:</p><formula xml:id="formula_6">X s = [x s 1 , . . . , x s 2 ] and X o = [x o 1 , . . . , x o 2 ]</formula><p>. Given X , X s , and X o , the goal of relation extraction is to predict a relation r 2 R (a predefined relation set) that holds be- tween the entities or "no relation" otherwise.</p><p>After applying an L-layer GCN over word vec- tors, we obtain hidden representations of each to- ken that are directly influenced by its neighbors no more than L edges apart in the dependency tree. To make use of these word representations for re- lation extraction, we first obtain a sentence repre- sentation as follows (see also <ref type="figure">Figure 2</ref> left):</p><formula xml:id="formula_7">h sent = f h (L) = f GCN(h (0) ) ,<label>(3)</label></formula><p>where h (l) denotes the collective hidden represen- tations at layer l of the GCN, and f :</p><formula xml:id="formula_8">R d⇥n ! R d</formula><p>is a max pooling function that maps from n output vectors to the sentence vector. We also observe that information close to entity tokens in the dependency tree is often central to relation classification. Therefore, we also obtain a subject representation h s from h (L) as follows</p><formula xml:id="formula_9">h s = f h (L) s 1 :s 2 ,<label>(4)</label></formula><p>as well as an object representation h o similarly. Inspired by recent work on relational learning between entities ( <ref type="bibr">Santoro et al., 2017;</ref><ref type="bibr" target="#b6">Lee et al., 2017)</ref>, we obtain the final representation used for classification by concatenating the sentence and the entity representations, and feeding them through a feed-forward neural network (FFNN):</p><formula xml:id="formula_10">h final = FFNN [h sent ; h s ; h o ] .<label>(5)</label></formula><p>This h final representation is then fed into a linear layer followed by a softmax operation to obtain a probability distribution over relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Contextualized GCN</head><p>The network architecture introduced so far learns effective representations for relation extraction, but it also leaves a few issues inadequately ad- dressed. First, the input word vectors do not con- tain contextual information about word order or disambiguation. Second, the GCN highly depends on a correct parse tree to extract crucial informa- tion from the sentence (especially when pruning is performed), while existing parsing algorithms produce imperfect trees in many cases.</p><p>To resolve these issues, we further apply a Con- textualized GCN (C-GCN) model, where the input word vectors are first fed into a bi-directional long short-term memory (LSTM) network to gener- ate contextualized representations, which are then used as h (0) in the original model. This BiL- STM contextualization layer is trained jointly with the rest of the network. We show empirically in Section 5 that this augmentation substantially im- proves the performance over the original model. We note that this relation extraction model is conceptually similar to graph kernel-based mod- els ( <ref type="bibr" target="#b14">Zelenko et al., 2003)</ref>, in that it aims to utilize local dependency tree patterns to inform relation classification. Our model also incorporates crucial off-path information, which greatly improves its robustness compared to shortest dependency path- based approaches. Compared to tree-structured models (e.g., Tree-LSTM ( <ref type="bibr">Tai et al., 2015)</ref>), it not only is able to capture more global informa- tion through the use of pooling functions, but also achieves substantial speedup by not requiring re- cursive operations that are difficult to parallelize. For example, we observe that on a Titan Xp GPU, training a Tree-LSTM model over a minibatch of 50 examples takes 6.54 seconds on average, while training the original GCN model takes only 0.07 seconds, and the C-GCN model 0.08 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Incorporating Off-path Information with Path-centric Pruning</head><p>Dependency trees provide rich structures that one can exploit in relation extraction, but most of the information pertinent to relations is usually con- tained within the subtree rooted at the lowest com- mon ancestor (LCA) of the two entities. Previous studies ( <ref type="bibr" target="#b12">Xu et al., 2015b;</ref><ref type="bibr">Miwa and Bansal, 2016)</ref> have shown that removing tokens outside this scope helps relation extraction by eliminating ir- relevant information from the sentence. It is there- fore desirable to combine our GCN models with tree pruning strategies to further improve perfor- mance. However, pruning too aggressively (e.g., keeping only the dependency path) could lead to loss of crucial information and conversely hurt ro- bustness. For instance, the negation in <ref type="figure">Figure 1</ref> is neglected when a model is restricted to only look- ing at the dependency path between the entities. Similarly, in the sentence "She was diagnosed with cancer last year, and succumbed this June", the dependency path She diagnosed!cancer is not sufficient to establish that cancer is the cause of death for the subject unless the conjunction depen- dency to succumbed is also present. Motivated by these observations, we propose path-centric pruning, a novel technique to incor- porate information off the dependency path. This is achieved by including tokens that are up to dis- tance K away from the dependency path in the LCA subtree. K = 0, corresponds to pruning the tree down to the path, K = 1 keeps all nodes that are directly attached to the path, and K = 1 retains the entire LCA subtree. We combine this pruning strategy with our GCN model, by directly feeding the pruned trees into the graph convolu- tional layers. <ref type="bibr">2</ref> We show that pruning with K = 1 achieves the best balance between including rele- vant information (e.g., negation and conjunction) and keeping irrelevant content out of the resulting pruned tree as much as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>At the core of fully-supervised and distantly- supervised relation extraction approaches are sta- tistical classifiers, many of which find syntac- tic information beneficial. For example, <ref type="bibr" target="#b10">Mintz et al. (2009)</ref> explored adding syntactic features to a statistical classifier and found them to be use- ful when sentences are long. Various kernel-based approaches also leverage syntactic information to measure similarity between training and test ex- amples to predict the relation, finding that tree-based kernels ( <ref type="bibr" target="#b14">Zelenko et al., 2003)</ref> and depen- dency path-based kernels ( <ref type="bibr" target="#b1">Bunescu and Mooney, 2005</ref>) are effective for this task.</p><p>Recent studies have found neural models ef- fective in relation extraction. <ref type="bibr" target="#b15">Zeng et al. (2014)</ref> first applied a one-dimensional convolutional neu- ral network (CNN) with manual features to encode relations. <ref type="bibr">Vu et al. (2016)</ref> showed that combin- ing a CNN with a recurrent neural network (RNN) through a voting scheme can further improve per- formance. <ref type="bibr" target="#b17">Zhou et al. (2016)</ref> and <ref type="bibr">Wang et al. (2016)</ref> proposed to use attention mechanisms over RNN and CNN architectures for this task.</p><p>Apart from neural models over word sequences, incorporating dependency trees into neural models has also been shown to improve relation extrac- tion performance by capturing long-distance rela- tions. <ref type="bibr" target="#b12">Xu et al. (2015b)</ref> generalized the idea of de- pendency path kernels by applying a LSTM net- work over the shortest dependency path between entities. <ref type="bibr" target="#b7">Liu et al. (2015)</ref> first applied a recur- sive network over the subtrees rooted at the words on the dependency path and then applied a CNN over the path. Miwa and Bansal (2016) applied a Tree-LSTM ( <ref type="bibr">Tai et al., 2015</ref>), a generalized form of LSTM over dependency trees, in a joint entity and relation extraction setting. They found it to be most effective when applied to the subtree rooted at the LCA of the two entities.</p><p>More recently, <ref type="bibr" target="#b0">Adel et al. (2016)</ref> and <ref type="bibr" target="#b16">Zhang et al. (2017)</ref> have shown that relatively simple neural models (CNN and augmented LSTM, re- spectively) can achieve comparable or superior performance to dependency-based models when trained on larger datasets. In this paper, we study dependency-based models in depth and show that with a properly designed architecture, they can outperform and have complementary advantages to sequence models, even in a large-scale setting.</p><p>Finally, we note that a technique similar to path- centric pruning has been applied to reduce the space of possible arguments in semantic role la- beling ( <ref type="bibr" target="#b2">He et al., 2018)</ref>. The authors showed prun- ing words too far away from the path between the predicate and the root to be beneficial, but reported the best pruning distance to be 10, which almost always retains the entire tree. Our method differs in that it is applied to the shortest dependency path between entities, and we show that in our tech- nique the best pruning distance is 1 for several dependency-based relation extraction models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baseline Models</head><p>We compare our models with several competitive dependency-based and neural sequence models.</p><p>Dependency-based models. In our main ex- periments we compare with three types of dependency-based models. (1) A logistic regres- sion (LR) classifier which combines dependency- based features with other lexical features. (2) Shortest Dependency Path LSTM (SDP-LSTM) ( <ref type="bibr" target="#b12">Xu et al., 2015b</ref>), which applies a neural sequence model on the shortest path between the subject and object entities in the dependency tree. <ref type="formula" target="#formula_7">(3)</ref> Tree-LSTM ( <ref type="bibr">Tai et al., 2015)</ref>, which is a recursive model that generalizes the LSTM to arbitrary tree structures. We investigate the child-sum variant of Tree-LSTM, and apply it to the dependency tree (or part of it). In practice, we find that modifying this model by concatenating dependency label em- beddings to the input of forget gates improves its performance on relation extraction, and therefore use this variant in our experiments. Earlier, our group compared (1) and (2) with sequence models ( <ref type="bibr" target="#b16">Zhang et al., 2017)</ref>, and we report these results; for (3) we report results with our own implemen- tation.</p><p>Neural sequence model. Our group presented a competitive sequence model that employs a position-aware attention mechanism over LSTM outputs (PA-LSTM), and showed that it outper- forms several CNN and dependency-based models by a substantial margin ( <ref type="bibr" target="#b16">Zhang et al., 2017</ref>). We compare with this strong baseline, and use its open implementation in further analysis. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>We conduct experiments on two relation extrac- tion datasets: (1) TACRED: Introduced in ( <ref type="bibr" target="#b16">Zhang et al., 2017)</ref>, TACRED contains over 106k men- tion pairs drawn from the yearly TAC KBP 4 chal- lenge. It represents 41 relation types and a spe- cial no relation class when the mention pair does not have a relation between them within these cat- egories. Mentions in TACRED are typed, with subjects categorized into person and organization, and objects into 16 fine-grained types (e.g., date and location). We report micro-averaged F 1 scores on this dataset as is conventional. <ref type="formula" target="#formula_3">(2</ref>  <ref type="bibr" target="#b16">Zhang et al., 2017</ref>) by selecting the model with the me- dian dev F 1 from 5 independent runs and report- ing its test F 1 . We also use the same "entity mask" strategy where we replace each subject (and ob- ject similarly) entity with a special SUBJ-&lt;NER&gt; token. For all models, we also adopt the "multi- channel" strategy by concatenating the input word embeddings with POS and NER embeddings.</p><p>Traditionally, evaluation on SemEval is con- ducted without entity mentions masked. However, as we will discuss in Section 6.4, this method en- courages models to overfit to these mentions and fails to test their actual ability to generalize. We therefore report results with two evaluation proto- cols: (1) with-mention, where mentions are kept for comparison with previous work; and (2) mask- mention, where they are masked to test the gener- alization of our model in a more realistic setting.</p><p>Due to space limitations, we report model train- ing details in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results on the TACRED Dataset</head><p>We present our main results on the TACRED test set in <ref type="table">Table 1</ref>. We observe that our GCN model System with-m mask-m</p><formula xml:id="formula_11">SVM † (Rink+2010) 82.2 - SDP-LSTM † (Xu+2015b) 83.7 - SPTree † (Miwa+2016)</formula><p>84.4 - PA-LSTM ‡ (Zhang+2017) 82. <ref type="bibr">7</ref> 75.3</p><p>Our Model (C-GCN) 84.8 ⇤ 76.5 ⇤ outperforms all dependency-based models by at least 1.6 F 1 . By using contextualized word rep- resentations, the C-GCN model further outper- forms the strong PA-LSTM model by 1.3 F 1 , and achieves a new state of the art. In addition, we find our model improves upon other dependency- based models in both precision and recall. Com- paring the C-GCN model with the GCN model, we find that the gain mainly comes from improved recall. We hypothesize that this is because the C- GCN is more robust to parse errors by capturing local word patterns (see also Section 6.2).</p><p>As we will show in Section 6.2, we find that our GCN models have complementary strengths when compared to the PA-LSTM. To leverage this result, we experiment with a simple interpolation strategy to combine these models. Given the out- put probabilities P G (r|x) from a GCN model and P S (r|x) from the sequence model for any relation r, we calculate the interpolated probability as</p><formula xml:id="formula_12">P (r|x) = ↵ · P G (r|x) + (1 ↵) · P S (r|x)</formula><p>where ↵ 2 [0, 1] is chosen on the dev set and set to 0.6. This simple interpolation between a GCN and a PA-LSTM achieves an F 1 score of 67.1, outper- forming each model alone by at least 2.0 F 1 . An interpolation between a C-GCN and a PA-LSTM further improves the result to 68.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results on the SemEval Dataset</head><p>To study the generalizability of our proposed model, we also trained and evaluated our best C- GCN model on the SemEval test set ( <ref type="table" target="#tab_2">Table 2</ref>). We find that under the conventional with-entity eval- uation, our C-GCN model outperforms all exist- ing dependency-based neural models on this sep- C-GCN GCN Tree-LSTM <ref type="figure">Figure 3</ref>: Performance of dependency-based models under different pruning strategies. For each model we show the F 1 score on the TACRED dev set averaged over 5 runs, and error bars indicate standard deviation of the mean estimate. K = 1 is equivalent to using the subtree rooted at the LCA.</p><p>arate dataset. Notably, by properly incorporating off-path information, our model outperforms the previous shortest dependency path-based model (SDP-LSTM). Under the mask-entity evaluation, our C-GCN model also outperforms PA-LSTM by a substantial margin, suggesting its generalizabil- ity even when entities are not seen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Effect of Path-centric Pruning</head><p>To show the effectiveness of path-centric prun- ing, we compare the two GCN models and the Tree-LSTM when the pruning distance K is var- ied. We experimented with K 2 {0, 1, 2, 1} on the TACRED dev set, and also include results when the full tree is used. As shown in <ref type="figure">Figure 3</ref>, the performance of all three models peaks when K = 1, outperforming their respective depen- dency path-based counterpart (K = 0). This con- firms our hypothesis in Section 3 that incorporat- ing off-path information is crucial to relation ex- traction. <ref type="bibr">Miwa and Bansal (2016)</ref> reported that a Tree-LSTM achieves similar performance when the dependency path and the LCA subtree are used respectively. Our experiments confirm this, and further show that the result can be improved by path-centric pruning with K = 1.</p><p>We find that all three models are less effective when the entire dependency tree is present, indi- cating that including extra information hurts per- formance. Finally, we note that contextualizing the GCN makes it less sensitive to changes in the tree structures provided, presumably because the C-GCN GCN PA-LSTM <ref type="figure">Figure 4</ref>: Dev set performance with regard to distance between the entities in the sentence for C-GCN, GCN and PA-LSTM. Error bars indicate standard deviation of the mean estimate over 5 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Dev  model can use word sequence information in the LSTM layer to recover any off-path information that it needs for correct relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis &amp; Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Ablation Study</head><p>To study the contribution of each component in the C-GCN model, we ran an ablation study on the TACRED dev set ( <ref type="table" target="#tab_4">Table 3</ref>). We find that: <ref type="formula" target="#formula_1">(1)</ref> The entity representations and feedforward layers contribute 1.0 F 1 . <ref type="formula" target="#formula_3">(2)</ref> When we remove the de- pendency structure (i.e., setting˜Asetting˜ setting˜A to I), the score drops by 3.2 F 1 . (3) F 1 drops by 10.3 when we remove the feedforward layers, the LSTM compo- nent and the dependency structure altogether. <ref type="formula" target="#formula_9">(4)</ref> Removing the pruning (i.e., using full trees as in- put) further hurts the result by another 9.7 F 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Complementary Strengths of GCNs and PA-LSTMs</head><p>To understand what the GCN models are capturing and how they differ from a sequence model such as the PA-LSTM, we compared their performance Benoit B. Mandelbrot, a maverick mathematician who developed an innovative theory of roughness and applied it to physics, biology, finance and many other fields, died Thursday in Cambridge, Mass.</p><p>Anil Kumar, a former director at the consulting firm McKinsey &amp; Co, pleaded guilty on Thursday to providing inside information to Raj Rajaratnam, the founder of the Galleon Group, in exchange for payments of at least $ 175 million from 2004 through 2009. In a career that spanned seven decades, Ginzburg authored several groundbreaking studies in various fields --such as quantum theory, astrophysics, radio-astronomy and diffusion of cosmic radiation in the Earth's atmosphere --that were of "Nobel Prize caliber," said Gennady Mesyats, the director of the Lebedev Physics Institute in Moscow, where Ginzburg worked .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Institute</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation</head><p>Dependency Tree Edges <ref type="table">Table 4</ref>: The three dependency edges that contribute the most to the classification of different relations in the TACRED dev set. For clarity, we removed edges which 1) connect to common punctuation (i.e., commas, periods, and quotation marks), 2) connect to common prepositions (i.e., of, to, by), and 3) connect between tokens within the same entity. We use PER, ORG for entity types of PERSON, ORGANIZATION. We use S-and O-to denote subject and object entities, respectively. We also include edges for more relations in the supplementary material.</p><formula xml:id="formula_13">per:children S-PER son son ! O-PER S-PER survived per:other family S-PER stepson niece ! O-PER O-PER stepdaughter per:employee of a member S-PER worked S-PER played per:schools attended S-PER graduated S-PER earned S-PER attended org:founded founded ! O-DATE established ! O-DATE was founded org:number of employees S-ORG has S-ORG ! employs O-NUMBER employees org:subsidiaries S-ORG O-ORG S-ORG ! 's O-ORG ! division org:shareholders buffett O-PER shareholder ! S-ORG largest shareholder</formula><p>over examples in the TACRED dev set. Specifi- cally, for each model, we trained it for 5 indepen- dent runs with different seeds, and for each exam- ple we evaluated the model's accuracy over these 5 runs. For instance, if a model correctly classifies an example for 3 out of 5 times, it achieves an ac- curacy of 60% on this example. We observe that on 847 (3.7%) dev examples, our C-GCN model achieves an accuracy at least 60% higher than that of the PA-LSTM, while on 629 (2.8%) examples the PA-LSTM achieves 60% higher. This comple- mentary performance explains the gain we see in <ref type="table">Table 1</ref> when the two models are combined.</p><p>We further show that this difference is due to each model's competitive advantage <ref type="figure">(Figure 4</ref>): dependency-based models are better at handling sentences with entities farther apart, while se- quence models can better leverage local word pat- terns regardless of parsing quality (see also <ref type="figure">Fig- ure 6)</ref>. We include further analysis in the supple- mentary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Understanding Model Behavior</head><p>To gain more insights into the C-GCN model's be- havior, we visualized the partial dependency tree it is processing and how much each token's final representation contributed to h sent ( <ref type="figure" target="#fig_3">Figure 5</ref>). We find that the model often focuses on the depen- dency path, but sometimes also incorporates off- path information to help reinforce its prediction. The model also learns to ignore determiners (e.g., "the") as they rarely affect relation prediction.</p><p>To further understand what dependency edges contribute most to the classification of different re- lations, we scored each dependency edge by sum- ming up the number of dimensions each of its con- nected nodes contributed to h sent . We present the top scoring edges in <ref type="table">Table 4</ref>. As can be seen in the table, most of these edges are associated with indicative nouns or verbs of each relation. <ref type="bibr">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Entity Bias in the SemEval Dataset</head><p>In our study, we observed a high correlation be- tween the entity mentions in a sentence and its relation label in the SemEval dataset. Bashardost was born in 1965 in the southern Ghanzi province and his family migrated to Iran and then to Pakistan after successive coup and factional fighting in Afghanistan . <ref type="figure">Figure 6</ref>: Dev set examples where either the C-GCN (upper) or the PA-LSTM (lower) predicted correctly in five independent runs. For each example, the predicted and pruned dependency tree corresponding to K = 1 in path-centric pruning is shown, and the shortest dependency path is thickened. We omit edges to punctuation for clarity. The first example shows that the C-GCN is effective at leveraging long-range dependencies while reducing noise with the help of pruning (while the PA-LSTM predicts no relation twice, org:alternate names twice, and org:parents once in this case). The second example shows that the PA-LSTM is better at leveraging the proximity of the word "migrated"</p><p>regardless of attachment errors in the parse (while the C-GCN is misled to predict per:country of birth three times, and no relation twice).</p><p>phenomenon. <ref type="bibr">6</ref> We started by simplifying every sentence in the SemEval training and dev sets to "subject and object", where subject and object are the actual entities in the sentence. Surprisingly, a trained PA-LSTM model on this data is able to achieve 65.1 F 1 on the dev set if GloVe is used to initialize word vectors, and 47.9 dev F 1 even without GloVe initialization. To further evaluate the model in a more realistic setting, we trained one model with the original SemEval training set (unmasked) and one with mentions masked in the training set, following what we have done for TACRED (masked). While the unmasked model achieves a 83.6 F 1 on the original SemEval dev set, F 1 drops drastically to 62.4 if we replace dev set entity mentions with a special &lt;UNK&gt; token to simulate the presence of unseen entities. In con- trast, the masked model is unaffected by unseen entity mentions and achieves a stable dev F 1 of 74.7. This suggests that models trained without entities masked generalize poorly to new examples with unseen entities. Our findings call for more careful evaluation that takes dataset biases into ac- count in future relation extraction studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We showed the success of a neural architecture based on a graph convolutional network for re- lation extraction. We also proposed path-centric pruning to improve the robustness of dependency- based models by removing irrelevant content with- out ignoring crucial information. We showed through detailed analysis that our model has com- plementary strengths to sequence models, and that the proposed pruning technique can be effectively applied to other dependency-based models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>I</head><label></label><figDesc>Figure 1: An example modified from the TAC KBP challenge corpus. A subtree of the original UD dependency tree between the subject ("he") and object ("Mike Cane") is also shown, where the shortest dependency path between the entities is highlighted in bold. Note that negation ("not") is off the dependency path.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples and the pruned dependency trees where the C-GCN predicted correctly. Words are shaded by the number of dimensions they contributed to h sent in the pooling operation, with punctuation omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>We exper- imented with PA-LSTM models to analyze this ALBA -the Bolivarian Alternative for the Americas -was founded by Venezuelan President Hugo Chavez and Cuban leader Fidel Castro in 2004 and also includes Bolivia , Nicaragua and the Caribbean island of Dominica .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>F 1 scores on SemEval.  † marks results re-
ported in the original papers;  ‡ marks results pro-
duced by using the open implementation. The last two 
columns show results from with-mention evaluation 
and mask-mention evaluation, respectively. ⇤ marks 
statistically significant improvements over PA-LSTM 
with p &lt; .05 under a bootstrap test. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>An ablation study of the best C-GCN model. 
Scores are median of 5 models. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>died Relation: org:founded_by Relation: per:city_of_death Rajaratnam Raj to founder</head><label></label><figDesc></figDesc><table>Group 
the 

of 
the 
Galleon 

Thursday Cambridge 

in 
Mass 

Mandelbrot 

Benoit B. 

</table></figure>

			<note place="foot" n="1"> We therefore treat the dependency graph as undirected, i.e. 8i, j, Aij = Aji.</note>

			<note place="foot" n="2"> For our C-GCN model, the LSTM layer still operates on the full sentence regardless of the pruning.</note>

			<note place="foot" n="3"> https://github.com/yuhaozhang/tacred-relation 4 https://tac.nist.gov/2017/KBP/index.html</note>

			<note place="foot" n="5"> We do notice the effect of dataset bias as well: the name &quot;Buffett&quot; is too often associated with contexts where shareholder relations hold, and therefore ranks top in that relation.</note>

			<note place="foot" n="6"> We choose the PA-LSTM model because it is more amenable to our experiments with simplified examples.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Arun Chaganty, Kevin Clark, Sebastian Schuster, Ivan Titov, and the anonymous review-ers for their helpful suggestions. This material is based in part upon work supported by the National Science Foundation under Grant No. IIS-1514268. Any opinions, findings, and conclusions or recom-mendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Comparing convolutional neural networks to traditional models for slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL-HLT)</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A shortest path dependency kernel for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing (EMNLP 2005)</title>
		<meeting>the Conference on Human Language Technology and Empirical Methods in Natural Language Processing (EMNLP 2005)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Syntax for semantic role labeling, to be, or not to be</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuchao</forename><surname>Shexia He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2004 Interactive poster and demonstration sessions</title>
		<meeting>the ACL 2004 Interactive poster and demonstration sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A dependency-based neural network for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Rion Snow, and Dan Jurafsky</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic relation classification via convolutional neural networks with simple negative sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP 2015)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP 2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1785" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improved neural relation detection for knowledge base question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazi</forename><surname>Saidul Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Computational Linguistics (COLING 2014)</title>
		<meeting>the 24th International Conference on Computational Linguistics (COLING 2014)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attentionbased bidirectional long short-term memory networks for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">207</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
