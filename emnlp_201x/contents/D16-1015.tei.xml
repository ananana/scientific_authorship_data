<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Semantic Parsing via Answer Type Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>Santa Barbara</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izzeddin</forename><surname>Gur</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>Santa Barbara</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>Santa Barbara</addrLine>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mudhakar</forename><surname>Srivatsa</surname></persName>
							<email>msrivats@us.ibm.com</email>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>Santa Barbara</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Semantic Parsing via Answer Type Inference</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="149" to="159"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this work, we show the possibility of inferring the answer type before solving a factoid question and leveraging the type information to improve semantic parsing. By replacing the topic entity in a question with its type, we are able to generate an abstract form of the question , whose answer corresponds to the answer type of the original question. A bidirectional LSTM model is built to train over the abstract form of questions and infer their answer types. It is also observed that if we convert a question into a statement form, our LSTM model achieves better accuracy. Using the predicted type information to rerank the logical forms returned by AgendaIL, one of the leading semantic parsers, we are able to improve the F1-score from 49.7% to 52.6% on the WE-BQUESTIONS data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large scale knowledge bases (KB) like Freebase ( <ref type="bibr" target="#b7">Bollacker et al., 2008)</ref>, <ref type="bibr">DBpedia (Auer et al., 2007)</ref>, and YAGO ( <ref type="bibr">Suchanek et al., 2007</ref>) that store the world's factual information in a structured fash- ion have become substantial resources for people to solve questions. KB-based factoid question answer- ing (KB-QA) that attempts to find exact answers to natural language questions has gained much atten- tion recently. KB-QA is a challenging task due to the representation variety between natural language and structural knowledge in KBs.</p><p>As one of the promising KB-QA techniques, se- mantic parsing maps a natural language question into its semantic representation (e.g., logical forms).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranking</head><p>F 1 # Improved Qs AgendaIL 49.7 - w/ Oracle Types@10 57.3 +234 w/ Oracle Types@20 58.7 +282 w/ Oracle Types@50 60.1 +331 w/ Oracle Types@All 60.5 +345 It uses a logical language with predicates closely re- lated to KB schema, and constructs a dictionary that maps relations to KB predicates. The problem then reduces to generating candidate logical forms, rank- ing them, and selecting one to derive the final an- swer.</p><p>In this work, we propose an answer type pre- diction model that can improve the ranking of the candidate logical forms generated by seman- tic parsing. The type of an entity, e.g., person, organization, location, carries very useful information for various down-stream natural lan- guage processing tasks such as co-reference resolu- tion ( <ref type="bibr">Recasens et al., 2013)</ref>, knowledge base popu- lation ( <ref type="bibr" target="#b10">Carlson et al., 2010)</ref>, relation extraction ( <ref type="bibr" target="#b19">Yao et al., 2012)</ref>, and question answering ( <ref type="bibr">Lin et al., 2012)</ref>. Although the potential clues for answer type from the question has been employed in the recent work AgendaIL <ref type="bibr" target="#b5">(Berant and Liang, 2015</ref>) at the lex- ical level, <ref type="table" target="#tab_0">Table 1</ref> suggests that there is yet a large room for further improvement by explicitly enforc-ing answer type. Inspired by this observation, we aim to directly predict the KB type of the answer from the question. In contrast to a small set of pre- defined types as used in previous answer type pre- diction methods (e.g., <ref type="bibr">(Li and Roth, 2002)</ref>), KBs could have thousands of fine-grained types. Take "When did Shaq come into the NBA?" as a running example. We aim to predict the KB type of its an- swer as SportsLeagueDraft. <ref type="bibr">1</ref> The value of typing answers in a fine granularity can be appreciated from two perspectives: (1) Since each entity in a KB like Freebase has a few types, answer type could help prune answer candidates, (2) since each predicate in the KB has a unique type schema, answer type can help rank logical forms.</p><p>The key challenge of using answer types to re- rank logic forms and hence their corresponding an- swers, is that it shall be done before the answer is found. Otherwise, there is no need to further infer its type. Inspired by the observation that the an- swer type of a question is invariant as long as the type of the topic entity (Shaq) remains the same (DraftedAthlete), we define abstract ques- tion as the question where the topic entity mention is replaced by its corresponding KB type. For the aforementioned example, the best candidate abstract question is "When did DraftedAthlete come into the NBA?" and the answer to this question is SportsLeagueDraft. Hence, we can reduce the answer type prediction task to abstract question an- swering.</p><p>The first step in our method is question ab- straction, in which we generate candidate abstract questions based on the context of question and its candidate topic entities. We build a bidirectional LSTM network over the question that recursively computes vector representations for the past and future contexts of an entity mention. Based on these context representations, we predict the right type of the entity mention. Next, in order to bet- ter utilize the syntactic features of the question, we convert the question form into a normal state- ment form by using dependency tree of the ques- tion. For the running example, after perform- ing the conversion, the abstract question becomes "DraftedAthlete come when into the NBA?" <ref type="bibr">1</ref> KB type of answer ("1992 NBA Draft") in the context.</p><p>We then construct a bidirectional LSTM neural net- work over this final representation of the question and predict the type of the answer. Using the in- ferred answer type, we are able to improve the result of AgendaIL ( <ref type="bibr" target="#b5">Berant and Liang, 2015)</ref> on WebQues- tions ( <ref type="bibr" target="#b6">Berant et al., 2013</ref>) from 49.7% to 52.6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>The knowledge base we work with consists of triples in subject-predicate-object form. It can be represented as K = {(e 1 , p, e 2 ) : e 1 , e 2 ∈ E, p ∈ P}, where E denotes the set of entities (e.g., ShaquilleOneal), and P denotes the set of bi- nary predicates (e.g., Drafted). A knowledge base in this format can be visualized as a graph where en- tities are nodes, and predicates are directed edges between entities. Freebase is used in this work as the knowledge base. It has more than 41M entities, 596M facts, and 24K types.</p><p>Types are an integral part of the Freebase schema. Each entity e in Freebase has a set of categories (types) it belongs to, and this information can be obtained by checking the out-going predicates (Type.Object.Type) from e.</p><p>For example, ShaquilleOneal has 20 Freebase types including Person, BasketballPlayer, DraftedAthlete, Celebrity, and FilmActor. For a specific question involving ShaquilleOneal, among these types, only a few will be relevant.</p><p>Each predicate in Freebase is from a subject en- tity to an object entity, and has a type signature. It has a unique expected types for its subject and ob- ject, independent of the individual subject and ob- ject entities themselves. For example, the predicate People.Person.Profession expects its sub- ject to be of Person type and its object to be of Profession type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Question Abstraction</head><p>The type of the topic entity rather than the entity it- self is essential for inferring the answer type, which is invariant as the topic entity changes within the same class. For example, independent of which NBA player (with DraftedAthlete type) is the topic entity of this question "When did Shaq come into the NBA", the type of the answer is always go- ing to be SportsLeagueDraft in Freebase. Pre- dicting this distinct type among the large number of candidate types in Freebase is a challenging task. We propose a two-step solution for this problem. In the first step, we compute a confidence score for each possible KB type for a given topic entity us- ing a bidirectional LSTM network. The second step prunes candidate types using the entity type infor- mation in Freebase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formulation</head><p>Given a natural language question and its topic en- tity mention, question abstraction is to predict types of the mention in the question context. Formally, let q = (x 1 , x 2 , . . . , x L ) denote the question, m be the topic entity mention in q, and T = {t 1 , t 2 , . . . , t K } the set of all types in KB. Given q and m, we com- pute a probability distribution o ∈ R K×1 over T , where o k denotes the likelihood of t k being the cor- rect type of m in q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Scoring Topic Entity Types with LSTM</head><p>Model. We formulate question abstraction as a clas- sification problem. A bidirectional LSTM network is built over q whose output is computed from the nodes that correspond to the words of m. <ref type="figure" target="#fig_0">Fig. 1</ref> il- lustrates the model for the question "When did Shaq come into the NBA?"</p><p>Let u(x) ∈ R D×1 denote the vector space em- bedding of word x. Forward and backward outputs</p><formula xml:id="formula_0">− → h l , ← − h l ∈ R D h ×1 of bidirectional LSTM are recur- sively computed by − → h l , − → c l = LST M (u(x l ), − → h l−1 , − → c l−1 ) (1) ← − h l , ← − c l = LST M (u(x l ), ← − h l+1 , ← − c l+1 ) (2)</formula><p>as described in <ref type="bibr">Graves (2012)</ref>, where</p><formula xml:id="formula_1">− → c l , ← − c l ∈ R D h ×1 stand for LSTM cell states.</formula><p>To encode the context of m to the final output, we apply an AVERAGE pooling layer when computing the output. For each output node r ∈ [i, j] (i and j correspond to the starting and ending indices of m in q), we compute final forward and backward outputs by</p><formula xml:id="formula_2">− → v r = AV G( − → h 1 , . . . , − → h r ) (3) ← − v r = AV G( ← − h r , . . . , ← − h n ),<label>(4)</label></formula><p>where AV G stands for average pooling.</p><p>We take the average of outputs at each output node</p><formula xml:id="formula_3">− → v = AV G( − → v i , . . . , − → v j ) (5) ← − v = AV G( ← − v i , . . . , ← − v j )<label>(6)</label></formula><p>as the forward and backward outputs of the whole network. The final representation v of the network is obtained by concatenating − → v and ← − v .</p><p>For question q, the probability distribution o over types is computed by</p><formula xml:id="formula_4">s(q) = W hy v (7) o(q) = sof tmax(s(q)),<label>(8)</label></formula><p>where W hy ∈ R K×(2D h ) since v is the concatena- tion of two vectors of dimension D h , where D h is the hidden vector dimension.</p><p>Objective Function and Learning. Given an input question q with a topic entity mention m, LSTM network computes the probability distribu- tion o(q) ∈ R K×1 as in <ref type="bibr">(8)</ref>. Let y(q) ∈ R K×1 de- note the true target distribution over T for q, where y k (q) = 1/n if t k is a correct type, y k (q) = 0 oth- erwise, and n is the number of correct types. We use the cross-entropy loss function between y(q) and o(q), and define the objective function over all train- ing data as</p><formula xml:id="formula_5">J(θ) = − q K k=1 y k (q) log o k (q) + λ 2 θ 2 ,</formula><p>where λ denotes the regularization parameter, and θ represents the set of all model parameters to be learned. We use stochastic gradient descent with RMSProp (Tieleman and Hinton, 2012) for mini- mizing the objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pruning</head><p>Let T e represent the set of KB types for entity e. We define the set of candidate types for entity mention m as</p><formula xml:id="formula_6">C m = e T e ,</formula><p>where e is a possible match of m in KB. We only need to score the types in C m . Once the hidden rep- resentation v is computed by LSTM, we use subma- trix W hy <ref type="bibr">[C m</ref> ] that consists of rows of W hy corre- sponding to the types in C m as the scoring matrix in (7). This returns the final scores for candidate types in C m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conversion to Statement Form</head><p>The objective of the conversion is to canonicalize question form into declarative statement (subject- relation-object) form. We use a simple pattern-based method that relies on dependency tree 2 ( <ref type="bibr">Manning et al., 2014</ref>). It decides whether the sub-trees of the root need reordering based on their dependency re- lations 3 . Before obtaining the dependency tree, we retrieve named entity (NER) tags of the question tokens. We replace a group of question tokens corresponding a named entity with a special token, ENTITY, to sim- plify the parse tree. In <ref type="figure" target="#fig_1">Figure 2</ref>, the question is first transformed to "what boarding school did ENTITY go to?" Each question is represented by the root's <ref type="bibr">2</ref> We use Stanford CoreNLP dependency parser 3 http://universaldependencies.org   dependency relations to its sub-trees in the original order, e.g., (dep, aux, nsubj, nmod). We clus- ter all these sequences and detect the patterns that appear at least 5 times in the training data. These patterns are then manually mapped to their corre- sponding conversion (pattern vs. mapping in <ref type="figure" target="#fig_1">Figure  2</ref>). Once the recomposition order of the sub-trees is determined by the conversion mapping, we finalize the reordering of the question tokens by keeping the order of words within the sub-trees same as the orig- inal order in the question. The example in <ref type="figure" target="#fig_1">Figure  2</ref> becomes "ENTITY go to what boarding school" with its corresponding sub-tree conversion mapping (nsubj, root, nmod, dep). If no mapping is cre- ated for a pattern, we keep the order of the words exactly as they occur in the original question form.</p><p>The motivation behind conversion is to overcome the potential semantic confusion stemming from varities in syntactic structures. To exemplify, con- sider two hypothetical questions "who plays X in Y?" and "who does Z play in Y?", where X is a FilmCharacter, Y is a Film, and Z is a FilmActor, with answer types FilmActor and FilmCharacter, respectively. With conversion, we aim to transform second question into "Z play who in Y", while leaving the first one as it is. Not- ing that the order of words affects the output of our answer type inference network, our intuition is to let the model distinguish better between such questions using their syntactic structure in this way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Answer Type Prediction</head><p>Given a reordered question with topic entity mention m, and a topic entity type t e ∈ T , our task is to predict a probability distribution o ∈ R K×1 over the answer types.</p><p>A topic entity type t e ∈ T is described as a set of words, {x i }. Let u(x i ) ∈ R D×1 represent the vector space embedding of x i , the representation of t e is computed by the average encoding,</p><formula xml:id="formula_7">u(t e ) = 1 |{x i }| x i u(x i ).<label>(9)</label></formula><p>As the first step, we replace the words of entity mention m with topic entity type t e , and obtain a new input word sequence r. t e is treated as one word and encoded by Eq. 9. We construct a bidi- rectional LSTM network over this input sequence r, whose output node corresponds to the question word. The output of the network is a probability dis- tribution over types denoting the likelihood of being the answer type. <ref type="figure" target="#fig_2">Figure 3</ref> shows how the network is constructed for the running example. The same average pooling described in Section 3.2 is applied to obtain the final forward and backward output vec- tors − → v and ← − v from the output node (this time, sin- gle output node) of network. The final output vec- tor v for prediction is obtained by concatenating − → v , and ← − v . The distribution o is computed by a stan- dard softmax layer. The learning is performed by the same cross-entropy loss and objective function described in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Reranking by Answer Type</head><p>In this section, we describe how to rerank logical forms based on our answer type prediction model.</p><formula xml:id="formula_8">Reranking Model. Let l 1 , l 2 , .</formula><p>. . , l N be the log- ical forms generated for question q by a semantic parser, e.g., AgendaIL. Each logical form has a score from the semantic parser. Meanwhile, our answer type prediction model generates a score for the an- swer type of each logical form. Therefore, we can represent each logical form l i using a pair of scores: the score from semantic parser and the score from our type prediction model. Suppose we know which logical forms are "correct", using the two scores as input, we train a logistic regression model with cross-entropy loss to learn a binary classifier for pre- dicting the correct logical forms. We rerank the top- k logical forms using their probability computed by the trained logistic regression model, and select the one with the highest probability. Finally, we run the selected logical form against KB to retrieve the an- swer. We select the optimal value of k from [1, N ] using the training data. For AgendaIL on WebQues- tions, we find that k = 80 gives the best result.</p><p>Training Data Selection. We now discuss which logical forms are "correct", i.e., how to select the positive examples to train the logistic regression model. Because a question can have more than one answer, we use the F 1 score, the harmonic mean of precision and recall, to evaluate logical forms. We select all the logical forms with F 1 &gt; 0 as the set of positive examples. However, taking all the log- ical forms with F 1 = 0 as negative examples will not work well. Even though the F 1 score of a log- ical form is 0, its answer type could still be correct. Therefore, we use the following trick: If there is a positive example with answer type t, we do not treat any other logical form with answer type t as neg- ative example. The logical forms having F 1 = 0, with the aforementioned exception, are then selected as the final set of negative examples. Our empirical study shows this trick works well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>In this section, we describe the datasets, model train- ing, and experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Dataset and Evaluation Metrics</head><p>Datasets. To evaluate our method, we use the We- bQuestions dataset <ref type="bibr" target="#b6">(Berant et al., 2013</ref>), which con- tains 5,810 questions crawled via Google Suggest API. The answers to these questions are annotated from Freebase using Amazon Mechanical Turk. The data is split into training and test sets of size 3,778 and 2,032 questions, respectively. This dataset has been popularly used in question answering and se- mantic parsing.</p><p>The SimpleQuestions (Bordes et al., 2015) con- tains 108,442 questions written in natural language by English-speaking human annotators. This dataset is a collection of question/Freebase-fact pairs rather than question/answer pairs. The data 4 is split and provided as training(75,910), test <ref type="bibr">(21,</ref><ref type="bibr">687)</ref>, and val- idation(10,845) sets. Each question is mapped to the subject, relation, and object of the corresponding Freebase fact. This dataset is only used for training the question abstraction model. Training Data Preparation. Since WebQues- tions only provides question-answer pairs along with annotated topic entities, we need to figure out the type information, which can be used as training data. We obtain simulated types as follows: We retrieve 1- hop and 2-hop predicates r from/to annotated topic entity e in Freebase. For each relation r, we query (e, r, ?) and (?, r, e) against Freebase and retrieve the candidate answers r a . The F 1 value of each candidate answer r a is computed with respect to the annotated answer. The subject and object types of the relation r with the highest F 1 value is selected as the simulated type for the topic entity and the an- swer. When there are multiple such relations, we obtain multiple simulated types for topic entity and answer, one from each relation. We treat each of them as correct with equal probability.</p><p>Candidate Logical Forms for Evaluation. To obtain candidate logical forms, we train AgendaIL ( <ref type="bibr" target="#b5">Berant and Liang, 2015)</ref> on WebQuestions with beam size 200 using the publicly available code 5 by the authors.</p><p>Evaluation Metric. We report average F 1 score of the reranked logical forms using the predicted an- swer types as the main evaluation metric. It is a com- mon performance measure in question answering as questions might have multiple answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Experimental Setup</head><p>We use 50 dimensional word embeddings, which are initialized by the 50 dimensional pre-trained word vectors 6 from GloVe ( <ref type="bibr">Pennington et al., 2014</ref>), and updated in the training process. Hyperparameters are tuned on the development set. The size of the LSTM hidden layer is set at 50. We use RMSProp <ref type="bibr" target="#b14">(Tieleman and Hinton, 2012</ref>) with a learning rate of 0.005 and mini-batch size of 32 for the optimization. We use a dropout layer with probability 0.5 for reg- ularization. We implemented the LSTM networks using Theano <ref type="bibr">(Theano Development Team, 2016)</ref>.</p><p>Identifying Topic Entity. We use Stanford NER tagger ( <ref type="bibr">Manning et al., 2014</ref>) to identify topic en- tity span for both training and test data. For en- tity linking, annotated mention span is mapped to a ranked list of candidate Freebase entities using Free- base Search API for the test data. For the training data, we use the gold Freebase topic entity linkings of each question provided by WebQuestions, com- ing from its question generation process.</p><p>Question Abstraction. We first pre-train the LSTM model described in Section 3.2 on the Sim- pleQuestions dataset. Then, we update the pre- trained model on the training portion of WebQues- tions data where the simulated topic entity types are used as true labels. We use the detected topic en- tity mentions to obtain candidate matching entities in the KB using Freebase Search API. We use top-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F1</head><p>( <ref type="bibr" target="#b6">Berant et al., 2013)</ref> 35.7 ( <ref type="bibr" target="#b18">Yao and Van Durme, 2014)</ref> 33.0 ( <ref type="bibr" target="#b4">Berant and Liang, 2014)</ref> 39.9 ( <ref type="bibr" target="#b2">Bao et al., 2014)</ref> 37.5 ( <ref type="bibr" target="#b8">Bordes et al., 2014)</ref> 39.2 ( <ref type="bibr" target="#b17">Yang et al., 2014)</ref> 41.3 <ref type="figure" target="#fig_0">(Dong et al., 2015b)</ref> 40.8 <ref type="bibr" target="#b20">(Yao, 2015)</ref> 44.3 <ref type="bibr" target="#b5">(Berant and Liang, 2015)</ref> 49.7 ( <ref type="bibr" target="#b21">Yih et al., 2015)</ref> 52.5 ( <ref type="bibr" target="#b15">Reddy et al., 2016)</ref> 50.3 <ref type="figure" target="#fig_0">(Xu et al., 2016)</ref> 53.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">(Yih et al., 2015) (w/ Freebase API) 48.4 (Yih et al., 2015) (w/o ClueWeb) 50.9 (Xu et al., 2016) (w/o Wikipedia) 47.1 Our Approach (w/o SimpleQuestions) 51.6 Our Approach</head><p>52.6 <ref type="table">Table 3</ref>: Comparison of our reranking-by-type system with sev- eral existing works on WebQuestions.</p><p>3 entities returned for the pruning step of Question Abstraction on the test examples. Answer Type Prediction. We train Answer Type Prediction model using the simulated topic entity and answer types for each question. We perform the answer type prediction on test data using the pre- dicted topic entity type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Results</head><p>Our main result is presented in <ref type="table">Table 3</ref>. Our system adds 2.9% absolute improvement over AgendaIL, and achieves 52.6% in F 1 measure. <ref type="bibr" target="#b21">Yih et al. (2015)</ref> achieve 52.5% by leveraging ClueWeb and S-MART ( <ref type="bibr" target="#b16">Yang and Chang, 2015)</ref>, an advanced entity linking system. Xu et al. (2016) achieve 53.3% by lever- aging Wikipedia and S-MART. If tested without Clueweb/Wikipedia/S-MART, their F1 scores are 48.4% and 47.1%, respectively. When our method is tested without using SimpleQuestions data for pre- training question abstraction module, it attains F1 score of 51.6%.</p><p>In <ref type="table" target="#tab_5">Table 4</ref>, we present some question ex- amples where our method can select a bet- ter logical form.</p><p>Take the question "who did <ref type="bibr">[australia]</ref> fight in the first world war?" as an example.</p><p>Our topic entity type pre- diction module returns MilitaryCombatant,  <ref type="table">Table 6</ref>: Ablation analysis of modules of our method.</p><p>Gain/Loss columns denote the number of questions where the F1 score of our selected logical form is greater/less than that of the top ranked logical forms from AgendaIL.</p><p>StatisticalRegion, and Kingdom as the top- 3 results for the type of "australia" in this ques- tion, which indicates that it exploits the context of this short question successfully. The abstract question is "[military combatant] fight who in the first world war?" for which our system returns MilitaryCombatant, MilitaryConflict, and MilitaryCommander as answer types with probabilities 0.73, 0.25, and 0.005, respectively, MilitaryCombatant is indeed the right answer type. This example shows the effect of abstraction in channeling the context in the most relevant direction to find the right answer type. In <ref type="table" target="#tab_6">Table 5</ref>, we provide a comparison of the selected logical forms based on AgendaIL rankings and our rankings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Ablation Analysis</head><p>In this section, we evaluate the effect of individual components of our model. Note that the answer type prediction model described in Section 5 can work independently from question abstraction and form conversion. We develop the following variants i) Base, ii) Base + Conversion, iii) Base + Abstrac- tion, iv) Base + Abstraction + Conversion, where Base corresponds to a model that infers answer types without employing abstraction or form conversion. We train/test each variant separately. <ref type="table">Table 6</ref> shows each component contributes and question abstrac- tion does help boost the performance.</p><p>Suppose we perform answer type prediction with- out question abstraction, and feed " <ref type="bibr">[australia]</ref> fight who in the first world war?" into the answer type prediction model (Base + Conversion). The predicted answer type is Location. Unfortu- nately, there is neither a 1-hop or 2-hop correct re- lation from/to Australia with the expected type Location nor a correct (with positive F 1) candi-   date logical form with the answer type Location. This shows that through question abstraction, a bet- ter logical form is selected for this question.</p><p>To exemplify another benefit of question ab- straction, consider the question "where does [marta] play soccer?" The top 3 entity link- ings via Freebase Search API for "marta" are MetropolitanAtlantaRapidTransit- Authority, Marta, and SantaMarta, where the correct entity is the second one. Our question abstraction system returns FootballPlayer as the top topic entity type prediction that is indeed corresponding to the correct entity. Utilizing the context via question abstraction we are able to recover useful information when the entity linking is uncertain. <ref type="table">Table 6</ref> also shows that the conversion to state- ment form also helps, especially together with Ab- straction. In the above example, the model without Conversion (Base + Abs) predicts the answer type for "where does [football player] play soccer" as SportsFacility, whereas the full model, con- sidering Conversion as well, finds the answer type for "[football player] play soccer where" as SportsTeam which is the better type in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Error Analysis</head><p>We present a further analysis of our approach by classifying the type inference errors made on ran- domly sampled 100 questions. 9% of the er- rors are due to inference at incorrect granular- ity (e.g., City instead of Location). 12% of the errors are the result of incorrect answer labels (hence incorrect answer types) or question ambigu- ity (e.g., "where is dwight howard now?"). 11% of them are incorrect, but acceptable inferences, e.g., BookWrittenWork instead of BookEdition for question "what dawkins book to read first?" 39% of the errors are due to the sparsity problem: They are made on questions whose answer type ap- pears less than 5 times in the training data (e.g., DayOfYear). The remaining 29% of them are due to incorrect question abstraction. In most of the question abstraction errors, the predicted topic en- tity type is semantically close to the correct type. In other cases such as "what did joey jordison play in slipknot?" where we predict FilmActor as the topic entity type while Musician is the correct one. In these cases, the answer type inference is not able to correct the abstraction error. These 29% of errors also contain the entity linking errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>156</head><p>Freebase QA has been studied from two differ- ent perspectives: grounded QA systems that work directly on KBs and general purpose ungrounded QA systems. <ref type="bibr">Kwiatkowski et al. (2013)</ref> generates KB agnostic intermediary CCG parses of questions which are grounded afterwards given a KB. <ref type="bibr" target="#b8">Bordes et al. (2014)</ref> uses a vector space embedding ap- proach to measure the semantic similarity between question and answers. <ref type="bibr" target="#b18">Yao and Van Durme (2014)</ref>, <ref type="bibr" target="#b3">Bast and Haussmann (2015)</ref> and <ref type="bibr" target="#b21">Yih et al. (2015)</ref> exploit a graph centric approach where a grounded subgraph query is generated from question and then executed against a KB. In this work, we propose a neural answer type inference method that can be in- corporated in existing grounded semantic parsers as a complementary feature to improve ranking of the candidate logical forms.</p><p>Berant and Liang (2015) uses lambda DCS logi- cal language with predicates from Freebase. In their approach, types are included as a part of unary lexi- con for building the logical forms from natural lan- guage questions. However, no explicit type infer- ence is exploited. We show that such information could indeed be useful for selecting logical forms.</p><p>There have been a series of studies investigating the expected answer type of a question in different contexts such as <ref type="bibr">Li and Roth (2002)</ref>, <ref type="bibr">Lally et al. (2012)</ref>, and <ref type="bibr" target="#b1">Balog and Neumayer (2012)</ref>. Most of these approaches classify the questions into a small set of types. Even when the set of classes is more fine-grained, e.g., 50 classes in <ref type="bibr">Li and Roth (2002)</ref>, they cannot be used for our purpose as it would re- quire nontrivial mapping between these categories and a much larger number of KB types. Further- more, these methods often rely on a rich set of hand crafted features and external resources. <ref type="bibr">Sun et al. (2015)</ref> uses Freebase types to learn the relevance of candidate answers to a given question via an association model. Their model directly ranks the answer candidates by utilizing types, whereas ours ranks the logical forms via predicting answer type. In this sense, we are able to take advantage of both logical form and type inference. <ref type="bibr">Su et al. (2015)</ref> exploits answer typing to facilitate knowl- edge graph search, but their input is graph query in- stead of natural language question. They predict an- swer types using additional relevance feedback for graph queries, while our algorithm directly infers answer types from input questions. On the question abstraction side, our work is related to a recent study <ref type="bibr" target="#b11">(Dong et al., 2015a</ref>) which classifies entity mentions into 22 types derived from DBpedia. They use a multilayer perceptron over a fixed size window and a recurrent neural network for the representations of context and entity mention, respectively. Instead, we use a bidirectional LSTM network to exploit the full context more flexibly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In this paper, we present a question answer type in- ference framework and leverage it to improve se- mantic parsing. We define the notion of abstract question as the class of questions that can be an- swered by type instead of entity. Question an- swer type inference is then reduced to "question ab- straction" and "abstract question answering", both of which are formulated as classification problems. Question abstraction is performed by exploiting the topic entity and its context in question via an LSTM network . A separate neural network is trained to exploit the abstraction to make the final question an- swer type inference. Our method improves the rank- ing of logical forms returned by AgendaIL on the WEBQUESTIONS dataset. In the future, we would like to investigate how the abstraction and explicit type inference can be incorporated in the early stage of semantic parsing for generating better candidate logical forms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Bi-directional LSTM model for question abstraction. Green circles represent the forward sequence's hidden vectors, while the red circles denote the backward sequence's. shaq (the topic entity mention) is the single output node of the network.</figDesc><graphic url="image-1.png" coords="3,72.00,57.82,226.80,226.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Conversion: red relations form the input pattern</figDesc><graphic url="image-2.png" coords="4,313.20,57.82,226.80,194.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Bi-directional LSTM model over the final representation of the question. Green and red circles are corresponding to forward and backward hidden vectors, respectively. The output node is when.</figDesc><graphic url="image-3.png" coords="5,72.00,57.82,226.81,194.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>What if the correct answer type is enforced? On We-

bQuestions, we remove those with incorrect answer types in the 

top-k logical forms returned by AgendaIL (Berant and Liang, 

2015), a leading semantic parsing system, and report the new 

average F1 score as well as the number of questions with an 

improved F1 score. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Top-5 most common patterns with mappings.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Example questions where our type prediction helps select a better logical form. The F1 gain shows the difference between 

the F1 score of the logical form we select and the top ranked logical form from AgendaIL. 

Questions and Selected Logical Forms 
1. what are some books that mark twain wrote? 
AgendaIL: (MarkTwain -Influence.InfluenceNode.InfluencedBy -?) 
Ours: (MarkTwain -Book.Author.WorksWritten -?) 
2. what guitar does corey taylor play? 
AgendaIL: (? -Organization.Organization.Founders -CoreyTaylor) 
Ours: (CoreyTaylor -Music.GroupMember.InstrumentsPlayed -?) 
3. what type of government does france use? 
AgendaIL: (France -Government.GovernmentalJurisdiction.Government -?) 
Ours: (France -Location.Country.FormOfGovernment -?) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison of selected logical forms for some examples. Logical forms are simplified and canonicalized into (subject -

predicate -object) format for better readability, where ? corresponds to answer nodes. 

</table></figure>

			<note place="foot" n="4"> http://fb.ai/babi.</note>

			<note place="foot" n="5"> https://github.com/percyliang/sempre 6 http://nlp.stanford.edu/projects/glove/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the anonymous reviewers for their valuable comments, and Huan Sun for fruit-ful discussions. This research was sponsored in part by the Army Research Laboratory under cooperative agreements W911NF09-2-0053, NSF IIS 1528175, and NSF CCF 1548848. The views and conclu-sions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and dis-tribute reprints for Government purposes notwith-standing any copyright notice herein. <ref type="bibr">Alex Graves, 2012</ref>. Supervised Sequence Labelling with Recurrent Neural Networks, pages 5-13. Springer Berlin Heidelberg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke</head><p>Zettlemoyer. 2013. Scaling semantic parsers with on-the-fly ontology matching. In Empirical Methods on Natural Language Processing (EMNLP). Adam Lally, John M Prager, Michael C McCord, BK Boguraev, Siddharth Patwardhan, James Fan, Paul Fodor, and Jennifer Chu-Carroll. 2012. Question analysis: How watson reads a clue. IBM Journal of Research and Development. Xin <ref type="bibr">Li and Dan Roth. 2002. Learning question</ref> </p><note type="other">classi-fiers. In International Conference on Computational Linguistics (COLING). Thomas Lin, Mausam, and Oren Etzioni. 2012. No noun phrase left behind: Detecting and typing unlinkable entities. In Empirical Methods on Natural Language Processing (EMNLP).</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J Bethard, and David McClosky. 2014. The stanford corenlp natural language process-ing toolkit. In Annual Meeting of the Association for Computational Linguistics: System Demonstrations. Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word rep-resentation. In Empirical Methods on Natural Lan-guage Processing (EMNLP). Marta Recasens, Marie catherine De Marneffe, and</head><p>Christopher Potts. 2013. The life and death of dis-course entities: Identifying singleton mentions. Con-ference of the North American Chapter of the Associa-tion for Computational Linguistics: Human Language Technologies (NAACL-HLT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Siva Reddy, Oscar Täckström, Michael Collins, Tom</head><p>Kwiatkowski, Dipanjan Das, Mark Steedman, and Mirella Lapata. 2016. Transforming Dependency Structures to Logical Forms for Semantic Parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transactions of the Association for Computational Linguistics (TACL). Yu Su, Shengqi Yang, Huan Sun, Mudhakar Srivatsa, Sue</head><p>Kase, Michelle Vanni, and Xifeng Yan. 2015. Exploit-ing relevance feedback in knowledge graph search. In ACM International Conference on Knowledge Discov-ery and Data Mining (SIGKDD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard</head><p>Weikum. 2007. Yago: A core of semantic knowledge. In World Wide Web (WWW). Huan Sun, Hao Ma, Wen-tau Yih, Chen-Tse Tsai, Jingjing Liu, and Ming-Wei Chang. 2015. Open do-main question answering via semantic enrichment. In World Wide Web (WWW).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DBpedia: A nucleus for a web of open data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference (ISWC)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hierarchical target type identification for entity-oriented queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Neumayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Information and Knowledge Management (CIKM)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Knowledge-based question answering as machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">More accurate question answering on freebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Bast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elmar</forename><surname>Haussmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Information and Knowledge Management (CIKM)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic parsing via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imitation learning of agenda-based semantic parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods on Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD International Conference on Management of Data</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Question answering with subgraph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Large-scale simple question answering with memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Coupled semi-supervised learning for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estevam</forename><forename type="middle">R</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jr</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Web Search and Data mining (WSDM)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A hybrid neural model for type classification of entity mentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Question answering over freebase with multi-column convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
	<note>Theano Development Team</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Lecture 6.5-RMSProp, COURSERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Question answering on freebase via relation extraction and textual evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">S-mart: Novel tree-based structure learning algorithms applied to entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint relational embeddings for knowledged-based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Chul</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haechang</forename><surname>Rim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods on Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Information extraction over structured data: Question answering with freebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised relation discovery with sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lean question answering over freebase from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic parsing via staged query graph generation: Question answering with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingwei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
