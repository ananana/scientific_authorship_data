<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reinforcing the Topic of Embeddings with Theta Pure Dependence for Text Classification *</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xing</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexian</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reinforcing the Topic of Embeddings with Theta Pure Dependence for Text Classification *</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>For sentiment classification, it is often recognized that embedding based on dis-tributional hypothesis is weak in capturing sentiment contrast-contrasting words may have similar local context. Based on broader context, we propose to incorporate Theta Pure Dependence (TPD) into the Paragraph Vector method to reinforce topical and sentimental information. TPD has a theoretical guarantee that the word dependency is pure, i.e., the dependence pattern has the integral meaning whose underlying distribution can not be conditionally factorized. Our method outperforms the state-of-the-art performance on text classification tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word embeddings can be learned by training a neural probabilistic language model or a uni- fied neural network architecture for various NLP tasks ( <ref type="bibr" target="#b1">Bengio et al., 2003;</ref><ref type="bibr" target="#b3">Collobert and Weston, 2008;</ref><ref type="bibr" target="#b4">Collobert et al., 2011</ref>). In global context-aware neural language model <ref type="bibr" target="#b8">(Huang et al., 2012)</ref>, the global context vector is a weighted average of all word embeddings of a single docu- ment/paragraph. After trained with all word em- beddings belonging to the current paragraph, a re- sulting Paragraph Vector can be obtained. Actu- ally, Le and Mikolov's Paragraph Vector ( <ref type="bibr" target="#b10">Le and Mikolov, 2014</ref>) is trained based on the log-linear neural language model ( <ref type="bibr" target="#b13">Mikolov et al., 2013a</ref>).</p><p>For text classification, using a straightforward extension of language model (e.g.</p><p>Le and Mikolov's Paragraph Vector) is considered not to be sensible. Embeddings learned for text classifi- cation should be very different from that learned for language modeling. For example, language * Corresponding authors: Yuexian Hou and Peng Zhang. models often calculate the probability of a sen- tence, therefore this is a good movie and this is a bad movie may not be discriminated from each other. In sentiment analysis task, the semantic representation of words needs to tell word good from bad, even if the two words have the same local context. For this reason, the local depen- dency is insufficient to model topical or sentiment information. Fortunately, if we have the global context of good like interesting or amazing, the sentiment meaning of the embedding will be ex- plicit. However, the training of log-linear neural language model is based on local word dependen- cies (e.g., the co-occurrence of the words in a local window). Thus, Paragraph Vector can not explic- itly model the word dependencies for those words that do not frequently appear in a local window but are actually closely dependent on each other.</p><p>In this paper, our aim is to extend the Paragraph Vector with global context which can capture topi- cal or sentiment information effectively. However, if one explicitly considers the dependency patterns that are beyond the local window level, there is a possibility that the noisy dependency patterns can be involved and modeled in the distributed repre- sentation methods. Moreover, there should be an unique and explicit topical meaning in the patterns to guarantee no ambiguity in the global context. Therefore, we need a dependency mining method that not only models the long range dependency patterns, but also provides a theoretical guarantee that the dependency patterns are pure. Here, the "pure" dependency pattern is an integral seman- tic meaning/concept that cannot be factorized into sub dependency patterns.</p><p>In the language of statistics, Conditional Pure Dependence (CPD) means that the underlying dis- tribution of the dependency patterns cannot be fac- torized under certain conditions (e.g., priors, ob- served words, etc.). It has been proved that CPD is the high-level pure dependence in ( <ref type="bibr" target="#b7">Hou et al., 2013</ref>). However, judging CPD is NP-hard <ref type="bibr" target="#b2">(Chickering et al., 2004</ref>). Fortunately, Theta Pure De- pendence (TPD) is the sufficient criteria of CPD and can be identified in O(N) time, where N is the number of words ( <ref type="bibr" target="#b7">Hou et al., 2013)</ref>. This finding motivates us to adopt TPD as the global context. Moreover, compared with other conventional co- occurrence-based methods, such as the Apriori al- gorithm <ref type="bibr" target="#b0">(Agrawal et al., 1993)</ref>, TPD based on the Information Geometry (IG) framework has a solid theoretical interpretations in statistics to guarantee the dependence is pure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Modeling Topic with TPD</head><p>Compared with local context, global context can usually capture the text topic more precisely. It is easy to get local context by a sliding window. We define the centered word as the current word and the other words in the window as local con- text words. Global context words are extracted from all the documents in the corpus and can be divided into two parts: a) the words in the current document but outside of the local context window; b) the words never appeared in the document but in the corpus. The following example shows the words mentioned above, and the topic (the scene of filming) is easily captured by TPD:</p><p>• TPD: scene camera acting movie Text: there [is great atmosphere in the scene from the location , the] lighting , the fog and such , but the camera should be slowly following the killer. . .</p><p>The bracket stands for the local context window, and the size of window is 5, i.e. there are five lo- cal context words (in italics) in both sides of the current word (in bold). Global context words are underlined in the example. In order to model the topic explicitly, the depen- dence pattern should report one and only one topi- cal meaning. TPD has a theoretical guarantee that the dependency has an integral meaning whose un- derlying distribution can not be conditionally fac- torized. Formally, given a set of binary random variables X = {X 1 , . . . , X n }, where X i denotes the occurrence (X i = 1) or absence (X i = 0) of the i-th word. Then the n-order TPD over X can be defined as follows. DEFINITION 1. (TPD): X = {X 1 , . . . , X n } is of n-order Theta Pure Dependence (TPD), iff the n-order θ coordinate θ 12...n is significantly differ- ent from zero. ( <ref type="bibr" target="#b7">Hou et al., 2013)</ref> TPD can be effectively identified by an explicit statistical test procedure: Log Likelihood Ratio Test (LLRT) ( <ref type="bibr" target="#b16">Nakahara and Amari, 2002</ref>) for θ- coordinate of IG. ( <ref type="bibr" target="#b7">Hou et al., 2013)</ref> Here, we introduce two negative examples to further emphasize the importance of utilizing TPD. Example 1: can, with, of. The joint distri- bution of this words combination can be uncon- ditionally factorized directly, since the occurrence of any word does not necessarily imply the occur- rence of others. Example 2: London, Chelsea, Sherlock Holmes. As we all know, both Chelsea and Sherlock Holmes are closely related to Lon- don. Chelsea and Sherlock Holmes are two rela- tively independent topics, i.e. they are conditional independent given London. Although the three phrases are unconditionally dependent, their joint distribution can be conditionally factorized. Thus the dependency in both two examples can not be pure.</p><p>To explain TPD and the characteristic "pure" intuitively, let us look at a typical example of TPD: climate, conference, Copenhagen. The co- occurrence of the three words implies an un- separable high-level semantic entity compared with the two negative examples, introduced above. In negative examples, the high frequency of words co-occurrence can be explained as some kind of "coincidence", because each of them or their pair- wise combinations has a high frequency, indepen- dently. However, the co-occurrence of TPD words cannot be fully explained as the random coinci- dence of, e.g., the co-occurrence of Copenhagen and conference (which can be any other confer- ences in Copenhagen) and the occurrence of cli- mate.</p><p>The word "pure" in <ref type="bibr" target="#b7">Hou et al. (2013)</ref> means that the joint probability distribution of these words is significantly different from the product of lower- order joint distributions or marginal distributions, w.r.t all possible decompositions. More formally, it requires that the joint distribution cannot be factorized unconditionally (UPD) or conditionally (CPD) in the language of graphical model. Let x i ∈ {0, 1} denote the value of X i . Let p(x), x = [x 1 , x 2 , . . . , x n ] T , be the joint probability dis- tribution over X. Then the definitions of UPD and CPD are as follows: DEFINITION 2. (UPD): X = {X 1 , . . . , X n } is of n-order Unconditional Pure Dependence (UPD), iff it can NOT be unconditionally fac- torized, i.e., there does NOT exist a k-partition <ref type="bibr" target="#b7">Hou et al., 2013)</ref> DEFINITION 3. (CPD): X = {X 1 , . . . , X n } is of n-order Conditional Pure Dependence (CPD), iff it can NOT be conditionally factorized, i.e., there does NOT exist C 0 ⊂ X and a k-partition</p><formula xml:id="formula_0">{C 1 , C 2 , . . . , C k } of X, k &gt; 1, such at p(x) = p(c 1 ) * p(c 2 ). . .p(c k ), where p(c i ), i = 1, . . . , k, is the joint distribution over C i . (</formula><formula xml:id="formula_1">{C 1 , C 2 , . . ., C k } of V = X − C 0 , k &gt; 1, such at p(v|c 0 ) = p(c 1 |c 0 ) * p(c 2 |c 0 ). . .p(c k |c 0 ),</formula><note type="other">where p(v|c 0 ) is the conditional joint distribution over V given C 0 , and p(c i |c 0 ), i = 1, 2, . . . , k, is the conditional joint distribution over C i given C 0 . In case that C 0 is an empty set, we define p(c 0 ) = 1. (Hou et al., 2013)</note><p>Actually, CPD is stricter than UPD, and the dependence which just satisfies UPD is not pure enough to model the global context. Therefore, "pure" in our paper refers to the characteristic of CPD. However judging CPD is NP-hard. It is proved that a significant nonzero n-order θ param- eter (TPD) entails the n-order CPD/UPD in <ref type="bibr" target="#b7">Hou et al. (2013)</ref>. The highest-order coordinate pa- rameter in IG is a proper metric for the purity (i.e., the unique semantics) of high-order depen- dence. A pattern is TPD, iff the n-order θ coor- dinate θ 12...n is significantly different from zero. Moreover, The Log Likelihood Ratio Test imple- mented in the mixed coordinates can test whether θ 12...n is significantly different from zero.</p><p>Contrasting to TPD, the semantic coupling among the associations in the two negative exam- ples is much weaker. In conclusion, can, with, of cannot give an explicit topic and London, Chelsea, Sherlock Holmes includes at least two topics. the co-occurrence of words in TPD (e.g. climate, conference, Copenhagen) implies an un-separable (pure) high-level semantic entity. A sufficient and unbroken meaning of dependence can not only supply the context but also avoid the ambiguity (or noise) in global context. Therefore, the meaning of pure is important in such a global context mod- eling method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Global PV-DBOW and Dependence Vectors</head><p>A version of Paragraph Vector in Le and Mikolov (2014) PV-DBOW is extended with TPD to a new model: Global PV-DBOW (Glo-PV-DBOW). TPD has been extracted from the corpus before training. Given a sequence of training words w 1 , w 2 , w 3 , . . . , w T and the global context glo t of w t , the objective of Glo-PV-DBOW is to maximize the average log probability:</p><formula xml:id="formula_2">L = 1 T T ∑ t=1 [ ∑ −c≤j≤c,j̸ =0</formula><p>log p(w t |w t+j ) + log p(w t |glo t ) + log p(w t |doc t ) ]</p><p>where c is the local context window size. The in- dicator of the document that the current word w t belongs to is denoted by doc t . Further, we define p(w t |glo t ) in equation <ref type="formula" target="#formula_4">(2)</ref>:</p><formula xml:id="formula_4">p(w t |glo t ) = A ∏ a [ p(w t |dep a t )p(w t |w a 1 , w a 2 , . . . , w a N ) ]<label>(2)</label></formula><p>The indicator of the a-th w t 's TPD pattern is de- noted as dep a t and can be trained to be a distributed representation of TPD: dependence vector v dep a t . This (N+1)-order TPD consists of N+1 words: w a 1 , w a 2 . . . w a N and w t . The energy function of w t and w i = (w t+j , doc t , dep a t ) is uniform as follows:</p><formula xml:id="formula_5">E(w t , w i ) = −v wt T v w i<label>(3)</label></formula><p>We define the energy function of TPD words:</p><formula xml:id="formula_6">E(w t , w a 1 , w a 2 , . . . w a N ) = − 1 N N ∑ n=1 v wt T v w a n<label>(4)</label></formula><p>The resulting predictive distributions are given by</p><formula xml:id="formula_7">p(w t |w i ) = exp(v wt T v w i ) ∑ W m=1 exp(v wm T v w i )<label>(5)</label></formula><formula xml:id="formula_8">p(w t |w a 1 , w a 2 , . . . , w a N ) = exp( 1 N ∑ N n=1 v wt T v w a n ) ∑ W m=1 exp( 1 N ∑ N n=1 v wm T v w a n )<label>(6)</label></formula><p>Hierarchical softmax <ref type="bibr" target="#b15">(Morin and Bengio, 2005</ref>) is adopted to reduce the cost of computation. The binary tree is specified with a Huffman tree, and the Huffman code of pseudo words m i in w t 's Huffman path is denoted as x m i . For more about hierarchical softmax we used, please re- fer to <ref type="bibr" target="#b14">(Mikolov et al., 2013b</ref>). Using stochas- tic gradient descent (SGD), distributed representa- tions of the word, dependence and document have been trained. The update procedure of v w i = (v w t+j , v doct , v dep a t ) is as same as the procedure described in ( <ref type="bibr" target="#b14">Mikolov et al., 2013b</ref>). Thus, the pseudo code for training TPD words is listed indi- vidually:</p><formula xml:id="formula_9">SGD FOR TRAINING THE TPD WORDS 1 v wt ← current word 2 v w a ave ← 1 N ∑ N n=1 v w a n 3 err ← 0 4 for ∀m i 5 do g ← (1 − x m i − σ(v m i T v w a ave )) * α 6 err+ = g * 1 N * v m i 7 m i + = g * v w a ave</formula><p>8 for n ← 1 to N 9 do v w a n + = err</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Apriori (not a pure dependency method) is con- trastively adopted to implement Glo-PV-DBOW. Glo-PV-DBOW-TPD and Glo-PV-DBOW-Apri are all evaluated in two text classification tasks: sentiment analysis and topic discovery. The suffix (e.g., -2, -5) of our global method name denotes the order of dependency (the number of words in a dependence pattern). The order of dependency is changed because we want to show the superi- ority of the high-order TPD. The high-order TPD provides the more rich and explicit global context than the lower-order one since the high-order TPD cannot be reduced to the random coincidence of lower-order dependencies. We cross-validate the hyperparameters and set the local context window size as 10, the dimen- sion of embeddings as 100. In sentiment anal- ysis task, Apriori's minimum support and TPD's theta 0 is respectively set as 0.004 and 1.4. While in topic discovery task, Apriori's minimum sup- port and TPD's theta 0 is around 0.020 and 2.0 re- spectively. Since the classification accuracy of the approaches compared is a single result, we do not include any results for test of significance in our method and only report the average accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sentiment Analysis on Movie Reviews</head><p>The binary sentiment classification on the IMDB dataset proposed by <ref type="bibr" target="#b11">(Maas et al., 2011</ref>) is con- ducted. Results in <ref type="figure" target="#fig_1">Fig.1</ref> show that global methods' performance is more stable than PV-DBOW's. Moreover, TPD works much better than Apriori, especially in the high-order dependence. Note that TPD-5 works better than TPD-2, while Apri- 5 works worse than Apri-2. It can be explained that the Apriori algorithm is short of an explicit statistical test procedure to guarantee the pure de- pendence. Therefore, the Apriori algorithm is not suitable for generating the high-order dependence.  Instead, the high-order TPD can provide the rich and explicit global context for the model. Mean- while, it is verified that our method is good at cap- turing sentiment contrast. <ref type="table">Table 1</ref> shows that Glo-PV-DBOW with 5-order TPD achieves the state-of-the-art performance. A promising result is an improvement of more than 2% over result published in <ref type="bibr" target="#b10">Le and Mikolov (2014)</ref>. Note that the algorithm process of Para- graph Vector ( <ref type="bibr" target="#b10">Le and Mikolov, 2014</ref>) is much more complex than PV-DBOW's. Paragraph Vec- tor includes an extra inference stage. In addition, Paragraph Vector's document vector is a combina- tion of two vectors: one learned by PV-DBOW and the other learned by Distributed Memory Model of Paragraph Vectors (PV-DM) ( <ref type="bibr" target="#b10">Le and Mikolov, 2014</ref>). The combined document vector has 800 dimensions, while all vectors in our experiments only have 100 dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Topic Discovery on News</head><p>The 20 Newsgroups dataset is a collection of ap- proximately 20,000 newsgroup documents, parti- tioned across 20 different newsgroups. We fol- low ( <ref type="bibr" target="#b5">Crammer et al., 2012</ref>) to create binary prob- lems from the dataset by creating binary decision problems of choosing between two similar groups. Therefore, the dataset is split into two sub-datasets as follows: comp: comp.sys.ibm.pc.hardware vs. comp.sys.mac.hardware and sci: sci.electronics vs. sci.med. Similarly, 1800 examples balanced between the two labels were selected for each problem.</p><p>The classification accuracy on each sub-dataset <ref type="table">Table 1</ref>: The performance of our method com- pared with other approaches on the IMDB dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Accuracy rate BoW (bnc) <ref type="bibr" target="#b11">(Maas et al., 2011)</ref> 87.80% Full+Unlabeled+BoW ( <ref type="bibr" target="#b11">Maas et al., 2011)</ref> 88.89% WRRBM ( <ref type="bibr" target="#b6">Dahl et al., 2012)</ref> 87.42% WRRBM + BoW (bnc) ( <ref type="bibr" target="#b6">Dahl et al., 2012)</ref> 89.23% SVM-bi ( <ref type="bibr" target="#b17">Wang and Manning, 2012)</ref> 89.16% NBSVM-bi ( <ref type="bibr" target="#b17">Wang and Manning, 2012)</ref> 91.22% PV-DBOW ( <ref type="bibr" target="#b10">Le and Mikolov, 2014)</ref> 90.79% Paragraph Vector ( <ref type="bibr" target="#b10">Le and Mikolov, 2014)</ref> 92.58% Sentence Vector + RNN-LM + NB-LM 92.57% <ref type="bibr" target="#b12">(Mesnil et al., 2014)</ref> mvCNNo&amp;w <ref type="bibr" target="#b9">(Johnson and Zhang, 2015)</ref> 93.34% Glo-PV-DBOW-Apri-2 93.76% Glo-PV-DBOW-Apri-5 92.41% Glo-PV-DBOW-TPD-2 94.83% Glo-PV-DBOW-TPD-5 95.05% is recorded in <ref type="table" target="#tab_0">Table 2</ref>. Compared with Confidence-weighted ( <ref type="bibr" target="#b5">Crammer et al., 2012</ref>) and PV-DBOW ( <ref type="bibr" target="#b10">Le and Mikolov, 2014)</ref>, our extended models achieve the highest accuracy on each sub- dataset. Moreover, TPD as a pure dependence works better than Apriori when they provide the global context for our model. The topical infor- mation is effectively reinforced in embeddings by incorporating TPD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis on Word Embeddings</head><p>The cosine similarity of each word pair in 20 Newsgroups is computed. We list four center words and their nearest neighbors in PV-DBOW and Glo-PV-DBOW groups respectively. The rankings are labeled in front of neighbor words, and some notable neighbor words are in bold.</p><p>From <ref type="table" target="#tab_1">Table 3</ref>, we can see that the statistical information of corpus like words co-occurrence can be mined by TPD. Therefore, the Glo-PV- DBOW's embeddings are context-aware and it can help a lot for classification tasks. The top 40 near- est neighbors of ibm are investigated, and we find macintosh and mac appeared in the PV-DBOW group but not in the Glo-PV-DBOW group. In the corpus, the topic of documents is either ibm or mac. If we perform a classification task on "ibm versus mac", it will be hard to classify in the PV- DBOW group. That is because PV-DBOW tends to regard ibm and mac both as computers. How- ever, the two different computer brands are distin- guished in Glo-PV-DBOW. Further, ibm and mac co-occur rarely in one document, and the statisti- cal information is noted by TPD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper proposes to incorporate Theta Pure De- pendence into Paragraph Vector to capture more topical and sentimental information in the con- text. The extended model is applied to a sen- timent classification task and a topical detection task. Our accuracy outperforms the state-of-the- art result on the movie and news datasets. The ap- proach can be improved further to fully leverage the un-factorized sense of high-order Theta Pure Dependence. In future, we will explore the appli- cations of dependence distributed representation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Box plot of classification accuracy over a local method (PV-DBOW) and 4 global methods (Apri-2/5, TPD-2/5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The performance of our method com-
pared to other approaches on 20 Newsgroup. 

Model 
Comp 
Sci 
Confidence-weighted 
94.39% 
97.56% 
(Crammer et al., 2012) 
PV-DBOW 
92.60% 
98.02% 
(Le and Mikolov, 2014) 
Glo-PV-DBOW-Apri-2 
94.56% 
98.42% 
Glo-PV-DBOW-Apri-5 
94.43% 
98.13% 
Glo-PV-DBOW-TPD-2 
94.59% 99.20% 
Glo-PV-DBOW-TPD-5 
95.47% 98.74% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 : Nearest neighbors of words ranking list based on cosine similarity.</head><label>3</label><figDesc></figDesc><table>Center word PV-DBOW 
Glo-PV-DBOW 

ibm 

1:aix 
2:pc 
. . . 
23:macintosh 
34:mac 

1:aix 
2:pc 
3:pc's 
4:austin 
5:workstations 

mac 

1:macintosh 
2:quicktime 
3:portable 
4:utilities 
5:macs 

1:macintosh 
2:apple's 
3:quicktime 
4:apple 
5:macs 

486 

1:386 
2:486dx 
3:33mhz 
4:486dx2 
5:cpu 

1:386 
2:cpu 
3:486dx 
4:486dx2 
5:33mhz 

Kingston 

1:aix 
2:mike 
3:sharks 
4:jones 
5:ibm 

1:aix 
2:ibm 
3:jones 
4:sharks 
5:mike 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mining association rules between sets of items in large databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Imieli´nskiimieli´nski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD Record</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="207" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large-sample learning of bayesian networks is np-hard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Maxwell Chickering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1287" to="1330" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A unified aagrawal1993miningrchitecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Confidence-weighted linear classification for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1891" to="1926" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Training restricted boltzmann machines on word observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>George E Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mining pure high-order word associations via information geometry for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexian</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01255</idno>
		<title level="m">Semi-supervised learning with multi-view embedding: Theory and application with convolutional neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5335</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ICLR Workshop</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international workshop on artificial intelligence and statistics</title>
		<meeting>the international workshop on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
<note type="report_type">Citeseer</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Information-geometric measure for neural spikes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Nakahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun-Ichi</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2269" to="2316" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
