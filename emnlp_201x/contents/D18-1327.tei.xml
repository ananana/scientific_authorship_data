<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Source Syntactic Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Currey</surname></persName>
							<email>a.currey@sms.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Edinburgh</orgName>
								<orgName type="institution" key="instit2">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
							<email>kheafiel@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Edinburgh</orgName>
								<orgName type="institution" key="instit2">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Source Syntactic Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2961" to="2966"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2961</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce a novel multi-source technique for incorporating source syntax into neural machine translation using linearized parses. This is achieved by employing separate en-coders for the sequential and parsed versions of the same source sentence; the resulting representations are then combined using a hierarchical attention mechanism. The proposed model improves over both seq2seq and parsed baselines by over 1 BLEU on the WMT17 English→German task. Further analysis shows that our multi-source syntactic model is able to translate successfully without any parsed input, unlike standard parsed methods. In addition, performance does not deteriorate as much on long sentences as for the baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation (NMT) typically makes use of a recurrent neural network (RNN) -based encoder and decoder, along with an at- tention mechanism ( <ref type="bibr">Bahdanau et al., 2015;</ref><ref type="bibr">Cho et al., 2014;</ref><ref type="bibr" target="#b7">Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b20">Sutskever et al., 2014</ref>). However, it has been shown that RNNs require some supervision to learn syntax ( <ref type="bibr">Bentivogli et al., 2016;</ref><ref type="bibr" target="#b12">Linzen et al., 2016;</ref><ref type="bibr" target="#b19">Shi et al., 2016)</ref>. Therefore, explicitly incor- porating syntactic information into NMT has the potential to improve performance. This is partic- ularly true for source syntax, which can improve the model's representation of the source language.</p><p>Recently, there have been a number of propos- als for using linearized representations of parses within standard NMT ( <ref type="bibr">Aharoni and Goldberg, 2017;</ref><ref type="bibr" target="#b10">Li et al., 2017;</ref><ref type="bibr" target="#b15">Nadejde et al., 2017)</ref>. Lin- earized parses are advantageous because they can inject syntactic information into the models with- out significant changes to the architecture. How- ever, using linearized parses in a sequence-to- sequence (seq2seq) framework creates some chal- lenges, particularly when using source parses. First, the parsed sequences are significantly longer than standard sentences, since they contain node labels as well as words. Second, these systems often fail when the source sentence is not parsed. This can be a problem for inference, since the external parser may fail on an input sentence at test time. We propose a method for incorporat- ing linearized source parses into NMT that ad- dresses these challenges by taking both the se- quential source sentence and its linearized parse simultaneously as input in a multi-source frame- work. Thus, the model is able to use the syntac- tic information encoded in the parse while falling back to the sequential sentence when necessary. Our proposed model improves over both standard and parsed NMT baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Seq2seq Neural Parsing</head><p>Using linearized parse trees within sequential frameworks was first done in the context of neural parsing. <ref type="bibr" target="#b22">Vinyals et al. (2015)</ref> parsed using an at- tentional seq2seq model; they used linearized, un- lexicalized parse trees on the target side and sen- tences on the source side. In addition, as in this work, they used an external parser to create syn- thetic parsed training data, resulting in improved parsing performance. <ref type="bibr" target="#b0">Choe and Charniak (2016)</ref> adopted a similar strategy, using linearized parses in an RNN language modeling framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">NMT with Source Syntax</head><p>Among the first proposals for using source syntax in NMT was that of <ref type="bibr" target="#b13">Luong et al. (2016)</ref>, who in- troduced a multi-task system in which the source data was parsed and translated using a shared en- coder and two decoders. More radical changes to the standard NMT paradigm have also been pro- posed. <ref type="bibr" target="#b2">Eriguchi et al. (2016)</ref> introduced tree-to- sequence NMT; this model took parse trees as in- put using a tree-LSTM ( <ref type="bibr" target="#b21">Tai et al., 2015)</ref> encoder. <ref type="bibr">Bastings et al. (2017)</ref> used a graph convolutional encoder in order to take labeled dependency parses of the source sentences into account. <ref type="bibr" target="#b5">Hashimoto and Tsuruoka (2017)</ref> added a latent graph parser to the encoder, allowing it to learn soft dependency parses while simultaneously learning to translate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Linearized Parse Trees in NMT</head><p>The idea of incorporating linearized parses into seq2seq has been adapted to NMT as a means of injecting syntax. <ref type="bibr">Aharoni and Goldberg (2017)</ref> first did this by parsing the target side of the train- ing data and training the system to generate parsed translations of the source input; this is the in- verse of our parse2seq baseline. Similarly, <ref type="bibr" target="#b15">Nadejde et al. (2017)</ref> interleaved CCG supertags with words on the target side, finding that this improved translation despite requiring longer sequences.</p><p>Most similar to our multi-source model is the parallel RNN model proposed by <ref type="bibr" target="#b10">Li et al. (2017)</ref>. Like multi-source, the parallel RNN used two en- coders, one for words and the other for syntax. However, they combined these representations at the word level, whereas we combine them on the sentence level. Their mixed RNN model is also similar to our parse2seq baseline, although the mixed RNN decoder attended only to words. As the mixed RNN model outperformed the parallel RNN model, we do not attempt to compare our model to parallel RNN. These models are similar to ours in that they incorporate linearized parses into NMT; here, we utilize a multi-source frame- work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Multi-Source NMT</head><p>Multi-source methods in neural machine transla- tion were first introduced by <ref type="bibr" target="#b23">Zoph and Knight (2016)</ref> for multilingual translation. They used one encoder per source language, and combined the resulting sentence representations before feeding them into the decoder. <ref type="bibr" target="#b3">Firat et al. (2016)</ref> expanded on this by creating a multilingual NMT system with multiple encoders and decoders. Libovick`Libovick`y and Helcl (2017) applied multi-source NMT to multimodal translation and automatic post-editing and explored different strategies for combining at- tention over the two sources. In this paper, we apply the multi-source framework to a novel task, syntactic neural machine translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NMT with Linearized Source Parses</head><p>We propose a multi-source method for incorporat- ing source syntax into NMT. This method makes use of linearized source parses; we describe these parses in section 3.1. Throughout this paper, we refer to standard sentences that do not contain any explicit syntactic information as sequential; see <ref type="table" target="#tab_0">Table 1</ref> for an example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Linearized Source Parses</head><p>We use an off-the-shelf parser, in this case Stan- ford CoreNLP ( <ref type="bibr" target="#b14">Manning et al., 2014</ref>), to create binary constituency parses. These parses are lin- earized as shown in <ref type="table" target="#tab_0">Table 1</ref>. We tokenize the opening parentheses with the node label (so each node label begins with a parenthesis) but keep the closing parentheses separate from the words they follow. For our task, the parser failed on one training sentence of 5.9 million, which we dis- carded, and succeeded on all test sentences. It took roughly 16 hours to parse the 5.9 million training sentences.</p><p>Following <ref type="bibr" target="#b18">Sennrich et al. (2016b)</ref>, our networks operate at the subword level using byte pair encod- ing (BPE) with a shared vocabulary on the source and target sides. However, the parser operates at the word level. Therefore, we parse then break into subwords, so a leaf may have multiple tokens without internal structure.</p><p>The proposed method is tested using both lex- icalized and unlexicalized parses. In unlexical- ized parses, we remove the words, keeping only the node labels and the parentheses. In lexical- ized parses, the words are included. <ref type="table" target="#tab_0">Table 1</ref> shows an example of the three source sentence for- mats: sequential, lexicalized parse, and unlexical- ized parse. Note that the lexicalized parse is sig- nificantly longer than the other versions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-Source</head><p>We propose a multi-source framework for inject- ing linearized source parses into NMT. This model consists of two identical RNN encoders with no shared parameters, as well as a standard RNN de- coder. For each target sentence, two versions of the source sentence are used: the sequential (stan- dard) version and the linearized parse (lexicalized or unlexicalized). Each of these is encoded simul-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example Sentence sequential history is a great teacher . lexicalized parse (ROOT (S (NP (NN history ) ) (VP (VBZ is ) (NP (DT a ) (JJ great ) (NN teacher ) ) ) (. . ) ) ) unlexicalized parse (ROOT (S (NP (NN ) ) (VP (VBZ ) (NP (DT ) (JJ ) (NN ) ) ) (. . ) ) ) target sentence</head><p>die Geschichte ist ein großartiger Lehrmeister . taneously using the encoders; the encodings are then combined and used as input to the decoder. We combine the source encodings using the hi- erarchical attention combination proposed by <ref type="bibr">Libovick`bovick`y and Helcl (2017)</ref>. This consists of a sep- arate attention mechanism for each encoder; these are then combined using an additional attention mechanism over the two separate context vectors. This multi-source method is thus able to combine the advantages of both standard RNN-based en- codings and syntactic encodings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>We base our experiments on the WMT17 (Bojar et al., 2017) English (EN) → German (DE) news translation task. All 5.9 million parallel train- ing sentences are used, but no monolingual data. Validation is done on newstest2015, while new- stest2016 and newstest2017 are used for testing. We train a shared BPE vocabulary with 60k merge operations on the parallel training data. For the parsed data, we break words into subwords af- ter applying the Stanford parser. We tokenize and truecase the data using the Moses tokenizer and truecaser ( <ref type="bibr" target="#b9">Koehn et al., 2007</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation</head><p>The models are implemented in Neural Mon- key ( <ref type="bibr" target="#b11">Helcl and Libovick`Libovick`y, 2017)</ref>. They are trained using Adam ( <ref type="bibr" target="#b8">Kingma and Ba, 2015)</ref> and have minibatch size 40, RNN size 512, and dropout probability 0.2 ( <ref type="bibr" target="#b4">Gal and Ghahramani, 2016)</ref>. We train to convergence on the validation set, using BLEU ( <ref type="bibr" target="#b16">Papineni et al., 2002</ref>) as the metric.</p><p>For sequential inputs and outputs, the maximum sentence length is 50 subwords. For parsed inputs, we increase maximum sentence length to 150 sub- words to account for the increased length due to the parsing labels; we still use a maximum output length of 50 subwords for these systems.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seq2seq</head><p>The proposed models are compared against two baselines. The first, referred to here as seq2seq, is the standard RNN-based neural machine transla- tion system with attention ( <ref type="bibr">Bahdanau et al., 2015)</ref>. This baseline does not use the parsed data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parse2seq</head><p>The second baseline we consider is a slight modi- fication of the mixed RNN model proposed by <ref type="bibr" target="#b10">Li et al. (2017)</ref>. This uses an identical architecture to the seq2seq baseline (except for a longer max- imum sentence length in the encoder). Instead of using sequential data on the source side, the lin- earized parses are used. We allow the system to attend equally to words and node labels on the source side, rather than restricting the attention to words. We refer to this baseline as parse2seq. <ref type="table" target="#tab_1">Table 2</ref> shows the performance on EN→DE trans- lation for each of the proposed systems and the baselines, as approximated by BLEU score. The multi-source systems improve strongly over both baselines, with improvements of up to 1.5 BLEU over the seq2seq baseline and up to 1.1 BLEU over the parse2seq baseline. In addi- tion, the lexicalized multi-source systems yields slightly higher BLEU scores than the unlexical- ized multi-source systems; this is surprising be- cause the lexicalized systems have significantly longer sequences than the unlexicalized ones. Fi- nally, it is interesting to compare the seq2seq and parse2seq baselines. Parse2seq outperforms</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Source Data 2016 2017 parse2seq seq 0.6 0.5 multi-source lex seq + seq 23.6 20.0 seq + null 23.1 19.3 multi-source unlex seq + seq 23.7 19.9 seq + null 23.6 20.9 <ref type="table">Table 3</ref>: BLEU scores on newstest2016 and new- stest2017 when no parsed data is used during inference.</p><p>seq2seq by only a small amount compared to multi-source; thus, while adding syntax to NMT can be helpful, some ways of doing so are more effective than others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Inference Without Parsed Sentences</head><p>The parse2seq and multi-source systems require parsed source data at inference time. However, the parser may fail on an input sentence. There- fore, we examine how well these systems do when given only unparsed source sentences at test time. <ref type="table">Table 3</ref> displays the results of these experi- ments. For the parse2seq baseline, we use only sequential (seq) data as input. For the lexical- ized and unlexicalized multi-source systems, two options are considered: seq + seq uses identical sequential data as input to both encoders, while seq + null uses null input for the parsed encoder, where every source sentence is "( )".</p><p>The parse2seq system fails when given only se- quential source data. On the other hand, both multi-source systems perform reasonably well without parsed data, although the BLEU scores are worse than multi-source with parsed data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">BLEU by Sentence Length</head><p>For models that use source-side linearized parses (multi-source and parse2seq), the source se- quences are significantly longer than for the seq2seq baseline. Since NMT already performs relatively poorly on long sentences ( <ref type="bibr">Bahdanau et al., 2015)</ref>, adding linearized source parses may exacerbate this issue. To detect whether this oc- curs, we calculate BLEU by sentence length.</p><p>We bucket the sentences in newstest2017 by source sentence length. We then compute BLEU scores for each bucket for the seq2seq and parse2seq baselines and the lexicalized multi- source system. The results are in <ref type="figure" target="#fig_2">Figure 1</ref>.</p><p>In line with previous work on NMT on long sen- tences ( <ref type="bibr">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b10">Li et al., 2017)</ref>, we see a significant deterioration in BLEU for longer Length of Source Sentence BLEU seq2seq parse2seq multi-source sentences for all systems. In particular, although the parse2seq model outperformed the seq2seq model overall, it does worse than seq2seq for sen- tences containing more than 30 words. This indi- cates that parse2seq performance does indeed suf- fer due to its long sequences. On the other hand, the multi-source system outperforms the seq2seq baseline for all sentence lengths and does particu- larly well for sentences with over 50 words. This may be because the multi-source system has both sequential and parsed input, so it can rely more on sequential input for very long sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we presented a multi-source method for effectively incorporating linearized parses of the source data into neural machine translation. This method, in which the parsed and sequential versions of the sentence were both taken as input during training and inference, resulted in gains of up to 1.5 BLEU on EN→DE translation. In ad- dition, unlike parse2seq, the multi-source model translated reasonably well even when the source sentence was not parsed.</p><p>In the future, we will explore adding back- translated ( <ref type="bibr" target="#b17">Sennrich et al., 2016a</ref>) or copied <ref type="bibr" target="#b1">(Currey et al., 2017</ref>) target data to our multi-source sys- tem. The multi-source model does not require all training data to be parsed; thus, monolingual data can be used even if the parser is unreliable for the synthetic or copied source sentences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: BLEU by sentence length on newstest2017 for baselines and lexicalized multi-source.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Example source training sentence with sequential, lexicalized parse, and unlexicalized parse versions. We include the corresponding target sentence for reference.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>BLEU scores on newstest2016 and new-
stest2017 datasets for the baselines, unlexicalized (un-
lex), and lexicalized (lex) systems. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was funded by the Amazon Academic Research Awards program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Roee Aharoni and Yoav Goldberg. 2017. Towards string-to-tree neural machine translation. In Pro-ceedings of the 55th Annual Meeting of the ACL, pages 132-140. Association for Computational Lin-guistics.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Parsing as language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kook</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2331" to="2336" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Copied monolingual data improves low-resource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Currey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tree-to-sequence attentional neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the ACL</title>
		<meeting>the 54th Annual Meeting of the ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="823" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-way, multilingual neural machine translation with a shared attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="866" to="875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural machine translation with source-side latent graph parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="125" to="135" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural Monkey: An open-source tool for sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Helcl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Libovick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="17" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Libovick`y</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the ACL</title>
		<meeting>the 45th Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Modeling source syntax for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the ACL</title>
		<meeting>the 55th Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="688" to="697" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attention strategies for multi-source sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Libovick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">`</forename><surname>Libovick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">`</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Helcl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the ACL</title>
		<meeting>the 55th Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="196" to="202" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Assessing the ability of LSTMs to learn syntax-sensitive dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="521" to="535" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-task sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the ACL</title>
		<meeting>the 52nd Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predicting target language CCG supertags improves neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Nadejde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="68" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BLEU: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the ACL</title>
		<meeting>the 40th Annual Meeting of the ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the ACL</title>
		<meeting>the 54th Annual Meeting of the ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the ACL</title>
		<meeting>the 54th Annual Meeting of the ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Does string-based neural MT learn source syntax?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkit</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1526" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-source neural translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="30" to="34" />
		</imprint>
	</monogr>
	<note>Proceedings of NAACL-HLT</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
