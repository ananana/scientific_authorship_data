<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Determining Semantic Textual Similarity using Natural Deduction Proofs</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitomi</forename><surname>Yanaka</surname></persName>
							<email>hitomiyanaka@g.ecc.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koji</forename><surname>Mineshima</surname></persName>
							<email>mineshima.koji@ocha.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">Ochanomizu University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascual</forename><surname>Martínez-Gómez</surname></persName>
							<email>pascual.mg@aist.go.jp</email>
							<affiliation key="aff2">
								<orgName type="department">Artificial Intelligence Research Center</orgName>
								<orgName type="institution">AIST Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Bekki</surname></persName>
							<email>bekki@is.ocha.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">Ochanomizu University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Determining Semantic Textual Similarity using Natural Deduction Proofs</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="681" to="691"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Determining semantic textual similarity is a core research subject in natural language processing. Since vector-based models for sentence representation often use shallow information, capturing accurate semantics is difficult. By contrast, logical semantic representations capture deeper levels of sentence semantics, but their symbolic nature does not offer graded notions of tex-tual similarity. We propose a method for determining semantic textual similarity by combining shallow features with features extracted from natural deduction proofs of bidirectional entailment relations between sentence pairs. For the natural deduction proofs, we use ccg2lambda, a higher-order automatic inference system, which converts Combinatory Categorial Grammar (CCG) derivation trees into semantic representations and conducts natural deduction proofs. Experiments show that our system was able to outperform other logic-based systems and that features derived from the proofs are effective for learning textual similarity.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Determining semantic textual similarity (STS) is one of the most critical tasks in informa- tion retrieval and natural language processing. Vector-based sentence representation models have been widely used to compare and rank words, phrases or sentences using various similarity and relatedness scores <ref type="bibr" target="#b33">(Wong and Raghavan, 1984;</ref><ref type="bibr" target="#b27">Mitchell and Lapata, 2010;</ref><ref type="bibr" target="#b17">Le and Mikolov, 2014)</ref>. Recently, neural network-based sentence representation models <ref type="bibr" target="#b28">(Mueller and Thyagarajan, 2016;</ref><ref type="bibr" target="#b14">Hill et al., 2016</ref>) have been proposed for learning textual similarity. However, these vector- based models often use shallow information, such as words and characters, and whether they can account for phenomena such as negation and quantification is not clear. Consider the sentences: Tom did not meet some of the players and Tom did not meet any of the players. If functional words such as some or any are ignored or represented as the same vector, then these sentences are to be represented by identical vectors. However, the first sentence implies that there is a player who Tom did not meet, whereas the second sentence means that Tom did not meet anyone, so the sentences have different meanings.</p><p>Conversely, logic-based approaches have been successful in representing the meanings of com- plex sentences, having had a positive impact for applications such as recognizing textual entail- ment ( <ref type="bibr" target="#b24">Mineshima et al., 2015</ref><ref type="bibr" target="#b25">Mineshima et al., , 2016</ref><ref type="bibr" target="#b0">Abzianidze, 2015</ref><ref type="bibr" target="#b1">Abzianidze, , 2016</ref>. However, purely logic-based ap- proaches only assess entailment or contradic- tion relations between sentences and do not offer graded notions of semantic similarity.</p><p>In this paper, we propose to leverage logic cues to learn textual similarity. Our hypothesis is that observing proof processes when testing the seman- tic relations is predictive of textual similarity. We show that our approach can be more effective than systems that ignore these logic cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Vector-based models of semantic composition have been widely studied with regards to calculat- ing STS. <ref type="bibr">Lapata (2008, 2010)</ref> pro-posed a sentence vector model involving word vector addition or component-wise multiplica- tion. Addition and multiplication are commuta- tive and associative and thus ignore word order. <ref type="bibr" target="#b30">Polajnar et al. (2015)</ref> proposed a discourse-based sentence vector model considering extra-intra sen- tential context. Also, a categorical compositional distributional semantic model has been developed for recognizing textual entailment and for cal- culating STS ( <ref type="bibr" target="#b13">Grefenstette and Sadrzadeh, 2011;</ref><ref type="bibr" target="#b15">Kartsaklis et al., 2014;</ref><ref type="bibr" target="#b16">Kartsaklis and Sadrzadeh, 2016)</ref>. However, these previous studies are mostly concerned with the structures of basic phrases or sentences and do not address logical and func- tional words such as negations and connectives. Neural network-based models of semantic compo- sition <ref type="bibr" target="#b28">(Mueller and Thyagarajan, 2016;</ref><ref type="bibr" target="#b14">Hill et al., 2016</ref>) have also been proposed. Although these models achieve higher accuracy, their end-to-end nature introduces challenges in the diagnosis of the reasons that make two sentences to be similar or dissimilar to each other. These diagnosis capa- bilities may play an important role in making the system explainable and also to guide future system improvements in a more precise manner. Our ap- proach presented in this paper is partially inspired by the latter two objectives.</p><p>Meanwhile, some previous studies have pro- posed logic systems for capturing the seman- tic relatedness of sentences. The Meaning Fac- tory ( <ref type="bibr" target="#b8">Bjerva et al., 2014</ref>) uses both shallow and logic-based features for learning textual similarity. In this system, the overlap of predicates and entail- ment judgments are extracted as logic-based fea- tures. <ref type="bibr">UTexas (Beltagy et al., 2014b</ref>) uses Prob- abilistic Soft Logic for learning textual similarity. In this system, each ground atom in the logical for- mulas has a probability based on distributional se- mantics of a word. The weights of the logical for- mulas are calculated from the probabilities of their ground atoms and are extracted as features. These previous studies improved the accuracy by using logic-based features derived from the entailment results of first-order theorem proving in addition to using shallow features such as sentence lengths.</p><p>In our study, we determine the semantic sim- ilarity of sentences based on the conception of proof-theoretic semantics . The key idea is that not only the entailment results but also the theorem proving process can be considered as features for learning textual sim- ilarity. That is, by taking into account not only whether a theorem is proved but also how it is proved, we can capture the semantic relationships between sentence pairs in more depth.</p><p>Another difference between our study and pre- vious logic systems is that we use higher-order predicate logic. Higher-order predicate logic is able to represent complex sentence semantics such as generalized quantifiers more precisely than first-order predicate logic. In addition, higher- order predicate logic makes the logical structure of a sentence more explicit than first-order predi- cate logic does, so it can simplify the process of proof search <ref type="bibr" target="#b22">(Miller and Nadathur, 1986</ref>). <ref type="figure" target="#fig_1">Figure 1</ref> shows an overview of the system which extracts features for learning textual similarity from logical proofs. To produce semantic repre- sentations of sentences and prove them automati- cally, we use ccg2lambda <ref type="bibr">(Martínez-Gómez et al., 2016)</ref>, which is a semantic parser combined with an inference system based on natural deduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Overview</head><p>First, sentences are parsed into syntactic trees based on Combinatory Categorial Grammar (CCG) <ref type="bibr" target="#b32">(Steedman, 2000</ref>). CCG is a syntactic the- ory suitable for semantic composition from syn- tactic structures. Meaning representations are ob- tained based on semantic templates and combi- natory rules for the CCG trees. Semantic tem- plates are defined manually based on formal se- mantics. Combinatory rules specify the syntactic behaviors of words and compositional rules for the CCG trees. In ccg2lambda, two wide-coverage CCG parsers, C&amp;C <ref type="bibr" target="#b10">(Clark and Curran, 2007)</ref> and EasyCCG ( <ref type="bibr" target="#b18">Lewis and Steedman, 2014)</ref>, are used for converting tokenized sentences into CCG trees robustly. According to a previous study <ref type="bibr" target="#b21">(Martínez-Gómez et al., 2017)</ref>, EasyCCG achieves higher accuracy. Thus, when the output of both C&amp;C and EasyCCG can be proved, we use EasyCCG's output for creating features.</p><p>Second, the meanings of words are described using lambda terms. Semantic representations are obtained by combining lambda terms in accor- dance with the meaning composition rules spec- ified in the CCG tree. The semantic represen- tations are based on Neo-Davidsonian event se- mantics <ref type="bibr" target="#b29">(Parsons, 1990;</ref><ref type="bibr" target="#b24">Mineshima et al., 2015)</ref>, in which every verb is decomposed into a predi- cate over events and a set of functional expressions   relating the events. Adverbs and prepositions are also represented as predicates over events.</p><p>Third, we attempt to prove entailment relations between sentence pairs. For this purpose, we use Coq <ref type="bibr" target="#b7">(Bertot and Castran, 2010)</ref>, which can be used for efficient theorem-proving for natural lan- guage inference using both first-order and higher- order logic ( <ref type="bibr" target="#b24">Mineshima et al., 2015</ref>). Coq's proof calculus is based on natural deduction <ref type="bibr" target="#b31">(Prawitz, 1965)</ref>, a proof system based on inference rules called introduction and elimination rules for log- ical connectives. The inference system imple- mented in ccg2lambda using Coq achieves effi- cient automatic inference by feeding a set of pre- defined tactics and user-defined proof-search tac- tics to its interactive mode. The natural deduc- tion system is particularly suitable for injecting external axioms during the theorem-proving pro- cess <ref type="bibr" target="#b21">(Martínez-Gómez et al., 2017)</ref>.</p><p>Finally, features for learning textual similar- ity are extracted from the proofs produced by ccg2lambda during the theorem-proving process. In this study, we experimented with logistic re- gression, support vector regression and random forest regression, finding that random forest re- gression was the most effective. We therefore chose random forest regression for learning tex- tual similarity, with its hyperparameters being op- timized by grid search. The mean squared error (MSE) was used to measure the prediction perfor- mance of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proof Strategy for Learning Textual Similarity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview of the proof strategy</head><p>Sentence similarity depends on complex elements, such as word overlaps and semantic relations. We capture the similarity between the sentence pair (A, B) as a function of the provability of bidirec- tional entailment relations for (A, B) and combine it with shallow features. After obtaining logical formulas A ′ and B ′ from A and B, we attempt to prove the bidirectional entailment relations, A ′ ⇒ B ′ and B ′ ⇒ A ′ . If the initial natural deduc- tion proofs fail, we re-run the proof, adding rel- evant external axioms or skipping unproved sub- goals until the proof is completed. After that, fea- tures for learning textual similarity are extracted by quantifying the provability of the bidirectional entailment relations.</p><p>The details of the procedure are as follows. First, we attempt a natural deduction proof without using external axioms, aiming to prove entailment relations, A ′ ⇒ B ′ and B ′ ⇒ A ′ . If both fail, then we check whether A ′ contradicts B ′ , which amounts to proving the negation of the original conclusion, namely A ′ ⇒ ¬B ′ and B ′ ⇒ ¬A ′ .</p><p>The similarity of a sentence pair tends to be higher when the negation of the conclusion can be proved, compared with the case where nei- ther the conclusion nor its negation can be proved. In the SICK (Sentences Involving Compositional Knowledge) dataset <ref type="bibr" target="#b19">(Marelli et al., 2014</ref>) (see Section 6.1 for details), 70% of the sentence pairs annotated as contradictory are assigned a related- ness score in <ref type="bibr">[3,</ref><ref type="bibr">5)</ref>.</p><p>Next, if we fail to prove entailment or contradic- tion, that is, we cannot prove the conclusion or its negation, we identify an unproved sub-goal which is not matched by any predicate in the premise. We then attempt to prove A ′ ⇒ B ′ and B ′ ⇒ A ′ using axiom injection, following the method in- troduced in <ref type="bibr" target="#b21">Martínez-Gómez et al. (2017)</ref>. In ax- iom injection, unproved sub-goals are candidates to form axioms. We focus only on predicates that share at least one argument with both the premise and the conclusion. This means that an axiom can be generated only if there is a predicate p in the pool of premises and a predicate q in a sub-goal and p and q share a variable in an argument posi- tion, possibly with the same case (e.g., Subject or Object).</p><p>In generating axioms, the semantic relation- ships between the predicates in the premise and those in the conclusion are checked using lexical knowledge. In this study, we use WordNet <ref type="bibr" target="#b23">(Miller, 1995)</ref> as the source of lexical knowledge. Linguis- tic relations between predicates are checked in the following order: inflections, derivationally related forms, synonyms, antonyms, hypernyms, similar- ities, and hyponyms. If any one of these relations is found in the lexical knowledge, an axiom can be generated. Again, if the proof fails, we attempt <ref type="figure">Figure 2</ref>: Example of the inference rules used in natural deduction. P, P 1 , . . . P n are formulas in the premise, while G, G 1 , G 2 are formulas in the goal. The initial formulas are at the top, with the formulas obtained by applying the inference rules shown below.</p><formula xml:id="formula_0">G : A ∧ B ∧-INTRO G1 : A G2 : B P : A1 ∧ A2 ∧ · · · ∧ An ∧-ELIM P1 : A1, P2 : A2, . . . , Pn : An G : A → B →-INTRO P : A G : B P1 : A → B P2 : A →-ELIM P : B G : ∃xA(x) ∃-INTRO G1 : A(x) P : ∃xA(x) ∃-ELIM P1 : A(x) P1 : A(t) P2 : t = u =-ELIM P : A(u)</formula><p>to prove the negation of the conclusion using the axiom injection mechanism.</p><p>If the proof by axiom injection fails because of a lack of lexical knowledge, we obtain sentence similarity information from partial proofs by sim- ply accepting the unproved sub-goals and forcibly completing the proof. After the proof is com- pleted, information about the generated axioms and skipped sub-goals is used to create features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Proving entailment relations</head><p>As an illustration of how our natural deduction proof works, consider the case of proving entail- ment for the following sentence pair:</p><p>A: A man is singing in a bar. B: A man is singing. The sentences A and B are mapped onto logical formulas A ′ and B ′ based on event semantics via CCG-based semantic composition, as follows.</p><formula xml:id="formula_1">A ′ : ∃e 1 x 1 x 2 (man(x 1 ) ∧ sing(e 1 ) ∧ (subj(e 1 ) = x 1 ) ∧ bar(x 2 ) ∧ in(e 1 , x 2 )) B ′ : ∃e 1 x 1 (man(x 1 ) ∧ sing(e 1 ) ∧ (subj(e 1 ) = x 1 ))</formula><p>First, we attempt a natural deduction proof of A ′ ⇒ B ′ , setting A ′ as the premise and B ′ as the goal of the proof. Then A ′ and B ′ are decomposed according to the inference rules. <ref type="figure">Figure 2</ref> shows the major inference rules we use in the proofs. Inference rules in natural deduction are divided into two types: introduction rules and elimination rules. Introduction rules specify how</p><formula xml:id="formula_2">P0 : ∃e1x1x2(man(x1) ∧ sing(e1) ∧ (subj(e1) = x1) ∧ bar(x2) ∧ in(e1, x2)) G0 : ∃e1x1(man(x1) ∧ sing(e1) ∧ (subj(e1) = x1)) ∃-ELIM (P0), ∃-INTRO (G0) P1 : man(x1) ∧ sing(e1) ∧ (subj(e1) = x1) ∧ bar(x2) ∧ in(e1, x2) G1 : man(x1) ∧ sing(e1) ∧ (subj(e1) = x1) ∧-ELIM (P1), ∧-INTRO (G1)</formula><p>P2 : man(x1), P3 : sing(e1), P4 : subj(e1) = x1 P5 : bar(x2), P6 : in(e1, x2), G2 : man(x1), G3 : sing(e1), G4 : subj(e1) = x1 to prove a formula in the goal, decomposing a goal formula into smaller sub-goals. Elimination rules specify how to use a premise, decomposing a for- mula in the pool of premises into smaller ones.</p><p>The proof process for A ′ ⇒ B ′ is shown in <ref type="figure" target="#fig_2">Fig- ure 3</ref>. Here A ′ is initially set to the premise P 0 and B ′ to the goal G 0 . P 0 and G 0 are then decomposed using elimination rules (∧-ELIM, ∃-ELIM) and intro- duction rules (∧-INTRO, ∃-INTRO). Then we obtain a set of premise formulas P = {P 2 , P 3 , P 4 , P 5 , P 6 }, and a set of sub-goals G = {G 2 , G 3 , G 4 }. The proof is performed by searching for a premise P i whose predicate and arguments match those of a given sub-goal G j . If such a logical premise is found, the sub-goal is removed. In this example, the sub-goals G 2 , G 3 , and G 4 match the premises P 2 , P 3 , and P 4 , respectively. Thus, A ′ ⇒ B ′ can be proved without introducing axioms.</p><p>Second, we attempt the proof in the opposite direction, B ′ ⇒ A ′ , by switching P 0 and G 0 in <ref type="figure" target="#fig_2">Figure 3</ref>. Again, by applying inference rules, we obtain the following sets of premises P and sub- goals G:</p><formula xml:id="formula_3">P = {P2 : man(x1), P3 : sing(e1), P4 : subj(e1) = x1} G = {G2 : man(x1), G3 : sing(e1), G4 : subj(e1) = x1, G5 : bar(x2), G6 : in(e1, x2))}</formula><p>Here, the two sub-goals G 5 and G 6 do not match any of the premises, so the attempted proof of B ′ ⇒ A ′ fails. We therefore attempt to inject additional axioms, but in this case no predicate in P shares the argument x 2 of the predicates bar(x 2 ) and in(e 1 , x 2 ) in G. Thus, no axiom can be generated. To obtain information from a partial proof, we forcibly complete the proof of B ′ ⇒ A ′ by skipping the unproved sub-goals bar(x) and in(e 1 , x 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Proving the contradiction</head><p>The proof strategy illustrated here can be straight- forwardly applied to proving the contradiction. In natural deduction, a negative formula of the form ¬A can be defined as A → False ("the formula A implies the contradiction"), by using a proposi- tional constant False to encode the contradiction. Thus, the inference rules for negation can be taken as special cases of implication rules, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. As an illustration, let us consider the following sentence pair:</p><p>A: No man is singing. B: There is a man singing loudly. <ref type="figure" target="#fig_4">Figure 5</ref> shows the proof process. The sentences A and B are mapped to P 0 and P 1 , respectively, via compositional semantics and the goal G 0 is set to False. By decomposing P 1 using elimination rules and then by combining P 2 , P 3 , and P 4 , we can obtain P 6 . From P 0 and P 6 we can then derive the contradiction.</p><p>These proofs are performed by an automated prover implemented on Coq, using tactics for first- order theorem proving. When a proof is success- ful, Coq outputs the resulting proof (a proof term), from which we can extract detailed information such as the number of proof steps and the types of inference rules used. In addition to the entail- ment/contradiction result, information about the proof process is used to create features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Description of the Features</head><p>To maximize accuracy when learning textual sim- ilarity, we adopt a hybrid approach that uses both logic-based features extracted from the natural de- duction proof and other, non-logic-based features. All features are scaled to the [0, 1] range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Logic-based Features</head><p>We propose 15 features consisting of nine different types of logic-based features. Six of these feature types are derived from the bidirectional natural de- duction proofs: six features are extracted from the direct proof (A ′ ⇒ B ′ ) and another six from the reverse proof (B ′ ⇒ A ′ ). The remaining three feature types are derived from semantic represen- tations of the sentence pairs. The feature types are as follows. Logical inference result. As stated in Section 4, we include features to distinguish the case where either the conclusion or its negation can be proved P0 : ¬∃e1∃x1(man(x1) ∧ sing(e1) ∧ (subj(e1) = x1)) P1 : ∃e1∃x1(man(x1) ∧ sing(e1) ∧ (subj(e1) = x1) ∧ loudly(e1)) G0 : False ∃-ELIM, ∧-ELIM (P2) P2 : man(x1), P3 : sing(e1), P4 : subj(e1) = x1, P5 : loudly(e1) from the one where neither can be proved. If the conclusion can be proved, the feature is set to 1.0. If the negation of the conclusion can be proved, the feature is set to 0.5. If neither can be proved, the feature is set to 0.0. Axiom probabilities. The probability of an ax- iom and the number of axioms appearing in the proof are used to create features. The probability of an axiom is defined as the inverse of the length of the shortest path that connects the senses in the is-a (hypernym/hyponym) taxonomy in WordNet. When multiple axioms are used in the proof, the average of the probabilities of the axioms is ex- tracted as a feature. If the proof can be completed without using axioms, the feature is set to 1.0. Proved sub-goals. Given that proofs can be ob- tained either by proving all the sub-goals or skip- ping unproved sub-goals, we use the proportion of proved sub-goals as a feature. Our assumption is that if there are more unproved sub-goals then the sentence pair is less similar. When there are m logical formulas in the premise pool and n proved sub-goals, we set the feature to n/m. If the theo- rem can be proved without skipping any sub-goals, the feature is set to 1.0. It may be the case that the number of sub-goals is so large that some sub- goals remain unproved even after axiom injection. Since the proportion of unproved sub-goals is de- creased by axiom injection, we use the proportion of unproved sub-goals both with and without ax- iom injection as features. Cases in unproved sub-goals. Subject or object words can affect the similarity of sentence pairs. Therefore, the number of each case in unproved sub-goals, like subj(e 1 ) in <ref type="figure" target="#fig_2">Figures 3 and 5</ref>, is used as a feature. Here, we count subjective, ob- jective, and dative cases. Proof steps. In general, complex theorems are dif- ficult to prove and in such cases the sentence pairs are considered to be less similar. We therefore use the number of Coq's proof steps, namely the num- ber of inference rule applications in a given proof, as a feature. Inference rules. The complexity of a natural de- duction proof can be measured in terms of the in- ference rules used for each proof step. We there- fore extract the relative frequency with which each inference rule is used in the proof as a feature. We check seven inference rules for natural deduction using Coq (cf. <ref type="figure">Figure 2</ref>): introduction and elimi- nation rules for conjunction (∧-INTRO, ∧-ELIM), im- plication (→-INTRO, →-ELIM), and existential quan- tification (∃-INTRO, ∃-ELIM), and the elimination rule for equality (=-ELIM). Predicate overlap. Intuitively, the more predi- cates overlap between the premise and the conclu- sion, the more likely it is that the inference can be proved. We therefore use the proportion of pred- icates that overlap between the premise and the conclusion as a feature. Semantic type overlap. Each semantic represen- tation in higher-order logic has a semantic type, such as Entity for entities and Prop for proposi- tions. As with predicates, we use the degree of se- mantic type overlap between the premise and the conclusion as a feature. Existence of negative clauses. Whether or not the premise or conclusion contain negative clauses is an effective measure of similarity. In semantic rep- resentations, negative clauses are represented by the negation operator ¬, so we check for negation operators in the premise and the conclusion and set this feature to 1.0 if either contains one.</p><formula xml:id="formula_4">G : ¬A ¬-INTRO P : A G : False P1 : ¬A P2 : A ¬-ELIM P : False</formula><formula xml:id="formula_5">∃-INTRO, ∧-INTRO (P2) P6 : ∃e1∃x1(man(x1) ∧ sing(e1) ∧ subj(e1) = x1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Non-logic-based Features</head><p>We also use the following eight non-logic-based features. Noun/verb overlap. We extract and lemmatize all nouns and verbs from the sentence pairs and use the degrees of overlap of the noun and verb lem- mas as features. Part-of-speech overlap.</p><p>We obtain part-of- speech (POS) tags for all words in the sentence pairs by first tokenizing them with the Penn Tree- bank Project tokenizer 1 and then POS tagging them with C&amp;C POS tagger <ref type="bibr" target="#b11">(Curran and Clark, 2003)</ref>. The degree of overlap between the sen- tences' POS tags is used as a feature.</p><p>Synset overlap. For each sentence in the pair, we obtain the set containing all the synonym lemmas (the synset) for the words in the sentence. The degree of overlap between the sentences' synsets is used as a feature.</p><p>Synset distance. For each word in the first sen- tence, we compute the maximum path similarity between its synset and the synset of any other word in the second sentence. Then, we use the average of maximum path similarities as a feature.</p><p>Sentence length. If the conclusion sentence is long, there will possibly be many sub-goals in the proof. We therefore use the average of the sen- tence lengths and the difference in length between the premise and the conclusion sentences as fea- tures.</p><p>String similarity. We use the similarity of the se- quence of characters within the sentence pairs as a feature. The Python Difflib 2 function returns the similarity between two sequences as a floating- point value in <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. This measure is given by 2.0 * M/T , where T is the total number of ele- ments in both sequences and M is the number of matches. This feature is 1.0 if the sequences are identical and 0.0 if they have nothing in common.</p><p>Sentence similarity from vector space models. We calculate sentence similarity by using three major vector space models, TF-IDF, latent se- mantic analysis (LSA) <ref type="bibr" target="#b12">(Deerwester et al., 1990)</ref>, and latent Dirichlet allocation (LDA) ( <ref type="bibr" target="#b9">Blei et al., 2003)</ref>. We use these cosine similarities as features.</p><p>Existence of passive clauses. Passive clauses have an influence on similarity. In CCG trees, passive clauses are represented using the syntactic category S pss \N P . We check for the occurrence of passive clauses in the premise and conclusion, and if either of them contains a passive clause then the feature is set to 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence1</head><p>Sentence2 Entailment Score 23</p><p>There is no biker jumping in the air. A lone biker is jumping in the air no 4.2 1412</p><p>Men are sawing logs. Men are cutting wood. yes 4.5 9963 The animal is grazing on the grass. The cop is sitting on a police bike. unknown 1 <ref type="table">Table 1</ref>: Examples in the SICK dataset with different entailment labels and similarity scores.  6 Experiments and Evaluation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Conditions</head><p>We evaluated our system 3 using two datasets: the SemEval-2014 version of the SICK dataset ( <ref type="bibr" target="#b19">Marelli et al., 2014</ref>) and the SemEval- 2012 version of the MSR-paraphrase video corpus dataset (MSR-vid) ( <ref type="bibr" target="#b2">Agirre et al., 2012</ref>). The experimental conditions were as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">The SICK dataset</head><p>The SICK dataset is a dataset for studying STS as well as for recognizing textual entailment (RTE). It was originally developed for evaluating com- positional distributional semantics, so it contains logically challenging expressions such as quan- tifiers, negations, conjunctions and disjunctions. The dataset contains 9927 sentence pairs with a 5000/4927 training/test split. These sentence pairs are manually annotated with three types of labels yes (entailment), no (contradiction), or unknown (neutral) as well as a semantic relatedness scores in <ref type="bibr">[1,</ref><ref type="bibr">5]</ref> (see <ref type="table">Table 1</ref> for a sample). In this dataset, sentence pairs whose gold entail- ment labels are no tend to be scored a little more highly than the average, whereas those whose la- bels are unknown have a wide range of scores. Thus, we set the baseline of the relatedness score to 5 when the gold entailment label was yes and to 3 when the label was no or unknown.</p><p>We compared our system with the fol- lowing systems: the state-of-the-art neural network-based system <ref type="bibr" target="#b28">(Mueller and Thyagarajan, 2016</ref>); the best system ( <ref type="bibr" target="#b34">Zhao et al., 2014</ref>) from SemEval-2014; and two of the logic- 3 Available at https://github.com/mynlp/ccg2lambda. based systems stated in Section 2: namely The Meaning Factory ( <ref type="bibr" target="#b8">Bjerva et al., 2014</ref>) and UTexas <ref type="figure" target="#fig_1">(Beltagy et al., 2014b)</ref>. The Pearson cor- relation coefficient γ, Spearman's rank correlation coefficient ρ, and the MSE were used as the eval- uation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">The MSR-vid dataset</head><p>The MSR-vid dataset is our second dataset for the STS task and contains 1500 sentence pairs with a 750/750 training/test split. All sentence pairs are annotated with semantic relatedness scores in the range <ref type="bibr">[0,</ref><ref type="bibr">5]</ref>. We used this dataset to compare our system with the best system from SemEval- 2012 <ref type="bibr">(Bär et al., 2012</ref>) and the logic-based UTexas system ( <ref type="bibr" target="#b5">Beltagy et al., 2014a</ref>). We used the Pear- son correlation coefficient γ as the evaluation met- ric. <ref type="table" target="#tab_1">Table 2</ref> shows the results of our experiments with the SICK dataset. Although the state-of-the-art neural network-based system yielded the best re- sults overall, our system achieved higher scores than SemEval-2014 submissions, including the two logic-based systems (The Meaning Factory and UTexas), in terms of Pearson correlation and Spearman's correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>The main reason for our system's lower per- formance in terms of MSE is that some theorems could not be proved because of a lack of lexical knowledge. In the current work, we only consider word-level knowledge (word-for-word paraphras- ing); we may expand the knowledge base in the future by using more external resources.</p><p>As we mentioned above, the sentence pairs an- notated as unknown produced a wide range of scores. The Pearson correlation of the unknown portion of the SICK dataset was 0.766, which sug- gests that our logic-based system can also be ap- plied to neutral sentence pairs. <ref type="table">Table 3</ref> shows the results of our experiments with the MSR-vid dataset. These results also in- dicate that our logic-based system achieved higher accuracy than the other logic-based systems. γ SemEval2012 Best Score 0.873 Our system 0.853 <ref type="bibr">Beltagy et al. (2014)</ref> 0.830 <ref type="table">Table 3</ref>: Results on the test split of MSR-vid.  <ref type="table">Table 4</ref>: Results when training our regressor with each feature group in isolation. <ref type="table">Table 4</ref> shows evaluation results for each feature group in isolation, showing that inference rules and predicate overlaps are the most effective fea- tures. Compared with the non-logic-based fea- tures, the logic-based features achieved a slightly higher accuracy, a point that will be analyzed in more detail in the next section. Overall, our re- sults show that combining logic-based features with non logic-based ones is an effective method for determining textual similarity. <ref type="table">Table 5</ref> shows some examples for which the pre- diction score was better when using logic-based features than when using non-logic-based ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Positive examples and error analysis</head><p>For IDs 642 and 1360, one sentence contains a passive clause while the other sentence does not. In such cases, the sentence pairs are not superfi- cially similar. By using logical formulas based on event semantics we were able to interpret the sen- tence containing the passive clause correctly and judge that the passive and non-passive sentences are similar to each other.</p><p>In ID 891, one sentence contains a negative clause while the other does not. Using shallow features, the word overlap is small and the predic- tion score was much lower than the correct score. Our logic-based method, however, interpreted the first sentence as a negative existential formula of the form ¬∃xP(x) and the second sentence as an existential formula ∃xP ′ (x). Thus, it could easily handle the semantic difference between the posi- tive and negative sentences.</p><p>In ID 1158, by contrast, the proportion of word overlap is so high that the prediction score with non-logic-based features was much higher than the correct score. Our method, however, was able to prove the contradiction using an antonym axiom of the form ∀x(remove(x) → ¬add(x)) from WordNet and thus predict the score correctly.</p><p>In ID 59, the proportion of word overlap is low, so the prediction score with non-logic-based features was lower than the correct score. Our method, however, was able to prove the partial en- tailment relations for the sentence pair and thus predict the score correctly. Here the logic-based method captured the common meaning of the sen- tence pair: both sentences talk about the kids play- ing in the leaves.</p><p>Finally, in ID 71, the prediction score with non- logic-based features was much higher than the cor- rect score. There are two reasons for this phe- nomenon: negations tend to be omitted in non- logic-based features such as TF-IDF and the pro- portion of word overlap is high. However, as logical formulas and proofs can handle negative clauses correctly, our method was able to predict the score correctly. <ref type="table" target="#tab_2">Table 6</ref> shows examples where using only logic- based features produced erroneous results. In ID 3974, the probability of axiom ∀x(awaken(x) → up(x)) was low (0.25) and thus the prediction score was lower than the correct score. Likewise, in ID 4833, the probability of axiom ∀x(file(x) → do(x)) was very low (0.09) and thus the pre- diction score was negatively affected. In these cases, we need to consider phrase-level axioms such as ∀x(awaken(x) → wake up(x)) and ∀x(file nail(x) → do manicure(x)) using a paraphrase database. This, however, is an issue for future study. In ID 1941, the system wrongly proved the bidirectional entailment relations by Pred Pred</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID Sentence Pair</head><p>Gold +logic -logic Entailment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>642</head><p>A person is climbing a rock with a rope, which is pink. 5.0 4.9 4.1 Yes A rock is being climbed by a person with a rope, which is pink.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1360</head><p>The machine is shaving the end of a pencil. 4.7 4.6 3.8 Yes A pencil is being shaved by the machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>891</head><p>There is no one on the shore. 3.6 3.7 2.6 No A bunch of people is on the shore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1158</head><p>A woman is removing ingredients from a bowl. 3.3 3.5 4.1 No A woman is adding ingredients to a bowl.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>59</head><p>Kids in red shirts are playing in the leaves. 3.9 3.8 3.1 Unknown Three kids are jumping in the leaves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>71</head><p>There is no child lying in the snow and making snow angels.</p><p>3.3 3.3 4.1 Unknown Two people in snowsuits are lying in the snow and making snow angels. <ref type="table">Table 5</ref>: Examples for which our regressor trained only with logic-based features performs better than when using non-logic features. "Gold": correct score, "Pred+logic": prediction score only with logic- based features, "Pred-logic": prediction score only with non-logic-based features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID Sentence Pair</head><p>Gold System Axiom 3974 A girl is awakening.  adding external axioms, so the prediction score was much higher than the correct score. Set- ting the threshold for the probability of an axiom may be an effective way of improving our axiom- injection method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have developed a hybrid method for learn- ing textual similarity by combining features based on logical proofs of bidirectional entailment rela- tions with non-logic-based features. The results of our experiments on two datasets show that our system was able to outperform other logic-based systems. In addition, the results show that infor- mation about the natural deduction proof process can be used to create effective features for learning textual similarity. Since these logic-based features provide accuracy improvements that are largely additive with those provided by non-logic-based features, neural network-based systems may also benefit from using them.</p><p>In future work, we will refine our system so that it can be applied to other tasks such as ques- tion answering. Compared with neural network- based systems, our natural deduction-based sys- tem can not only assess how similar sentence pairs are, but also explain what the sources of simi- larity/dissimilarity are by referring to information about sub-goals in the proof. Given this interpreta- tive ability, we believe that our logic-based system may also be of benefit to other natural language processing tasks, such as question answering and text summarization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>CCG</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: System overview.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The proof process for the example entailment relation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Inference rules of negation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Proof process for the contradiction example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>A girl is doing a manicure. ∀x(file(x) → do(x)) 1941 A woman is putting the baby into a trash can. A person is putting meat into a skillet. 1.0 3.3 ∀x(woman(x) → person(x)) ∀x(trash(x) → skillet(x)) ∀x(baby(x) → meat(x))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Results on the test split of SICK dataset.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 6 :</head><label>6</label><figDesc>Error examples when training the regressor only with logic-based features.</figDesc><table></table></figure>

			<note place="foot" n="1"> ftp://ftp.cis.upenn.edu/pub/treebank/public html/ tokenization.html 2 https://docs.python.org/3.5/library/difflib.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the three anonymous reviewers for their detailed comments. This work was supported by JST CREST Grant Number JPMJCR1301, Japan.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A tableau prover for natural logic and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasha</forename><surname>Abzianidze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP-15)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP-15)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2492" to="2502" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Natural solution to FraCaS entailment problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasha</forename><surname>Abzianidze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the 5th Joint Conference on Lexical and Computational Semantics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="64" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SemEval-2012 Task 6: A 689 pilot on semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval-2012)</title>
		<meeting>the 6th International Workshop on Semantic Evaluation (SemEval-2012)<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="385" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">UKP: Computing semantic textual similarity by combining multiple content similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bär</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval-2012)</title>
		<meeting>the Sixth International Workshop on Semantic Evaluation (SemEval-2012)<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="435" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Contextpassing and underspecification in dependent type semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Bekki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koji</forename><surname>Mineshima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Modern Perspectives in Type Theoretical Semantics, Studies of Linguistics and Philosophy</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11" to="41" />
		</imprint>
	</monogr>
	<note>Stergios Chatzikyriakidis and Zhaohui Luo</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Probabilistic soft logic for semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Islam Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL-2014)</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (ACL-2014)<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1210" to="1219" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">UTexas: Natural language semantics using distributional semantics and probabilistic logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Islam</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014)</title>
		<meeting>the 8th International Workshop on Semantic Evaluation (SemEval-2014)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="796" to="801" />
		</imprint>
		<respStmt>
			<orgName>Association for Computational Linguistics and Dublin City University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Interactive Theorem Proving and Program Development: Coq&apos;Art The Calculus of Inductive Constructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Bertot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Castran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Meaning Factory: Formal semantics for recognizing textual entailment and determining semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Bjerva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Van Der Goot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malvina</forename><surname>Nissim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014)</title>
		<meeting>the 8th International Workshop on Semantic Evaluation (SemEval-2014)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="642" to="646" />
		</imprint>
		<respStmt>
			<orgName>Association for Computational Linguistics and Dublin City University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Widecoverage efficient statistical parsing with CCG and log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="493" to="552" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Investigating GIS and smoothing for maximum entropy taggers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth conference on European chapter</title>
		<meeting>the tenth conference on European chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Experimental support for a categorical compositional distributional model of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP-2011)</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP-2011)<address><addrLine>Edinburgh, Scotland, UK. Asso</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1394" to="1404" />
		</imprint>
	</monogr>
	<note>ciation for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning distributed representations of sentences from unlabelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1367" to="1377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Resolving lexical ambiguity in tensor regression models of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL-2014)</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (ACL-2014)<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="212" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributional inclusion hypothesis for tensor-based composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics: Technical Papers (COLING-2016)</title>
		<meeting>the 26th International Conference on Computational Linguistics: Technical Papers (COLING-2016)<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2849" to="2860" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
	<note>ICML-2014)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A* CCG parsing with a supertag-factored model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP-2014)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP-2014)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="990" to="1000" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A SICK cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC-2014)</title>
		<meeting>the 9th International Conference on Language Resources and Evaluation (LREC-2014)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
	<note>European Language Resources Association</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">2016. ccg2lambda: A 690 compositional semantics system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascual</forename><surname>Martínez-Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koji</forename><surname>Mineshima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Bekki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-2016 System Demonstrations</title>
		<meeting>ACL-2016 System Demonstrations<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="85" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On-demand injection of lexical knowledge for recognising textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascual</forename><surname>Martínez-Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koji</forename><surname>Mineshima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Bekki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2017)</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2017)<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="710" to="720" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Some uses of higher-order logic in computational linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopalan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nadathur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 24th Annual Meeting of the Association for Computational Linguistics<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1986" />
			<biblScope unit="page" from="247" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">WordNet: A lexical database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Higher-order logical inference with compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koji</forename><surname>Mineshima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascual</forename><surname>Martínez-Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Bekki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP2015)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP2015)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2055" to="2061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Building compositional semantics and higher-order inference system for a wide-coverage Japanese CCG parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koji</forename><surname>Mineshima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ribeka</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascual</forename><surname>Martínezgómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Bekki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2236" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08)</title>
		<meeting>the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08)<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="236" to="244" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1429" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Siamese recurrent architectures for learning sentence similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Thyagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI-2016)</title>
		<meeting>the 30th AAAI Conference on Artificial Intelligence (AAAI-2016)<address><addrLine>Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2786" to="2792" />
		</imprint>
	</monogr>
	<note>Association for the Advancement of Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Events in The Semantics of English: a Study in Subatomic Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terence</forename><surname>Parsons</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An exploration of discourse-based sentence spaces for compositional distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Polajnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Linking Computational Models of Lexical, Sentential and Discourse-level Semantics</title>
		<meeting>the 1st Workshop on Linking Computational Models of Lexical, Sentential and Discourse-level Semantics<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Natural Deduction-A ProofTheoretical Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dag</forename><surname>Prawitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Almqvist &amp; Wiksell</title>
		<imprint>
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<title level="m">The Syntactic Process</title>
		<meeting><address><addrLine>Cambridge, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Vector space model of information retrieval: A reevaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K M</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">V</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 7th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1984" />
			<biblScope unit="page" from="167" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ECNU: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014)</title>
		<meeting>the 8th International Workshop on Semantic Evaluation (SemEval-2014)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="271" to="277" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics and Dublin City University</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
