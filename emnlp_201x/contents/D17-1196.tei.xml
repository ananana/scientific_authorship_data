<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Quantum Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivano</forename><surname>Basile</surname></persName>
							<email>ivano.basile@sns.it</email>
							<affiliation key="aff0">
								<orgName type="institution">Scuola Normale Superiore</orgName>
								<address>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tamburini</surname></persName>
							<email>fabio.tamburini@unibo.it</email>
							<affiliation key="aff1">
								<orgName type="institution">FICLIT -University of Bologna</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Quantum Language Models</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1840" to="1849"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents a new approach for building Language Models using the Quantum Probability Theory, a Quantum Language Model (QLM). It mainly shows that relying on this probability calculus it is possible to build stochastic models able to benefit from quantum correlations due to interference and entanglement. We extensively tested our approach showing its superior performances, both in terms of model perplexity and inserting it into an automatic speech recognition evaluation setting, when compared with state-of-the-art language modelling techniques.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Quantum Mechanics Theory (QMT) is one of the most successful theories in modern science. De- spite its effectiveness in the physics realm, the at- tempts to apply it in other domains remain quite limited, excluding, of course, the large quantity of studies regarding Quantum Information Process- ing on quantum computers.</p><p>Only in recent years some scholars tried to em- body principles derived from QMT into their spe- cific fields, for example, by the Information Re- trieval community ( <ref type="bibr" target="#b36">Zuccon et al., 2009;</ref><ref type="bibr" target="#b16">Melucci and van Rijsbergen, 2011;</ref><ref type="bibr" target="#b8">González and Caicedo, 2011;</ref><ref type="bibr" target="#b15">Melucci, 2015)</ref> and in the domain of cog- nitive sciences and decision making <ref type="bibr" target="#b12">(Khrennikov, 2010;</ref><ref type="bibr" target="#b4">Busemeyer and Bruza, 2012;</ref><ref type="bibr" target="#b0">Aerts et al., 2013</ref>). In the machine learning field ( <ref type="bibr" target="#b1">Arjovsky et al., 2016;</ref><ref type="bibr" target="#b34">Wisdom et al., 2016;</ref><ref type="bibr" target="#b10">Jing et al., 2017)</ref> have used unitary evolution matrices for building deep neural networks obtaining interesting results, but we have to observe that their works do not ad- here to QMT and use unitary evolution operators in a way not allowed by QMT. In recent years, also the Natural Language Processing (NLP) commu- nity started to look at QMT with interest and some studies using it have already been presented ( <ref type="bibr" target="#b3">Blacoe et al., 2013;</ref><ref type="bibr" target="#b13">Liu et al., 2013;</ref><ref type="bibr" target="#b32">Tamburini, 2014;</ref><ref type="bibr" target="#b11">Kartsaklis et al., 2016)</ref>.</p><p>Language models (LM) are basic tools in NLP used in various applications, such as Automatic Speech Recognition (ASR), machine translation, part-of-speech tagging, etc., and were traditionally modeled by using N-grams and various smoothing techniques. Among the dozen of tools for comput- ing N-gram LM, we will refer to CMU-SLM (with Good-Turing smoothing) <ref type="bibr" target="#b5">(Clarkson and Rosenfeld, 1997</ref>) and IRSTLM (with Linear Witten-Bell smoothing) <ref type="bibr" target="#b6">(Federico et al., 2008)</ref>; the latter is the tool used in <ref type="bibr">Kaldi (Povey et al., 2011b</ref>), one of the most powerful and used open-source ASR pack- age that we will use for some of the experiments presented in the following sections.</p><p>In recent years new techniques from the Neural Networks (NN) domain have been introduced in order to enhance the performances of such models. Elman recurrent NN, as used in the RNNLM tool <ref type="bibr" target="#b18">(Mikolov et al., 2010</ref><ref type="bibr" target="#b19">(Mikolov et al., , 2011</ref>), or Long Short-Term Memory NN, as in the tool LSTMLM <ref type="bibr" target="#b29">(Soutner and Müller, 2015)</ref>, produce state-of-the-art perfor- mances for current language models.</p><p>This paper presents a different approach for building LM based on quantum probability the- ory. Actually, we present a QLM applicable only to problems defined on a small set of different to- kens. This is a "proof-of-concept" study and our main aim is to show the potentialities of such ap- proach rather than building a complete application for solving this problem for any setting.</p><p>The paper is organized as follows: we provide background on Quantum Probability Theory in Section 2 followed by the description of our pro- posed Quantum Language Model in Section 3. We then discuss some numerical issues mainly related to the optimisation procedure in Section 4, and in Section 5 we present the experiments we did to validate our approach. In Section 6 we discuss our results and draw some provisional conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Quantum Probability Theory</head><p>In QMT the state of a system is usually described, in the most general case, by using density matrices over an Hilbert space H. More specifically, a den- sity matrix ρ is a positive semidefinite Hermitian matrix of unit trace, namely ρ † = ρ, Tr(ρ) = 1, and it is able to encode all the information about the state of a quantum system 1 .</p><p>The measurable quantities, or observables, of the quantum system are associated to Hermitian matrices O defined on H. The axioms of QMT specify how one can make predictions about the outcome of a measurement using a density matrix:</p><p>• the possible outcomes of a projective mea- surement of an observable O are its eigenval- ues {λ j };</p><p>• the probability that the outcome of the mea- surement is λ j is P (λ j ) = Tr(ρΠ λ j ) = Tr(Π λ j ρ), where Π λ j is the projector on the eigenspace of O associated to λ j . Note that in the following we will use some proper- ties of these kind of measurements, namely</p><formula xml:id="formula_0">Π † λ j = Π λ j and Π 2 λ j = Π λ j ;</formula><p>• after the measurement the system state col- lapses in the following fashion: if the out- come of the measurement was λ j , the col- lapse is</p><formula xml:id="formula_1">ρ = Π λ j ρΠ λ j Tr(Π λ j ρΠ λ j )</formula><p>where the denominator is needed for trace normalization;</p><p>• time evolution of states using a fixed time step is described by a unitary matrix U over H, i.e. U † U = I, where I is the identity ma- trix. Given a state ρ t , at a specific time t, the system evolution without measurements modifies the state as:</p><formula xml:id="formula_2">ρ t+1 = U ρ t U † .</formula><p>See for example <ref type="bibr" target="#b21">(Nielsen and Chuang, 2010)</ref> or <ref type="bibr" target="#b33">(Vedral, 2007</ref>) for a complete introduction on QPT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Quantum Language Models</head><p>In this section we describe our approach to build QLM that can compute probabilities for the oc- currence of a sequence w = (w 1 , w 2 , ..., w n ) of length n, composed using N different symbols, the vocabulary containing all the words in the model, i.e. for every symbol w in the sequence w ∈ {0, ..., N − 1}. We define a set of orthogonal N -dimensional vectors {e w : w ∈ {0, ..., N −1}}, spanning the complex space H = C N ; to measure the probability of a symbol w, collapsing the state over the space spanned by e w , we use the projec- tor Π w = e w e † w . Note that all the words in the vocabulary have been encoded as numbers corre- sponding to the N dimensions of the vector space H.</p><p>Our method is sequential, from QMT point of view, in the sense that we use a quantum system that produces a single symbol upon measurement.</p><p>The basic idea is that the probabilistic informa- tion for a given sequence w = (w 1 , w 2 , ..., w n ) is encoded in the density matrix that results from the following process:</p><formula xml:id="formula_3">• Inititalisation Cond.Prob.: P (w 1 ; ρ 0 , U ) = Tr(ρ 0 Π w 1 ) Projection: ρ 1 = Πw 1 ρ 0 Πw 1 Tr(Πw 1 ρ 0 Πw 1 ) Evolution: ρ 1 = U ρ 1 U † • Recurrence (i = 2, .., n) Cond.Prob.: P (w i |w 1 , ..., w i−1 ; ρ 0 , U ) = Tr(ρ i−1 Π w i ) Projection: ρ i = Πw i ρ i−1 Πw i Tr(Πw i ρ i−1 Πw i ) Evolution: ρ i = U ρ i U † • Termination P (w|ρ 0 , U ) = P (w 1 ; ρ 0 , U ) · n i=2 P (w i |w 1 , ..., w i−1 ; ρ 0 , U )</formula><p>The total probability P (w|ρ 0 , U ) for the given sequence is thus obtained, in the termination step, by multiplying the conditional probability P (w i |w 1 , ..., w i−1 ; ρ 0 , U ) for each word in the se- quence.</p><p>We then use the initial density matrix ρ 0 and the time evolution unitary matrix U as parameters to optimise the perplexity Γ, evaluated on a training corpus of sequences S,</p><formula xml:id="formula_4">Γ(ρ 0 , U ) = exp − 1 C w∈S log P (w|ρ 0 , U )</formula><p>which quantifies the uncertainty of the model. C is the number of tokens in the corpus.</p><p>Minimising Γ is equivalent of learning a model by fixing all the model parameters, a typical pro- cedure in the machine learning domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Ancillary system</head><p>The problem with this setup is that the 'quan- tum effects' are completely washed out by the measurements on the system by using projec- tors. The resulting expression for the probability P (w|ρ 0 , U ) for a sequence w is identical to that obtained using a classical Markov model.</p><p>To solve this issue, our approach is to avoid the complete collapse of the state after each symbol measurement using a common technique in QMT: we introduce an ancillary system described by a fictitious D-dimensional Hilbert space, H ancilla = C D , and we couple the original system to the an- cillary system. The resulting DN -dimensional Hilbert space is</p><formula xml:id="formula_5">H 2 ≡ H ancilla ⊗ H = C DN</formula><p>where ⊗ denotes the Kronecker product for matri- ces and D can be seen as a free hyper-parameter of the model. On this new space the projectors are now given by Π (2)</p><formula xml:id="formula_6">w = I D ⊗ Π w , where I D is the D-dimensional identity matrix.</formula><p>The advantage of using this method is that the time evolution for the coupled system creates non- trivial correlations between the two entangled sys- tems such that measuring and collapsing the sym- bol state keeps some information about the whole sequence stored in the ancillary part of the state. This information is then reshuffled into the symbol state via time evolution, resulting in a 'memory ef- fect' that takes the whole sequence of symbols into account, thereby extending the idea behind the N- grams approach. Larger D values will results in more memory of this system and, of course, in a larger number of parameters to learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">System evolution</head><p>We need to specify the system evolution for our coupled system. The simplest approach is to use a unitary DN × DN matrix U that acts on the en- tangled Hilbert space as shown before; it can be specified by (DN ) 2 real parameters with a suit- able parametrization <ref type="bibr" target="#b30">(Spengler et al., 2010</ref>) that ensures the unitarity of U . However, in our pre- liminary experiments this approach resulted in an insufficient 'memory' capability for the QLM and in a very complex and slow minimisation proce- dure.</p><p>A different approach could be introduced by us- ing a specific unitary matrix for each word, but this would lead to an enormous amount of parameters to learn with the optimization procedure.</p><p>There are a lot of techniques in NLP to repre- sent single words with dense vectors (see for ex- ample ( <ref type="bibr" target="#b17">Mikolov et al., 2013</ref>) for the so called word embeddings). Following this idea, we can repre- sent every symbol in our system with a specific p- dimensional vector trained using one of the avail- able techniques w → (α 1 (w), ..., α p (w)) or fixed randomly.</p><p>We then work with a set of p DN ×DN unitary matrices U = (U 1 , ..., U p ), one for each compo- nent of the word vector, that are used to dynami- cally build a different system evolution matrix for each word in this way:</p><formula xml:id="formula_7">V (w) ≡ p i=1 U α i (w) i</formula><p>This results in p(DN ) 2 complex or 2p(DN ) 2 real parameters to be learned.</p><p>Essentially, we treat the words in our problem in different ways: the evolution operator for each word V (w) is build by using a combination of the operators U defined for each word-vector compo- nent, while, considering the system projection, we treat each word as one basis vector for the space H.</p><p>Note that the choice to use a set {V (w)} of operators, one for each word w, does not violate the linearity of quantum mechanics: let K be the quantum operation</p><formula xml:id="formula_8">K(ρ) = w V (w)Π (2) w ρ Π (2) w V † (w)</formula><p>defined using projectors and evolution matrices. Then K is a valid (i.e. a Completely Positive Trace-preserving) evolution map that exactly re- produces our results in the sequence of evolutions and collapses. The number of evolutionary operators is a trade- off: as we said before, defining only one op- erator U resulted in a poor performance of the proposed method in all the relevant experiments, while defining an operator for each word would produce too many parameters to be learned. The trade-off that we chose is to use one operator for each word-vector component, and build the set {V (w)} from them as described above while pre- serving unitarity.</p><p>With regard to the initial density matrix ρ 0 , we have to define it combining the initial density ma- trix of our system, ρ s 0 , and the initial density ma- trix of the ancilla, ρ a 0 . We defined ρ s 0 as a diagonal N × N matrix containing the classical Maximum Likelihood probability Estimation to have a spe- cific symbol at the first sequence position:</p><formula xml:id="formula_9">ρ s 0 = 1 |S| w∈S Π w 1</formula><p>where S is again the set of all sequences in the training set and w 1 is the first word in each se- quence w. With regard to the ancilla system we do not know anything about it and thus we have to define</p><formula xml:id="formula_10">ρ a 0 as the D × D diagonal matrix ρ a 0 = I D Tr(I D )</formula><p>.</p><p>Consequently we can define ρ 0 as</p><formula xml:id="formula_11">ρ 0 = ρ a 0 ⊗ ρ s 0 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The final model</head><p>Putting all the ingredients together, we can fi- nally write down the formula for the probability P (w|ρ 0 , U) for a sequence w in the QLM speci- fied by ρ 0 and U. The product of conditional prob- abilities simplifies because of the normalising de- nominators added at each collapse and time evolu- tion step. The result is:</p><formula xml:id="formula_12">P (w|ρ 0 , U) = Tr(Π (2) wn ...V † (w 2 )Π (2) w 2 V † (w 1 ) Π (2) w 1 ρΠ (2) w 1 V (w 1 )Π (2) w 2 V (w 2 )...Π (2) wn )<label>(1)</label></formula><p>Using the fact that projectors have many zero en- tries one can also re-express this trace of the prod- uct of DN × DN matrices in terms of the trace of the product of D × D matrices. The formula for P (w|ρ 0 , U) then simplifies to our final result</p><formula xml:id="formula_13">P (w|ρ 0 , U) = Tr(T † RT )<label>(2)</label></formula><p>where the matrices R and T are defined as follows:</p><p>• in terms of entries R i,j with indices i, j = 0, ..., D − 1, the matrix R is given by</p><formula xml:id="formula_14">R i,j = [ρ 0 ] N i+w 1 ,N j+w 1 .</formula><p>Note that only the value of first symbol in the sequence, w 1 , enters in the expression. This is to be expected since R derives from the ini- tial density matrix ρ 0 ;</p><p>• analogously, the matrix T that encodes the chain of combined collapses and time evolutions is given by the product T = T (2) T (3) ...T (n) , where the matrices T (k) are given in entries, with indices i, j = 0, ..., D− 1, by</p><formula xml:id="formula_15">T (k) i,j = [V (w k−1 )] N i+w k−1 ,N j+w k .</formula><p>These matrices can be pre-calculated for ev- ery pair of the involved symbols, so that the calculation of P (w|ρ 0 , U) for all the se- quences will be very fast.</p><p>The detailed calculation for obtaining the equation (2) can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Optimisation and Numerical Issues</head><p>In order to optimise the parameters U we numer- ically minimise the perplexity Γ computed on a given training corpus of sequences S. This re- quires that the matrices U remain strictly unitary at every step of the minimisation procedure and it can be accomplished in various ways.</p><p>The most straightforward way is to employ an explicit parametrization for unitary matrices, as was done in <ref type="bibr" target="#b30">(Spengler et al., 2010</ref>). Due to the transcendental functions employed in this parametrisation, this approach resulted in a func- tional form for Γ that has proven to be very chal- lenging to minimise efficiently in our experiments.</p><p>A more elegant and efficient approach is to con- sider the entries of U as parameters (thereby en- suring a polynomial functional form for Γ) and to employ techniques of differential geometry to keep the parameters from leaving the unitary sub- space at each minimisation step. This can be done using a modification of the approach outlined in <ref type="bibr" target="#b31">(Tagare, 2011</ref>) that considers the unitary matri- ces subspace as a manifold, the Stiefel manifold U(DN ). It is then possible to project the gradient f of a generic function f (M ) of the matrix vari- able M on the tangent space of the Stiefel mani- fold and build a line search algorithm that sweeps out curves on this manifold so that at each point the parameters are guaranteed to form a unitary matrix.</p><p>In our case we have multiple unitary matrices U = (U 1 , ..., U p ). This simply results having curves defined on U(DN ) p , parametrised by a p- dimensional vector of DN ×DN unitary matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Formula for the gradient</head><p>To implement the curvilinear search method de- scribed in (Tagare, 2011) one needs an expression for the gradient G = <ref type="figure">(G 1 , .</ref>.., G p ) of the proba- bility function. This gradient is organised in a p- dimensional vector of DN × DN matrices, such that the component G j is obtained by computing the matrix derivative of P (w|ρ 0 , U) with respect to U j either analytically or by applying some nu- merical estimate of the gradients, for example by using finite differences. The latter method, when working with thousands or millions of variables can be very time consuming and, usually, an ex- plicit analytic formula for the gradient accelerates considerably all the required processing.</p><p>A lengthy analytic computation results in an ex- plicit result. Firstly, we introduce the following objects:</p><p>• The spectral decomposition of U j , given by U j = S j D j S † j , guaranteed to exist by the spectral theorem. S j is unitary and the di- agonal matrix D j contains the eigenvalues (u j1 , ..., u jDN ) of U j , j = 1, ..., p.</p><p>• The DN × DN matrices C j (α) defined, in entries, by</p><formula xml:id="formula_16">[C j (α)] ab = u ja α − u jb α u ja − u jb if u ja = u jb [C j (α)] ab = αu ja α−1 if u ja = u jb</formula><p>where u is the complex conjugate of u.</p><p>• The D × DN matrices Q k given in entries by</p><formula xml:id="formula_17">(Q k ) jA = δ N j+w k ,A</formula><p>where j = 0, ..., D − 1, A = 0, ..., DN − 1.</p><p>• The lesser and greater products associated to the construction of system evolution matrices</p><formula xml:id="formula_18">V &lt;j (w) = j−1 i=1 U α i (w) i V &gt;j (w) = n i=j+1 U α i (w) i .</formula><p>With these ingredients, the resulting formula for the components G j of the gradient is</p><formula xml:id="formula_19">G j = 2S j n k=2 S † j V &lt;j (w k−1 ) † Q T k−1 k−1 l=2 T (l) † RT n l=k+1 T (l) † Q k V &gt;j (w k−1 ) † S j · C j (α j (w k−1 )) S † j<label>(3)</label></formula><p>where · denotes the element-wise matrix product. Again, all the detailed calculations for obtaining the analytic expression (3) for the gradient G j can be found in the supplementary material. Using Tagare's method we can project the gra- dient onto the Stiefel manifold and build a curvi- linear search algorithm for the minimisation.</p><p>To achieve this aim, Tagare proposed an Armijo-Wolfe line search inserted into a simple gradient descent procedure. We developed an ex- tension of this algorithm combining the minimiza- tion over the Steifel manifold technique with a Moré-Thuente (1994) line search and a Conju- gate Gradient minimisation algorithm that uses the PolakRibì ere method for the combination of gra- dients and search directions ( <ref type="bibr" target="#b22">Nocedal and Wright, 2006</ref>). All the experiments presented in the next section were performed using these methods.</p><p>The minimisation uses random mini-batches that increase their size during the training: they start with approximately one tenth of the training set dimension and increase to include all the in- stances using a parametrised logistic function. As stopping criterion we used the minimum of the perplexity function over the validation set as sug- gested in <ref type="bibr" target="#b2">(Bengio, 2012;</ref><ref type="bibr" target="#b27">Prechelt, 2012</ref>) for other machine learning techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data</head><p>The TIMIT corpus is a read speech corpus designed to provide speech data for acoustic- phonetic studies and for the development and eval- uation of automatic speech recognition systems ( <ref type="bibr" target="#b7">Garofolo et al., 1990)</ref>. It contains broadband recordings of 630 speakers of eight major dialects of American English and includes time-aligned or- thographic, phonetic and word transcriptions as well as a 16-bit, 16 kHz speech waveform file for each utterance.</p><p>In the speech community, the TIMIT corpus is the base for a standard phone-recognition task with specific evaluation procedures described in detail in ( <ref type="bibr" target="#b14">Lopes and Perdigao, 2011</ref>). We stick completely to this evaluation to test the effective- ness of our proposed model adopting, among the other procedures, the same splitting between the different data sets: the training set contains 3696 utterances (140225 phones), the validation set 400 utterances (15057 phones) and the test set 192 ut- terances (7215 phones).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Results</head><p>We tested the proposed model by setting up two different evaluations: the first is an intrinsic evalu- ation of LM performances in terms of global per- plexity on the TIMIT testset; the second is an extrinsic evaluation in which we replace the LM tools provided with the Kaldi ASR toolkit ( <ref type="bibr" target="#b25">Povey et al., 2011b</ref>) with our model in order to check the final system performances in a phone-recognition task and comparing them with the other state-of- the-art LM techniques briefly introduced in Sec- tion 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Intrinsic evaluation</head><p>The first experiment consisted in an evaluation of models perplexity (PPL) on the TIMIT testset. We compared the QLM model with two N-gram im- plementations, namely CMU-SLM <ref type="bibr" target="#b5">(Clarkson and Rosenfeld, 1997</ref>) and IRSTLM <ref type="bibr" target="#b6">(Federico et al., 2008)</ref>, and two recurrent NN models able to produce state-of-the-art results in language mod- elling, the RNNLM ( <ref type="bibr" target="#b18">Mikolov et al., 2010</ref><ref type="bibr" target="#b19">Mikolov et al., , 2011</ref>) and the LSTMLM ( <ref type="bibr" target="#b29">Soutner and Müller, 2015)</ref> packages. <ref type="table">Table 1</ref> shows the results of the intrinsic evalu- ation. With regard to RNNLM and LSTMLM re- sults, only the best hyper-parameters combination after a lot of experiments, optimizing them on the validation set, has been inserted into the <ref type="table">Table.</ref> With regard to QLM, all the presented ex- periments are based on artificial word vectors produced randomly using values from the set {−1, 0, 1} instead of real word embeddings. Ev- ery word vector is different from the others and we decided not to use real embeddings in order to test the core QMT method without adding the contex-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Parameters</p><note type="other">PPL CMU-SLM 2-gram 15.49 (Good-Turing 3-gram 14.28 smoothing) 4-gram 15.62 5-gram 17.33 IRSTLM 2-gram 15.47 (linear Witten- 3-gram 14.07 Bell smoothing) 4-gram 15.55 5-gram 17.53 RNNLM 280 neurons 13.32 LSTMLM 25 neurons, 1 layer 13.17 QLM</note><p>N=48, p=4, D=10 13.44 N=48, p=4, D=20 13.15 N=48, p=4, D=30 13.10 N=48, p=4, D=40 12.99 <ref type="table">Table 1</ref>: Perplexity (PPL) of the tested language- modelling techniques on the TIMIT testset. All the QLM results in bold face are better than the other systems we tested.</p><p>tual information, contained in word embeddings, that could have helped our approach to obtain bet- ter performances, at least in principle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Extrinsic evaluation</head><p>The "TIMIT recipe" contained in the Kaldi dis- tribution 2 reproduces exactly the same evalua- tion settings described in (Lopes and Perdigao, 2011) for a phone recognition task based on this corpus. Moreover, Kaldi provides some n-best rescoring scripts that apply RNNLM hypothesis rescoring and interpolate the results with the stan- dard N-gram model results used in the evaluation. We slightly modified these scripts to work with LSTMLM and QLM in order to test different mod- els using the same setting. This allowed us to re- place the LM used in Kaldi and experiment with all the systems evaluated in the previous section. <ref type="table">Table 2</ref> outlines the results we obtained replac- ing the LM technique into Kaldi ASR package w.r.t. the different ASR systems that the TIMIT recipe implements. These systems are built on top of MFCC, LDA, MLLT, fMLLR with CMN 3 fea- tures (see <ref type="bibr" target="#b25">(Povey et al., 2011b;</ref><ref type="bibr" target="#b28">Rath et al., 2013</ref>) for all acronyms references and a complete feature or recipe descriptions).</p><p>For this extrinsic evaluation we used the best models we obtained in the previous experiments interpolating their log-probability results for each utterance with the original bigram (or trigram) log-probability using a linear model with a ratio 0.25/0.75 between the original N-gram LM and the tested one as suggested in the standard Kaldi rescoring script. For this test we rescored the 10,000-best hypothesis.</p><p>We have to say that in this experiment we were not trying to build the best possible phone recog- niser, but simply to compare the relative perfor- mances of the analysed LM techniques showing the effectiveness of QLM when used in a real ap- plication. Thus absolute Phone Error Rate is not so important here and it can be certainly possible to devise recognisers with better performances by applying more sophisticated techniques. For ex- ample ( <ref type="bibr" target="#b23">Peddinti et al., 2015</ref>) presented a method for lattice rescoring in Kaldi that exhibits better performances than the n-best rescoring we used to interpolate between n-grams and the tested mod- els, but modifying it in order to test LSTMLM and QLM presented a lot of problems and thus we decided to use the simpler n-best approach. For completeness, the last column of <ref type="table">Table 2</ref> out- lines the results obtained using this lattice rescor- ing method with RNNLM as described in <ref type="bibr" target="#b23">(Peddinti et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and conclusions</head><p>We presented a new technique for building LM based on QMT, and its probability calculus, test- ing it extensively both with intrinsic and extrinsic evaluation methods.</p><p>The PPL results for the intrinsic evaluation, outlined in <ref type="table">Table 1</ref>, show a clear superiority of the proposed method when compared with state-of-the-art techniques such as RNNLM and LSTMLM. It is interesting to note that even using D = 20, that means a system containing a quar- ter of parameters, therefore much less 'memory', w.r.t. the system with D = 40, we obtain a PPL performance better than the other methods.</p><p>With regard to the second experiment we made, an extrinsic evaluation where we replaced the LM of an ASR system with the LM produced by all the tested methods (see <ref type="table">Table 2</ref>), QLM consistently exhibits the best performances for all the tested ASR systems from the Kaldi "TIMIT recipe". De- spite using a n-best technique in this evaluation for hypothesis rescoring, that is known to perform worse than the lattice rescoring method proposed in ( <ref type="bibr" target="#b23">Peddinti et al., 2015)</ref>, the QLM performances are even better than this method.</p><p>The approach we have presented in this paper is not without problems: the number of different word types in the considered language has to be small in order to keep the model computationally tractable. Even if the code we used in the evalu- ations is analytically highly optimised, the train- ing of this model is rather slow and requires rele- vant computational resources even for small prob- lems. On the contrary, inference is very quick, faster than the RNNLM and LSTMLM packages we tested.</p><p>The main research question that drove this work was to verify if the distinguishing properties of quantum probability theory, namely interference and system entaglement that could allow the an- cilla to have a "potentially infinite" memory, were enough to build stochastic systems more power- ful than those built using classical probabilities or those built using recurrent NN. Our main aim was not to build a complete model to handle all possible LM scenarios, but to present a "proof-of- concept" study to test the potentialities of this ap- proach. For this reason we tried to keep the model as simple as possible using orthogonal projectors: for measuring probabilities, projecting the system state, each word is mapped onto a single basis vec- tor and the dimension of the system Hilbert space, N , is equal to the number of different words. Given the matrix dimensions that we have to man- age when we add the ancilla, DN × DN , this set- ting does not scale to real LM problems (e.g. the Brown corpus), even though the calculations are performed using D × D submatrices, but allowed us to successfully verify the research question. For the same reason out-of-vocabulary words cannot be handled in this model because there are no ba- sis vectors assigned to them.</p><p>In order to overcome these limitations, this work can be extended by using generalized quan- tum measurements projectors (POVM) and by us- ing a different structure for the system Hilbert space: instead of mapping each word onto a sin- gle basis vector we can span this space using as basis the same p-basis vectors used to define the V matrices. In this way we will project the system state on a generic word vector built as a superposi-  <ref type="table">Table 2</ref>: Phone-recognition performances, in terms of Phone Error Rate, for the TIMIT dataset and the different Kaldi ASR models, rescoring the 10,000-best solutions with the tested LM techniques in- terpolated with the IRSTLM bigrams and trigrams LM (the standard LM used in Kaldi). In boldface the best performing system and in italics the second best. Kaldi ASR systems descriptions: tri1 = a triphone model using 13 dim. MFCC+∆+∆∆; tri2 = tri1+LDA+MLLT; tri3 = tri2+SAT; SGMM2 = Semi-supervised Gaussian Mixture Model <ref type="bibr" target="#b9">(Huang and Hasegawa-Johnson, 2010;</ref><ref type="bibr" target="#b24">Povey et al., 2011a</ref>); Dan NN = DNN model by <ref type="bibr" target="#b35">(Zhang et al., 2014;</ref>.</p><p>tion on the p-basis. Such improvement would re- duce dramatically the dimensions of the matrices to Dp × Dp potentially mitigating the computa- tional issue. Moreover, this would solve also the problem of out-of-vocabulary words allowing for a proper management of the large set of different words typical of real applications. We are still working on these improvements and we will hope to get a complete model soon.</p><p>With this contribution we would like to raise also some interest in the community to analyse and develop more effective techniques, both on the modelling and minimisation/learning sides, to allow to build real world application based on this framework. QMT and its probability calculus seem to be promising methodologies to enhance the performances of our systems in NLP and cer- tainly deserve further investigations.</p></div>
			<note place="foot" n="1"> † marks the conjugate transpose of a vector/matrix and Tr(·) is the trace of a matrix.</note>

			<note place="foot" n="2"> https://github.com/kaldi-asr/kaldi 3 MFCC: Mel-Frequency Cepstral Coefficients; LDA: Linear Discriminant Analysis; MLTT: Maximum Likelihood Linear Transform; fMLLR: feature space Maximum Likelihood Linear Regression; SAT: Speaker Adapted Training, i.e. train on fMLLR-adapted features; CMN: Cepstral Mean Normalization.</note>

			<note place="foot" n="4"> https://www.cineca.it/en</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We acknowledge the CINECA 4 award no. HP10C7XVUO under the ISCRA initiative, for the availability of HPC resources and support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quantum structure and human thought</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Aerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Broekaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liane</forename><surname>Gabora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Sozzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">274276</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unitary evolution recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine LearningICML&apos;16</title>
		<meeting>the 33rd International Conference on International Conference on Machine LearningICML&apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1120" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Practical recommendations for gradient-based training of deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade: Second Edition</title>
		<editor>Grégoire Montavon,Genevì eve B. Orr, and KlausRobert Müller</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="437" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A quantum-theoretic approach to distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Blacoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elham</forename><surname>Kashefi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="847" to="857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Quantum Models of Cognition and Decision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><forename type="middle">R</forename><surname>Busemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">D</forename><surname>Bruza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Statistical language modeling using the cmu-cambridge toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EUROSPEECH &apos;97</title>
		<meeting>EUROSPEECH &apos;97</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="2707" to="2710" />
		</imprint>
	</monogr>
	<note>ISCA</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">IRSTLM: an open source toolkit for handling large scale language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2008, 9th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1618" to="1621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Darpa timit acoustic-phonetic continuous speech corpus cd-rom</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lori</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pallett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><surname>Dahlgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DARPA, TIMIT Acoustic-Phonetic Continuous Speech Corpus CDROM</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Quantum latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">A</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval Theory</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">6931</biblScope>
			<biblScope unit="page" from="52" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised training of gaussian mixture models by conditional entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jui Ting</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual Conference of the International Speech Communication Association, INTERSPEECH 2010</title>
		<meeting>the 11th Annual Conference of the International Speech Communication Association, INTERSPEECH 2010</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1353" to="1356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tunable efficient unitary neural networks (EUNN) and their application to RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tena</forename><surname>Dubcek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Peurifoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">A</forename><surname>Skirlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Tegmark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Soljacic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtyfourth International Conference on Machine Learning-ICML2017</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Workshop on Semantic Spaces at the Intersection of NLP, Physics and Cognitive Science</title>
		<meeting>the 2016 Workshop on Semantic Spaces at the Intersection of NLP, Physics and Cognitive Science</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">221</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Ubiquitous Quantum Structure: From Psychology to Finance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khrennikov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>SpringerVerlag</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A novel classifier based on quantum computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghu</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="484" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Phoneme recognition on the timit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carla</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Perdigao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ivo Ipsic, editor, Speech Technologies</title>
		<meeting><address><addrLine>Rijeka</addrLine></address></meeting>
		<imprint>
			<publisher>InTech</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval and Quantum Mechanics. The Information Retrieval Series 35</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Melucci</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer-Verlag Berlin Heidelberg</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Quantum mechanics and information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Melucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Topics in Information Retrieval</title>
		<editor>Massimo Melucci and Ricardo Baeza-Yates</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="125" to="155" />
		</imprint>
	</monogr>
<note type="report_type">Heidelberg</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Workshop at ICLR</title>
		<meeting>of Workshop at ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukás</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernock´ycernock´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Makuhari, Chiba, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rnnlmrecurrent neural network language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaň</forename><surname>Cernock´ycernock´y</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ASRU 2011</title>
		<meeting>ASRU 2011</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Line search algorithms with guaranteed sufficient decrease</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><forename type="middle">J</forename><surname>Moré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Thuente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Softw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="286" to="307" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Isaac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<title level="m">Quantum Computation and Quantum Information: 10th Anniversary Edition</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Numerical Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">JHU aspire system: Robust LVCSR with tdnns, ivector adaptation and RNN-LMS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vimal</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<meeting><address><addrLine>Scottsdale, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The subspace gaussian mixture model-a structured model for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Akyazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Kai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagendra</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariya</forename><surname>Rastrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">C</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Speech Lang</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="404" to="439" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagendra</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirko</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Silovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Stemmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Vesely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2011 Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<publisher>IEEE Signal Processing Society</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parallel training of dnns with natural gradient and parameter averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations-ICLR2015</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Early stopping-but when?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lutz</forename><surname>Prechelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade: Second Edition</title>
		<editor>Grégoire Montavon,Genevì eve B. Orr, and KlausRobert Müller</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="53" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improved feature processing for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vesel´yvesel´y</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Annual Conference of the International Speech Communication Association-INTERSPEECH2013</title>
		<meeting>the 14th Annual Conference of the International Speech Communication Association-INTERSPEECH2013<address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="109" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soutner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luděk</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Language and Speech Processing: Third International Conference</title>
		<editor>AdrianHoria Dediu, Carlos Martín-Vide, and Klára Vicsi</editor>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015-11-24" />
			<biblScope unit="page" from="267" to="274" />
		</imprint>
	</monogr>
	<note>SLSP</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A composite parameterization of unitary groups, density matrices and subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Spengler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrix</forename><forename type="middle">C</forename><surname>Hiesmayr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physics A: Mathematical and Theoretical</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">38</biblScope>
			<biblScope unit="page">385306</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Tagare</surname></persName>
		</author>
		<title level="m">Notes on optimization on Stiefel manifolds</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>Yale University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Are quantum classifiers promising?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tamburini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Italian Conference on Computational Linguistics CLiC-it</title>
		<meeting>the First Italian Conference on Computational Linguistics CLiC-it<address><addrLine>Pisa</addrLine></address></meeting>
		<imprint>
			<publisher>Pisa University Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="360" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Introduction to Quantum Information Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlatko</forename><surname>Vedral</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Oxford University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fullcapacity unitary recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Les</forename><surname>Atlas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4880" to="4888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving deep neural network acoustic models using generalized maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Trmal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="215" to="219" />
		</imprint>
	</monogr>
	<note>2014 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The quantum probability ranking principle for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leif</forename><forename type="middle">A</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval Theory</title>
		<editor>Leif et al. Azzopardi</editor>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5766</biblScope>
			<biblScope unit="page" from="232" to="240" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
