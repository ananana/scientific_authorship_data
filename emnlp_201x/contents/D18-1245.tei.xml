<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Level Structured Self-Attentions for Distantly Supervised Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 2216</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhua</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingguang</forename><surname>Han</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Accenture Labs Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dadong</forename><surname>Wan</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Accenture Labs Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">† ADAPT Centre</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Level Structured Self-Attentions for Distantly Supervised Relation Extraction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2216" to="2225"/>
							<date type="published">October 31-November 4, 2018. 2018. 2216</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Attention mechanisms are often used in deep neural networks for distantly supervised relation extraction (DS-RE) to distinguish valid from noisy instances. However, traditional 1-D vector attention models are insufficient for the learning of different contexts in the selection of valid instances to predict the relationship for an entity pair. To alleviate this issue, we propose a novel multi-level structured (2-D matrix) self-attention mechanism for DS-RE in a multi-instance learning (MIL) framework using bidirectional recurrent neural networks. In the proposed method, a structured word-level self-attention mechanism learns a 2-D matrix where each row vector represents a weight distribution for different aspects of an instance regarding two entities. Targeting the MIL issue, the structured sentence-level attention learns a 2-D matrix where each row vector represents a weight distribution on selection of different valid instances. Experiments conducted on two publicly available DS-RE datasets show that the proposed framework with a multi-level struc-tured self-attention mechanism significantly outperform state-of-the-art baselines in terms of PR curves, P@N and F1 measures.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction is a fundamental task in infor- mation extraction (IE), which studies the issue of predicting semantic relations between pairs of en- tities in a sentence ( <ref type="bibr" target="#b17">Zelenko et al., 2003;</ref><ref type="bibr" target="#b1">Bunescu and Mooney, 2005;</ref><ref type="bibr" target="#b21">Zhou et al., 2005)</ref>. One crucial problem in RE is the relative lack of large-scale, high-quality labeled data. In recent years, one commonly used and effective technique for deal- ing with this challenge is the distant supervision method via knowledge bases (KBs) ( <ref type="bibr" target="#b11">Mintz et al., 2009;</ref><ref type="bibr" target="#b12">Riedel et al., 2010;</ref><ref type="bibr" target="#b4">Hoffmann et al., 2011</ref>), which assumes that if one entity pair appearing in some sentences can be observed in a KB with a certain relationship, then these sentences will be labeled as the context of this entity pair and this relationship. The distant supervision strategy is an effective and efficient method for automatically la- beling large-scale training data. However, it also introduces a severe mislabelling problem due to the fact that a sentence that mentions two enti- ties does not necessarily express their relation in a KB ( <ref type="bibr" target="#b13">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b18">Zeng et al., 2015)</ref>.</p><p>Plenty of research work has been proposed to deal with distantly supervised data and has achieved significant progress, especially with the rapid development of deep neural net- works (DNN) for relation extraction in recent years ( <ref type="bibr" target="#b19">Zeng et al., 2014</ref><ref type="bibr" target="#b18">Zeng et al., , 2015</ref><ref type="bibr" target="#b9">Lin et al., 2016</ref><ref type="bibr" target="#b8">Lin et al., , 2017a</ref>; <ref type="bibr" target="#b15">Wang et al., 2016;</ref><ref type="bibr" target="#b22">Zhou et al., 2016;</ref><ref type="bibr" target="#b5">Ji et al., 2017;</ref><ref type="bibr" target="#b16">Yang et al., 2017;</ref><ref type="bibr" target="#b20">Zeng et al., 2017</ref>). DNN models under an MIL framework for DS- RE have become state-of-the-art, replacing statis- tical methods, such as feature-based and graphi- cal models ( <ref type="bibr" target="#b12">Riedel et al., 2010;</ref><ref type="bibr" target="#b4">Hoffmann et al., 2011;</ref><ref type="bibr" target="#b13">Surdeanu et al., 2012</ref>). In the MIL frame- work for distantly supervised RE, each entity pair often has multiple instances where some are noisy and some are valid. The attention mechanism in DNNs, such as convolutional (CNN) and recurrent neural networks (RNN), is an effective way to se- lect valid instances by learning a weight distribu- tion over multiple instances. However, there are two important representation learning problems in DNN-based distantly supervised RE: (1) Problem I: entity pair-targeted context representation learn- ing from an instance; and (2) Problem II: valid in- stance selection representation learning over mul- tiple instances. The former can use a word-level attention mechanism to learn a weight distribu- tion on words and then a weighted sentence rep- resentation regarding two entities; the latter can employ a sentence-level attention mechanism to learn a weight distribution on multiple instances so that valid sentences with higher weights can be fo- cused and selected, and noisy instances with lower weights are suppressed.</p><p>Both the word-level and sentence-level atten- tion mechanisms in previous work on the RE task are simple 1-D vectors which are learned using the hidden states of the RNN, or via pooling from either the RNNs' hidden states or convolved n- grams ( <ref type="bibr" target="#b19">Zeng et al., 2014</ref><ref type="bibr" target="#b18">Zeng et al., , 2015</ref><ref type="bibr" target="#b22">Zhou et al., 2016;</ref><ref type="bibr" target="#b15">Wang et al., 2016;</ref><ref type="bibr" target="#b5">Ji et al., 2017;</ref><ref type="bibr" target="#b16">Yang et al., 2017)</ref>. The deficiency of the 1-D attention vec- tor is that it only focuses on one or a small number of aspects of the sentence, or one or a small num- ber of instances ( <ref type="bibr" target="#b10">Lin et al., 2017b</ref>), with the result that different semantic aspects of the sentence, or different multiple valid sentences are ignored, and cannot be utilised.</p><p>Inspired by the structured self-attentive sen- tence embedding in <ref type="bibr" target="#b10">Lin et al. (2017b)</ref>, we propose a novel multi-level structured (2-D) self-attention mechanism (MLSSA) in a bidirectional LSTM- based (BiLSTM) <ref type="bibr" target="#b3">(Hochreiter and Schmidhuber, 1997</ref>) MIL framework to alleviate two problems in the distantly supervised RE. Regarding Prob- lem I, we propose a 2-D matrix-based word- level attention mechanism, which contains mul- tiple vectors, each focusing on different aspects of the sentence for better context representation learning. In terms of Problem II, we propose a 2-D sentence-level attention mechanism for mul- tiple instance learning, where it contains multi- ple vectors, each focusing on different valid in- stances for a better sentence selection. "struc- tured" indicates that the weight vectors in the learned 2-D matrix try to construct a structural de- pendency relationship by learning different weight distributions for different contexts or instances given the entity pair. We can see that our struc- tured attention mechanism is different from that in <ref type="bibr">Kim et al. (2017)</ref> which incorporates richer structural distributions and are simple extensions of the basic attention procedure. We verify the proposed framework on two distantly supervised RE datasets, namely the New York Times (NYT) dataset ( <ref type="bibr" target="#b12">Riedel et al., 2010</ref>) and the DBpedia Por- tuguese dataset ( <ref type="bibr" target="#b0">Batista et al., 2013)</ref>. Experi- mental results show that our MLSSA framework significantly outperforms state-of-the-art baseline systems in terms of different evaluation metrics.</p><p>The main contributions of this paper include:</p><p>(1) we propose a novel multi-level structured (2- D) self-attention mechanism for DS-RE which can make full use of input sequences to learn different contexts, without integrating extra re- sources; (2) we propose a 2-D matrix-based word- level attention for better context representation learning targeting two entities; (3) we propose a 2- D sentence-level attention mechanism over mul- tiple instances to select different valid instances; and (4) we verify the proposed framework on two publicly available distantly supervised datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Most existing work on distant supervision data mainly focuses on denoising the data under the MIL strategy by learning a valid sentence rep- resentation or features, and then selecting one or more valid instances for relation classifica- tion ( <ref type="bibr" target="#b12">Riedel et al., 2010;</ref><ref type="bibr" target="#b4">Hoffmann et al., 2011;</ref><ref type="bibr" target="#b13">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b18">Zeng et al., 2015;</ref><ref type="bibr" target="#b9">Lin et al., 2016</ref><ref type="bibr" target="#b8">Lin et al., , 2017a</ref><ref type="bibr" target="#b22">Zhou et al., 2016;</ref><ref type="bibr" target="#b5">Ji et al., 2017;</ref><ref type="bibr" target="#b20">Zeng et al., 2017;</ref><ref type="bibr" target="#b16">Yang et al., 2017)</ref>. <ref type="bibr" target="#b12">Riedel et al. (2010)</ref> and <ref type="bibr" target="#b13">Surdeanu et al. (2012</ref>) use a graphical model and MIL to select the valid sentences and classify the relations. However, these models are based on statistical methods and feature engineering, i.e. extracting sentence fea- tures using other NLP tools. <ref type="bibr" target="#b18">Zeng et al. (2015)</ref> proposed a piece-wise CNN (PCNN) method to automatically learn sentence-level features and se- lect one valid instance for the relation classifi- cation. The one-sentence-selection strategy does not make full use of the supervision information among multiple instances. <ref type="bibr" target="#b9">Lin et al. (2016)</ref> and <ref type="bibr" target="#b5">Ji et al. (2017)</ref> introduce an attention mechanism to the PCNN-based MIL framework to select informative sentences, which outperforms all baseline systems on the NYT data set. However, their attention mechanism is only a sentence-level model without incorporating word- level attention. <ref type="bibr" target="#b22">Zhou et al. (2016)</ref> introduce a word-level attention model to the BiLSTM-based MIL framework and obtain significant improve- ments on the SemEval2010 ( <ref type="bibr" target="#b2">Hendrickx et al., 2010</ref>) data set. <ref type="bibr" target="#b15">Wang et al. (2016)</ref> extend the sin- gle word-level attention model to multiple word levels in CNNs to discern patterns in heteroge- neous contexts of the input sentence, and achieve best performance on the SemEval2010 data set. However, these two works were not targeting the distantly supervised RE problem. <ref type="bibr" target="#b16">Yang et al. (2017)</ref> experiment with word-level and sentence-level attention models in the bidirec- tional RNN on the NYT dataset on the basis of the open source DS-RE system, 1 and verify that a two-level attention mechanism achieves best per- formance compared to PCNN/CNN models. Both the word-level and sentence-level attention models are 1-D vectors.</p><p>From previous work, we can see that the at- tention mechanism in DNNs has made signifi- cant progress on the RE task. However, both word-level and sentence-level attention models are still based on 1-D vectors which have the follow- ing insufficiencies: (1) although the 1-D atten- tion model can learn weights for different con- texts, it only focuses on one or very few aspects of a single sentence ( <ref type="bibr" target="#b10">Lin et al., 2017b)</ref>, or one or very few instances; (2) in order to allow the at- tention mechanism to learn more aspects of the sentence, or different instances, extra knowledge needs to be integrated, such as the work in <ref type="bibr" target="#b5">Ji et al. (2017)</ref> and <ref type="bibr" target="#b8">Lin et al. (2017a)</ref>. The former in- tegrates entity descriptions generated from Free- base and Wikipedia as supplementary background knowledge to disambiguate the entity. The latter introduces a multilingual framework which em- ploys a monolingual attention mechanism to uti- lize the information within monolingual texts, and further uses a cross-lingual attention mechanism to consider the information consistency and com- plementarity among cross-lingual texts. However, extra resources are difficult to obtain in many prac- tical scenarios.</p><p>In order to alleviate the burden of integrating extra knowledge, and make full use of the input sentence (i.e. learning different aspects of context and focusing on different valid instances), we pro- pose a multi-level structured self-attention mecha- nism in a BiLSTM-based MIL framework without integrating extra resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>The distantly supervised RE can be formalised as follows: given an entity pair (e 1 , e 2 ), a bag G con- taining J instances, and the relation label r for G, the goal of the training process is to denoise these instances by selecting valid candidates based on r, and the goal of the testing process is to denoise multiple instances by selecting valid candidates to predict the relation r for G.</p><p>To alleviate the aforementioned two problems, improving the following two representation learn- ing issues is clearly important for a DNN-based RE classifier:</p><p>• Entity pair-targeted context representation:</p><p>The model should have the capability to learn a better context representation from the input sentence targeting the entity pair;</p><p>• Instance selection representation: The model should have the capability to learn a better weight distribution over multiple instances to select valid instances regarding an entity pair.</p><p>Motivated by these two issues, we propose a multi-level structured self-attention framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>The proposed framework consists of three parts as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The first part includes the input layer, embedding layer and BiLSTM layer which transform the input sequence at different time steps to LSTM hidden states.</p><p>The second part implements the entity pair- targeted context representation learning, includ- ing:</p><p>• a structured word-level self-attention layer:</p><p>this generates a set of summation weight vec- tors (or a 2-D matrix) taking the LSTM hid- den states as input. Each vector in the 2-D matrix represents the weights for different as- pects of the input sentence.</p><p>• a structured context representation layer: the weight vectors learned by the 2-D word-level self-attention are dotted with the BiLSTM hidden states. Accordingly, a 2-D matrix or a set of weighted LSTM hidden state vectors, denoted as "M L1 " in <ref type="figure" target="#fig_0">Figure 1</ref>, is obtained. Each weighted vector represents a sentence embedding reflecting a different aspect of the sentence targeting the entity pair. By this means, a dependency parsing-like structure of the input sentence can be constructed, ob- taining different semantic representations of the sentence for the two entities in question.</p><p>• a flattened representation layer: this concate- nates each vector in the 2-D matrix of the sentence embedding to one vector. Then, the The first and second parts operate on the single instance level, i.e. given a bag G and feeding each instance into the framework, the structured word- level self-attention mechanism will construct J in- dividual structured sentence representations corre- sponding to J input instances.</p><p>The third part targets the instance selection rep- resentation learning issue, and operates on the bag level, i.e. considering weighted context represen- tations of all instances in the bag G and learning probability distributions to distinguish informative from noisy sentences. This part includes:</p><p>• a structured sentence-level attention model:</p><p>this has a similar structure to the structured word-level attention mechanism, except that it generates a set of summation weight vec- tors for all input instances in the same bag G. Each vector is a weight distribution over all instances. Accordingly, the 2-D sentence- level matrix is expected to learn a set of dif- ferent weight distributions focusing on differ- ent informative instances. As a result, infor- mative sentences are expected to contribute more with higher weights, and noisy sen- tences are expected to contribute less with smaller weights, to the relation classification.</p><p>• an averaged sentence-level attention layer: the 2-D sentence-level attention matrix is av- eraged and converted to a 1-D vector.</p><p>• a selection representation layer: the 1-D av- eraged attention vector is dotted with the out- put of the flattened representation layer. Ac- cordingly, a 1-D vector, denoted as "M L2 " in <ref type="figure" target="#fig_0">Figure 1</ref>, is obtained which represents an av-eraged weighted selection representation of multiple sentences.</p><p>• an output layer: this connects to a softmax layer and produces a probability distribution corresponding to relation classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Structured Word-Level Self-Attention and its Penalisation Function</head><p>Given a bag G = (S 1 , S 2 , . . . , S J ) containing J instances, and a sentence S j in G consisting of N tokens, S j can be represented using a sequence of word embeddings, as in <ref type="formula" target="#formula_0">(1)</ref>:</p><formula xml:id="formula_0">S j = (e 1 , e 2 , . . . , e N )<label>(1)</label></formula><p>where e i is a d-dimension vector for the i-th word, and S j is the j-th instance in G.</p><p>We denote the hidden state of the BiLSTM as in <ref type="formula" target="#formula_1">(2)</ref>:</p><formula xml:id="formula_1">H = (h 1 , h 2 , . . . , h N ) T<label>(2)</label></formula><p>where h t is a concatenation of the forward hidden state − → h t and the backward hidden state ← − h t at time step t. T is the transpose operation. If the size of each unidirectional LSTM is u, then H has the size 2u-by-N .</p><p>Then, the structured word-level self-attention mechanism is defined as in (3):</p><formula xml:id="formula_2">A L1 = sof tmax(W L1 s2 tanh(W L1 s1 H))<label>(3)</label></formula><p>where L1 stands for the first-level attention mech- anism, i.e. the word-level; W L1 s1 is a weight matrix of size d L1 a × 2u, where d L1 a is a hyper-parameter for the number of neurons in the attention network; W L1 s2 is a weight matrix with the shape r L1 × d L1 a , where r L1 (r L1 &gt; 1) is the hyper-parameter repre- senting the size of multiple vectors in the 2-D at- tention matrix. The size of r L1 is defined based on how many different aspects of the sentence need to be focused on; A L1 is the annotation matrix of size r L1 ×N . We can see that in A L1 , there are r L1 attention vectors for the N -token input sentence.</p><p>Finally, we compute the r L1 weighted sums by multiplying the annotation matrix A L1 and BiL- STM hidden states H. The resulting structured sentence representation M L1 is (4):</p><formula xml:id="formula_3">M L1 = A L1 H T<label>(4)</label></formula><p>where M L1 has the shape r L1 × 2u. It can be seen that the traditional 1-D sentence representation is extended to a 2-D representation (r L1 &gt; 1).</p><p>Subsequently, the output of the flattened repre- sentation layer for the instance S j in G is <ref type="formula" target="#formula_4">(5)</ref>:</p><formula xml:id="formula_4">O L1 j = ReLU (W L1 o M F T L1 + b L1 o )<label>(5)</label></formula><p>where W L1 o is the weight matrix that has the shape v-by-r L1 * 2u, where v is the amount of neurons in the ReLU -based MLP layer; M F T L1 is the flattened structured sentence representation which is a con- catenated vector of each row in M L1 and has the dimension r L1 * 2u; b L1 o is the bias vector of size v; O L1 j is the aggregated sentence representation of the j-th instance in the bag G with size v.</p><p>Then, the output of all instances in G from the flattened representation layer is denoted as in <ref type="formula" target="#formula_5">(6)</ref>:</p><formula xml:id="formula_5">O L1 = (O L1 1 , O L1 2 , . . . , O L1 J ) T<label>(6)</label></formula><p>where O L1 has the shape of v × J. As in <ref type="bibr" target="#b10">Lin et al. (2017b)</ref>, the penalisation term for the structured word-level attention is as in <ref type="formula">(7)</ref>:</p><formula xml:id="formula_6">P L1 = ||(A L1 A T L1 − I)|| 2 F (7)</formula><p>where || · || F is the Frobenius norm of a matrix. I is an identity matrix. Minimising this penalisation term means that we learn an orthogonal matrix for A L1 so that each row in A L1 only focuses on a single aspect of semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Structured Sentence-Level Self-Attention and Averaged Selection Representation</head><p>Taking O L1 as the input to the structured 2-D sentence-level attention model, the annotation ma- trix A L2 is calculated as in <ref type="formula" target="#formula_7">(8)</ref>:</p><formula xml:id="formula_7">A L2 = sof tmax(W L2 s2 tanh(W L2 s1 O L1 ))<label>(8)</label></formula><p>where W L2 s1 is the weight matrix of size d L2 a × v, and d L2 a is the number of neurons in the atten- tion network; W L2 s2 is the weight matrix of shape r L2 × d L2 a , where r L2 (r L2 &gt; 1) is the hyper- parameter representing the size of multiple vectors in the 2-D sentence-level attention matrix. The r L2 multiple vectors are expected to focus on dif- ferent informative instances for the relation classi- fication; A L2 is the sentence-level annotation ma- trix of size r L2 × J. We can see that the traditional 1-D sentence-level attention model is expanded to a multi-vector attention (r L2 &gt; 1).</p><p>Then, we average the 2-D A L2 to a 1-D vector ¯ A L2 which has the dimension of J.</p><p>Accordingly, we calculate the averaged weighted sum by multiplying ¯ A L2 and the ag- gregated sentence representation O L1 , with the resulting instance selection representation M L2 being (9):</p><formula xml:id="formula_8">M L2 = ¯ A L2 · (O L1 ) T<label>(9)</label></formula><p>where M L2 has the size of v. The probability distribution of the predicted re- lation type, i.e. the final output for relation predic- tion, can be calculated as in <ref type="formula" target="#formula_0">(10)</ref>:</p><formula xml:id="formula_9">p(ˆ y|G) = sof tmax(W L2 o tanh(M L2 ) + b L2 o ) (10)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss Function and Optimisation</head><p>The total loss of the network is the summation of the penalisation term P L1 , softmax loss in Eq. <ref type="formula" target="#formula_0">(10)</ref> and the L2 regularisation loss. We use the ADAM optimiser ( <ref type="bibr" target="#b7">Kingma and Ba, 2014)</ref> to minimize the loss function on the mini- batch basis which is randomly selected from the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We use two distantly supervised datasets, namely the NYT corpus (NYT) and the DBpedia Por- tuguese dataset (PT), 2 to verify our method.</p><p>In the NYT dataset, there are 53 relationships including a special relation NA which indicates a None Relation between two entities. The train- ing set contains 580,888 sentences, 292,484 entity pairs and 19,429 relational facts (Non-NA). The test set contains 172,448 sentences, 96,678 entity pairs and 1,950 relational facts (Non-NA). There are 19.24% and 22.57% entity pairs corresponding to multiple instances in the training set and test set, respectively.</p><p>The DBpedia Portuguese dataset is smaller, containing just 10 relationships including a spe- cial relation Other. After preprocessing the orig- inal dataset, we obtain 96,847 sentences, 85,528 entity pairs and 77,321 relational facts (Non- Other). There are 8.61% entity pairs correspond- ing to multiple instances in the whole dataset. As in <ref type="bibr" target="#b0">Batista et al. (2013)</ref>, we use two different set- tings for the training and test sets: (1) a manually <ref type="bibr">2</ref> There are several reasons to use the Portuguese dataset: (i) the data sets reported in previous work, such as the KBP data, are not publicly available, or (ii) SemEval data sets which are not distantly supervised data. Google has also released a dataset (https://github.com/google-research- datasets/relation-extraction-corpus), but it is smaller and only has 4 relation types. For all these reasons, the Portuguese data is a better option to verify our method. reviewed subset that contains 602 sentences (PT- MANUAL) as the test set; and (2) 70%-30% out of the whole data as the training set and test set, respectively (PT-SPLIT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Word Embeddings and Relative Position Features</head><p>For the NYT dataset, we use the 200-dimensional word vectors pre-trained using the NYT corpus; <ref type="bibr">3</ref> for the PT dataset, we use a pre-trained 300- dimensional word vector model. <ref type="bibr">4</ref> For the two- word entities in the data set, we use underscore to connect them as one word. The word embeddings of unknown words are intialised using the normal distribution with the standard deviation 0.05. Sim- ilar to previous work, we also use position embed- dings specified by entity pairs. It is defined as the combination of the relative distances from the cur- rent word to head or tail entities ( <ref type="bibr" target="#b19">Zeng et al., 2014</ref><ref type="bibr" target="#b18">Zeng et al., , 2015</ref><ref type="bibr" target="#b9">Lin et al., 2016</ref>). ) models with or without an attention mechanism. In order to carry out a fair comparison, we select CNN+ATT, PCNN+ATT, BiGRU+ATT (bidirectional gated recurrent unit) and BiGRU+2ATT models as base- lines on the NYT data, PCNN+ATT and Bi- GRU+2ATT as baselines on the PT data, where ATT indicates that the model has a sentence-level attention mechanism, and 2ATT indicates that the model has a 1-D word-level and a 1-D sentence- level attention. <ref type="bibr">5</ref> To show the incremental effectiveness of struc- tured 2-D word-level and 2-D sentence-level self- attention mechanisms, we use two different set- tings for our MLSSA system: (1) MLSSA-1: this has a 2-D word-level self-attention and a 1-D sentence-level attention, i.e. A L2 in <ref type="figure" target="#fig_0">Figure 1</ref> is a 1-D vector. This system is used to verify the con- text representation learning targeting Problem I;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines and Our MLSSA Systems</head><p>(2) MLSSA-2: both the word-level and sentence- level attentions are structured 2-D matrices. This system verifies the instance selection representa- tion learning targeting Problem II.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experiment Setup and Evaluation Metrics</head><p>Following previous work, we use different evalu- ation metrics on these two datasets. For the NYT dataset:</p><p>• Overall evaluation: all training data is used for the model training, and all test data is used for the evaluation in terms of Precision- Recall (PR) curves;</p><p>• P@N evaluation: we select those entity pairs that have more than one instance to carry out the comparison in terms of the precision at n (P@N) measure. <ref type="bibr">6</ref> As in <ref type="bibr" target="#b9">Lin et al. (2016)</ref>, there are three settings: (1) One: for each testing entity pair corresponding to multiple instances, we randomly select one sentence to predict the relation; (2) Two: for each test- ing entity pair with multiple instances, we randomly select two sentences for the rela- tion extraction; and (3) All: for each entity pair having multiple instances, we use all of them to predict the relation. Note that these three selections are only applied to the test set, and we keep all sentences in the training data for model building.</p><p>For the PT dataset, we use Macro F1 to evaluate system performance. <ref type="bibr">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Hyper-parameter Settings</head><p>We use cross-validation to determine the hyper- parameters of our system regarding two different settings and datasets. The in-common and dif- ferent parameters for our two systems and two datasets are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">PR Curves on NYT Dataset</head><p>The comparison results for the NYT test set are shown in <ref type="figure" target="#fig_2">Figure 2</ref>. We have the following obser- vations: (1) BiGRU+ATT outperforms CNN+ATT and PCNN+ATT in terms of the PR curve, show- ing that it can learn a better semantic representa- tion from the sequential input; (2) BiGRU+2ATT has better overall performance compared to Bi- GRU+ATT, showing that word-level attention is beneficial to sentence-level attention compared to single-attention models, i.e. the sentence- level attention model can select more informa- tive sentences based on a more reasonable sen- tence embedding learned by the word-level atten- tion model; (3) MLSSA-1 outperforms all baseline systems in terms of the PR curve, which demon- strates that the structured 2-D word-level atten- tion model can learn a better sentence representa- tion by focusing on different aspects of the sen- tence, so that the sentence-level attention has a better chance of selecting the most informative sentences; and (4) the PR curve of MLSSA-2 is higher than that of MLSSA-1, demonstrating that the 2-D sentence-level attention model can better select the most informative sentences compared to the 1-D sentence-level attention model targeting those entity pairs with multiple instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">P@N Evaluation on NYT Dataset</head><p>The results on the NYT dataset regarding P@100, P@200, P@300 and the mean of three set- tings for each model are shown in <ref type="table">Table 2</ref>. From the table, we have similar observations to the PR Curves: (1) BiGRU+2ATT outper- forms CNN+ATT, PCNN+ATT and BiGRU+ATT in most cases in terms of all P@N scores; and (2) MLSSA-1 and MLSSA-2 significantly outper- form all baselines for all measures. We observe that MLSSA-1 performs better than MLSSA-2 on tasks One and Two, but worse on All. We infer that in our 2-D sentence-level attention model, we  <ref type="table">Table 2</ref>: Precision values for the top-100, top-200, and top-300 relation instances that are randomly selected in terms of one, two and all sentences. set r L2 to 9, but there are only one and two in- stances for selection in tasks One and Two, so the 2-D matrix cannot demonstrate its full potential. However, in All, many entity pairs contain multi- ple or more than 9 instances, so it can learn a better 2-D matrix to focus on different instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Results on PT Dataset</head><p>Based on results from the NYT dataset, we choose PCNN+ATT and BiGRU+2ATT as representative baselines to compare against our MLSSA-1/2 sys- tems on the PT test sets. The results in terms of Macro F1 are shown in <ref type="table" target="#tab_2">Table 3</ref>. It can be seen that on both test sets, our MLSSA-2 model achieved the best performance which shows that the structured 2-D word-level and sentence-level self-attention models can be well applied to datasets of a smaller scale and with a smaller ratio of multiple instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9">Examples and Analysis</head><p>In order to show the effectiveness of structured self-attention mechanisms, we show some exam-  ples by visualising the attentions on different as- pects of a sentence, and on different sentences comparing with BiLSTM+2ATT model. <ref type="figure" target="#fig_4">Figure 3</ref> shows the comparison of word-level attention mechanism between BiGRU+2ATT and MLSSA-1 reflecting their capability of context representation learning (Problem I). MLSSA-2 has a similar probability distribution to MLSSA- 1 in terms of this example.</p><p>The pink fonts indicate lower probability and red indicates higher probability. We observe that: (1) BiGRU+2ATT mainly focuses on one word baltimore. We can see that it has little attention on the entity word maryland. In this example, the comma implies a semantic relationship loca- tion/location/contains for the entity pair (Mary- land, Baltimore). However, BiGRU+2ATT allo- cates quite a small probability to it; and (2) we can see that our model focuses on different words via different attention vectors (9 in total). Words with a red background have a high probability of 0.98 or so. For rows 5, 6, 8 and 9, the focus is on the BLANK tokens. In both systems, the max- imum time step is set to 70, which indicates that shorter sentences are padded with BLANK tokens and longer sentences are cut off. The last row shows the summation of 9 annotation vectors, and it constructs a dependency-like context of the re- lation for the entity pair. Attentions on different words are attributed to the penalisation P L1 which is optimised to learn orthogonal eigenvectors. <ref type="figure" target="#fig_5">Figure 4</ref> shows the comparison of sentence- level attentions between BiGRU+2ATT, MLSSA- 1 and MLSSA-2. The first, second and third columns are probability distributions over multi-  ple instances. The entity pair is (vinod khosla, sun microsystems), and their relation is Busi- ness/Person/Company. From this figure, we ob- serve that: (1) BiGRU+2ATT allocates high prob- abilities to Sentences 1 and 2 by learning the context of "a founder of", but does not recog- nise that "co-founder" is semantically the same as "founder"; and (2) our two models almost evenly focus on all sentences because they ex- press the same semantic concept of "a person is a founder of a company" in terms of the given en- tity pair. Therefore, the structured self-attention mechanism is helpful to learn a better representa- tion and select informative sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>This paper has proposed a multi-level structured self-attention mechanism for distantly supervised RE. In this framework, the traditional 1-D word- level and sentence-level attentions are extended to 2-D structured matrices which can learn differ- ent aspects of a sentence, and different informa- tive instances. Experimental results on two dis- tant supervision data sets show that (1) the struc- tured 2-D word-level attention can learn a bet- ter sentence representation; (2) the structured 2- D sentence-level attention and averaged selec- tion can perform better selection from multiple in- stances for relation classification; (3) the proposed framework significantly outperforms state-of-the- art baseline systems for a range of different mea- sures, which verifies its effectiveness on two rep- resentation learning issues. A subsequent manual investigation via examples also show its effective- ness on two representation learning issues.</p><p>In future work, we will build a domain-specific distant supervision dataset with a higher ratio of multiple instances and compare our system with others. Furthermore, we will consider not using RNNs or CNNs, but a deeper neural networks with only attentions for distantly supervised RE, similar to the work in <ref type="bibr" target="#b14">Vaswani et al. (2017)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Multi-level structured self-attention framework for distantly supervised RE</figDesc><graphic url="image-1.png" coords="4,85.76,62.80,423.30,379.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Neural RE systems have become the state-of- the-art, such as CNN-based (Zeng et al., 2014; Lin et al., 2017a), Piecewise CNN-based (Zeng et al., 2015; Lin et al., 2016; Ji et al., 2017), and BiLSTM-based (Zhou et al., 2016</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison results of a variety of methods in terms of precision/recall curves.</figDesc><graphic url="image-2.png" coords="8,72.00,199.13,239.62,179.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of word-level attentions.</figDesc><graphic url="image-4.png" coords="9,106.81,214.43,381.20,90.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of sentence-level attentions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Hyper-parameter settings</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : Results on the PT test sets</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> https://github.com/frankxu2004/ TensorFlow-NRE</note>

			<note place="foot" n="3"> https://catalog.ldc.upenn.edu/ ldc2008t19 4 https://s3-us-west-1.amazonaws.com/ fasttext-vectors/wiki.pt.vec 5 All the baseline systems are obtained from https:// github.com/thunlp/NRE and https://github. com/thunlp/TensorFlow-NRE.</note>

			<note place="foot" n="6"> P@N considers only the topmost results returned by the model. 7 Regarding the metric, we keep the evaluation consistent with the work in Batista et al. (2013) where they used F1 to measure their RE systems on the Portuguese dataset, in order to maintain a fair comparison with their work using the same metric.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Yuyun Huang and Utsab Barman from University College Dublin, and Jer Hayes, Ed-ward Burgin and other colleagues from Accen-ture Labs Dublin for helpful comments, discus-sion and facilities. We would like to thank the re-viewers for their valuable and constructive com-ments and suggestions. This research is sup-ported by the ADAPT Centre for Digital Con-tent Technology, funded under the SFI Research Centres Programme (Grant 13/RC/2106), and by SFI Industry Fellowship Programme 2016 (Grant 16/IFB/4490), and is supported by Accenture Labs Dublin.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploring dbpedia and wikipedia for portuguese semantic relationship extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David Soares</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><forename type="middle">J</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linguamatica</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Subsequence kernels for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="171" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</title>
		<meeting>the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="94" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledge-based weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th ACL-HLT</title>
		<meeting>the 49th ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with sentence-level attention and entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First (AAAI) Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First (AAAI) Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3060" to="3066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luong</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename></persName>
		</author>
		<idno>abs/1702.00887</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural relation extraction with multi-lingual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th ACL</title>
		<meeting>the 55th ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="34" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th ACL</title>
		<meeting>the 54th ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th ACL and the 4th AFNLP</title>
		<meeting>the Joint Conference of the 47th ACL and the 4th AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 European Conference on Machine Learning and Knowledge Discovery in Databases: Part III</title>
		<meeting>the 2010 European Conference on Machine Learning and Knowledge Discovery in Databases: Part III</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Relation classification via multi-level attention cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th ACL</title>
		<meeting>the 54th ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1298" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-level attention-based neural networks for distant supervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tin</forename><surname>Lok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihai</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Irish Conference on Artificial Intelligence and Cognitive Science (AICS)</title>
		<meeting>Irish Conference on Artificial Intelligence and Cognitive Science (AICS)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014</title>
		<meeting>COLING 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Incorporating relation paths in neural relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1769" to="1778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd ACL</title>
		<meeting>the 43rd ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attentionbased bidirectional long short-term memory networks for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th ACL</title>
		<meeting>the 54th ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="207" to="212" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
