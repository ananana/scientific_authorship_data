<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HFT-CNN: Learning Hierarchical Category Structure for Multi-label Short Text Categorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Shimura</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Interdisciplinary Graduate School</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyi</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Yamanashi</orgName>
								<address>
									<addrLine>4-3-11</addrLine>
									<postCode>400-8511</postCode>
									<settlement>Takeda</settlement>
									<region>Kofu</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumiyo</forename><surname>Fukumoto</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Yamanashi</orgName>
								<address>
									<addrLine>4-3-11</addrLine>
									<postCode>400-8511</postCode>
									<settlement>Takeda</settlement>
									<region>Kofu</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Engineering</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HFT-CNN: Learning Hierarchical Category Structure for Multi-label Short Text Categorization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="811" to="816"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>811</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We focus on the multi-label categorization task for short texts and explore the use of a hierarchical structure (HS) of categories. In contrast to the existing work using non-hierarchical flat model, the method leverages the hierarchical relations between the categories to tackle the data sparsity problem. The lower the HS level, the worse the categorization performance. Because lower categories are fine-grained and the amount of training data per category is much smaller than that in an upper level. We propose an approach which can effectively utilize the data in the upper levels to contribute catego-rization in the lower levels by applying a Con-volutional Neural Network (CNN) with a fine-tuning technique. The results using two benchmark datasets show that the proposed method, Hierarchical Fine-Tuning based CNN (HFT-CNN) is competitive with the state-of-the-art CNN based methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Short text categorization is widely studied since the recent explosive growth of online social net- working applications <ref type="bibr" target="#b13">(Song et al., 2014</ref>).</p><p>In contrast with documents, short texts are less topic-focused in texts.</p><p>Major attempts to tackle the problem is to expand short texts with knowledge extracted from the textual cor- pus, machine-readable dictionaries, and thesauri ( <ref type="bibr" target="#b11">Phan et al., 2008;</ref><ref type="bibr" target="#b14">Wang et al., 2008;</ref><ref type="bibr" target="#b1">Chen et al., 2011;</ref><ref type="bibr" target="#b17">Wu et al., 2012)</ref>. However, because of domain-independent nature of dictionaries and thesauri, it is often the case that the data distri- bution of the external knowledge is different from the test data collected from some specific domain, which deteriorates the overall performance of cat- egorization. A methodology which maximizes the impact of pre-defined domains/categories is needed to improve categorization performance.</p><p>More recently, many authors have attempted to apply deep learning techniques including CNN ( <ref type="bibr" target="#b16">Wang et al., 2015;</ref><ref type="bibr" target="#b21">Zhang and Wallace, 2015;</ref><ref type="bibr" target="#b15">Wang et al., 2017)</ref>, the atten- tion based CNN ( <ref type="bibr" target="#b18">Yang et al., 2016)</ref>, bag-of-words based CNN <ref type="bibr" target="#b3">(Johnson and Zhang, 2015a)</ref>, and the combination of CNN and recurrent neural network ( <ref type="bibr" target="#b7">Lee and Dernoncourt, 2016;</ref><ref type="bibr" target="#b19">Zhang et al., 2016)</ref> to text categorization. Most of them demon- strated that neural network models are powerful for learning features from texts, while they fo- cused on single-label or a few labels problem. Several efforts have been made to multi-labels <ref type="bibr" target="#b4">(Johnson and Zhang, 2015b;</ref><ref type="bibr" target="#b10">Liu et al., 2017</ref>). Liu et al. explored a family of new CNN models which are tailored for extreme multi-label classi- fication ( <ref type="bibr" target="#b10">Liu et al., 2017)</ref>. They used a dynamic max pooling scheme, a binary cross-entropy loss, and a hidden bottleneck layer to improve the overall performance. The results by using six benchmark datasets where the label-set sizes are up to 670K showed that their method attained at the best or second best in comparison with seven state-of-the-art methods including FastText ( <ref type="bibr" target="#b5">Joulin et al., 2017)</ref> and bag-of-words based CNN <ref type="bibr" target="#b3">(Johnson and Zhang, 2015a</ref>). However, all of these attempts aimed at utilizing a large volume of data.</p><p>We address the problem of multi-label short text categorization and explore the use of a HS of cat- egories. The lower level of categories are fine- grained compared to the upper level of categories. Moreover, it is often the case that the amount of training data in a lower level is much smaller than that in an upper level which deteriorates the over- all performance of categorization. We propose an approach which can effectively utilize the data in the upper levels to contribute categorization in lower levels by applying fine-tuning to the CNN which can learn a HS of categories and incorporate granularity of categories into categorization. We transferred the parameters of CNN trained from upper to lower levels according to the HS, and finely tuned parameters. The main contributions of our work can be summarized: (1) We propose a method that maximizes the impact of pre-defined categories to alleviate data sparsity in multi-label short texts. (2) We empirically examined a fine- tuning with CNN that fits to learn a HS of cate- gories defined by lexicographers, and <ref type="formula">(3)</ref> The re- sults show that our method is competitive to the state-of-the-art CNN based methods by using two benchmark datasets, especially it is effective for categorization of short texts consisting of a few words with a large number of labels.  <ref type="figure" target="#fig_0">Figure 1</ref> is based on <ref type="bibr" target="#b6">(Kim, 2014)</ref>. Let x i ∈ R k be the k-dimensional word vector with the i-th word in a sentence obtained by applying skip-gram model provided in fastText <ref type="bibr">1</ref> . A sentence with length n is represented as</p><formula xml:id="formula_0">x 1:n = [x 1 , x 2 , · · · , x n ] ∈ R nk . A convolution filter w ∈ R hk is applied</formula><p>to a window size of h words to produce a new fea- ture, c i = f (w·x i:i+h−1 +b) where b ∈ R indicates a bias term and f refers to a non-linear activation function. We applied this convolution filter to each possible window size in the sentence and obtained a feature map, m ∈ R n−h+1 . As shown in <ref type="figure" target="#fig_0">Fig- ure 1</ref>, we then apply a max pooling operation over the feature map and obtain the maximum valuêvaluê m as a feature of this filter. We obtained multi- ple filters by varying window sizes and multiple features. These features form a pooling layer and are passed to a fully connected layer. In the fully connected layer, we applied dropout ( <ref type="bibr" target="#b2">Hinton et al., 2012</ref>). The dropout randomly sets values in the layer to 0. Finally, we obtained the probability dis- tribution over categories. The network is trained with the objective that minimizes the binary cross- entropy (BCE) of the predicted distributions and the actual distributions by performing stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hierarchical structure learning</head><p>Our key idea is to use a fine-tuning technique in CNN to tackle the data sparsity problem, espe- cially a lower level of a HS. Following a HS, we transferred the parameters of CNN trained in the upper levels to the lower levels which are worse trained because of the lack of data, and then finely tuned parameters of CNN for lower levels ( <ref type="figure" target="#fig_0">Figure  1</ref>). This approach can effectively utilize the data in the upper levels to contribute categorization in the lower levels.</p><p>Fine-tuning is motivated by the observation that the earlier features of CNN contain more generic features that should be effective for many tasks, but later layers of the CNN becomes progressively more specific to the details of the classes contained in the original dataset. The motivation is identical to a HS of categories as we first learn to distinguish among generic categories at the upper level of a hierarchy, then learns lower level distinctions by using only within the appropriate top level of the HS. We note that fine-tuning the last few layers are usually sufficient for transfer learning as the last few layers become more specific features. How- ever, the HS consisting of deep level needs to fine- tune the early layers as well because the distance between the upper and lower level of categories is significant. For this reason, we transferred two layers shown in <ref type="figure" target="#fig_0">Figure 1</ref>, i.e., a layer obtained by word embedding and the convolutional layer. We used them as an initial parameter to learn the sec- ond level of a hierarchy. We repeated this pro- cedure from the top level to the bottom level of a hierarchy. We note that a HS consists of many levels. We fine-tune between adjacent layers only because they are more correlated with each other compared to distant layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-label categorization</head><p>Each test instance is classified into categories with probabilities/scores by applying HFT-CNN. We  <ref type="table" target="#tab_0">Tr  Te  C  RCV1  4  23,149 781,265  103  Amazon670K  9 490,449</ref> 153,025 670,091 then utilize a constraint of a HS to obtain final results which differs from the existing work on non-hierarchical flat model (Johnson and Zhang, 2015a; <ref type="bibr" target="#b10">Liu et al., 2017</ref>). This is done by using two scoring functions: One is a Boolean Scoring Function (BSF). Another is a Multiplicative Scor- ing Function (MSF). Both functions set a thresh- old value and categories whose scores exceed the threshold value are considered for selection. The difference is that BSF has a constraint that a cate- gory can only be selected if its ancestor categories are selected. MSF does not have such a constraint, i.e., we extracted all the categories whose scores exceeded the threshold value and sorted them in descending order as the system's assignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data and HFT-CNN model setting</head><p>We selected two benchmark datasets having a HS from the extreme classification repository 2 : RCV1 ( <ref type="bibr" target="#b9">Lewis et al., 2004</ref>) and Amazon670K ( <ref type="bibr" target="#b8">Leskovec and Krevl, 2015)</ref>. All the documents in RCV1 and item descriptions in Amazon670K are tagged by using Tree Tagger <ref type="bibr" target="#b12">(Schmid, 1995)</ref>. We used nouns, verbs, and adjectives. We then applied fastText. Each dataset has an official training and test sets. We used each fold in the experiments. We choose titles from the training and test set on RCV1. The maximum number of words in the ti- tle was 13 words. Each text of Amazon670K con- sists of a product name and its item description. We extracted the first 13 words from each item de- scription and used them in the experiments. <ref type="table" target="#tab_0">Table  1</ref> presents the statistics on the datasets. We di- vided the training data into two folds; we used 5% to tuning the parameters, and the remains to train the models. Our model setting is shown in <ref type="table">Table  2</ref> 3 . In the experiments, we run three times for each model and obtained the averaged performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Metrics</head><p>We used the standard F1 measure. Furthermore, we evaluated our method by two rank-based eval- uation metrics: the precision at top k, P@k and the Normalized Discounted Cumulated Gains, NDCG@k which are commonly used for com- paring extreme multi-label classification meth- ods ( <ref type="bibr" target="#b10">Liu et al., 2017</ref>). We calculated P@k and NDCG@k for each test data and then obtained an average over all the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Basic results</head><p>We compared HFT-CNN with a method which has hierarchical-based categorization but without fine-tuning (WoFT-CNN) and Flat model to ex- amine the effect of the fine-tuning. WoFT-CNN shows that we independently trained parameters of CNN for each level and trained parameters are not transferred. Flat means that we simply applied our CNN model. The results are shown in Ta- ble 3. The HFT-CNN is better than WoFT-CNN and Flat model except for Micro-F1 obtained by WoFT-CNN(M) in Amazon670K. We also found that the overall results obtained by MSF were bet- ter to those obtained by BSF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparison with state-of-the-art method</head><p>We chose XML-CNN as a comparative method because their method attained at the best or sec- ond best compared to the seven existing methods in six benchmark datasets ( <ref type="bibr" target="#b10">Liu et al., 2017)</ref>. Origi- nal XML-CNN is implemented by using Theano 4 , while we implemented HFT-CNN by Chainer 5 . In order to avoid the influence of differences in li- braries, we implemented XML-CNN by Chainer and compared it with HFT-CNN. We used the author-provided implementation in Chainer's ver- sion of XML-CNN. We recall that we set convo- lutional filters with the window sizes to <ref type="bibr">(2,</ref><ref type="bibr">3,</ref><ref type="bibr">4)</ref> and the stride size to 1 because of short text. To make a fair comparison, we also evaluated XML- CNN with the same window sizes and stride size as HFT-CNN. Liu et al. evaluated their method by using P@k and NDCG@k. We used their metrics as well as F1 measure. We did not set a threshold value on BSF and MSF when we evaluated by using these metrics, but instead, we used a ranked list of cate-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Description</head><p>Values <ref type="table" target="#tab_0">Description  Values  Input word vectors fastText  Filter region size  (2,3,4)  Stride size  1  Feature maps (m)  128  Filters  128 × 3  Activation function  ReLu  Pooling  1-max pooling  Dropout  Randomly selected  Dropout rate1  0.25  Dropout rate2  0.5  Hidden layers  1,024  Batch sizes  100  Learning rate  Predicted by Adam  Epoch  40</ref> with early stopping Loss function BCE loss over sigmoid activation Threshold value for BSF and MSF 0.5 <ref type="table">Table 2</ref>: HFT-CNN model settings: Dropout rate1 shows dropout immediately after embedding layer, and Dropout rate2 refers to dropout in a fully connected layer.  <ref type="table">Table 3</ref>: Basic results: (B) and (M) refer to a BSF and MSF, respectively. Bold font shows the best result within each line. The method marked with " * " indi- cates the score is not statistically significant compared to the best one. We used a t-test, p-value &lt; 0.05.</p><formula xml:id="formula_1">Metric RCV1 F1 HFT(B) WoFT(B) HFT(M) WoFT(M) Flat</formula><p>gories assigned to the test instance. The results are shown in <ref type="table">Table 4</ref>. HFT-CNN with BSF/MSF has the best scores with statistical significance com- pared to both of the XML-CNNs. On RCV1, HFT-CNN(B) in P@1 and NDCG@1 were worse than XML-CNN(1), while HFT-CNN(M) with the same metrics were statistically significant com- pared to XML-CNN(1). This is not surprising be- cause hierarchical fine-tuning does not contribute to the accuracy at the top level as the trained pa- rameters on the top level have not changed in the level.</p><p>We also examined the affection on each system performance by the depth of a hierarchical struc- ture. <ref type="figure">Figure 2</ref> shows Micro-F1 at each hierarchi- cal level. The deeper the hierarchical level, the worse the system's performance. However, HFT- CNN is still better than XML-CNNs. The im- provement by MSF was 1.00 ∼ 1.34% by Micro- F1 and 3.77 ∼ 10.07% by Macro-F1 on RCV1. On Amazon670K, the improvement was 1.10 ∼ 9.26% by Micro-F1 and 1.10 ∼ 3.60% by Macro- F1. This shows that hierarchical fine-tuning fits to learn the hierarchical category structure.</p><p>We recall that we focused on the multi-label problem.   <ref type="formula">(1)</ref>   <ref type="table">Table 4</ref>: Comparative results: "1" and "2" of XML show the stride size=1 and 2 by XML-CNN, respec- tively. "G" stands for NDCG.</p><p>that Micro-F1 obtained by HFT-CNN and XML- CNNs were not statistically significant difference in the number of categories, while Macro-F1 by HFT-CNN except for the number of 13 categories was constantly better to XML-CNNs. On Ama- zon670K data, when the number of categories as- signed to the short text is less than 38, HFT-CNN was better than XML-CNNs or HFT-CNN was not statistically significant compared to XML-CNNs by both F1-scores. However, when it exceeds 39, HFT-CNN was worse than XML-CNNs. One pos- sible reason is the use of BSF: a category can only be selected if its ancestor categories are selected. Therefore, once the test data could not be classi- fied into categories correctly, their child categories also cannot be correctly assigned to the test data. In contrast, as shown in <ref type="figure">Figure 5</ref>, HFT-CNN by MSF was better than XML-CNNs in both Micro and Macro F1 even in the deep level of a hierarchy. From the observations, a robust scoring function is needed for further improvement.</p><p>It is important to note that how the ratio of training data affects the overall performance as we focused on the data sparsity problem. 4 shows Micro and Macro-F1 against a ratio of the training data. Overall, the curves show that more training helps the performance, while the curves obtained by HFT-CNN drop slowly com- pared to other methods in both datasets and evalu- ation metrics. From the observations mentioned in the above, we can conclude that fine-tuning works well, especially in the cases that the number of the training data per category is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have presented an approach to multi-label cat- egorization for short text. The comparative re- sults with XML-CNN showed that HFT-CNN is competitive, especially for the cases that there ex- ists only a small amount of training data. Fu- ture work will include: (i) incorporating lexical semantics such as named entities and domain- specific senses for further improvement, (ii) ex- tending the method to utilize label dependency constraints ( <ref type="bibr" target="#b0">Bi and Kwok, 2011)</ref>, and (iii) improv- ing the accuracy of the top ranking categories to deal with P@1 and NDCG@1 metrics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: HFT-CNN model</figDesc><graphic url="image-1.png" coords="2,73.94,62.81,214.37,138.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2</head><label></label><figDesc>Hierarchical Fine-Tuning based CNN 2.1 CNN architecture Similar to other CNN (Johnson and Zhang, 2015a; Liu et al., 2017), our HFT-CNN model shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figures 3 illustrates Micro-F1 and Macro-F1 against the number of categories per short text. We can see from RCV1 in Figure 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :Figure 3 :Figure 4 :Figure 5 :</head><label>2345</label><figDesc>Figure 2: Performance in each hierarchical level: HFT-CNN used BSF</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Data Statistics: #L shows the depth of a hier- archy. Tr and Te refer to the # of training and test data, respectively. C indicates the total # of categories.</head><label>1</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> https://github.com/facebookresearch/fastText</note>

			<note place="foot" n="2"> manikvarma.org/downloads/XC/XMLRepository.html 3 Our source code including Chainer&apos;s version of XMLCNN is available at: HTTP://github.com/ShimShim46/HFTCNN.</note>

			<note place="foot" n="4"> https://drive.google.com/file/d/1Wwy1MNkrJRXZM3WN ZNywa94c2-iEh 6U/view 5 https://chainer.org</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous review-ers for their helpful comments. This work is sup-ported in part by Support Center for Advanced Telecommunications Technology Research, Foun-dation and the Grant-in-aid for the Japan Society for the Promotion of Science, No.17K00299.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-Label Classification on Tree-and DAG-Structured Hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 28th Internation Conference on Mathine Learning</title>
		<meeting>of the 28th Internation Conference on Mathine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Short Text Classification Improved by Learning Multi-Granularity Topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 22nd International Joint Conference on Artificial Intelligence</title>
		<meeting>of the 22nd International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1776" to="1781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Improving Neural Networks by Preventing CoAdaptation of Feature Detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Effective Use of Word Order for Text Categorization with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-Supervised Convolutional Neural Networks for Text Categorization vis Region Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="919" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bag of Tricks for Efficient Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 15th Conference of the European Chapter of the Association for Conputational Linguistics</title>
		<meeting>of the 15th Conference of the European Chapter of the Association for Conputational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Sentence Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>of the 2014 Conference on Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sequential ShortText classification with Recurrent and Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dernoncourt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies</title>
		<meeting>of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="515" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krevl</surname></persName>
		</author>
		<title level="m">SNAP Datasets: Stanford Large Network Dataset Collection</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">RCV1: A New Benchmark Collection for Text Categorization Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Learning for Extreme Multi-Label Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to Classify Short and Sparse Text &amp; Web with Hidden Topics from Large-Scale Data Collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Horiguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th International World Wide Web Conference</title>
		<meeting>of the 17th International World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improvements in Part-of-Speech Tagging with an Application to German</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the EACL SIGDAT Workshop</title>
		<meeting>of the EACL SIGDAT Workshop</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="47" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bie</surname></persName>
		</author>
		<title level="m">Short Text Classification: A Survey. Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="635" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ConceptBased Short Text Classification and Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-R</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 23rd ACM International Conference on Information and Knowledge Management</title>
		<meeting>of the 23rd ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1069" to="1078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Combining Knowledge with Deep Convolutional Neural Networks for Short Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>of the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2915" to="2921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic Clustering and Convolutional Neural Network for Short Text Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="352" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Probabilistic Taxonomy for Text Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2012 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>of the 2012 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="481" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical Attention Networks for Document Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies</title>
		<meeting>of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dependency Sensitive Convolutional Neural Networks for Modeling Sentences and Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies</title>
		<meeting>of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1512" to="1521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lease</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>of the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="155" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Sensitivity Analysis of (and Practitioners&apos; Guide to) Convolutional Neural Networks for Sentence Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 8th International Joint Conference on Natural Language Processing</title>
		<meeting>of the 8th International Joint Conference on Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="253" to="263" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
