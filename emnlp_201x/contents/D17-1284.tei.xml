<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:32+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Entity Linking via Joint Encoding of Types, Descriptions, and Context</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Gupta</surname></persName>
							<email>nitishg@seas.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Pennsylvania Philadelphia</orgName>
								<orgName type="institution" key="instit2">University of California Irvine</orgName>
								<orgName type="institution" key="instit3">University of Pennsylvania Philadelphia</orgName>
								<address>
									<region>PA, CA, PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
							<email>sameer@uci.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Pennsylvania Philadelphia</orgName>
								<orgName type="institution" key="instit2">University of California Irvine</orgName>
								<orgName type="institution" key="instit3">University of Pennsylvania Philadelphia</orgName>
								<address>
									<region>PA, CA, PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
							<email>danroth@seas.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Pennsylvania Philadelphia</orgName>
								<orgName type="institution" key="instit2">University of California Irvine</orgName>
								<orgName type="institution" key="instit3">University of Pennsylvania Philadelphia</orgName>
								<address>
									<region>PA, CA, PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Entity Linking via Joint Encoding of Types, Descriptions, and Context</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2681" to="2690"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>For accurate entity linking, we need to capture various information aspects of an entity , such as its description in a KB, contexts in which it is mentioned, and struc-tured knowledge. Additionally, a linking system should work on texts from different domains without requiring domain-specific training data or hand-engineered features. In this work we present a neural, modular entity linking system that learns a unified dense representation for each entity using multiple sources of information, such as its description, contexts around its mentions, and its fine-grained types. We show that the resulting entity linking system is effective at combining these sources, and performs competitively, sometimes out-performing current state-of-the-art systems across datasets, without requiring any domain-specific training data or hand-engineered features. We also show that our model can effectively &quot;embed&quot; entities that are new to the KB, and is able to link its mentions accurately.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Entity linking, the task of identifying the real-world entity a mention in text refers to, provides the abil- ity to ground text to existing knowledge bases, and thus supports multiple natural language understand- ing, and knowledge acquisition tasks.</p><p>A key challenge for successful entity linking is the need to capture semantic and background information at various levels of granularity. For example, to resolve the mention "India" in "India plays a match in England today" to the correct en- tity, India cricket team, one needs to use * Work performed while these authors were at UIUC. mention-level context to identify that the sentence refers to a sports team (using plays and match), use document-level context to identify the sport, and information about the entity to realize that India cricket team is a sports team and the string "India" may refer to it. The problem has been studied extensively by employing a variety of machine learning, and inference methods, in- cluding a pipeline of deterministic modules ( <ref type="bibr" target="#b17">Ling et al., 2015)</ref>, simple classifiers <ref type="bibr" target="#b5">(Cucerzan, 2007;</ref><ref type="bibr" target="#b26">Ratinov et al., 2011</ref>), graphical models <ref type="bibr" target="#b7">(Durrett and Klein, 2014</ref>), classifiers augmented with ILP infer- ence ( <ref type="bibr" target="#b4">Cheng and Roth, 2013)</ref>, and more recently, neural approaches ( <ref type="bibr" target="#b11">He et al., 2013;</ref><ref type="bibr" target="#b8">Francis-Landau et al., 2016)</ref>.</p><p>We present a neural approach to linking 1 that learns a dense unified representation of entities by encoding the semantic and background informa- tion from multiple sources -encyclopedic entity descriptions, entity-type information, and the con- texts the entity occurs in -thus capturing differ- ent aspects of the "meaning" of an entity. Hence, we overcome the shortcomings of several existing models that do not capture all these aspects. For example, methods, such as Vinculum ( <ref type="bibr" target="#b17">Ling et al., 2015)</ref>, do not make use of the local context of the mention ("plays" and "match") while others, such as Berkeley-CNN (Francis- <ref type="bibr" target="#b8">Landau et al., 2016)</ref>, do not take entity-types into account. Our proposed model uses compositional training to ensure that the learned entity representation captures the vari- ous information sources available to it, making it quite modular. Specifically, we introduce encoders for the different sources of information about the entity, and encourage the entity embedding to be similar to all of the encoded representations.</p><p>A key requirement for information extraction systems is their ability to work across texts from various domains. Some methods <ref type="bibr" target="#b8">(Francis-Landau et al., 2016;</ref><ref type="bibr" target="#b22">Nguyen et al., 2016;</ref><ref type="bibr" target="#b12">Hoffart et al., 2011</ref>) train parameters on domain-specific linked data, thus hampering their ability to generalize to new domains. By only making use of indirect su- pervision that is available in Wikipedia/Freebase, we refrain from using domain specific training data, and produce a domain-independent linking system. Our comprehensive evaluation on recent entity link- ing benchmarks reveals that the resulting entity linker compares favorably to state-of-the-art sys- tems across datasets, even those that have hand- engineered features or use dataset-specific train- ing. We hence show that our model not only lever- ages all the available information for each entity effectively, but is also robust to missing informa- tion, such as entities without links/description in Wikipedia or with incomplete entity types.</p><p>In the real-world, new entities are regularly added to the knowledge bases, thus, it is impor- tant for any entity linking system to be extendable to such entities, especially the ones that do not have any existing linked mentions. By the virtue of our model's modular nature, it can easily incorporate new entities not present during training. Specifi- cally, we show that our model can perform accurate linking for new entities, without having to re-train the existing entity representations, only using their description and types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Existing approaches for entity linking differ in sev- eral ways, including the machine learning models, the types of training data, and the kinds of informa- tion used about the entities.</p><p>Many existing approaches use links and informa- tion from Wikipedia as the only source of supervi- sion to build the entity linking system. These ap- proaches use sparse entity and mention-context rep- resentations, such as, based on the Wikipedia cate- gories <ref type="bibr" target="#b5">(Cucerzan, 2007)</ref>, weighted bag of words in the entity description and mention context <ref type="bibr" target="#b14">(Kulkarni et al., 2009;</ref><ref type="bibr" target="#b26">Ratinov et al., 2011</ref>), hand crafted features based on partial string matches, punctua- tions in entity name ( <ref type="bibr" target="#b19">McNamee et al., 2009)</ref>, etc. Heuristics ( <ref type="bibr" target="#b20">Mihalcea and Csomai, 2007)</ref> or linear classifiers ( <ref type="bibr" target="#b3">Bunescu and Pasca, 2006;</ref><ref type="bibr" target="#b5">Cucerzan, 2007;</ref><ref type="bibr" target="#b26">Ratinov et al., 2011;</ref><ref type="bibr" target="#b19">McNamee et al., 2009)</ref> are used over these features to rank entity candi- dates for linking. Recently, neural models have been proposed as a way to support better general- ization over the sparse features; e.g., using feed- forward networks on bag-of-words of the entity context ( <ref type="bibr" target="#b11">He et al., 2013)</ref>, or using entity-class in- formation from KB ( ).</p><p>Some models ignore the entity's description on Wikipedia, but rather, only rely on the context from links to learn entity representations ( <ref type="bibr" target="#b15">Lazic et al., 2015)</ref>, or use a pipeline of existing annotators to filter entity candidates ( <ref type="bibr" target="#b17">Ling et al., 2015)</ref>. Our model is similar to these approaches by only using information from Wikipedia; however, we do not use hand-crafted features, and use multiple sources of information such as local and document-level entity context, KB descriptions, and entity types, to learn explicit entity representation.</p><p>Few recent entity linking approaches <ref type="bibr" target="#b12">(Hoffart et al., 2011;</ref><ref type="bibr" target="#b7">Durrett and Klein, 2014;</ref><ref type="bibr" target="#b22">Nguyen et al., 2016;</ref><ref type="bibr" target="#b8">Francis-Landau et al., 2016</ref>) use manually- annotated domain specific training data to learn the linking system. AIDA <ref type="bibr" target="#b12">(Hoffart et al., 2011)</ref>, for example, evaluate their system on test set from CoNLL-YAGO dataset but also train on the training data from the same dataset. Berkeley- CNN (Francis- <ref type="bibr" target="#b8">Landau et al., 2016)</ref>, that uses CNNs operating over different granularity of entity and mention contexts, also follows this training regime and trains separate models for each dataset. Such approaches can be prohibitive in many applications as it encourages the model to over-fit to the pecu- liarities of different datasets and domains.</p><p>Other forms of information, apart from descrip- tions, and context from linked data, are also uti- lized for linking. Many approaches perform joint inference over the linking decisions in a docu- ment ( <ref type="bibr" target="#b21">Milne and Witten, 2008;</ref><ref type="bibr" target="#b26">Ratinov et al., 2011;</ref><ref type="bibr" target="#b12">Hoffart et al., 2011;</ref><ref type="bibr" target="#b9">Globerson et al., 2016)</ref>, iden- tify mentions that do not link to any existing en- tity (NIL) ( <ref type="bibr" target="#b3">Bunescu and Pasca, 2006;</ref><ref type="bibr" target="#b26">Ratinov et al., 2011)</ref>, and cluster NIL-mentions ( <ref type="bibr" target="#b36">Wick et al., 2013;</ref><ref type="bibr" target="#b15">Lazic et al., 2015</ref>) to discover new entities. Few approaches jointly model entity linking, and other related NLP tasks to improve linking, such as, coreference resolution ( <ref type="bibr" target="#b10">Hajishirzi et al., 2013)</ref>, rela- tional inference <ref type="bibr" target="#b4">(Cheng and Roth, 2013)</ref>, and joint coreference with typing <ref type="bibr" target="#b7">(Durrett and Klein, 2014</ref>). In our model, we use fine-grained type information of the entity as an auxiliary distant supervision to improve mention-context representation but do not use intermediate typing decisions for linking.</p><p>Many approaches that learn entity embeddings for other applications have also been proposed,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Jointly Embedding Entity Information</head><p>Knowledge bases contain different kinds of infor- mation about entities such as textual description, linked mentions (in Wikipedia), and types (in Free- base). For accurate linking, it is often necessary to combine information from these various sources.</p><p>Here, we describe our model that encodes informa- tion about the set of entities E using dense unified representation for linking (v e ∈ R d , ∀e ∈ E). In particular, we use existing mentions in Wikipedia to encode the context ( § 3.1), textual descriptions from Wikipedia to encode background information ( § 3.2), and fine-grained types from Freebase as structured topical knowledge ( § 3.3). <ref type="figure" target="#fig_0">Figure 1</ref> pro- vides an overview of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoding the Mention Context, C</head><p>Consider the example mention in <ref type="figure" target="#fig_0">Figure 1</ref> that con- tains two mentions, "India" and "England". In order to disambiguate "India" to the correct entity, a linking system would need to utilize both the local context (played and match), and the docu- ment context (to identify the sport). However, the model needs to represent context such that the se- mantics are preserved, e.g. "England" should not be linked to a sports team even though it shares the context with "India". In this section, we describe how we encode these two types of context, using a LSTM-based encoder to capture the lexical and syntactical local-context of a mention (v local m ), and a feed-forward network to encode the document- level topical knowledge (v doc m ), and combine them in a single representation for each mention (v m ).</p><p>Local-Context Encoder Given a mention m in the sentence s = w 1 , . . . , m, . . . , w N , we use LSTM encoders on the left (w 1 , . . . , m) and right (m, . . . , w N ) contexts of the mention separately, and then combine it to form the local context representation of the mention <ref type="figure" target="#fig_3">(Fig. 2</ref>). More precisely, we formulate an LSTM as h i , s i = LSTM(u i , h i−1 , s i−1 ), u i ∈ R dw is the input em- bedding of the i-th token in the sequence, and   Mention-Context Encoder We combine the lo- cal (v local m ) and document (v doc m ) level context vec- tors by concatenating them, and passing them through a single-layer feed-forward network to ob- tain the mention context embedding v m ∈ R d . In order to learn the entity representation v e such that it encodes all of its mentions' contexts, we intro- duce an objective that encourages the context rep- resentation v m to be similar to v e (where mention m is a link to entity e), and dissimilar to other can- didates <ref type="bibr">4</ref> . Precisely, we maximize the probability of predicting the correct entity from the mention- context vector as P text (e|m) = exp (vm·ve) c k ∈Cm exp (vm·vc k ) , <ref type="bibr">2</ref> We reverse the token sequence in the right context so that right-LSTM starts at the last token and ends at the mention. <ref type="bibr">3</ref> We use rectified linear unit (ReLU) as the non-linear activation throughout this paper. <ref type="bibr">4</ref> Details on candidate generation in <ref type="bibr">Sec 4</ref> where C m is the set of candidate entities. Given all the mentions in Wikipedia, we jointly optimize the entity representations, and the context encoders by maximizing the following log-likelihood:</p><formula xml:id="formula_0">h i−1 , s i−1 ∈ R l is</formula><formula xml:id="formula_1">L text = 1 M M i=1 log P text (e m (i) |m (i) )<label>(1)</label></formula><p>where m (i) is the i th mention in the linked data, and e m (i) is the entity the mention refers to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoding Entity Description, D</head><p>The textual description about entities in Wikipedia can provide a useful source of background infor- mation about the entity, and thus has been used in many existing linking systems. Given the de- scription as a sequence of words, we first embed each word to a d w -dimensional vector resulting in a sequence of vectors w 1 , . . . , w n . To encode this description as a fixed size vector, we use a Convolution Neural Network (CNN), similar to Francis-Landau et al. <ref type="formula" target="#formula_1">(2016)</ref>, with global average pooling, to obtain v e desc ∈ R d . In order for the entity representation v e to encode its description, we use a similar objective as in the previous section § 3.1, i.e. we maximize the probability P desc (e|v e desc ), and learn the parameters by maximizing the log-likelihood L desc , defined similarly as (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Encoding Fine-Grained Types, E</head><p>Fine-grained types provide a source of structured information that is quite readily available, often more easily than the description or linked data (e.g. Freebase contains tens of millions of entities with types but Wikipedia only contains descriptions for a few million). These types have been shown to be quite useful for linking ( <ref type="bibr" target="#b17">Ling et al., 2015)</ref>, since an accurate prediction of types from the mention, and its match with the entity types can often resolve many challenging ambiguities.</p><p>Here, we focus on being able to represent the different types at the entity level, leaving mention- level type information to the next section. Each entity has multiple types T e ⊂ T from the type set T introduced by <ref type="bibr" target="#b18">Ling and Weld (2012)</ref>. We com- pute the probability P (t|e) of type t being relevant to entity e as σ(v t · v e ), where σ is the sigmoid function, v e ∈ R d is the entity representation, and v t ∈ R d is the embedding of type t in T . We maxi- mize the log-likelihood of the type information to jointly learn entity and type representations:</p><formula xml:id="formula_2">L etype = 1 |E| e∈E log t∈Te P (t|e) t / ∈Te (1 − P (t |e))</formula><p>3.4 Type-Aware Context Representation, T Apart from being able to represent the types of the entities, it is also important for our linker to be able to represent the type information at the men- tion level. In the example in <ref type="figure" target="#fig_0">Fig. 1</ref>, although the mention "India" is prominently used to refer to the country, it is evident from the sentence that it refers to a Sports Team. The context-encoder cap- tures this information in an unstructured manner, thus it will be useful for the encoder to directly utilize this supervision. This is a similar setup as <ref type="bibr" target="#b17">Ling et al. (2015)</ref> and <ref type="bibr" target="#b27">Shimaoka et al. (2017)</ref> that use noisy distant supervision to train a fine-grained type predictor for mentions.</p><p>In order for the context encoders, and type em- beddings to directly inform each other, we intro- duce an objective L mtype between every v m and v t if type t belongs to T e for the entity e that m refers to. This objective is similar to L etype from § 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Learning Unified Entity Representations</head><p>In the sections above we described different en- coder models to capture entity-context informa- tion (local-and document-level), entity-description from a KB, and fine-grained types in a single entity representation vector. To learn the entity represen- tations, and parameters of the encoders, we jointly maximize the total objective:</p><formula xml:id="formula_3">{v e }, Θ = argmax {ve},Θ L text + L desc + L etype + L mtype</formula><p>where {v e } is the set of entity representations, and Θ is set of parameters for the different encoders. One advantage of having such a joint, modular ob- jective is that it is robust to missing information, i.e. entities with missing mentions, types, or de- scriptions will still obtain accurate representations learned using other sources of information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Entity Linking</head><p>Given a document, and mentions marked in it for disambiguation, we perform a two-step procedure to link them to an entity. First, we find a set of candidate entities, and their prior scores using a pre-computed dictionary. We then use our mention- context encoder to estimate the semantic similarity of each mention with the vector representations of each entity candidate, and combine the results from the two sources for making linking decisions.</p><p>A typical KB contains millions of entities, which makes it prohibitively expensive to compute a sim- ilarity score between each mention and all entities in the KB. Prior work has shown that, for a given mention, aggressively pruning the set of possible entities to a small subset hurts performance only negligibly, while making the linker extremely ef- ficient. For each mention m, we generate a set of candidate entities C m = {c j } ⊂ E using Cross- Wikis (Spitkovsky and Chang, 2012), a dictionary computed from a Google crawl of the web that stores the frequency with which a mention links to a particular entity. To generate C m we choose the top−30 entities for each mention string, and normalize this frequency across the chosen candi- dates to compute P prior (e|m). In the literature, such a dictionary is often built from the anchor links in <ref type="bibr">Wikipedia (Ratinov et al., 2011;</ref><ref type="bibr" target="#b12">Hoffart et al., 2011</ref>) but <ref type="bibr" target="#b17">Ling et al. (2015)</ref> show using CrossWikis gives improved prior scores and candidate recall.</p><p>For each mention m, we use our learned mention-context encoder from § 3.1 to encode the mention's context as v m , and estimate the distribu- tion over the candidates using P text (e|m). We treat these two pieces of evidence; pre-computed prior probability, and the context-based probability, as independent, disjunctive sources of signal, and thus combine them to compute P (e|m) as:</p><formula xml:id="formula_4">P (e|m) = P prior (e|m) + P text (e|m) − (P prior (e|m) * P text (e|m)) (2) ˆ e m = argmax e∈Cm P (e|m)<label>(3)</label></formula><p>wherê e m is the predicted entity that the mention m should be disambiguated to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation Setup</head><p>Here we provide a detailed description of how we train our models, benchmark datasets, linking sys- tems we compare to, and the evaluation metrics.</p><p>Training Data Our primary source of informa- tion about the entities is Wikipedia (dump dated 2016/09/20). We use existing links in Wikipedia, with the anchors as mentions, and links as the true entity, as input to the context encoder (see § 3.1). As the description of each entity ( § 3.2), we use the first 100 tokens of the entity's Wikipedia page (same as Francis- <ref type="bibr" target="#b8">Landau et al. (2016)</ref>). To ob- tain entity types (see § 3.3), we extract the types for each entity from Freebase and map them to the 112 fine-grained types introduced by <ref type="bibr" target="#b18">Ling and Weld (2012)</ref>. For context and description encoders, we use pre-trained 300-dimensional case-sensitive word embeddings by <ref type="bibr" target="#b25">Pennington et al. (2014)</ref> as the first layer that is not updated during training.</p><p>Hyper-parameters We perform coarse-grained tuning of the hyper-parameters using a fraction of the training data. The vectors for the enti- ties, types, contexts, and descriptions are of size d = 200. The size of the local context encoder LSTM hidden layer l, local context output, and the document-context encoder output D m is set to 100(= l = D m ). The document context vocabu- lary contains |V G | = 1.5 million strings. We use dropout ( <ref type="bibr" target="#b31">Srivastava et al., 2014</ref>) with a probability of 0.4. Additionally, we use word-dropout where we replace a random subset of tokens (mention- strings) in the local (document) context with "unk" (rate of 0.4 and 0.6 for local and document con- text respectively). We use Adam ( <ref type="bibr" target="#b13">Kingma and Ba, 2014</ref>) for optimization, with learning rate 0.005 and mini-batches of size 1000.</p><p>Existing Approaches We compare our ap- proach to the following five entity-linking mod- els: (1) Plato ( <ref type="bibr" target="#b15">Lazic et al., 2015)</ref>, an unsupervised generative model that uses indirect-supervision from Wikipedia and an additional corpus of 50 million unlabeled webpages, (2) Wikifier ( <ref type="bibr" target="#b26">Ratinov et al., 2011</ref>), an unsupervised linker that uses hand-crafted features to rank candidates, (3) Vin- culum ( <ref type="bibr" target="#b17">Ling et al., 2015)</ref>, a modular, unsupervised pipeline system, (4) AIDA <ref type="bibr" target="#b12">(Hoffart et al., 2011</ref>), a supervised linker trained on CoNLL data and uses hand-crafted features, and <ref type="formula">(5)</ref>   <ref type="table">Table 1</ref>: Entity Linking Performance: Accuracy of existing systems, and variations of our model on gold mentions. The model using context informa- tion is labeled C, entity-description as D, context- typing as T, and entity-type encoding as E. Existing models marked in Italics* train domain-specific linkers for each dataset. Our system performs competitively to these systems, and outperforms Plato (Sup) that uses the same indirect supervision.</p><p>a prediction is only considered correct if the sys- tem mention boundaries match the gold annota- tion, and the predicted link is correct (we compare against these by extracting mentions with Stanford- NER). On the other hand, systems like Plato, AIDA, and Berkeley-CNN assume mentions are provided, and evaluate using the linking accuracy for gold- mentions. Further, the approaches we compare here (including ours) do not predict NIL entities for the datasets evaluated on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>In this section we present various experiments to evaluate the performance of our proposed entity- linking system. Specifically, we focus on the fol- lowing questions: (1) how effective is our model in combining different information on standard link- ing benchmarks, without requiring domain specific information ( § 6.1), (2) is our model able to ac- commodate unseen entities by using their types, or description, without re-training the entity represen- tations ( § 6.2), and (3) how does the model perform on fine-grained mention typing, a task it is not di- rectly trained for, compared to approaches designed for the task ( § 6.3). Further, Sec 6.4 presents exam- ples to show the effect of encoding different kinds of information in a unified entity representation.   <ref type="table">Table 2</ref>: Results for ACE-2004: F1 is calcu- lated for predicted mentions, and accuracy on gold- mentions. Results for Wikifier and AIDA are from ( <ref type="bibr" target="#b17">Ling et al., 2015</ref>). All systems use the same men- tion extraction protocol showing the difference in F1 is due to linking performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Entity Linking</head><p>In <ref type="table">Table 1</ref> we present linking accuracy for our models that vary in the information they use. We see that the model that only encodes the context- information, Model C (L = L text ) consistently per- forms better than picking the entity with the high- est prior probability from CrossWikis, indicating that the model is able to utilize the context across datasets. On incorporating the description with context (Model CD) we see improvement in the performance on ACE-2005, but slight decrease in CoNLL, suggesting the entity descriptions are not extremely useful for the latter (it contains rare en- tities, many short and incomplete sentences, and specific entities as annotations for metonymic men- tions, as also observed by <ref type="bibr" target="#b17">Ling et al. (2015)</ref>). On introducing the entity type-aware loss in Model CT to the context-only model, we see significantly improved results for all datasets, demonstrating that explicitly modeling fine-grained types helps learning a better context encoder and, in turn, type- aware entity representations. Combining descrip- tions with this model (Model CDT) shows further gains in accuracy indicating that our model is able to exploit complementary information from the two sources. Finally, on introducing explicit entity-type encoding, Model CDTE performs the best on two of the four datasets. As we will see in § 6.2, encod- ing entity-type information also allows our models to easily generalize to new entities. On comparison to existing systems we see that all our variants outperform Plato's indirectly- supervised model trained on Wikipedia, which is the same information our Model C and CD use. Their semi-supervised model, that is additionally trained on 50 million web-pages, performs much  In <ref type="table">Table 2</ref> we present results for our models on ACE-2004. Our model outperforms the Wikifier and Vinculum systems that only use information from Wikipedia, and AIDA, by a significant margin, indicating its possible over-fitting to the CoNLL domain. Hence, it shows our model's ability to perform accurate linking across different datasets without using domain-specific information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Cold-Start Entities</head><p>In realistic situations, new entities are regularly added to the knowledge base with little or no linked data for them. Hence, it is important for any infor- mation extraction system that learns entity repre- sentations to be easily extendable for such entities without needing to be re-trained. In this section, we consider the use of our approach to this setting.</p><p>In particular, for each such new entity, we need to determine their embedding using only their de- scription and/or type information. For a new entity for which only the description is available, we di- rectly set its embedding to be the output of the entity-description encoder without any need for learning. If only fine-grained types are available, we learn the new entity-embedding by optimizing the objective L etype . In case both description and types are available, we jointly maximize the simi- larity of the entity embedding with the output of the entity-description, and the type encoders (i.e. opti- mize L desc and L etype ). Note that we only learn the embeddings of each new entity, keeping all other     <ref type="bibr" target="#b18">and Weld, 2012)</ref> and neural-LSTM model of <ref type="bibr" target="#b27">Shimaoka et al. (2017)</ref>.</p><p>Their SSIR-Full model that uses a biLSTM layer, an attention layer, combined with hand-crafted fea- tures is state-of-art for this task. parameters of our model (Model CDTE) fixed.</p><p>To evaluate this setting of new entities, we ran- domly select 1000 rare entities from Wikipedia that are not used during training. Among all mentions of these entities in Wikipedia, we only keep the mentions for which our candidate generation gen- erates more than one candidate, resulting in 3791 mentions. On average, each mention had 6 candi- date entities, and further, as priors are not available in this setting, we only rely on the context proba- bility for linking, making this a challenging task.</p><p>We present the results of using different types of information about the entity for this data in <ref type="table" target="#tab_4">Table 3</ref>. It is surprising that randomly initialized embed- dings for these new entities perform better than ran- dom guessing, suggesting our model is sometimes able to eliminate the wrong candidates purely based on their learned embedding, i.e. an entity with a random embedding has a higher likelihood of being the correct entity. More importantly, we see that our model variants that utilize the available entity information are able to link much more accurately (47-60% error reduction). Further, using both de- scription and types results in the best embeddings for these new entities (∼ 80% accuracy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Fine-Grained Typing</head><p>Since entity embeddings are trained to be both, context and type-aware, we evaluate whether they can be used to predict fine-grained types for men- tions from context (using v m and v t ). Compared to existing systems trained specifically for this task, embeddings from our approach (Model CDTE) per- forms competitively (see <ref type="table" target="#tab_6">Table 4</ref>). In particular, our model performs better than the neural-LSTM model of <ref type="bibr" target="#b27">Shimaoka et al. (2017)</ref>, suggesting that our multi-task linking, and typing loss facilitates effective encoding of mention contexts. 12th Asian Nations Cup finals are hosted by Lebanon until this October 29. Model CD: Lebanon football team Model CT: Lebanon (correct) Model CDTE: Lebanon (correct)</p><p>Yugoslav midfielder Petrovic scored twice as PSV Eind- hoven romped to a 6-0 win. Model CD: Zeljko Petrovic (correct) Model CT: Vladimir Petrovic Model CDTE: Zeljko Petrovic (correct)</p><p>Ince was clambering over a wall at the Republican stadium during an under-21 clash. Model CD: Ince Model CT: Tom Ince Model CDTE: Paul Ince (correct) <ref type="table">Table 5</ref>: Example predictions by our models: Model CT (Ex.1) and CD (Ex.2) predict correctly when correct type prediction or background knowl- edge is sufficient, respectively. Only Model CDTE (Ex.3) predicts correctly when combination of con- text, types, and background knowledge is required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Example Predictions</head><p>In <ref type="table">Table 5</ref> we show the prediction from different variants of our model for a few example mentions. In the first example, detecting the type of the men- tion is crucial, and thus we see both Model CT and CDTE are able to predict accurately. On the other hand, predicting the type of the mention is not especially useful in Example 2, and background factual knowledge from the entity description is needed (which models CD and CDTE are able to encode). Example 3 shows a challenging example where the appropriate combination of context, type prediction, and background knowledge is needed, that our Model CDTE is able to combine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Motivated by the need to provide accurate entity- linking systems that are able to incorporate mul- tiple sources of information, and do not require domain-specific datasets or hand-crafted features, we presented a novel neural approach to linking. We proposed a compositional training objective to learn unified entity embeddings that encode the va- riety of information available for each entity: its un- structured textual description, local and document contexts for its mentions, and sets of fine-grained types attached to it. The joint formulation allows the model to fruitfully combine the various sources of information, providing accurate linking on mul- tiple datasets, generalization to new entities with missing linked data, and the use of entity embed- dings for related tasks such as type prediction.</p><p>There are a number of avenues for future work. Further research will include encoding more struc- tured knowledge about the entities, such as their relations to other entities, to make their representa- tions semantically richer. We will investigate how we can use unstructured resources, such as the cor- pus of unlabeled webpages used by Plato, and noisy supervision from the Wikilinks corpus ( <ref type="bibr" target="#b28">Singh et al., 2012</ref>) in order to further improve the model. We will also evaluate our approach on substantially var- ied domains, such as discussion forums, and social media posts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the Model ( § 3): Each entity has a Wikipedia description, linked mentions in Wikipedia (only one shown), and fine-grained types from Freebase (only one shown). We encode local and document-level mention contexts ( § 3.1), entity-description ( § 3.2), and fine-grained entity-types ( § 3.3 &amp; § 3.4). Joint optimization ( § 3.5) over these provides the unified entity representations {v e }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>the previous output and the cell state of the LSTM, respectively. The left-LSTM is applied to the sequence (w 1 , . . . , m) with the last output − → h l m , while a different right-LSTM is ap-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of the mention context encoder plied to the sequence (w N ,. .. , m) 2 to produce ← − h r m. We concatenate these output [ − → h l m , ← − h r m ] and pass it through a single layer feed-forward network 3 to produce the local context representation of the mention (v local m ), where v local m ∈ R Dm. Note that this encoder will produce different representations for different mentions in the same sentence. Document-Context Encoder To represent the document context of a mention m, we use a bag-ofmention surfaces representation, v D ∈ {0, 1} |V G | , of the document, similar to Lazic et al. (2015). The vocabulary V G consists of all mention surfaces seen in our training data, e.g. USA, Nasser Hussain, Pearl Jam etc. Such a representation helps capture the topical and entity coherence information in the document by utilizing co-occurrence between entity surface forms. This sparse vector v D of bag-of-mention surfaces is compressed to a lowdimensional representation v doc m ∈ R Dm using a single layer feed-forward network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>F1</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIGER</head><label></label><figDesc>FIGER 47.4 69.2 65.5 SSIR-LSTM 55.6 75.1 71.7 SSIR-Full 59.6 78.9 75.3 Our Model 57.7 72.8 72.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>BerkCNN (Francis- Landau et al., 2016), a recent neural supervised approach that has variants that use hand-crafted features. Evaluation Setup We evaluate our approach on the following four datasets: CoNLL-YAGO (Hof- fart et al., 2011), ACE 2004 (NIST, 2004; Rati- nov et al., 2011), ACE 2005 (NIST, 2005; Ben- tivogli et al., 2010), and Wikipedia (Ratinov et al., 2011). For each of these datasets, we use the stan- dard test/development splits, but do not use any information from the training splits. End-to-end entity linking systems such as Vinculum and Wik- ifier perform an NER-style F1 evaluation where</figDesc><table>CoNLL 
ACE05 Wiki 
Test 
Dev 

Plato (Sup) 
79.7 
-
-
-
Plato (Semi-Sup) 
86.4 
-
-
-
AIDA* 
81.8 
-
-
-
BerkCNN:Sparse* 74.9 
-
83.6 
81.5 
BerkCNN:CNN* 
81.2 86.91 
84.5 
75.7 
BerkCNN:Full* 
85.5 
-
89.9 
82.2 

Priors 
68.5 
70.9 
81.1 
78.1 
Model C 
81.4 
83.4 
83.7 
86.1 
Model CD 
81.0 
83.2 
85.8 
86.1 
Model CT 
82.3 
83.9 
86.5 
88.2 
Model CDT 
82.5 
85.6 
86.8 
88.0 
Model CDTE 
82.9 
84.9 
85.6 
89.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Cold-Start Entities: Linking new enti-
ties by using different information to learn their 
embeddings. Our model is able to jointly utilize 
description and type information better. 

better. In comparison to AIDA and Berkeley-CNN, 
that train separate models on respective datasets, 
we perform better than AIDA and Berkeley-CNN's 
sparse and neural model. On combining features 
from CNN to the sparse model, the Berkeley-CNN 
models for each dataset outperform our model, but 
are unlikely to generalize across the datasets 5 . 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Typing Prediction: Performance on the 
FIGER (GOLD) dataset. Our performance is com-
petitive with FIGER (Ling </table></figure>

			<note place="foot" n="1"> The source code and the datasets are available at https://nitishgupta.github.io/neural-el</note>

			<note place="foot" n="5"> Ling et al. (2015) show that AIDA is unable to perform well on datasets it has not been trained on.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Mike Lewis, Xiao Ling, Ananya, Shyam Upadhyay, and the anonymous EMNLP reviewers for their valuable feedback. This work is supported in part by the US Defense Advanced Research Projects Agency (DARPA) un-der contract FA8750-13-2-0008, and in part by an Adobe Research faculty award. The views ex-pressed are those of the authors and do not reflect the official policy or position of the Dept. of De-fense, the U.S. Government, or Adobe.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Extending english ace 2005 corpus annotation with ground-truth links to wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Giuliano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Marchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Pianta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kateryna</forename><surname>Tymoshenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the National Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using encyclopedic knowledge for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Pasca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Chapter of the ACL (EACL)</title>
		<meeting>the European Chapter of the ACL (EACL)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Relational inference for wikification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>of the Conference on Empirical Methods in Natural Language essing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-scale named entity disambiguation based on Wikipedia data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silviu Cucerzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>of the Conference on Empirical Methods in Natural Language essing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="708" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Chains of reasoning over entities, relations, and text using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Chapter of the ACL (EACL)</title>
		<meeting>the European Chapter of the ACL (EACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A joint model for entity analysis: Coreference, typing, and linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Capturing semantic similarity for entity linking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Francis-Landau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the North American Association of Computational Linguistics (NAACL)</title>
		<meeting>the Annual Meeting of the North American Association of Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Collective entity resolution with multi-focal attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevena</forename><surname>Amir Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Lazic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Ringgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint coreference resolution and named-entity linking with multi-pass sieves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Zilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke S</forename><surname>Daniel S Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods for Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods for Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning entity representation for entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust disambiguation of named entities in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Amir</forename><surname>Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilaria</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>Fürstenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilyana</forename><surname>Taneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods for Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Collective annotation of wikipedia entities in web text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayali</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Plato: A selective context model for entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevena</forename><surname>Lazic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ringgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the National Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Design challenges for entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fine-grained entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel S Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the National Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Hltcoe approaches to knowledge base population at tac</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Gerber</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikesh</forename><surname>Garera</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Finin</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mayfield</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><forename type="middle">D</forename><surname>Piatko</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delip</forename><surname>Rao</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Wikify!: Linking documents to encyclopedic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Csomai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>ACM Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to link with Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>ACM Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="509" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint learning of local and global features for entity linking via neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thien Huu Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Fauceglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oktie</forename><surname>Rodriguez-Muro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfio</forename><surname>Hassanzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Massimiliano Gliozzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sadoghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings the International Conference on Computational Linguistics (COLING)</title>
		<meeting>the International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The ACE evaluation plan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nist</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The ace evaluation plan. US National Institute for Standards and Technology (NIST)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">S</forename><surname>Nist</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods for Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Local and global algorithms for disambiguation to wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>of the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural architectures for fine-grained entity type classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonse</forename><surname>Shimaoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Chapter of the ACL (EACL)</title>
		<meeting>the European Chapter of the ACL (EACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Wikilinks: A large-scale cross-document coreference corpus labeled via links to Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<pubPlace>University of Massachusetts, Amherst</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A cross-lingual dictionary for english wikipedia concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Modeling mention, context and entity with neural networks for entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhou</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Compositional learning of embeddings for relation paths in knowledge bases and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multilingual relation extraction using compositional universal schema</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the North American Association of Computational Linguistics (NAACL)</title>
		<meeting>the Annual Meeting of the North American Association of Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generalizing to unseen entities and entity pairs with row-less universal schema</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Chapter of the ACL (EACL)</title>
		<meeting>the European Chapter of the ACL (EACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A joint model for discovering and linking entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshal</forename><surname>Pandya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Workshop on Automated Knowledge Base Construction</title>
		<meeting>the 2013 Workshop on Automated Knowledge Base Construction</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
