<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What do character-level models learn about morphology? The case of dependency parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Vania</surname></persName>
							<email>c.vania@ed.ac.uk, andreasgrv@gmail.com, alopez@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Grivas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">What do character-level models learn about morphology? The case of dependency parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2573" to="2583"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2573</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>When parsing morphologically-rich languages with neural models, it is beneficial to model input at the character level, and it has been claimed that this is because character-level models learn morphology. We test these claims by comparing character-level models to an oracle with access to explicit morphological analysis on twelve languages with varying morphological typologies. Our results highlight many strengths of character-level models , but also show that they are poor at disam-biguating some words, particularly in the face of case syncretism. We then demonstrate that explicitly modeling morphological case improves our best model, showing that character-level models can benefit from targeted forms of explicit morphological modeling.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modeling language input at the character level ( <ref type="bibr" target="#b15">Ling et al., 2015;</ref><ref type="bibr" target="#b12">Kim et al., 2016</ref>) is effec- tive for many NLP tasks, and often produces bet- ter results than modeling at the word level. For parsing, <ref type="bibr" target="#b2">Ballesteros et al. (2015)</ref> have shown that character-level input modeling is highly effective on morphologically-rich languages, and the three best systems on the 45 languages of the CoNLL 2017 shared task on universal dependency parsing all use character-level models <ref type="bibr" target="#b10">(Dozat et al., 2017;</ref><ref type="bibr" target="#b19">Shi et al., 2017;</ref><ref type="bibr" target="#b4">Björkelund et al., 2017;</ref><ref type="bibr" target="#b24">Zeman et al., 2017)</ref>, showing that they are effective across many typologies.</p><p>The effectiveness of character-level models in morphologically-rich languages has raised a ques- tion and indeed debate about explicit modeling of morphology in NLP. <ref type="bibr" target="#b15">Ling et al. (2015)</ref> pro- pose that "prior information regarding morphol- ogy ... among others, should be incorporated" into character-level models, while Chung et al. * Work done while at the University of Edinburgh.</p><p>(2016) counter that it is "unnecessary to consider these prior information" when modeling charac- ters. Whether we need to explicitly model mor- phology is a question whose answer has a real cost: as <ref type="bibr" target="#b2">Ballesteros et al. (2015)</ref> note, morphologi- cal annotation is expensive, and this expense could be reinvested elsewhere if the predictive aspects of morphology are learnable from strings.</p><p>Do character-level models learn morphology? We view this as an empirical claim requiring em- pirical evidence. The claim has been tested implic- itly by comparing character-level models to word lookup models ( <ref type="bibr" target="#b17">Qian et al., 2016;</ref><ref type="bibr" target="#b3">Belinkov et al., 2017)</ref>. In this paper, we test it explicitly, ask- ing how character-level models compare with an oracle model with access to morphological anno- tations. This extends experiments showing that character-aware language models in Czech and Russian benefit substantially from oracle morphol- ogy ( <ref type="bibr" target="#b23">Vania and Lopez, 2017)</ref>, but here we focus on dependency parsing ( §2)-a task that benefits sub- stantially from morphological knowledge-and we experiment with twelve languages using a va- riety of techniques to probe our models.</p><p>Our summary finding is that character-level models lag the oracle in nearly all languages ( §3). The difference is small, but suggests that there is value in modeling morphology. When we tease apart the results by part of speech and dependency type, we trace the difference back to the character- level model's inability to disambiguate words even when encoded with arbitrary context ( §4). Specif- ically, it struggles with case syncretism, in which noun case-and thus syntactic function-is am- biguous. We show that the oracle relies on mor- phological case, and that a character-level model provided only with morphological case rivals the oracle, even when case is provided by another pre- dictive model ( §5). Finally, we show that the cru- cial morphological features vary by language ( §6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dependency parsing model</head><p>We use a neural graph-based dependency parser combining elements of two recent models <ref type="bibr" target="#b13">(Kiperwasser and Goldberg, 2016;</ref><ref type="bibr" target="#b25">Zhang et al., 2017)</ref>. Let w = w 1 , . . . , w |w| be an input sentence of length |w| and let w 0 denote an artificial ROOT to- ken. We represent the ith input token w i by con- catenating its word representation ( §2.3), e(w i ) and part-of-speech (POS) representation, p i . 1 Us- ing a semicolon (; ) to denote vector concatena- tion, we have:</p><formula xml:id="formula_0">x i = [e(w i ); p i ]<label>(1)</label></formula><p>We call x i the embedding of w i since it depends on context-independent word and POS represen- tations. We obtain a context-sensitive encoding h i with a bidirectional LSTM (bi-LSTM), which con- catenates the hidden states of a forward and back- ward LSTM at position i. Using h f i and h b i respec- tively to denote these hidden states, we have:</p><formula xml:id="formula_1">h i = [h f i ; h b i ]<label>(2)</label></formula><p>We use h i as the final input representation of w i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Head prediction</head><p>For each word w i , we compute a distribution over all other word positions j ∈ {0, ..., |w|}/i denot- ing the probability that w j is the headword of w i .</p><formula xml:id="formula_2">P head (w j | w i , w) = exp(a(h i , h j )) � |w| j � =0 exp(a(h i , h j � ))<label>(3)</label></formula><p>Here, a is a neural network that computes an as- sociation between w i and w j using model param- eters U a , W a , and v a .</p><formula xml:id="formula_3">a(h i , h j ) = v a tanh(U a h i + W a h j )<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Label prediction</head><p>Given a head prediction for word w i , we predict its syntactic label � k ∈ L using a similar network.</p><formula xml:id="formula_4">P label (� k | w i , w j , w) = exp(f (h i , h j )[k]) � |L| k � =1 exp(f (h i , h j )[k � ])<label>(5)</label></formula><p>where L is the set of output labels and f is a func- tion that computes label score using model param- eters U � , W � , and V � :</p><formula xml:id="formula_5">f (h i , h j ) = V � tanh(U � h i + W � h j )<label>(6)</label></formula><p>The model is trained to minimize the summed cross-entropy losses of both head and label predic- tion. At test time, we use the Chu-Liu-Edmonds ( <ref type="bibr" target="#b7">Chu and Liu, 1965;</ref><ref type="bibr" target="#b11">Edmonds, 1967)</ref> algorithm to ensure well-formed, possibly non-projective trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Computing word representations</head><p>We consider several ways to compute the word representation e(w i ) in Eq. 1: word. Every word type has its own learned vector representation.</p><p>char-lstm. Characters are composed using a bi-LSTM ( <ref type="bibr" target="#b15">Ling et al., 2015)</ref>, and the final states of the forward and backward LSTMs are concate- nated to yield the word representation.</p><p>char-cnn. Characters are composed using a convolutional neural network ( <ref type="bibr" target="#b12">Kim et al., 2016)</ref>.</p><p>trigram-lstm. Character trigrams are com- posed using a bi-LSTM, an approach that we pre- viously found to be effective across typologies <ref type="bibr" target="#b23">(Vania and Lopez, 2017)</ref>.</p><p>oracle. We treat the morphemes of a morpho- logical annotation as a sequence and compose them using a bi-LSTM. We only use universal in- flectional features defined in the UD annotation guidelines. For example, the morphological anno- tation of "chases" is �chase, person=3rd, num-SG, tense=Pres�.</p><p>For the remainder of the paper, we use the name of model as shorthand for the dependency parser that uses that model as input (Eq. 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Data We experiment on twelve languages with varying morphological typologies <ref type="table">(Table 1</ref>) in the Universal Dependencies (UD) treebanks version 2.0 ( . <ref type="bibr">2</ref> Note that while Ara- bic and Hebrew follow a root &amp; pattern typology, their datasets are unvocalized, which might reduce the observed effects of this typology. Following common practice, we remove language-specific dependency relations and multiword token anno- tations. We use gold sentence segmentation, to- kenization, universal POS (UPOS), and morpho- logical (XFEATS) annotations provided in UD.</p><p>Implementation and training Our Chainer ( <ref type="bibr" target="#b21">Tokui et al., 2015</ref> predictions (output of Eqs. 4 and 6). We set batch size to 16 for char-cnn and 32 for other models following a grid search. We apply dropout to the embeddings (Eq. 1) and the input of the head pre- diction. We use Adam optimizer with initial learn- ing rate 0.001 and clip gradients to 5, and train all models for 50 epochs with early stopping. For the word model, we limit our vocabulary to the 20K most frequent words, replacing less frequent words with an unknown word token. The char- lstm, trigram-lstm, and oracle models use a one- layer bi-LSTM with 200 hidden units to compose subwords. For char-cnn, we use the small model setup of <ref type="bibr" target="#b12">Kim et al. (2016)</ref>. <ref type="table" target="#tab_2">Table 2</ref> presents test results for every model on every language, establishing three results. First, they support previous find- ings that character-level models outperform word- based models-indeed, the char-lstm model out- performs the word model on LAS for all lan- guages except Hindi and Urdu for which the re- sults are identical. 3 Second, they establish strong baselines for the character-level models: the char- lstm generally obtains the best parsing accuracy, closely followed by char-cnn. Third, they demon- strate that character-level models rarely match the accuracy of an oracle model with access to ex- plicit morphology. This reinforces a finding of 4 Analysis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parsing Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Why do characters beat words?</head><p>In character-level models, orthographically simi- lar words share many parameters, so we would ex- pect these models to produce good representations of OOV words that are morphological variants of training words. Does this effect explain why they are better than word-level models?</p><p>Sharing parameters helps with both seen and unseen words <ref type="table" target="#tab_3">Table 3</ref> shows how the charac- ter model improves over the word model for both non-OOV and OOV words. On the agglutina- tive languages Finnish and Turkish, where the OOV rates are 23% and 24% respectively, we see the highest LAS improvements, and we see es- pecially large improvements in accuracy of OOV words. However, the effects are more mixed in other languages, even with relatively high OOV rates. In particular, languages with rich morphol- ogy like Czech, Russian, and (unvocalised) Arabic see more improvement than languages with mod- erately rich morphology and high OOV rates like Portuguese or Spanish. This pattern suggests that parameter sharing between pairs of observed train- ing words can also improve parsing performance. For example, if "dog" and "dogs" are observed in the training data, they will share activations in their context and on their common prefix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Why do morphemes beat characters?</head><p>Let's turn to our main question: what do character- level models learn about morphology? To answer it, we compare the oracle model to char-lstm, our best character-level model.   Pis mo ("letter") acts as the subject in (1), and as object in (2). This knowledge is available to the oracle via morphological case: in (1), the case of pis mo is nominative and in (2) it is accusative. Could this explain why the oracle outperforms the character model? To test this, we look at accuracy for word types that are empirically ambiguous-those that have more than one morphological analysis in the train- ing data. Note that by this definition, some am- biguous words will be seen as unambiguous, since they were seen with only one analysis. To make the comparison as fair as possible, we consider only words that were observed in the training data. <ref type="figure" target="#fig_0">Figure 1</ref> compares the improvement of the oracle on ambiguous and seen unambiguous words, and as expected we find that handling of ambiguous words improves with the oracle in almost all lan- guages. The only exception is Turkish, which has the least training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Morphological analysis disambiguates words</head><p>Morphology helps for nouns Now we turn to a more fine-grained analysis conditioned on the annotated part-of-speech (POS) of the depen-  dent. We focus on four languages where the ora- cle strongly outperforms the best character-level model on the development set: Finnish, Czech, German, and Russian. <ref type="bibr">4</ref> We consider five POS categories that are frequent in all languages and consistently annotated for morphology in our data: adjective (ADJ), noun (NOUN), pronoun (PRON), proper noun (PROPN), and verb (VERB). <ref type="table" target="#tab_5">Table 4</ref> shows that the three noun categories- ADJ, PRON, and PROPN-benefit substantially from oracle morphology, especially for the three fusional languages: Czech, German, and Russian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Morphology helps for subjects and objects</head><p>We analyze results by the dependency type of the dependent, focusing on types that interact with morphology: root, nominal subjects (nsubj), ob- jects (obj), indirect objects (iobj), nominal modi- fiers (nmod), adjectival modifier (amod), obliques (obl), and (syntactic) case markings (case). <ref type="figure">Figure 2</ref> shows the differences in the confu- sion matrices of the char-lstm and oracle for those words on which both models correctly predict the head. The differences on Finnish are small, which we expect from the similar overall LAS of both models. But for the fusional languages, a pat- tern emerges: the char-lstm consistently underper- forms the oracle on nominal subject, object, and indirect object dependencies-labels closely asso- ciated with noun categories. From inspection, it appears to frequently mislabel objects as nominal subjects when the dependent noun is morphologi- cally ambiguous. For example, in the sentence of <ref type="figure">Figure 3</ref>, Gelände ("terrain") is an object, but the char-lstm incorrectly predicts that it is a nominal subject. In the training data, Gelände is ambigu- ous: it can be accusative, nominative, or dative.</p><p>In German, the char-lstm frequently confuses objects and indirect objects. By inspection, we found 21 mislabeled cases, where 20 of them would likely be correct if the model had access to morphological case (usually dative). In Czech and Russian, the results are more varied: indi- rect objects are frequently mislabeled as objects, obliques, nominal modifiers, and nominal sub- jects. We note that indirect objects are relatively rare in these data, which may partly explain their frequent mislabeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Characters and case syncretism</head><p>So far, we've seen that for our three fusional languages-German, Czech, and Russian-the or- acle strongly outperforms a character model on nouns with ambiguous morphological analyses, particularly on core dependencies: nominal sub- jects, objects and indirect objects. Since the nomi- native, accusative, and dative morphological cases are strongly (though not perfectly) correlated with these dependencies, it is easy to see why the morphologically-aware oracle is able to predict them so well. We hypothesized that these cases are more challenging for the character model be- cause these languages feature a high degree of syncretism-functionally distinct words that have the same form-and in particular case syncretism. For example, referring back to examples (1) and (2), the character model must disambiguate pis mo from its context, whereas the oracle can directly disambiguate it from a feature of the word itself. <ref type="bibr">5</ref> To understand this, we first designed an exper- iment to see whether the char-lstm could success-fully disambiguate noun case, using a method sim- ilar to <ref type="bibr" target="#b3">(Belinkov et al., 2017)</ref>. We train a neu- ral classifier that takes as input a word represen- tation from the trained parser and predicts a mor- phological feature of that word-for example that its case is nominative (Case=Nom). The classi- fier is a feedforward neural network with one hid- den layer, followed by a ReLU non-linearity. We consider two representations of each word: its em- bedding (x i ; Eq. 1) and its encoding (h i ; Eq. 2). To understand the importance of case, we consider it alongside number and gender features as well as whole feature bundles.</p><p>The oracle relies on case <ref type="table">Table 5</ref> shows the results of morphological feature classification on Czech; we found very similar results in German and Russian <ref type="bibr">(Appendix A.2)</ref>. The oracle embed- dings have almost perfect accuracy-and this is just what we expect, since the representation only needs to preserve information from its input. The char-lstm embeddings perform well on number and gender, but less well on case. This results sug- gest that the character-level models still struggle to learn case when given only the input text. Com- paring the char-lstm with a baseline model which predicts the most frequent feature for each type in the training data, we observe that both of them show similar trends even though character models slightly outperforms the baseline model.</p><p>The classification results from the encoding are particularly interesting: the oracle still performs very well on morphological case, but less well on other features, even though they appear in   <ref type="table">Table 5</ref>: Morphological tagging accuracy from rep- resentations using the char-lstm and oracle embed- ding and encoder representations in Czech. Base- line simply chooses the most frequent tag. All means we concatenate all annotated features in UD as one tag.</p><p>the input. In the character model, the accuracy in morphological prediction also degrades in the encoding-except for case, where accuracy on case improves by 12%. These results make intuitive sense: representa- tions learn to preserve information from their input that is useful for subsequent predictions. In our parsing model, morphological case is very use- ful for predicting dependency labels, and since it is present in the oracle's input, it is passed almost completely intact through each represen- tation layer. The character model, which must disambiguate case from context, draws as much additional information as it can from surround- ing words through the LSTM encoder. But other features, and particularly whole feature bundles, are presumably less useful for parsing, so neither model preserves them with the same fidelity. <ref type="bibr">6</ref> Explicitly modeling case improves parsing ac- curacy Our analysis indicates that case is im- portant for parsing, so it is natural to ask: Can we improve the neural model by explicitly mod- eling case? To answer this question, we ran a set of experiments, considering two ways to aug- ment the char-lstm with case information: multi- task learning <ref type="bibr">(MTL;</ref><ref type="bibr" target="#b6">Caruana, 1997</ref>) and a pipeline model in which we augment the char-lstm model with either predicted or gold case. For example, we use �p, i, z, z, a, Nom� to represent pizza with nominative case. For MTL, we fol- low the setup of <ref type="bibr" target="#b20">Søgaard and Goldberg (2016)</ref>   LAS results when case information is added. We use bold to highlight the best results for models without explicit access to gold annota- tions. <ref type="bibr" target="#b9">Coavoux and Crabbé (2017)</ref>. We increase the biL- STMs layers from two to four and use the first two layers to predict morphological case, leaving out the other two layers specific only for parser. For the pipeline model, we train a morphological tag- ger to predict morphological case <ref type="bibr">(Appendix A.1)</ref>. This tagger does not share parameters with the parser. <ref type="table" target="#tab_8">Table 6</ref> summarizes the results on Czech, Ger- man, and Russian. We find augmenting the char- lstm model with either oracle or predicted case improve its accuracy, although the effect is dif- ferent across languages. The improvements from predicted case results are interesting, since in non- neural parsers, predicted case usually harms accu- racy ( <ref type="bibr" target="#b22">Tsarfaty et al., 2010)</ref>. However, we note that our taggers use gold POS, which might help. The MTL models achieve similar or slightly better per- formance than the character-only models, suggest- ing that supplying case in this way is beneficial. Curiously, the MTL parser is worse than the the pipeline parser, but the MTL case tagger is better than the pipeline case tagger (  observe that augmenting the char-lstm with either gold or predicted case improves the parsing per- formance for all languages, and indeed closes the performance gap with the full oracle, which has access to all morphological features. This is espe- cially interesting, because it shows using carefully targeted linguistic analyses can improve accuracy as much as wholesale linguistic analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Understanding head selection</head><p>The previous experiments condition their analysis on the dependent, but dependency is a relationship between dependents and heads. We also want to understand the importance of morphological fea- tures to the head. Which morphological features of the head are important to the oracle?</p><p>Composing features in the oracle To see which morphological features the oracle depends on when making predictions, we augmented our model with a gated attention mechanism follow- ing <ref type="bibr" target="#b14">Kuncoro et al. (2017)</ref>. Our new model attends to the morphological features of candidate head w j when computing its association with dependent w i (Eq. 3), and morpheme representations are then scaled by their attention weights to produce a final representation. Let f i1 , · · · , f ik be the k morphological features of w i , and denote by f i1 , · · · , f ik their correspond- ing feature embeddings. As in §2, h i and h j are the encodings of w i and w j , respectively. The mor- phological representation m j of w j is:</p><formula xml:id="formula_6">m j = [f j1 , · · · , f jk ] � k (7)</formula><p>where k is a vector of attention weights:</p><formula xml:id="formula_7">k = softmax([f j1 , · · · , f jk ] � Vh i )<label>(8)</label></formula><p>The intuition is that dependent w i can choose which morphological features of w j are most im- portant when deciding whether w j is its head.</p><p>Note that this model is asymmetric: a word only attends to the morphological features of its (sin- gle) parent, and not its (many) children, which may have different functions. <ref type="bibr">7</ref> We combine the morphological representation with the word's encoding via a sigmoid gating mechanism.</p><formula xml:id="formula_8">z j = g � h j + (1 − g) � m j (9) g = σ(W 1 h j + W 2 m j )<label>(10)</label></formula><p>where � denotes element-wise multiplication. The gating mechanism allows the model to choose between the computed word representation and the weighted morphological representations, since for some dependencies, morphological features of the head might not be important. In the final model, we replace Eq. 3 and Eq. 4 with the fol- lowing:</p><formula xml:id="formula_9">P head (w j |w i , w) = exp(a(h i , z j )) � N j � =0 exp a(h i , z j � ) (11) a(h i , z j ) = v a tanh(U a h i + W a z j ) (12)</formula><p>The modified label prediction is:</p><formula xml:id="formula_10">P label (� k |w i , w j , w) = exp(f (h i , z j )[k]) � |L| k � =0 exp(f (h i , z j )[k � ])<label>(13)</label></formula><p>where f is again a function to compute label score:</p><formula xml:id="formula_11">f (h i , z j ) = V � tanh(U � h i + W � z j )<label>(14)</label></formula><p>Attention to headword morphological features We trained our augmented model (oracle-attn) on Finnish, German, Czech, and Russian. Its accu- racy is very similar to the oracle model <ref type="table" target="#tab_12">(Table 8)</ref>, so we obtain a more interpretable model with no change to our main results. Next, we look at the learned attention vectors to understand which morphological features are im- portant, focusing on the core arguments: nominal subjects, objects, and indirect objects. Since our model knows the case of each dependent, this en- ables us to understand what features it seeks in po- tential heads for each case. For simplicity, we only report results for words where both head and label predictions are correct.   <ref type="figure" target="#fig_3">Figure 4</ref> shows how attention is distributed across multiple features of the head word. In Czech and Russian, we observe that the model at- tends to Gender and Number when the noun is in nominative case. This makes intuitive sense since these features often signal subject-verb agreement. As we saw in earlier experiments, these are fea- tures for which a character model can learn reli- ably good representations. For most other depen- dencies (and all dependencies in German), Lemma is the most important feature, suggesting a strong reliance on lexical semantics of nouns and verbs. However, we also notice that the model sometimes attends to features like Aspect, Polarity, and Verb- Form-since these features are present only on verbs, we suspect that the model may simply use them as convenient signals that a word is verb, and thus a likely head for a given noun.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Character-level models are effective because they can represent OOV words and orthographic regu- larities of words that are consistent with morphol- ogy. But they depend on context to disambiguate words, and for some words this context is insuffi- cient. Case syncretism is a specific example that our analysis identified, but the main results in Ta- ble 2 hint at the possibility that different phenom- ena are at play in different languages.</p><p>While our results show that prior knowledge of morphology is important, they also show that it can be used in a targeted way: our character-level models improved markedly when we augmented them only with case. This suggests a pragmatic re- ality in the middle of the wide spectrum between pure machine learning from raw text input and linguistically-intensive modeling: our new models don't need all prior linguistic knowledge, but they clearly benefit from some knowledge in addition to raw input. While we used a data-driven anal- ysis to identify case syncretism as a problem for neural parsers, this result is consistent with previ- ous linguistically-informed analyses <ref type="bibr" target="#b18">(Seeker and Kuhn, 2013;</ref><ref type="bibr" target="#b22">Tsarfaty et al., 2010)</ref>. We conclude that neural models can still benefit from linguis- tic analyses that target specific phenomena where annotation is likely to be useful.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: LAS improvements (oracle − char-lstm) for ambiguous and unambiguous words on development set.</figDesc><graphic url="image-1.png" coords="4,307.28,351.40,207.36,114.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Heatmaps of the difference between oracle vs. char-lstm confusion matrices for label prediction when both head predictions are correct (x-axis: predicted labels; y-axis: gold labels). Blue cells have higher oracle values, red cells have higher char-lstm values.</figDesc><graphic url="image-2.png" coords="6,72.00,62.81,453.54,177.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Feature</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The importance of morphological features of the head for subject and object predictions.</figDesc><graphic url="image-5.png" coords="9,307.28,210.08,218.26,73.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS LAS</head><label></label><figDesc></figDesc><table>In the oracle, morphological annotations disam-
biguate some words that the char-lstm must dis-
ambiguate from context. Consider these Russian 
sentences from Baerman et al. (2005): 

(1) MašaMašaˇMašačitaet pis mo 
Masha reads letter 
'Masha reads a letter.' Model → 
word 
char-lstm 
char-cnn 
trigram-lstm 
oracle 
o/c 

↓ Language Finnish 
85.7 80.8 90.6 88.4 89.9 87.5 89.7 87.0 90.6 88.8 +0.4 
Turkish 
71.4 61.6 74.7 68.6 74.4 67.9 73.2 65.9 75.3 69.5 +0.9 

Czech 
92.6 89.3 93.5 90.6 93.5 90.6 92.7 89.2 94.3 92.0 +1.4 
English 
90.6 88.9 91.3 89.4 91.7 90.0 90.4 88.5 91.7 89.9 +0.5 
German 
88.1 84.5 88.0 84.5 87.8 84.4 87.1 83.5 88.8 86.5 +2.0 
Hindi 
95.8 93.1 95.7 93.3 95.7 93.2 93.4 89.8 95.9 93.3 -
Portuguese 87.4 85.5 87.8 86.0 87.7 86.0 86.7 84.8 88.0 86.5 +0.5 
Russian 
92.4 90.1 94.0 92.4 93.8 92.1 92.0 89.5 94.4 93.3 +0.9 
Spanish 
89.4 86.9 89.8 87.4 90.0 87.3 88.6 85.5 90.0 87.7 +0.3 
Urdu 
91.1 87.0 91.2 87.1 91.3 87.2 88.6 83.5 90.9 87.0 -0.1 

Arabic 
75.5 70.9 76.7 72.1 76.6 72.2 74.6 68.9 76.7 72.7 +0.6 
Hebrew 
73.5 69.8 73.4 69.8 73.3 69.8 71.3 67.1 73.3 70.0 +0.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Unlabeled Attachment Score (UAS) and Labeled Attachment Score (LAS) on test set. The best 
accuracy for each language is highlighted in bold for all models, and for all non-oracle models. o/c: 
LAS improvement from char-lstm to oracle. 

Language 
dev 
LAS improvement 

%OOV non-OOV OOV 

Finnish 
23.0 
6.8 
17.5 
Turkish 
24.0 
4.6 
13.5 

Czech 
5.8 
1.4 
3.9 
English 
6.8 
0.7 
5.2 
German 
9.7 
0.9 
0.7 
Hindi 
4.3 
0.2 
0.0 
Portuguese 
8.1 
0.3 
1.3 
Russian 
8.4 
2.1 
6.9 
Spanish 
7.0 
0.4 
0.7 

Arabic 
8.0 
1.2 
7.3 
Hebrew 
9.0 
0.2 
1.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>LAS improvements (char-lstm − word) 
for non-OOV and OOV words on development set. 

(2) Na stole ležit pis mo 
on table lies letter 
'There's a letter on the table.' 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Labeled accuracy for different parts of speech on development set.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>and</head><label></label><figDesc></figDesc><table>Language Input 
Dev Test 

Czech 
char 
91.2 90.6 
char (multi-task) 
91.6 91.0 
char + predicted case 92.2 91.8 

char + gold case 
92.3 91.9 
oracle 
92.5 92.0 

German 
char 
87.5 84.5 
char (multi-task) 
87.9 84.4 
char + predicted case 87.8 86.4 

char + gold case 
90.2 86.9 
oracle 
89.7 86.5 

Russian 
char 
91.6 92.4 
char (multi-task) 
92.2 92.6 
char + predicted case 92.5 93.3 

char + gold case 
92.8 93.5 
oracle 
92.6 93.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 7 )</head><label>7</label><figDesc>. This in- dicates that the MTL model must learn to encode case in the model's representation, but must not learn to effectively use it for parsing. Finally, we</figDesc><table>Language %case 
Dev 
Test 

PL 
MT 
PL 
MT 

Czech 
66.5 95.4 96.7 95.2 96.6 
German 
36.2 92.6 92.0 90.8 91.4 
Russian 
55.8 95.8 96.5 95.9 96.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Case accuracy for case-annotated to-
kens, for pipeline (PL) vs. multitask (MT) setup. 
%case shows percentage of training tokens anno-
tated with case. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Our attention experiment results on devel-
opment set. 

</table></figure>

			<note place="foot" n="1"> This combination yields the best labeled accuracy according to Ballesteros et al. (2015).</note>

			<note place="foot" n="2"> For Russian we use the UD Russian SynTagRus treebank, and for all other languages we use the default treebank.</note>

			<note place="foot" n="3"> Note that Hindi and Urdu are mutually intelligible. Vania and Lopez (2017): character-level models are effective tools, but they do not learn everything about morphology, and they seem to be closer to oracle accuracy in agglutinative rather than in fusional languages.</note>

			<note place="foot" n="4"> This is slightly different than on the test set, where the effect was stronger in Turkish than in Finnish. In general, we found it difficult to draw conclusions from Turkish, possibly due to the small size of the data.</note>

			<note place="foot" n="5"> We are far from first to observe that morphological case is important to parsing: Seeker and Kuhn (2013) observe the same for non-neural parsers.</note>

			<note place="foot" n="6"> This finding is consistent with Ballesteros (2013) which performed careful feature analysis on morphologically rich languages and found that lemma and case features provide the highest improvement in a non-neural transition based parser compared to other features.</note>

			<note place="foot" n="7"> This is a simple and much less computationally demanding variant of the model of Dozat et al. (2017), which uses different views for each head/dependent role.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Clara Vania is supported by the Indonesian En-dowment Fund for Education (LPDP), the Cen-tre for Doctoral Training in Data Science, funded by the UK EPSRC (grant EP/L016427/1), and the University of Edinburgh. We would like to thank Yonatan Belinkov for the helpful dis-cussion regarding morphological tagging exper-iments. We thank Sameer Bansal, Marco Da-monte, Denis Emelin, Federico Fancellu, Sorcha Gilroy, Jonathan Mallinson, Joana Ribeiro, Naomi Saphra, Ida Szubert, Sabine Weber, and the anony-mous reviewers for helpful discussion of this work and comments on previous drafts of the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Baerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dunstan</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greville</forename><forename type="middle">G</forename><surname>Corbett</surname></persName>
		</author>
		<title level="m">The Syntax-Morphology Interface: A Study of Syncretism. Cambridge Studies in Linguistics</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Effective morphological feature selection with MaltOptimizer at the SPMRL 2013 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages</title>
		<meeting>the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improved transition-based parsing by modeling characters instead of words with LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="349" to="359" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What do neural machine translation models learn about morphology?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahim</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="861" to="872" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">IMS at the CoNLL 2017 UD shared task: CRFs and perceptrons meet neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Falenska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL</title>
		<meeting>the CoNLL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="40" to="51" />
			<pubPlace>Vancouver, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the shortest arborescence of a directed graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Sinica</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A character-level decoder without explicit segmentation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1693" to="1703" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multilingual lexicalized constituency parsing with wordlevel auxiliary tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximin</forename><surname>Coavoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Crabbé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="331" to="336" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stanford&apos;s graph-based neural dependency parser at the CoNLL 2017 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="20" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimum branchings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Edmonds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Research of the National Bureau of Standards B</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="233" to="240" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the 2016 Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Simple and accurate dependency parsing using bidirectional LSTM feature representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="313" to="327" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What do recurrent neural network grammars learn about syntax?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1249" to="1258" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Fermandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Universal Dependencies 2.0. LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<ptr target="http://hdl.handle.net/11234/1-1983" />
	</analytic>
	<monogr>
		<title level="j">Charles University</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Investigating language universal and specific properties in word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Peng Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1478" to="1488" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Morphological and syntactic case in statistical dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Seeker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="55" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Combining global models for parsing universal dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianze</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="31" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep multi-task learning with low level tasks supervised at lower layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="231" to="235" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Chainer: a next-generation open source framework for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiya</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Clayton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Statistical parsing of morphologically rich languages (SPMRL): What, how and whither</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Candito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Versley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Rehbein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lamia</forename><surname>Tounsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, SPMRL &apos;10</title>
		<meeting>the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, SPMRL &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">From characters to words to in between: Do we capture morphology?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2016" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">CoNLL 2017 shared task: Multilingual parsing from raw text to universal dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhani</forename><surname>Luotolahti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Badmaeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Memduh</forename><surname>Gokirmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Nedoluzhko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvie</forename><surname>Cinkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic Jr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaroslava</forename><surname>Hlavacova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Václava</forename><surname>Kettnerová</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zdenka</forename><surname>Uresova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenna</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stina</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Missilä</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Taji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herman</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuela</forename><surname>Sanguinetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Simi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valeria</forename><surname>Depaiva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Droganova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Héctor Martínez Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gr C ¸ ¨ Oltekin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<editor>Kayadelen, Mohammed Attia, Ali Elkahky, Zhuoran Yu, Emily Pitler, Saran Lertpradit, Michael Mandl, Jesse Kirchner, Hector Fernandez Alcalde, Jana Strnadová, Esha Banerjee, Ruli Manurung, Antonio Stella, Atsuko Shimada, Sookyoung Kwak, Gustavo Mendonca, Tatiana Lando, Rattima Nitisaroj, and Josie Li</editor>
		<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies<address><addrLine>Umut Sulubacak, Hans Uszkoreit, Vivien Macketanz, Aljoscha Burchardt, Kim Harris, Katrin Marheinecke, Georg Rehm, Tolga; Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dependency parsing as head selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="665" to="676" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
