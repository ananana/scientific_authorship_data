<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Document-Level Neural Machine Translation with Hierarchical Attention Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lesly</forename><surname>Miculicich</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Idiap Research Institute</orgName>
								<address>
									<country>Switzerland ‡</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhananjay</forename><surname>Ram</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Idiap Research Institute</orgName>
								<address>
									<country>Switzerland ‡</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Idiap Research Institute</orgName>
								<address>
									<country>Switzerland ‡</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Idiap Research Institute</orgName>
								<address>
									<country>Switzerland ‡</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Ecole Polytechnique Fédérale de Lausanne (EPFL)</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Document-Level Neural Machine Translation with Hierarchical Attention Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2947" to="2954"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2947</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural Machine Translation (NMT) can be improved by including document-level contex-tual information. For this purpose, we propose a hierarchical attention model to capture the context in a structured and dynamic manner. The model is integrated in the original NMT architecture as another level of abstraction, conditioning on the NMT model&apos;s own previous hidden states. Experiments show that hierarchical attention significantly improves the BLEU score over a strong NMT baseline with the state-of-the-art in context-aware methods, and that both the encoder and decoder benefit from context in complementary ways.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation (NMT) ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b18">Wu et al., 2016;</ref><ref type="bibr" target="#b12">Vaswani et al., 2017)</ref> trains an encoder-decoder network on sentence pairs to maximize the likelihood of predicting a target-language sentence given the correspond- ing source-language sentence, without consider- ing the document context. By ignoring discourse connections between sentences and other valuable contextual information, this simplification poten- tially degrades the coherence and cohesion of a translated document <ref type="bibr">(Hardmeier, 2012;</ref><ref type="bibr">Meyer and Webber, 2013;</ref><ref type="bibr" target="#b6">Sim Smith, 2017)</ref>. Recent studies ( <ref type="bibr" target="#b9">Tiedemann and Scherrer, 2017;</ref><ref type="bibr">Jean et al., 2017;</ref><ref type="bibr" target="#b15">Wang et al., 2017;</ref>) have demon- strated that adding contextual information to the NMT model improves the general translation per- formance, and more importantly, improves the co- herence and cohesion of the translated text ( <ref type="bibr">Bawden et al., 2018;</ref><ref type="bibr">Lapshinova-Koltunski and Hardmeier, 2017</ref>). Most of these methods use an ad- ditional encoder <ref type="bibr">(Jean et al., 2017;</ref><ref type="bibr" target="#b15">Wang et al., 2017)</ref> to extract contextual information from pre- vious source-side sentences. However, this re- quires additional parameters and it does not ex- ploit the representations already learned by the NMT encoder. More recently,  have shown that a cache-based memory network performs better than the above encoder-based methods. The cache-based memory keeps past context as a set of words, where each cell cor- responds to one unique word keeping the hidden representations learned by the NMT while trans- lating it. However, in this method, the word repre- sentations are stored irrespective of the sentences where they occur, and those vector representations are disconnected from the original NMT network.</p><p>We propose to use a hierarchical attention net- work (HAN) ( <ref type="bibr" target="#b21">Yang et al., 2016</ref>) to model the contextual information in a structured manner us- ing word-level and sentence-level abstractions. In contrast to the hierarchical recurrent neural net- work (HRNN) used by ( <ref type="bibr" target="#b15">Wang et al., 2017)</ref>, here the attention allows dynamic access to the context by selectively focusing on different sentences and words for each predicted word. In addition, we in- tegrate two HANs in the NMT model to account for target and source context. The HAN encoder helps in the disambiguation of source-word repre- sentations, while the HAN decoder improves the target-side lexical cohesion and coherence. The integration is done by (i) re-using the hidden rep- resentations from both the encoder and decoder of previous sentence translations and (ii) provid- ing input to both the encoder and decoder for the current translation. This integration method en- ables it to jointly optimize for multiple-sentences. Furthermore, we extend the original HAN with a multi-head attention ( <ref type="bibr" target="#b12">Vaswani et al., 2017)</ref> to cap- ture different types of discourse phenomena.</p><p>Our main contributions are the following: (i) We propose a HAN framework for translation to capture context and inter-sentence connections in a structured and dynamic manner. (ii) We in- tegrate the HAN in a very competitive NMT ar-chitecture ( <ref type="bibr" target="#b12">Vaswani et al., 2017)</ref> and show signif- icant improvement over two strong baselines on multiple data sets. (iii) We perform an ablation study of the contribution of each HAN configura- tion, showing that contextual information obtained from source and target sides are complementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Proposed Approach</head><p>The goal of NMT is to maximize the likelihood of a set of sentences in a target language repre- sented as sequences of words y = (y 1 , ..., y t ) given a set of input sentences in a source language x = (x 1 , ..., x m ) as:</p><formula xml:id="formula_0">max Θ 1 N N n=1 log(P Θ (y n |x n ))<label>(1)</label></formula><p>so, the translation of a document D is made by translating each of its sentences independently. In this study, we introduce dependencies on the pre- vious sentences from the source and target sides:</p><formula xml:id="formula_1">max Θ 1 N N n=1 log(P Θ (y n |x n , D x n , D y n )) (2)</formula><p>where D x n = (x n−k , ..., x n−1 ) and D y n = (y n−k , ..., y n−1 ) denote the previous k sentences from source and target sides respectively. The con- texts D x n and D y n are modeled with HANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hierarchical Attention Network</head><p>The proposed HAN has two levels of abstraction. The word-level abstraction summarizes informa- tion from each previous sentence j into a vector s j as:</p><formula xml:id="formula_2">q w = f w (h t )<label>(3)</label></formula><formula xml:id="formula_3">s j = MultiHead i (q w , h j i )<label>(4)</label></formula><p>where h denotes a hidden state of the NMT net- work. In particular, h t is the last hidden state of the word to be encoded, or decoded at time step t, and h j i is the last hidden state of the i-th word of the j-th sentence of the context. The function f w is a linear transformation to obtain the query q w . We used the MultiHead attention function pro- posed by <ref type="bibr" target="#b12">(Vaswani et al., 2017</ref>) to capture differ- ent types of relations among words. It matches the query against each of the hidden representations h j i (used as value and key for the attention). The sentence-level abstraction summarizes the contextual information required at time t in d t as: </p><formula xml:id="formula_4">q s = f s (h t ) (5) d t = FFN(MultiHead j (q s , s j ))<label>(6)</label></formula><p>where f s is a linear transformation, q s is the query for the attention function, FFN is a position-wise feed-forward layer ( <ref type="bibr" target="#b12">Vaswani et al., 2017)</ref>. Each layer is followed by a normalization layer (Lei Ba et al., 2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Context Gating</head><p>We use a gate (  to regulate the information at sentence-level h t and the contextual information at document-level d t . The intuition is that different words require different amount of context for translation:</p><formula xml:id="formula_5">λ t = σ(W h h t + W d d t )<label>(7)</label></formula><formula xml:id="formula_6">h t = λ t h t + (1 − λ t )d t<label>(8)</label></formula><p>where W h , W p are parameter matrices, and h t is the final hidden representation for a word x t or y t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Integrated Model</head><p>The context can be used during encoding or de- coding a word, and it can be taken from previously encoded source sentences, previously decoded tar- get sentences, or from previous alignment vectors (i.e. context vectors ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>). The different configurations will define the input query and values of the attention function. In this work we experiment with five of them: one at encod- ing time, three at decoding time, and one combin- ing both. At encoding time the query is a func- tion of the hidden state h xt of the current word to be encoded x t , and the values are the encoded states of previous sentences h j x i (HAN encoder). At decoding time, the query is a function of the hidden state h yt of the current word to be decoded y t , and the values can be (a) the encoded states of previous sentences h j x i (HAN decoder source), (b) the decoded states of previous sentences h j y i</p><p>(HAN decoder), and (c) the alignment vectors c j i (HAN decoder alignment). Finally, we combine complementary target-source sides of the context by joining HAN encoder and HAN decoder. <ref type="figure" target="#fig_0">Fig- ure 1</ref> shows the integration of the HAN encoder with the NMT model; a similar architecture is ap- plied to the decoder. The output˜houtput˜ output˜h t is used by the NMT model as replacement of h t during the final classification layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Evaluation Metrics</head><p>We carry out experiments with Chinese-to-English (Zh-En) and Spanish-to-English (Es-En) sets on three different domains: talks, subtitles, and news. TED Talks is part of the IWSLT 2014 and 2015 ( <ref type="bibr">Cettolo et al., 2012</ref><ref type="bibr">Cettolo et al., , 2015</ref>) evaluation campaigns 1 . We use dev2010 for development; and tst2010- 2012 (Es-En), tst2010-2013 (Zh-En) for testing. The Zh-En subtitles corpus is a compilation of TV subtitles designed for research on context ( <ref type="bibr" target="#b14">Wang et al., 2018)</ref>. In contrast to the other sets, it has three references to compare. The Es-En corpus is a subset of OpenSubtitles2018 (Lison and Tiede- mann, 2016) 2 . We randomly select two episodes for development and testing each. Finally, we use the Es-En News-Commentaries11 3 corpus which has document-level delimitation. We evaluate on WMT sets ( <ref type="bibr">Bojar et al., 2013)</ref>: newstest2008 for development, and newstest2009-2013 for testing. A similar corpus for Zh-En is too small to be com- parable. <ref type="table" target="#tab_2">Table 2</ref> shows the corpus statistics.</p><p>For evaluation, we use BLEU score <ref type="bibr" target="#b2">(Papineni et al., 2002</ref>) (multi-blue) on tokenized text, and we measure significance with the paired bootstrap re- sampling method proposed by Koehn (2004) (im- plementations by <ref type="bibr">Koehn et al. (2007)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Configuration and Training</head><p>As baselines, we use a NMT transformer, and a context-aware NMT transformer with cache mem- ory which we implemented for comparison fol- lowing the best model described by , with memory size of 25 words. We used the OpenNMT ( <ref type="bibr">Klein et al., 2017</ref>) implementation of the transformer network. The configuration is the same as the model called "base model" in the original paper <ref type="bibr" target="#b12">(Vaswani et al., 2017)</ref>. The encoder and decoder are composed of 6 hidden layers each. All hidden states have dimension of 512, dropout of 0.1, and 8 heads for the multi-head attention. The target and source vocabulary size is 30K. The optimization and regularization methods were the same as proposed by <ref type="bibr" target="#b12">Vaswani et al. (2017)</ref>. In- spired by  we trained the models in two stages. First we optimize the parameters for the NMT without the HAN, then we proceed to optimize the parameters of the whole network. We use k = 3 previous sentences, which gave the best performance on the development set. <ref type="table" target="#tab_1">Table 1</ref> shows the BLEU scores for different mod- els. The baseline NMT transformer already has better performance than previously published re- sults on these datasets, and we replicate previous previous improvements from the cache method over the this stronger baseline. All of our proposed HAN models perform at least as well as the cache method. The best scores are obtained by the com- bined encoder and decoder HAN model, which is significantly better than the cache method on all datasets without compromising training speed (2.3K vs 2.6K tok/sec). An important portion of the improvement comes from the HAN encoder, which can be attributed to the fact that the source- side always contains correct information, while the target-side may contain erroneous predictions at testing time. But combining HAN decoder with HAN encoder further improves translation perfor- mance, showing that they contribute complemen- tary information. The three ways of incorporating information into the decoder all perform similarly. <ref type="table" target="#tab_4">Table 3</ref> shows the performance of our best HAN model with a varying number k of previous sen- tences in the test-set. We can see that the best per- formance for TED talks and news is archived with 3, while for subtitles it is similar between 3 and 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Translation Performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Accuracy of Pronoun/Noun Translations</head><p>We evaluate coreference and anaphora using the reference-based metric: accuracy of pronoun translation (Miculicich Werlen and Popescu-Belis, 2017b), which can be extended for nouns. The list of evaluated pronouns is predefined in the met- ric, while the list of nouns was extracted using NLTK POS tagging <ref type="bibr">(Bird, 2006</ref>   of <ref type="table" target="#tab_3">Table 4</ref> shows the results. For nouns, the joint HAN achieves the best accuracy with a significant improvement compared to other models, showing that target and source contextual information are complementary. Similarity for pronouns, the joint model has the best result for TED talks and news. However, HAN encoder alone is better in the case of subtitles. Here HAN decoder produces mis- takes by repeating past translated personal pro- nouns. Subtitles is a challenging corpus for per- sonal pronoun disambiguation because it usually involves dialogue between multiple speakers.</p><note type="other">). The upper part TED Talks Subtitles News Zh-En Es-En Zh-En 4 Es-En Es-En</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Cohesion and Coherence Evaluation</head><p>We use the metric proposed by <ref type="bibr" target="#b17">Wong and Kit (2012)</ref> to evaluate lexical cohesion. It is defined as the ratio between the number of repeated and lex- ically similar content words over the total number of content words in a target document. The lexi- cal similarity is obtained using WordNet.  the next, and the results are averaged to get a doc- ument score. We employed the pre-trained LSA model Wiki-6 from ( <ref type="bibr" target="#b7">Stefanescu et al., 2014</ref>). Ta- ble 4 (bottom-right) shows the average coherence score of documents. The joint HAN model consis- tently obtains the best coherence score, but close to other HAN models. Most of the improvement comes from the HAN decoder. <ref type="table" target="#tab_7">Table 5</ref> shows an example where HAN helped to generate the correct translation. The first box shows the current sentence with the analyzed word in bold; and the second, the past context at source and target. For the context visualization we use the toolkit provided by <ref type="bibr" target="#b3">Pappas and Popescu-Belis (2017)</ref>. Red corresponds to sentences, and blue to words. The intensity of color is proportional to the weight. We see that HAN correctly translates the ambiguous Spanish pronoun "su" into the En- glish "his". The HAN decoder highlighted a previ- ous mention of "his", and the HAN encoder high- lighted the antecedent "Nathaniel". This shows that HAN can capture interpretable inter-sentence connections. More samples with different atten- tion heads are shown in the Appendix ??.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Statistical Machine Translation (SMT) Initial studies were based on cache memories (Tiede-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noun Translation</head><p>Pronoun Translation TED <ref type="table">Talks  Subtitles  News  TED Talks  Subtitles  News  Model</ref> Zh-En Es-En Zh-En Es-En Zh-En Es-En Zh-En Es-En Zh-En Es-En NMT     <ref type="formula" target="#formula_0">(2018)</ref> proposed test-sets for evaluating discourse in NMT, <ref type="bibr" target="#b13">Voita et al. (2018)</ref> shows that context-aware NMT improves the of anaphoric pronouns, and <ref type="bibr">Maruf and Haffari (2018)</ref> proposed a document-level NMT using memory-networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed a hierarchical multi-head HAN NMT model <ref type="bibr">5</ref> to capture inter-sentence connections. We integrated context from source and target sides by directly connecting representations from pre- vious sentence translations into the current sen- tence translation. The model significantly outper- forms two competitive baselines, and the ablation study shows that target and source context is com- plementary. It also improves lexical cohesion and coherence, and the translation of nouns and pro- nouns. The qualitative analysis shows that the model is able to identify important previous sen- tences and words for the correct prediction. In fu- ture work, we plan to explicitly model discourse connections with the help of annotated data, which may further improve translation quality. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Integration of HAN during encoding at time step t, ˜ h t is the context-aware hidden state of the word x t. Similar architecture is used during decoding.</figDesc><graphic url="image-1.png" coords="2,318.19,62.81,196.44,107.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>mann, 2010 ;</head><label>2010</label><figDesc>Gong et al., 2011). However, most of the work explicitly models discourse phe- nomena (Sim Smith, 2017) such as lexical co- hesion (Meyer and Popescu-Belis, 2012; Xiong et al., 2013; Loáiciga and Grisot, 2016; Pu et al., 2017; Mascarell, 2017), coherence (Born et al., 2017), and coreference (Rios Gonzales and Tuggener, 2017; Miculicich Werlen and Popescu- Belis, 2017a). Hardmeier et al. (2013) introduced the document-level SMT paradigm. Sentence-level NMT Initial studies on NMT en- hanced the sentence-level context by using mem- ory networks (Wang et al., 2016), self-attention (Miculicich Werlen et al., 2018; Zhang et al., 2016), and latent variables (Yang et al., 2017). Document-level NMT Tiedemann and Scherrer (2017) use the concatenation of multiple sentences as NMT's input/output, Jean et al. (2017) add a context encoder for the previous source sentence, Wang et al. (2017) includes a HRNN to summa- rize source-side context, and Tu et al. (2018) use a dynamic cache memory to store representations of previously translated words. Recently, Baw- den et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>BLEU score for the different configurations of the HAN model, and two baselines. The highest score 
per dataset is marked in bold. ∆ denotes the difference in BLEU score with respect of the NMT transformer. 
The significance values with respect to the NMT and the cache method are denoted by  * , and  † respectively. The 
repetitions correspond to the p-values:  *  
 † &lt; .05,  *  *  
 † † &lt; .01,  *  *  *  
 † † † &lt; .001. 

TED Talks 
Subtitles 
News 
Zh-En Es-En Zh-En Es-En Es-En 
Training 
0.2M 0.2M 2.2M 4.0M 0.2M 
Development 0.8K 
0.8K 
1.1K 
1.0K 
1.9K 
Test 
5.5K 
4.7K 
1.2K 
1.0K 13.5K 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Dataset statistics in # sentence pairs. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 (</head><label>4</label><figDesc></figDesc><table>bottom-left) displays the average ratio per tested 
document. In some cases, HAN decoder achieves 
the best score because it produces a larger quan-
tity of repetitions than other models. However, 
as previously demonstrated in 4.2, repetitions do 
not always make the translation better. Although 
HAN boosts lexical cohesion, the scores are still 
far from the human reference, so there is room for 
improvement in this aspect. 
For coherence, we use a metric based on Latent 
Semantic Analysis (LSA) (Foltz et al., 1998). LSA 
is used to obtain sentence representations, then co-
sine similarity is calculated from one sentence to 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Performance for variable context sizes k with the HAN encoder + HAN decoder.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>Transformer 40.16 65.97 46.65 61.79 47.94 63.44 68.00 69.71 65.83 47.22 + cache 40.87 66.75 46.00 61.87 49.91 63.53 68.66 69.97 66.27 49.34 + HAN encoder 41.93 67.75 46.78 61.52 50.06 64.05 69.17 71.04 68.56 49.57 + HAN decoder 41.61 67.35 46.78 61.99 50.03 64.02 69.36 70.50 67.03 49.33 + HAN encoder + HAN decoder 42.99 67.81 47.43 62.30 50.40 64.35 69.60 70.60 67.47 49.</figDesc><table>59 
Lexical cohesion 
Coherence 
NMT Transformer 
54.26 51.98 51.87 51.77 30.06 0.298 0.299 0.283 0.262 0.279 
+ HAN encoder 
54.87 52.35 51.89 52.33 30.34 0.304 0.299 0.285 0.262 0.280 
+ HAN decoder 
54.95 52.43 52.33 52.43 30.41 0.302 0.301 0.287 0.265 0.282 
+ HAN enc. + HAN dec. 
55.40 52.36 51.94 52.75 30.58 0.305 0.302 0.287 0.265 0.282 
Human reference 
56.08 57.02 54.81 58.19 35.12 0.310 0.314 0.296 0.270 0.298 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Evaluation on discourse phenomena. Noun and pronoun translation: Accuracy with respect to a human 
reference. Lexical cohesion: Ratio of repeated and lexically similar words over the number of content words. 
Coherence: Average cosine similarity of consecutive sentences (i.e. average of LSA word-vectors) 

Currently Translated Sentence 

Src.: y esto es un escape de su estado atormentado . 
Ref.: and that is an escape from his tormented state . 
Base: and this is an escape from its &lt; unk &gt; state . 
Cache: and this is an escape from their state . 
HAN: and this is an escape from his &lt; unk &gt; state . 
Context from Previous Sentences 
HAN decoder context with target. Query: his (En) 

HAN encoder context with source. Query: su (Es) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Example of pronoun disambiguation using 
HAN (TED Talks Es-En). 

</table></figure>

			<note place="foot" n="1"> https://wit3.fbk.eu 2 http://www.opensubtitles.org 3 http://opus.nlpl.eu/News-Commentary11.php</note>

			<note place="foot" n="4"> NIST BLEU: NMT transformer 35.99, cache 36.52, and HAN 37.15.</note>

			<note place="foot" n="5"> Code available at https://github.com/idiap/HAN_NMT. Project Towards Document-Level NMT (Miculicich Werlen, 2017)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful for the support of the European Union under the Horizon 2020 SUMMA project n. 688139, see www.summa-project.eu.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations<address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Validation of an automatic metric for the accuracy of pronoun translation (APT)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lesly</forename><surname>Miculicich Werlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Popescu-Belis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Discourse in Machine Translation</title>
		<meeting>the Third Workshop on Discourse in Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="17" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multilingual hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Popescu-Belis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1015" to="1025" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Consistent translation of repeated nouns using syntactic and semantic cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Mascarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Popescu-Belis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="948" to="957" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Coreference resolution of elided subjects and possessive pronouns in spanish-english statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annette</forename><forename type="middle">Rios</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gonzales</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Tuggener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="657" to="662" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On integrating discourse in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><forename type="middle">Sim</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Discourse in Machine Translation</title>
		<meeting>the Third Workshop on Discourse in Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="110" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Latent semantic analysis models on wikipedia and tasa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Stefanescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajendra</forename><surname>Banjade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasile</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC-2014)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC-2014)</meeting>
		<imprint>
			<publisher>ELRA</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Context adaptation in statistical machine translation using models with exponentially decaying cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing</title>
		<meeting>the 2010 Workshop on Domain Adaptation for Natural Language Processing<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="8" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural machine translation with extended context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Scherrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Discourse in Machine Translation</title>
		<meeting>the Third Workshop on Discourse in Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="82" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Context gates for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="87" to="99" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to remember translation history with a continuous cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Context-aware neural machine translation learns anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Voita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Serdyukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1264" to="1274" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Translating pro-drop languages with reconstruction models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploiting cross-sentence context for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2826" to="2831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Memory-enhanced decoder for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="278" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Extending machine translation evaluation metrics with lexical cohesion to document level</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Billy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1060" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lexical chain based cohesion models for document-level statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew Lim</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1563" to="1573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural machine translation with recurrent attention modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="383" to="387" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Variational neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="521" to="530" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
