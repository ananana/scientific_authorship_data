<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Relations such as Hypernymy: Identifying and Exploiting Hearst Patterns in Distributional Vectors for Lexical Entailment</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
							<email>roller@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Linguistics</orgName>
								<orgName type="institution" key="instit1">The University of Texas at Austin</orgName>
								<orgName type="institution" key="instit2">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
							<email>katrin.erk@mail.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Linguistics</orgName>
								<orgName type="institution" key="instit1">The University of Texas at Austin</orgName>
								<orgName type="institution" key="instit2">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Relations such as Hypernymy: Identifying and Exploiting Hearst Patterns in Distributional Vectors for Lexical Entailment</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2163" to="2172"/>
							<date type="published">November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We consider the task of predicting lexical entailment using distributional vectors. We perform a novel qualitative analysis of one existing model which was previously shown to only measure the prototypicality of word pairs. We find that the model strongly learns to identify hypernyms using Hearst patterns, which are well known to be predictive of lexical relations. We present a novel model which exploits this behavior as a method of feature extraction in an iterative procedure similar to Principal Component Analysis. Our model combines the extracted features with the strengths of other proposed models in the literature, and matches or outperforms prior work on multiple data sets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As the field of Natural Language Processing has de- veloped, more ambitious semantic tasks are starting to be addressed, such as Question Answering (QA) and Recognizing Textual Entailment (RTE). These systems often depend on the use of lexical resources like WordNet in order to infer entailments for indi- vidual words, but these resources are expensive to develop, and always have limited coverage.</p><p>To address these issues, many works have con- sidered on how lexical entailments can be derived automatically using distributional semantics. Some focus mostly on the use of unsupervised techniques, and study measures which emphasize particular word relations ( <ref type="bibr" target="#b0">Baroni and Lenci, 2011</ref>). Many are based on the Distributional Inclusion Hypothesis, which states that the contexts in which a hypernym appears are a superset of its hyponyms' contexts <ref type="bibr" target="#b20">(Zhitomirsky-Geffet and Dagan, 2005;</ref><ref type="bibr" target="#b5">Kotlerman et al., 2010)</ref>. More recently, a great deal of work has pushed toward using supervised methods ( <ref type="bibr" target="#b1">Baroni et al., 2012;</ref><ref type="bibr" target="#b13">Roller et al., 2014;</ref><ref type="bibr" target="#b19">Weeds et al., 2014;</ref><ref type="bibr" target="#b6">Kruszewski et al., 2015</ref>), varying by their experimental setup or proposed model. Yet the literature disagrees about which models are strongest ( <ref type="bibr" target="#b19">Weeds et al., 2014;</ref><ref type="bibr" target="#b13">Roller et al., 2014)</ref>, or even if they work at all ( ). Indeed,  showed that two exist- ing lexical entailment models fail to account for similarity between the antecedent and consequent, and conclude that such models are only learning to predict prototypicality: that is, they predict that cat entails animal because animal is usually en- tailed, and therefore will also predict that sofa en- tails animal. Yet it remains unclear why such models make for such strong baselines ( <ref type="bibr" target="#b19">Weeds et al., 2014;</ref><ref type="bibr" target="#b6">Kruszewski et al., 2015;</ref>.</p><p>We present a novel qualitative analysis of one pro- totypicality classifier, giving new insight into why prototypicality classifiers perform strongly in the lit- erature. We find the model overwhelmingly learns to identify hypernyms using Hearst patterns avail- able in the distributional space, like "animals such as cats" and "animals including cats." These pat- terns have long been used to identify lexical rela- tions <ref type="bibr" target="#b4">(Hearst, 1992;</ref><ref type="bibr" target="#b15">Snow et al., 2004</ref>).</p><p>We propose a novel model which exploits this be- havior as a method of feature extraction, which we call H-feature detectors. Using an iterative proce- dure similar to Principal Component Analysis, our model is able to extract and learn using multiple H- feature detectors. Our model also integrates overall word similarity and Distributional Inclusion, bring- ing together strengths of several models in the litera- ture. Our model matches or outperforms prior work on multiple data sets. The code, data sets, and model predictions are made available for future research. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Research on lexical entailment using distributional semantics has now spanned more than a decade, and has been approached using both unsupervised ( <ref type="bibr" target="#b18">Weeds et al., 2004;</ref><ref type="bibr" target="#b5">Kotlerman et al., 2010;</ref><ref type="bibr" target="#b7">Lenci and Benotto, 2012;</ref><ref type="bibr" target="#b14">Santus, 2013)</ref> and supervised techniques ( <ref type="bibr" target="#b1">Baroni et al., 2012;</ref><ref type="bibr" target="#b3">Fu et al., 2014;</ref><ref type="bibr" target="#b13">Roller et al., 2014;</ref><ref type="bibr" target="#b19">Weeds et al., 2014;</ref><ref type="bibr" target="#b6">Kruszewski et al., 2015;</ref><ref type="bibr" target="#b16">Turney and Mohammad, 2015;</ref><ref type="bibr" target="#b14">Santus et al., 2016</ref>). Most of the work in unsupervised methods is based on the Dis- tributional Inclusion Hypothesis ( <ref type="bibr" target="#b18">Weeds et al., 2004;</ref><ref type="bibr" target="#b20">Zhitomirsky-Geffet and Dagan, 2005)</ref>, which states that the contexts in which a hypernym appear should be a superset over its hyponyms' contexts.</p><p>This work focuses primarily on the supervised works in the literature. Formally, we consider meth- ods which treat lexical entailment as a supervised classification problem, which take as input the dis- tributional vectors for a pair of words, (H, w), and predict on whether the antecedent w entails the con- sequent H. <ref type="bibr">2</ref> One of the earliest supervised approaches was Concat ( <ref type="bibr" target="#b1">Baroni et al., 2012)</ref>. In this work, the con- catenation of the pair H, w was used as input to an off-the-shelf SVM classifier. At the time, it was very successful, but later works noted that it had major problems with lexical memorization <ref type="bibr" target="#b13">(Roller et al., 2014;</ref><ref type="bibr" target="#b19">Weeds et al., 2014;</ref>. That is, when the training and test sets were carefully con- structed to ensure they were completely disjoint, it performed extremely poorly. Nonetheless, Concat is continually used as a strong baseline in more recent work ( <ref type="bibr" target="#b6">Kruszewski et al., 2015)</ref>.</p><p>In response to these issues of lexical memoriza- tion, alternative models were proposed. Of particu- lar note are the Diff ( <ref type="bibr" target="#b3">Fu et al., 2014;</ref><ref type="bibr" target="#b19">Weeds et al., 2014</ref>) and Asym classifiers ( <ref type="bibr" target="#b13">Roller et al., 2014</ref>). The Diff model takes the vector difference H − w as input, while the Asym model uses both the vector difference and the squared vector difference as in- put. <ref type="bibr" target="#b19">Weeds et al. (2014)</ref> found that Concat moder- ately outperformed Diff, while <ref type="bibr" target="#b13">Roller et al. (2014)</ref> found that Asym outperformed Concat. Both Diff and Asym can also be seen as a form of supervised Distributional Inclusion Hypothesis, with the vector difference being analogous to the set-inclusion mea- sures of some unsupervised techniques ( <ref type="bibr" target="#b13">Roller et al., 2014</ref>). All of these works focused exclusively on hy- pernymy detection, rather than the more general task of lexical entailment.</p><p>Recently, other works have begun to analyze Con- cat and Diff for their ability to go beyond just hyper- nymy detection. <ref type="bibr" target="#b17">Vylomova et al. (2016)</ref> take an ex- tensive look at Diff's ability to model a wide variety of lexical relations and conclude it is generally ro- bust, and <ref type="bibr" target="#b6">Kruszewski et al. (2015)</ref> have success with a neural network model based on the Distributional Inclusion Hypothesis.</p><p>On the other hand,  analyze both Concat and Diff in their ability to detect general lex- ical entailment on five data sets: two consisting of only hypernymy, and three covering a wide variety of other entailing word relations. They find that both Concat and Diff fail, and analytically show that they are learning to predict the prototypicality of the con- sequent H, rather than the relationship between the antecedent and the consequent, and consider this a form of lexical memorization. They propose a new model, Ksim, which addresses their concerns, but lacks any notion of Distributional Inclusion. In par- ticular, they argue for directly including the cosine similarity of w and H as a term in a custom SVM kernel, in order to determine whether w and H are related all. Ultimately,  conclude that distributional vectors may simply be the wrong tool for the job.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data and Resources</head><p>Prior work on lexical entailment relied on a variety of data sets, each constructed in a different manner.</p><p>We focus on four different data sets, each of which has been used for evaluation in prior work. Two data sets contain only hypernymy relations, and two con- sider general lexical entailment.</p><p>Our first data set is LEDS, the Lexical Entail- ment Data Set, originally created by <ref type="bibr" target="#b1">Baroni et al. (2012)</ref>. The data set contains 1385 hyponym- hypernym pairs extracted directly from WordNet, forming a set of positive examples. Negative exam- ples were generated by randomly shuffling the orig- inal set of 1385 pairs. As such, LEDS only contains examples of hypernymy and random relations.</p><p>Another major data set has been BLESS, the Baroni and Lenci (2011) Evaluation of Semantic Spaces. The data set contains annotations of word relations for 200 unambiguous, concrete nouns from 17 broad categories. Each noun is annotated with its co-hyponyms, meronyms, hypernym and some ran- dom words. In this work, we treat hypernymy as positive, and other relations as negative.</p><p>These two data sets form our hypernymy data sets, but we cannot overstate their important differ- ences: LEDS is balanced, while BLESS contains mostly negative examples; negatives in BLESS in- clude both random pairs and pairs exhibiting other strong semantic relations, while LEDS only contains random pairs. Furthermore, all of the negative ex- amples in LEDS are the same lexical items as the positive items, which has strong implications on the prototypicality argument of .</p><p>The next data set we consider is Medical ( ). This data set contains high quality anno- tations of subject-verb-object entailments extracted from medical texts, and transformed into noun-noun entailments by argument alignments. The data con- tains 12,600 annotations, but only 945 positive ex- amples encompassing various relations like hyper- nymy, meronomy, synonymy and contextonymy. <ref type="bibr">3</ref> This makes it one of the most difficult data sets: it is both domain specific and highly unbalanced.</p><p>The final data set we consider is TM14, a varia- tion on the SemEval 2012 Shared Task of identifying the degree to which word pairs exhibit various rela- tions. These relationships include a small amount of hypernymy, but also many more uncommon rela-tions (agent-object, cause-effect, time-activity, etc). Relationships were binarized into (non-)entailing pairs by <ref type="bibr" target="#b16">Turney and Mohammad (2015)</ref>. The data set covers 2188 pairs, 1084 of which are entailing.</p><p>These two entailment data sets also contain im- portant differences, especially in contrast to the hy- pernymy data sets. Neither contains any random negative pairs, meaning general semantic similarity measures should be less useful; And both exhibit a variety of non-hypernymy relations, which are less strictly defined and more difficult to model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Distributional Vectors</head><p>In all experiments, we use a standard, count-based, syntactic distributional vector space. We use a cor- pus composed of the concatenation of Gigaword, Wikipedia, BNC and ukWaC. We preprocess the corpus using Stanford CoreNLP 3.5.2 <ref type="bibr" target="#b2">(Chen and Manning, 2014</ref>) for tokenization, lemmatization, POS-tagging and universal dependency parses. We compute a syntactic distributional space for the 250k most frequent lemmas by counting their dependency neighbors across the corpus. We use only the top 1M most frequent dependency attachments as contexts. We use CoreNLP's "collapsed dependencies", in which prepositional dependencies are collapsed e.g. "go to the store" emits the tuples (go, prep:to+store) and (store, prep:to −1 +go). After collecting counts, vectors are transformed using PPMI, SVD reduced to 300 dimensions, and normalized to unit length. The use of collapsed dependencies is very important, as we will see in Section 4, but other parameters are reasonably robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Motivating Analysis</head><p>As discussed in Section 2, the Concat classifier is a classifier trained on the concatenation of the word vectors, H, w. As additional background, we first review the findings of , who showed that Concat trained using a linear classifier is only able to capture notions of prototypicality; that is, Concat guesses that (animal, sofa) is a positive example because animal looks like a hypernym.</p><p>Formally, a linear classifier like Logistic Regres- sion or Linear SVM learns a decision hyperplane represented by a vectorˆpvectorˆ vectorˆp. Data points are compared to this plane with the inner product: those above the plane (positive inner product) are classified as entailing, and those below as non-entailing. Cru- cially, since the input features are the concatenation of the pair vectors H, w, the hyperplanê p vec- tor can be decomposed into separate H and w com- ponents. Namely, if we rewrite the decision planêplanê p = ˆ H, ˆ w, we find that each pair H, w is classi- fied using:</p><formula xml:id="formula_0">ˆ p H, w = ˆ H, ˆ w H, w = ˆ H H + ˆ w w.<label>(1)</label></formula><p>This analysis shows that, when the hyperplanê p is evaluated on a novel pair, it lacks any form of direct interaction between H and w like the inner prod- uct H w. Without any interaction terms, the Con- cat classifier has no way of estimating the relation- ship between the two words, and instead only makes predictions based on two independent terms, ˆ H andˆw andˆ andˆw, the prototypicality vectors. Furthermore, the Diff classifier can be analyzed in the same fashion and therefore has the same fatal property.</p><p>We agree with this prototypicality interpretation, although we believe it is incomplete: while it places a fundamental ceiling on the performance of these classifiers, it does not explain why others have found them to persist as strong baselines ( <ref type="bibr" target="#b19">Weeds et al., 2014;</ref><ref type="bibr" target="#b13">Roller et al., 2014;</ref><ref type="bibr" target="#b6">Kruszewski et al., 2015;</ref><ref type="bibr" target="#b17">Vylomova et al., 2016)</ref>. To approach this ques- tion, we consider a baseline Concat classifier trained using a linear model. This classifier should most strongly exhibit the prototypicality behavior accord- ing to Equation 1, making it the best choice for anal- ysis. We first consider the most pessimistic hypothe- sis: is it only learning to memorize which words are hypernyms at all?</p><p>We train the baseline Concat classifier using Lo- gistic Regression on each of the four data sets, and extract the vocabulary words which are most simi- lar to thê H half of the learned hyperplanê p. If the classifier is only learning to memorize the training data, we would expect items from the data to dom- inate this list of closest vocabulary terms. <ref type="table" target="#tab_0">Table 1</ref> gives the five most similar words to the learned hy- perplane, with bold words appearing directly in the data set.</p><p>Interestingly, we notice there are very few bold words at all in the list. In LEDS, we actually see  <ref type="table" target="#tab_0">Medical  TM14  material  goods  item  sensitiveness  structure lifeform  unlockable  tactility  object  item  succor  palate  process  equipment team-up  stiffness  activity</ref> herbivore non-essential content some hypernyms of data set items that do not even appear in the data set, and the Medical and TM14 words do not even appear related to the content of the data sets. Similar results were also found for Diff and Asym, and both when using Linear SVM and Logistic Regression. These lists cannot explain the success of the prototypicality classifiers in prior work. Instead, we propose an alternative interpreta- tion of the hyperplane: that of a feature detector for hypernyms, or an H-feature detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">H-Feature Detectors</head><p>Recall that distributional vectors are derived from a matrix M containing counts of how often words co-occur with the different syntactic contexts. This co-occurrence matrix is factorized using Singular Value Decomposition, producing both W , the ubiq- uitous word-embedding matrix, and C, the context- embedding matrix ( <ref type="bibr" target="#b8">Levy and Goldberg, 2014)</ref>:</p><formula xml:id="formula_1">M ≈ W C</formula><p>Since the word and context embeddings implicitly live in the same vector space ( <ref type="bibr" target="#b11">Melamud et al., 2015)</ref>, we can also compare Concat's hyperplane with the context matrix C. Under this interpretation, the Concat model does not learn what words are hy- pernyms, but rather what contexts or features are in- dicative of hypernymy. <ref type="table" target="#tab_1">Table 2</ref> shows the syntactic contexts with the highest cosine similarity to thê H prototype for each of the different data sets.</p><p>This view of Concat as an H-feature detector produces a radically different perspective on the classifier's hyperplane. Nearly all of the features learned take the form of Hearst patterns <ref type="bibr" target="#b4">(Hearst, 1992;</ref><ref type="bibr" target="#b15">Snow et al., 2004</ref>). The most recognizable and common pattern learned is the "such as" pat- tern, as in "animals such as cats". These patterns have been well known to be indicative of hyper- nymy for over two decades. Other interesting pat-LEDS BLESS nmod:such as+animal nmod:such as+submarine acl:relcl+identifiable nmod:such as+ship nmod:of −1 +determine nmod:such as+seal nmod:of −1 +categorisation nmod:such as+plane compound+many nmod:such as+rack nmod:such as+pot nmod:such as+rope terns are the "including" pattern ("animals includ- ing cats") and "many" pattern ("many animals"). Although we list only the six most similar context items for the data sets, we find similar contexts con- tinue to dominate the list for the next 30-50 items. Taken together, it is remarkable that the model iden- tified these patterns using only distributional vectors and only the positive/negative example pairs. How- ever, the reader should note these are not true Hearst patterns: Hearst patterns explicitly relate a hyper- nym and hyponym using an exact pattern match of a single co-occurrence. On the other hand, these H-features are aggregate indicators of hypernymy across a large corpus.</p><note type="other">Medical TM14 nmod:such as+patch amod+desire nmod:such as+skin amod+heighten nmod:including+skin nsubj −1 +disparate nmod:such as+tooth nmod:such as+honey nmod:such as+feather nmod:with −1 +body nmod:including+finger nsubj −1 +unconstrained</note><p>These learned features are much more inter- pretable than those found in the analysis of prior work like <ref type="bibr" target="#b13">Roller et al. (2014)</ref> and . <ref type="bibr" target="#b13">Roller et al. (2014)</ref> found no signals of H-features in their analysis of one classifier, but their model was focused on bag-of-words distributional vectors, which perform significantly worse on the task.  also performed an analysis of lexical entailment classifiers, and found weak signals like "such" and "of" appearing as prominent contexts in their classifier, giving an early hint of H-feature de- tectors, but not to such an overwhelming degree as we see in this work. Critically, their analysis fo- cused on a classifier trained on high-dimensional, sparse vectors, rather than focusing on context em- beddings as we do. By using these sparse vectors, their model was unable to generalize across simi- lar contexts. Additionally, their model did not make use of collapsed dependencies, making features like "such" much weaker signals of entailment and there- fore less dominant during analysis.</p><p>Among these remarkable lists, the LEDS and TM14 data sets stand out for having much fewer "such as" patterns compared to BLESS and Medi- cal. The reason for this is explained by the construc- tion of the data sets: since LEDS contains the same words used as both positive and negative examples, the classifier has a hard time picking out clear sig- nal. The TM14 data set, however, does not contain any such negative examples.</p><p>We hypothesize the TM14 data set contains too many diverse and mutually exclusive forms of lex- ical entailment, like instrument-goal (e.g. "honey" → "sweetness"). To test this, we retrained the model with only hypernymy as positive examples, and all other relations as negative. We find that "such as" type patterns become top features, but also some interesting data specific features, like "retailer of <ref type="bibr">[clothes]</ref>". Examining the data shows it contains many consumer goods, like "beverage" or "clothes", which explains these features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Proposed Model</head><p>As we saw in the previous section, Concat only acts as a sort of H-feature detector for whether H is a prototypical hypernym, but does not actually infer the relationship between H and w. Nonetheless, this is powerful behavior which should still be used in combination with the insights of other models like Ksim and Asym. To this end, we propose a novel model which exploits Concat's H-feature detector behavior, extends its modeling power, and adds two other types of evidence proposed in the literature: overall similarity, and distributional inclusion.</p><p>Our model works through an iterative procedure similar to Principal Component Analysis (PCA). Each iteration repeatedly trains a Concat classifier under the assumption that it acts as an H-feature de- tector, and then explicitly discards this information from the distributional vectors. By training a new H-feature detector on these modified distributional vectors, we can find additional features indicative of entailment which were missed by the first classifier. The entire procedure is iteratively repeated similar to how in Principal Component Analysis, the second principal component is computed after the first prin- cipal component has been removed from the data.</p><p>The main insight is that after training some H- feature detector using Concat, we can remove this prototype from the distributional vectors through the use of vector projection. Formally, the vector pro- jection of x onto a vectorˆpvectorˆ vectorˆp, projˆpprojˆ projˆp (x) finds the com- ponent of x which is in the direction ofˆpofˆ ofˆp, <ref type="figure" target="#fig_1">Figure 1</ref> gives a geometric illustration of the vector projection. If x forms the hypotenuse of a right tri- angle, projˆpprojˆ projˆp (x) forms a leg of the triangle. This also gives rise to the vector rejection, which is the vec- tor forming the third leg of the triangle. The vector rejection is orthogonal to the projection, and intu- itively, is the original vector after the projection has been removed:</p><formula xml:id="formula_2">projˆpprojˆ projˆp (x) = x ˆ p ˆ p ˆ p.</formula><formula xml:id="formula_3">rejˆprejˆ rejˆp (x) = x − projˆpprojˆ projˆp (x).</formula><p>Using the vector rejection, we take a learned H- feature detectorˆpdetectorˆ detectorˆp, and discard these features from each of the word vectors. That is, for every data point H, w, we replace it by its vector rejection and rescale it to unit magnitude:</p><formula xml:id="formula_4">H i+1 = rejˆprejˆ rejˆp (H)/rejˆprejˆ rejˆp (H) w i+1 = rejˆprejˆ rejˆp (w)/rejˆprejˆ rejˆp (w)</formula><p>A new classifier trained on the H i+1 , w i+1 data must now learn a different decision plane thanˆpthanˆ thanˆp, asˆp asˆ asˆp is no longer present in any data points. This repeti- tion of the procedure is roughly analogous to learn- ing the second principal component of the data; we wish to classify the pairs without using any informa- tion learned from the previous iteration.</p><p>This second classifier must perform strictly worse than the original, otherwise the first classifier would have learned this second hyperplane. Nonetheless, it will be able to learn new H-feature detectors which the original classifier was unable to capture. By repeating this process, we can find several H- feature detectors, ˆ p 1 , . . . , ˆ p n . Although the first, ˆ p 1 is the best possible single H-feature detector, each additional H-feature detector increases the model's representational power (albeit with diminishing re- turns).</p><p>This procedure alone does not address the main concern of : that these linear clas- sifiers never actually model any connection between H and w. To address this, we explicitly compare H and w by extracting additional information about how H and w interact with respect to each of the H-feature detectors. This additional information is then used to train one final classifier which makes the final prediction.</p><p>Concretely, in each iteration i of the procedure, we generate a four-valued feature vector F i , based on the H-feature detectorˆpdetectorˆ detectorˆp i . Each feature vector contains (1) the similarity of H i and w i (before pro- jection); (2) the featurê p i applied to H i ; (3) the H- feature detectorˆpdetectorˆ detectorˆp i applied to w i ; and (4) the differ- ence of 2 and 3.</p><formula xml:id="formula_5">F i (H i , w i , ˆ p i ) = H i w i , H i ˆ p i , w i ˆ p i , (H i − w i ) ˆ p i</formula><p>These four "meta"-features capture all the bene- fits of the H-feature detector (slots 2 and 3), while still addressing Concat's issues with similarity argu- ments (slot 1) and distributional inclusion (slot 4). The final feature's relation to the DIH comes from the observation of <ref type="bibr" target="#b13">Roller et al. (2014)</ref> that the vec- tor difference intuitively captures whether the hyper- nym includes the hyponym. The union of all the feature vectors F 1 , . . . , F n from repeated iteration form a 4n-dimensional fea- ture vector which we use as input to one final classi- fier which makes the ultimate decision. This classi- fier is trained on the same training data as each of the individual H-feature detectors, so our iterative pro- cedure acts only as a method of feature extraction.</p><p>For our final classifier, we use an SVM with an RBF-kernel, though decision trees and other non- linear classifiers also perform reasonably well. The nonlinear final classifier can be understood as do- ing a form of logical reasoning about the four slots: "animal" is a hypernym of "cat" because (1) they are similar words where (2) animal looks like a hyper- nym, but (3) cat does not, and (4) some "animal" contexts are not good "cat" contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Setup and Evaluation</head><p>In our experiments, we use a variation of 20-fold cross validation which accounts for lexical overlap. To simplify explanation, we first explain how we generate splits for training/testing, and then after- wards introduce validation methodology.</p><p>We first pool all the words from the antecedent (LHS) side of the data into a set, and split these lex- ical items into 20 distinct cross-validation folds. For each fold F i , we then use all pairs (w, H) where w ∈ F i as the test set pairs. That is, if "car" is in the test set fold, then "car → vehicle" and "car truck" will appear as test set pairs. The training set will then be every pair which does not contain any overlap with the test set; e.g. the training set will be all pairs which do not contain "car", "truck" or "ve- hicle" as either the antecedent or consequent. This ensures that both (1) there is zero lexical overlap be- tween training and testing and (2) every pair is used as an item in a test fold exactly once. One quirk of this setup is that all test sets are approximately the same size, but training sizes vary dramatically.</p><p>This setup differs from those of previous works like <ref type="bibr" target="#b6">Kruszewski et al. (2015)</ref> and , who both use single, fixed train/test/val sets without lexical overlap. We find our setup has several advan- tages over fixed sets. First, we find there can be con- siderable variance if the train/test set is regenerated with a different random seed, indicating that multi- ple trials are necessary. Second, fixed setups con- sistently discard roughly half the data as ineligible for either training or test, as lexical items appear in many pairs. Our CV-like setup allows us to evaluate performance over every item in the data set exactly once, making a much more efficient and representa- tive use of the original data set.</p><p>Our performance metric is F1 score. This is more  representative than accuracy, as most of the data sets are heavily unbalanced. We report the mean F1 scores across all cross validation folds.</p><note type="other">.843 .631 .240 .701 Nonlinear Models RBF .779 .574 .215 .705 Ksim .893 .488 .224 .707 Our model .901 .631 .260 .697</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Hyperparameter Optimization</head><p>In order to handle hyperparameter selection, we ac- tually generate the test set using fold i, and use fold i − 1 as a validation set (removing pairs which would overlap with test), and the remaining 18 folds as training (removing pairs which would over- lap with test or validation). We select hyperpa- rameters using grid search. For all models, we optimize over the regularization parameter C ∈ {10 −4 , 10 −3 , . . . , 10 4 }, and for our proposed model, the number of iterations n ∈ {1, . . . , 6}. All other hyperparameters are left as defaults provided by Scikit-Learn (Pedregosa et al., 2011), except for us- ing balanced class weights. Without balanced class weights, several of the baseline models learn degen- erate functions (e.g. always guess non-entailing).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>We compare our proposed model to several ex- isting and alternative baselines from the literature. Namely, we include a baseline Cosine classifier, which only learns a threshold which maximizes F1 score on the training set; three linear models of prior work, Concat, Diff and Asym; and the RBF and Ksim models found to be successful in <ref type="bibr" target="#b6">Kruszewski et al. (2015)</ref> and   Ksim+Asym, because Ksim is based on a custom SVM kernel which is not amenable to combinations. <ref type="table" target="#tab_3">Table 3</ref> the results across all four data sets for all of the listed models. Our proposed model improves significantly <ref type="bibr">4</ref> over Concat in the LEDS, BLESS and Medical data sets, indicating the benefits of combin- ing these aspects of similarity and distributional in- clusion with the H-feature detectors of Concat. The Concat+Asym classifier also improves over the Con- cat baseline, further emphasizing these benefits. Our model performs approximately the same as Ksim on the LEDS and TM14 data sets (no significant difference), while significantly outperforming it on BLESS and Medical data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Ablation Experiments</head><p>In order to evaluate how important each of the vari- ous F features are to the model, we also performed an ablation experiment where the classifier is not given the similarity (slot 1), prototype H-feature de- tectors (slots 2 and 3) or the inclusion features (slot 4). To evaluate the importance of these features, we fix the regularization parameter at C = 1, and train all ablated classifiers on each training fold with number of iterations n = 1, . . . , 6. <ref type="table" target="#tab_5">Table 4</ref> shows the decrease (absolute difference) in performance between the full and ablated models on the develop- ment sets, so higher numbers indicate greater feature importance.</p><p>We find the similarity feature is extremely impor- tant in the LEDS, BLESS and Medical data sets, therefore reinforcing the findings of . The similarity feature is especially impor- tant in the LEDS and BLESS data sets, where neg- ative examples include many random pairs. The detector features are moderately important for the Medical and TM14 data sets, and critically impor- tant on BLESS, where we found the strongest evi-dence of Hearst patterns in the H-feature detectors. Surprisingly, the detector features are moderately detrimental on the LEDS data set, though this can also be understood in the data set's construction: since the negative examples are randomly shuffled positive examples, the same detector signal will ap- pear in both positive and negative examples. Finally, we find the model performs somewhat robustly with- out the inclusion feature, but still is moderately im- pactful on three of the four data sets, lending further evidence to the Distributional Inclusion Hypothesis. In general, we find all three components are valu- able sources of information for identifying hyper- nymy and lexical entailment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Analysis by Number of Iterations</head><p>In order to evaluate how the iterative feature extrac- tion affects model performance, we fix the regular- ization parameter at C = 1, and train our model fixing the number of iterations to n = {1, . . . , 6}. We then measure the mean F1 score across the de- velopment folds and compare to a baseline which uses only one iteration. <ref type="figure" target="#fig_2">Figure 2</ref> shows these results across all four data sets, with the 0 line set at per- formance of the n = 1 baseline. Models above 0 benefit from the additional iterations, while models below do not.</p><p>In the figure, we see that the iterative pro- cedure moderately improves performance LEDS, while greatly improving the scores of BLESS and TM14, but on the medical data set, additional it- erations actually hurt performance. The differing curves indicate that the optimal number of itera- tions is very data set specific, and provides differing amounts of improvement, and therefore should be tuned carefully. The LEDS and BLESS curves indi- cate a sort of "sweet spot" behavior, where further iterations degrade performance.</p><p>To gain some additional insight into what is cap- tured by the various iterations of the feature extrac- tion procedure, we repeat the procedure from Sec- tion 4: we train our model on the entire BLESS data set using a fixed four iterations and regular- ization parameter. For each iteration, we compare its learned H-feature detector to the context embed- dings, and report the most similar contexts for each iteration in <ref type="table" target="#tab_7">Table 5</ref>.</p><p>The first iteration is identical to the one in Ta-   ble 2, as expected. The second iteration includes many H-features not picked up by the first itera- tion, mostly those of the form "X including Y". The third iteration picks up some data set specific signal, like "free-swimming <ref type="bibr">[animal]</ref>" and "value of <ref type="bibr">[computer]</ref>", and so on. By the fourth iteration, the fea- tures no longer exhibit any obvious Hearst patterns, perhaps exceeding the sweet spot we observed in <ref type="figure" target="#fig_2">Figure 2</ref>. Nonetheless, we see how multiple iter- ations of the procedure allows our model to capture many more useful features than a single Concat clas- sifier on its own.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We considered the task of detecting lexical entail- ment using distributional vectors of word meaning. Motivated by the fact that the Concat classifier acts as a strong baseline in the literature, we proposed a novel interpretation of the model's hyperplane. We found the Concat classifier overwhelmingly acted as a feature detector which automatically identifies Hearst Patterns in the distributional vectors.</p><p>We proposed a novel model that embraces these H-feature detectors fully, and extends their model- ing power through an iterative procedure similar to Principal Component Analysis. In each iteration of the procedure, an H-feature detector is learned, and then removed from the data, allowing us to iden- tify several different kinds of Hearst Patterns in the data. Our final model combines these H-feature de- tectors with measurements of general similarity and Distributional Inclusion, in order to integrate the strengths of different models in prior work. Our model matches or exceeds the performance of prior work, both on hypernymy detection and general lex- ical entailment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A vectorˆpvectorˆ vectorˆp is used to break x into two orthogonal components, its projection and the rejection overˆpoverˆ overˆp.</figDesc><graphic url="image-1.png" coords="6,115.20,57.83,140.41,97.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance of model on development folds by number of iterations. Plots show the improvement (absolute difference) in mean F1 over the model fixed at one iteration. Iteration 1 Iteration 2 Iteration 3 Iteration 4 nmod:such as+submarine nmod:including+animal amod+free-swimming advcl+crown nmod:such as+ship nmod:including+snail nmod:including −1 +thing advcl+victorious nmod:such as+seal nmod:including+insect nsubj −1 +scarcer nsubj+eaters nmod:such as+plane nmod:such as+crustacean nsubj −1 +pupate nsubj+kaine nmod:such as+rack nmod:such as+mollusc nmod:such as+mollusc nmod:at+finale nmod:such as+rope nmod:such as+insect nmod:of −1 +value nsubj+gowen nmod:such as+box nmod:such as+animal nmod:as −1 +exhibit nsubj+pillman</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Most similar words to the prototypê 
H learned by the 
Concat model. Bold items appear in the data set. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Most similar contexts to the prototypê 
H learned by 
the Concat model. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : Mean F1 scores for each model and data set.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 : Absolute decrease in mean F1 on the development sets with the different feature types ablated. Higher numbers</head><label>4</label><figDesc></figDesc><table>indicate greater feature importance. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Most similar contexts to the H-feature detector for each iteration of the PCA-like procedure. This model was trained on all 

data of BLESS. The first and second iterations contain clear Hearst patterns, while the third and fourth contain some data-specific 

and non-obvious signals. 

</table></figure>

			<note place="foot" n="1"> http://github.com/stephenroller/ emnlp2016 2 We use the notation w and H for word and hypernym. These variables refer to either the lexical items, or their distributional vectors, depending on context.</note>

			<note place="foot" n="3"> A term for entailments that occur in some contexts, but do not cleanly fit in other categories; e.g. hospital entails doctor.</note>

			<note place="foot" n="4"> Bootstrap test, p &lt; .01.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank I. Beltagy, Vered Shwartz, Subhashini Venugopalan, and the review-ers for their helpful comments and suggestions. This research was supported by the NSF grant IIS 1523637. We acknowledge the Texas Advanced Computing Center for providing grid resources that contributed to these results.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How we BLESSed distributional semantic evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics</title>
		<meeting>the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics<address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Entailment above the word level in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Quynh</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Chieh</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the European Chapter of the Association for Computational Linguists</title>
		<meeting>the 2012 Conference of the European Chapter of the Association for Computational Linguists<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning semantic hierarchies via word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiji</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 2014 Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1199" to="1209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marti A Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1992 Conference on Computational Linguistics</title>
		<meeting>the 1992 Conference on Computational Linguistics<address><addrLine>Nantes, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="539" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Directional distributional similarity for lexical inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Kotlerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan</forename><surname>Zhitomirsky-Geffet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="359" to="389" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deriving Boolean structures from distributional vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="375" to="388" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Identifying hypernyms in distributional semantic spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Benotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The First Joint Conference on Lexical and Computational Semantics</title>
		<meeting><address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="75" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Focused entailment graphs for Open IE propositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Computational Natural Language Learning</title>
		<meeting>the 2014 Conference on Computational Natural Language Learning<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="87" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Do supervised distributional methods really learn lexical inference relations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Remus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="970" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple word embedding model for lexical substitution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Vector Space Modeling for Natural Language Processing</title>
		<meeting>the First Workshop on Vector Space Modeling for Natural Language Processing<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inclusive yet selective: Supervised distributional hypernymy detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 International Conference on Computational Linguistics</title>
		<meeting>the 2014 International Conference on Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1025" to="1036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nine features in a random forest to learn taxonomical semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tin-Shing</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation<address><addrLine>Paris, France. Enrico Santus</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>University of Pisa</orgName>
		</respStmt>
	</monogr>
	<note>SLQS: An entropy measure. Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning syntactic patterns for automatic hypernym discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Experiments with three approaches to recognizing lexical entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="437" to="476" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Take and took, gaggle and goose, book and read: Evaluating the utility of vector differences for lexical relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vylomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08" />
			<biblScope unit="page" from="1671" to="1682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Characterising measures of lexical distributional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 International Conference on Computational Linguistics</title>
		<meeting>the 2004 International Conference on Computational Linguistics<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1015" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to distinguish hypernyms and co-hyponyms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoud</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 International Conference on Computational Linguistics</title>
		<meeting>the 2014 International Conference on Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2249" to="2259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The distributional inclusion hypotheses and lexical entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan</forename><surname>Zhitomirsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Geffet</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 2005 Annual Meeting of the Association for Computational Linguistics<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
