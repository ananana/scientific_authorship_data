<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Position Encoding Convolutional Neural Network Based on Dependency Tree for Relation Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlun</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shulei</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Position Encoding Convolutional Neural Network Based on Dependency Tree for Relation Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="65" to="74"/>
							<date type="published">November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>With the renaissance of neural network in recent years, relation classification has again become a research hotspot in natural language processing, and leveraging parse trees is a common and effective method of tackling this problem. In this work, we offer a new perspective on utilizing syntactic information of dependency parse tree and present a position encoding convolutional neural network (PECNN) based on dependency parse tree for relation classification. First, tree-based position features are proposed to encode the relative positions of words in dependency trees and help enhance the word representations. Then, based on a redefinition of &quot;context&quot;, we design two kinds of tree-based convolution kernels for capturing the semantic and structural information provided by dependency trees. Finally, the features extracted by convolution module are fed to a classifier for labelling the semantic relations. Experiments on the benchmark dataset show that PECNN outperforms state-of-the-art approaches. We also compare the effect of different position features and visualize the influence of tree-based position feature by tracing back the con-volution process.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation classification focuses on classifying the se- mantic relations between pairs of marked entities in given sentences ( <ref type="bibr" target="#b5">Hendrickx et al., 2010)</ref>. It is a fun- damental task which can serve as a pre-existing sys- tem and provide prior knowledge for information ex- * Corresponding authors traction, natural language understanding, informa- tion retrieval, etc. However, automatic recognition of semantic relation is challenging. Traditional fea- ture based approaches rely heavily on the quantity and quality of hand-crafted features and lexical re- sources, and it is time-consuming to select an op- timal subset of relevant features in order to maxi- mize performance. Though kernel based methods get rid of the feature selection process, they need elaborately designed kernels and are also computa- tionally expensive.</p><p>Recently, with the renaissance of neural network, deep learning techniques have been adopted to pro- vide end-to-end solutions for many classic NLP tasks, such as sentence modeling <ref type="bibr" target="#b16">(Socher, 2014;</ref><ref type="bibr" target="#b7">Kim, 2014</ref>) and machine translation ( <ref type="bibr" target="#b1">Cho et al., 2014</ref>). Recursive Neural Network (RNN) <ref type="bibr" target="#b14">(Socher et al., 2012</ref>) and Convolutional Neural Network (CNN) ( <ref type="bibr" target="#b23">Zeng et al., 2014</ref>) have proven powerful in relation classification. In contrast to traditional approaches, neural network based methods own the ability of automatic feature learning and alleviate the problem of severe dependence on human-designed features and kernels.</p><p>However, previous researches <ref type="bibr" target="#b14">(Socher et al., 2012)</ref> imply that some features exploited by tradi- tional methods are still informative and can help en- hance the performance of neural network in relation classification. One simple but effective approach is to concatenate lexical level features to features ex- tracted by neural network and directly pass the com- bined vector to classifier. In this way, <ref type="bibr" target="#b14">Socher et al. (2012)</ref>, <ref type="bibr" target="#b10">Liu et al. (2015)</ref> achieve better performances when considering some external features produced by existing NLP tools. Another more sophisticated method adjusts the structure of neural network ac- cording to the parse trees of input sentences. The results of ( <ref type="bibr" target="#b13">Li et al., 2015</ref>) empirically suggest syn- tactic structures from recursive models might offer useful power in relation classification. Besides rela- tion classification, parse tree also gives neural net- work a big boost in other NLP tasks ( <ref type="bibr" target="#b18">Tai et al., 2015)</ref>.  Dependency parse tree is valuable in relation clas- sification task. According to our observation, depen- dency tree usually shortens the distances between pairs of marked entities and helps trim off redundant words, when comparing with plain text. For exam- ple, in the sentence shown in <ref type="figure" target="#fig_1">Figure 1</ref>, two marked entities span the whole sentence, which brings much noise to the recognition of their relation. By con- trast, in the dependency tree corresponding to the sentence, the path between two marked entities com- prises only four words and extracts a key phrase "caused by" that clearly implies the relation of enti- ties. This property of dependency tree is ubiquitous and consistent with the Shortest Path Hypothesis which is accepted by previous studies <ref type="bibr" target="#b20">Xu et al., 2015a;</ref><ref type="bibr" target="#b21">Xu et al., 2015b)</ref>.</p><p>To better utilize the powerful neural network and make the best of the abundant linguistic knowledge provided by parse tree, we propose a position encod- ing convolutional neural network (PECNN) based on dependency parse tree for relation classification. In our model, to sufficiently benefit from the impor- tant property of dependency tree, we introduce the position feature and modify it in the context of parse tree. Tree-based position features encode the rela- tive positions between each word and marked en- tities in a dependency tree, and help the network pay more attention to the key phrases in sentences. Moreover, with a redefinition of "context", we de- sign two kinds of tree-based convolution kernels for capturing the structural information and salient fea- tures of sentences.</p><p>To sum up, our contributions are:</p><p>1) We propose a novel convolutional neural network with tree-based convolution kernels for relation classification.</p><p>2) We confirm the feasibility of transferring the po- sition feature from plain text to dependency tree, and compare the performances of different posi- tion features by experiments.</p><p>3) Experimental results on the benchmark dataset show that our proposed method outperforms the state-of-the-art approaches. To make the mech- anism of our model clear, we also visualize the influence of tree-based position feature on rela- tion classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recent studies usually present the task of relation classification in a supervised perspective, and tra- ditional supervised approaches can be divided into feature based methods and kernel methods. Feature based methods focus on extracting and selecting relevant feature for relation classifica- tion. <ref type="bibr" target="#b6">Kambhatla (2004)</ref> leverages lexical, syntactic and semantic features, and feeds them to a maxi- mum entropy model. <ref type="bibr" target="#b5">Hendrickx et al. (2010)</ref> show that the winner of SemEval-2010 Task 8 used the most types of features and resources, among all par- ticipants. Nevertheless, it is difficult to find an opti- mal feature set, since traversing all combinations of features is time-consuming for feature based meth- ods.</p><p>To remedy the problem of feature selection men- tioned above, kernel methods represent the input data by computing the structural commonness be- tween sentences, based on carefully designed ker- nels. Mooney and Bunescu (2005) split sentences into subsequences and compute the similarities us- ing the proposed subsequence kernel. Bunescu and Mooney <ref type="formula" target="#formula_2">(2005)</ref> propose a dependency tree kernel and extract information from the Shortest Depen- dency Path (SDP) between marked entities. Since kernel methods require similarity computation be- tween input samples, they are relatively computa- tionally expensive when facing large-scale datasets. Nowadays, deep neural network based ap- proaches have become the main solutions to relation classification. Among them, some handle this task by modifying sentence modeling methods. <ref type="bibr" target="#b14">Socher et al. (2012)</ref> build RNN on constituency trees of sen- tences, and apply the model to relation recognition task. <ref type="bibr" target="#b23">Zeng et al. (2014)</ref> propose the use of position feature for improving the performance of CNN in relation classification. dos <ref type="bibr" target="#b4">Santos et al. (2015)</ref> di- minish the impact of noisy class by using a pairwise ranking loss function based CNN. Meanwhile, in- spired by the ideas of traditional methods, some re- cent researches concentrate on mining information from the SDP. <ref type="bibr" target="#b21">Xu et al. (2015b)</ref> use a multichan- nel LSTM network to model the SDP in given sen- tences. <ref type="bibr" target="#b10">Liu et al. (2015)</ref> reserve the subtrees attached to the SDP and propose an augmented SDP based CNN. Neural network based methods offer the ad- vantage of automatic feature learning and also scale well with large amounts of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Model</head><p>Given a sentence s with two marked entities e1 and e2, we aim to identify the semantic relation between e1 and e2 in relation classification. As the set of target relations is predefined, this task can be formu- lated as a multi-class classification problem. In this section, we detailedly describe our proposed model designed for this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework</head><p>The schematic illustration of the framework is shown in <ref type="figure" target="#fig_2">Figure 2</ref>.</p><p>First, the dependency tree of a sentence is gen- erated by the Stanford Parser ( <ref type="bibr" target="#b8">Klein and Manning, 2003)</ref>. For each word in the tree, its word embed- ding and tree-based position features are concate- nated as its representation. The position feature of a word is determined by the relative position between the word and marked entities in the dependency tree.</p><p>Next, with tree-based kernels, convolution opera- tions are conducted on each node of the dependency tree. Compared with plain text, dependency tree could provide a word with more meaningful con- text, thus making tree-based kernel more effective. After convolution, we apply max-pooling over the extracted feature maps to capture the most important features.</p><p>At last, the output of max-pooling layer, i.e. the feature vector of input sentence, is fed to a softmax classifier for labelling the semantic relation of enti- ties in each sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Word Represetation</head><p>The representation of a word is composed of two parts: word embedding and tree-based position fea- ture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Word Embedding</head><p>Distributed representation of words in a vector space help learning algorithms to achieve better per- formance in NLP tasks ( <ref type="bibr" target="#b11">Mikolov et al., 2013</ref>). Such representation is usually called word embedding in recent works. High-quality word embedding is able to capture precise syntactic and semantic informa- tion by training unsupervisedly on large-scale cor- pora.</p><p>In our model, we initialize the word embeddings by pretraining them on a large corpus and further fine-tune them in training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Tree-based Position Feature</head><p>Position Feature (PF) is first proposed by <ref type="bibr" target="#b2">(Collobert et al., 2011</ref>) for semantic role labeling. ( <ref type="bibr" target="#b23">Zeng et al., 2014</ref>) exploit position feature as a substitute for traditional structure features in relation classifi- cation. The main idea of position feature is to map each discrete distance to a real-valued vector. It is similar to word embedding, except that words are replaced by discrete distances. For instance, let us examine again the sentence shown in <ref type="figure" target="#fig_1">Figure 1</ref>, <ref type="bibr">[Convulsions]</ref>  the relative distances of caused to Convulsions and fever are respectively 6 and −3. Each relative dis- tance is further mapped to a d pf (a hyperparameter) dimensional vector, which is randomly initialized. Supposing pf 6 and pf −3 are the corresponding vec- tors of distance 6 and −3, the position feature of caused is given by concatenating these two vectors</p><formula xml:id="formula_0">[pf 6 , pf −3 ].</formula><p>Position feature on plain text proves to be infor- mative (dos <ref type="bibr" target="#b4">Santos et al., 2015)</ref>, while it may suf- fer from several problems. According to our case study, adverbs or unrelated entities that appear be- tween two entities in a sentence could significantly affect the performance of position feature, as these words only change the relative distance to entities without providing any more useful information for relation classification. Similarly, position feature of- ten fails to handle sentences in which marked enti- ties are too far from each other.</p><p>On the other hand, dependency tree focuses on the action and agents in a sentence ( ), which is valuable for relation classification. As we have mentioned above, dependency tree is able to shorten the distances between pairs of marked enti- ties and help trim off redundant words. Therefore, it is straightforward and reasonable to transfer the position feature from plain text to dependency tree.</p><p>We propose two kinds of Tree-based Position Fea- ture which we refer as TPF1 and TPF2.</p><p>TPF1 encodes the relative distances of current word to marked entities in dependency trees. The "relative distance" here refers to the length of the shortest dependency path between current word and target entity. The sign of the distance is used to dis- tinguish whether current word is a descendant of tar- get entity. After calculating the relative distances of words in the tree, we can get their TPF1 by mapping relative distances to corresponding vectors, which is the same as the PF in plain text.</p><p>To more precisely describe the position of a word, TPF2 incorporates more information given by de- pendency tree. TPF2 represents the relative posi- tions between current word and marked entities by encoding their shortest paths. For a word and an en- tity, the shortest path between them can be separated by their lowest common ancestor, and the lengths of the two sub-paths are sufficient for encoding the shortest path and the relative position between the word and the entity. As a result, we formally rep- resent the relative position using a 2-tuple, in which two elements are the lengths of the two separated sub-paths respectively. Thereafter, each unique rel- ative position is mapped to a real-valued vector. For example, in <ref type="figure" target="#fig_3">Figure 3</ref>, the path between Con- vulsions and by is Convulsions→ caused←by. In TPF1, the relative distance of by to Convulsions is 2, the length of this path. In TPF2, the lowest common ancestor caused splits the path into two subpaths of length 1, so the relative position between Convul- sions and by is (1, 1) (encoded in 2-tuple). More ex- amples of the tree-based position features are shown in <ref type="figure" target="#fig_3">Figure 3</ref>.</p><p>TPF1 and TPF2 both offer good strategies for en- coding word position in dependency tree. TPF2 is more fine-grained than TPF1 and TPF1 is a simpli- fied version of TPF2.</p><p>In our model, for each word in dependency trees, its word embedding and tree-based position feature are concatenated to form its representation, which is subsequently fed to the convolutional layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Convolution Methods</head><p>In the classic CNN architecture of (Collobert et al., 2011) and its variants <ref type="bibr" target="#b7">(Kim, 2014</ref>), a convolution window covers a word and its context, i.e. its neigh- boring words. Thus convolution only captures local features around each word. Words that are not in a same window will not interact, even if they are syn- tactically related.</p><p>Compared with plain text, dependency tree could provide a word with more meaningful context. In a dependency tree, words are connected if they are in some dependency relationship. To capitalize on these syntactic information, we regard the parent and children of a word (i.e. nodes neighboring this word) as its new context. Changing the definition of "con- text" leads to modification of convolution kernel. To implement this idea, we design two kinds of tree- based kernels (Kernel-1 and Kernel-2), and apply them to sentences in dependency tree form.</p><p>Formally, for a word x in the dependency tree, let p be its parent and c 1 , · · · , c n be its n children. Their vector representation are respectively denoted by x, p, c 1 , · · · , c n ∈ R d . The convolution process of Kernel-1 is formulated as</p><formula xml:id="formula_1">z 1 xi =g(W 1 x · x + W 1 p · p + W 1 c · c i ) f or i = 1, · · · , n<label>(1)</label></formula><p>where z 1 xi ∈ R n 1 and n 1 is the number of Kernel-1, and W 1 x , W 1 p , W 1 c ∈ R n 1 ×d are weight parameters corresponding to the word, its parent and children respectively. g is the non-linear activation function. For leaf nodes which have no child, i.e. n = 0, we assign each of them a child of which the vector rep- resentation is 0. For the root node, p is set to be 0.</p><p>Similarly, the output of Kernel-2 is given by</p><formula xml:id="formula_2">z 2 xi =g(W 2 x · x + W 2 lc · c i + W 2 rc · c i+1 ) f or i = 1, · · · , n − 1<label>(2)</label></formula><p>where z 2 xi ∈ R n 2 and n 2 is the number of Kernel- 2, and W 2 x , W 2 lc , W 2 rc ∈ R n 2 ×d are weight parame- ters associated with the word and its two neighbor- ing children. If n ≤ 1, we simply add one or two 0 children, just like the zero padding strategy.</p><p>Kernel-1 aims at extracting features from words of multiple levels in dependency tree, while Kernel- 2 focuses on mining the semantic information be- tween words which share the same parent. Kernel- 1 and Kernel-2 both consider 3 words at a time because the experimental results of previous re- searches ( <ref type="bibr" target="#b23">Zeng et al., 2014;</ref><ref type="bibr" target="#b4">dos Santos et al., 2015)</ref> suggest that trigram features are relatively more use- ful in relation classification. And it is also straight- forward to extend these kernels to a larger size and apply them to other tasks.</p><p>After convolution with tree-based kernels, we ap- ply a global max-pooling operation over extracted features by taking the maximum value in each di- mension, which is formulated as</p><formula xml:id="formula_3">h 1 = elemax x,i z 1 xi (3) h 2 = elemax x,i z 2 xi (4)</formula><p>where h 1 ∈ R n 1 , h 2 ∈ R n 2 , and elemax is the op- eration which gives the element-wise maximum of all input vectors. As a consequence, the output of convolution process is [h 1 , h 2 ], the combination of features extracted by two kinds of kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Output and Training Objective</head><p>After convolution, the extracted feature is further passed to a fully connected softmax layer whose out- put is the probability distribution over all types of relations.</p><p>Since we treat the relation classification task as a multi-class classification problem, the training ob- jective is the cross-entropy error. For regularization, we apply dropout ( <ref type="bibr" target="#b17">Srivastava et al., 2014</ref>) to the fea- ture vector extracted by convolution and penalize the fully connected layer with l 2 regularizer as well.</p><p>Some other dependency tree based methods like ( <ref type="bibr" target="#b10">Liu et al., 2015)</ref>, (Xu et al., 2015a) and ( <ref type="bibr" target="#b21">Xu et al., 2015b</ref>), all focus on using different kinds of neu- ral networks to model the shortest dependency path (SDP) between entities. By contrast, PECNN ex- tracts features from the whole dependency tree, so that the information out of SDP will be taken into consideration as well. The empirical results of (dos <ref type="bibr" target="#b4">Santos et al., 2015)</ref> suggest that when position fea- tures exist, modeling the full sentence yields a bet- ter performance than only using the subsentence be- tween entities. With the help of tree-based position feature, our model is capable of evaluating the im- portance of different parts of dependency trees and tends to pay relatively more attention to SDP.</p><p>Some methods enhancing their performances by proposing dataset-specific strategies. dos <ref type="bibr" target="#b4">Santos et al. (2015)</ref> treat the class Other as a special class and omit its embedding. <ref type="bibr" target="#b20">Xu et al. (2015a)</ref> take the re- lation dimensionality into account and introduce a negative sampling strategy to double the number of training samples, which can be regarded as data aug- mentation. These strategies do not conflict with our model, but we decide not to integrate them into our methods as we aim to offer a general and effective feature extraction model for relation classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation Metric</head><p>To evaluate our method, we conduct experiments on the SemEval2010 Task 8 dataset which is a widely used benchmark for relation classification. The dataset contains 8, 000 training sentences and 2, 717 test sentences. In each sentence, two entities are marked as target entities.</p><p>The predefined target relations include 9 directed relations and an undirected Other class. The 9 directed relations are Cause-Effect, Component- Whole, Content-Container, Entity-Destination, Entity-Origin, Instrument-Agency, Member- Collection, Message-Topic and Product-Producer.</p><p>"Directed" here means, for example, Cause- Effect(e 1 , e 2 ) and Cause-Effect(e 2 , e 1 ) are two different relations. In another word, the direction- ality of relation also matters. And sentences that do not belong to any directed relation are labelled as Other. Therefore, relation classification on this dataset is a 19-class classification problem.</p><p>Following previous studies, we use the official evaluation metric, macro-averaged F1-score with di- rectionality taken into account and the Other class ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Details</head><p>Since there is no official validation set, 10% of the training sentences are taken out for hyperparameter tuning and early stopping.</p><p>When converting sentences to dependency trees, we note that some prepositions such as "by", "in" and "of", might be important clues to relation clas- sification. To reserve these valuable information, we use the Stanford Parser without the collapsed op- tion.</p><p>In the dataset, there are some entities consisting of multiple words, which make the calculation of rela- tive position ambiguous. To solve this problem, we take the last word as the representation of an entity, as the last word is usually the predominant word.</p><p>For word embeddings, we initialize them using the 300-dimensional word2vec vectors 1 . The vec- tors are trained on 100 billion words from Google News. Words not present in the word2vec vectors are initialized by sampling each dimension from a uniform distribution <ref type="bibr" target="#b7">(Kim, 2014)</ref>. Tree-based posi- tion features are 50-dimensional and initialized ran- domly. Therefore the representation of each word has dimensionality of 400.</p><p>We use ReLU as the activation function. The number of convolution kernels is 500 for each kind, 1, 000 in total. The dropout rate is 0.5, and the co- efficient of l 2 penalty of fully connected layer is set to 10 −6 . These parameters are selected through grid search on validation set. The network is trained with the Adadelta update rule <ref type="bibr" target="#b22">(Zeiler, 2012</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>The performances of our proposed model and other state-of-the-art methods are shown in <ref type="table" target="#tab_2">Table 1</ref>. First, we compare PECNN with the following baselines when no external lexical feature is used. <ref type="bibr" target="#b14">Socher et al. (2012)</ref> assign a vector and a matrix to each word for the purpose of semantic composi- tion, and build recursive neural network along con- stituency tree (MVRNN). It is noteworthy that this work is the first one who confirms the feasibility of applying neural network to relation classification. Without considering any external lexical feature and dataset-specific strategy, our model achieve an F 1 of 84.0, suggesting that tree-based position fea- tures and kernels are effective. Comparing with the CNN based on plain text, our model benefits from dependency tree based network and obtain a notable 71  Most of these baselines concatenate external lex- ical features to features extracted by neural network and directly pass the combined vector to classifier. SDP-LSTM represents lexical features as embed- dings and enhances its word representation. For fair comparison, we add three features (POS tags, NER tags and WordNet hypernyms of marked entities) to the vector extracted by our model and retrain the net- work. Thus, our model achieves an F 1 of 84.6 and outperforms all existing baselines in a fair condition where no data augmentation strategy is adopted. The enhancement we gain from external features is less, comparing with other baselines. This implies that our model is able to mine useful features from lim- ited resources, even no extra information is avail- able.   <ref type="table" target="#tab_5">Table 2</ref> summarizes the performances of proposed model when different position features are exploited. To concentrate on studying the effect of position fea- tures, we do not involve lexical features in this sec- tion. As the table shows, the position feature on plain text is still effective in our model and we ac- credit its satisfactory result to the dependency in- formation and tree-based kernels. The F 1 scores of tree-based position features are higher since they are "specially designed" for our model. Contrary to our expectation, the more fine-grained TPF2 does not yield a better performance than TPF1, and two kinds of TPF give fairly close results. One possible reason is that the influence of a more elaborated definition of relative position is minimal. As most sentences in this dataset are of short length and their dependency trees are not so complicated, replacing TPF1 with TPF2 usually brings little new structural information and thus results in a similar F 1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of Different Position</head><p>However, though the performances of different position features are close, tree-based position fea- ture is an essential part of our model. The F 1 score is severely reduced to 75.22 when we remove the tree-based position feature in PECNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effect of Tree-based Position Feature</head><p>For shallow CNN in NLP, visualization offers clear and convincing explanations for the mechanism of neural networks (dos <ref type="bibr" target="#b3">Santos and Gatti, 2014;</ref>. Moreover, it is easy to implement.</p><p>Note that in the max-pooling step, for each ker- nel, we select the feature which has the largest value. This feature corresponds to 3 words in the convolu-tion step, and we regard them as the most relevant words extracted by this kernel, with respect to the sentence . Since there are 1, 000 kernels in total, we count 3, 000 words (0 will be ignored) and calculate the proportion of each different word. Intuitively, the more important a word is in this task, the larger its proportion will be.</p><p>In <ref type="figure" target="#fig_5">Figure 4</ref>, we compare the proportions of words in the example sentence when tree-based position feature (TPF) is used and not. As we can see, the proportions of two entities, Convulsions and fever, and the phrase caused by all increase visibly with the presence of TPF, suggesting that TPF is effec- tive in helping the neural network pay more atten- tion to the crucial words and phrases in a sentence. The word occur is also picked up by our model since it is an important candidate clue to relation classifi- cation. Meanwhile, the influence of irrelevant entity DTaP is remarkably diminished as expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This work presents a dependency parse tree based convolutional neural network for relation classifica- tion. We propose tree-based position features to en- code the relative positions of words in a dependency tree. Meanwhile, tree-based convolution kernels are designed to gather semantic and syntactic informa- tion in dependency trees. Experimental results prove the effectiveness of our model. Comparing with plain text based CNN, our proposed kernels and po- sition features boost the performance of network by utilizing dependency trees in a new perspective.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>[</head><label></label><figDesc>Convulsions] that occur after DTaP are caused by a [fever].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A dependency tree example. Words in square brackets are marked entities. The red dashed-line arrows indicate the path between two entities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The framework of PECNN. The red and blue circles represent the word embeddings and tree-based position features of words. The yellow and green circles stand for the feature maps extracted by two kinds of convolution kernels respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of Tree-based Position Features. The red numbers are relative distances in TPF1. The blue 2-tuples are relative positions in TPF2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Following the ideas of (Collobert et al., 2011), Zeng et al. (2014) first solve relation classifica- tion using convolutional neural network (CNN). The position feature introduced by them proves effec- tive. dos Santos et al. (2015) build a similar CNN called CR-CNN but replace the objective function with a pairwise ranking loss. By treating the noisy class Other as a special class, this method achieves an F 1 of 84.1. The F 1 score is 82.7 if no special treatment is applied. The rest two baselines focus on modeling the Shortest Dependency Paths (SDP) between marked entities. Xu et al. (2015a)) (depLCNN) integrate the relation directionality into CNN and achieve an F 1 of 84.0 with a data augmentation strategy called negative sampling. Without such data augmenta- tion, their F 1 score is 81.9. Xu et al. (2015b) (SDP- LSTM) represent heterogeneous features as embed- dings and propose a multichannel LSTM based re- current neural network for picking up information along the SDP. Their F 1 score is 83.0 when only word embedding is used as the word representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization of the effect of tree-based position feature. The proportions of words change with the use of tree-based position feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>e1 that occur after DTaP are caused by a [fever] e2 .</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Comparison of different relation classification models. The symbol  *  indicates the results with special treatment of the 

class Other. The symbol • indicates the results with data augmentation strategy. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Features</head><label></label><figDesc></figDesc><table>Position Feature 
F 1 
plain text PF 
83.21 
TPF1 
83.99 
TPF2 
83.90 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different position features.</figDesc><table></table></figure>

			<note place="foot" n="1"> https://code.google.com/p/word2vec/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>This work is partially supported by the National High Technology Research and Development Pro-gram of China (Grant No. 2015AA015403) and the National Natural Science Foundation of China (Grant No. 61170091). We would also like to thank the anonymous reviewers for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A shortest path dependency kernel for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for sentiment analysis of short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cıcero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maıra</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 25th International Conference on Computational Linguistics (COLING)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Classifying relations by ranking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2004 on Interactive poster and demonstration sessions</title>
		<meeting>the ACL 2004 on Interactive poster and demonstration sessions</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">When are tree structures necessary for deep learning of representations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="2304" to="2314" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A dependency-based neural network for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="285" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Subsequence kernels for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mooney And Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bunescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="171" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discriminative neural sentence modeling by tree-based convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="2315" to="2325" />
		</imprint>
	</monogr>
	<note>Portugal</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Recursive Deep Learning for Natural Language Processing and Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
	<note>Beijing</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<idno>abs/1605.02688</idno>
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
	<note>Theano Development Team. arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic relation classification via convolutional neural networks with simple negative sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1721" />
			<biblScope unit="page" from="536" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1721" />
			<biblScope unit="page" from="1785" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
