<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dependency-based Discourse Parser for Single-Document Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhisa</forename><surname>Yoshida</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit2">NTT Corporation</orgName>
								<address>
									<addrLine>2-4 Hikaridai, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0237</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit2">NTT Corporation</orgName>
								<address>
									<addrLine>2-4 Hikaridai, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0237</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit2">NTT Corporation</orgName>
								<address>
									<addrLine>2-4 Hikaridai, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0237</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit2">NTT Corporation</orgName>
								<address>
									<addrLine>2-4 Hikaridai, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0237</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dependency-based Discourse Parser for Single-Document Summarization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1834" to="1839"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The current state-of-the-art single-document summarization method generates a summary by solving a Tree Knapsack Problem (TKP), which is the problem of finding the optimal rooted sub-tree of the dependency-based discourse tree (DEP-DT) of a document. We can obtain a gold DEP-DT by transforming a gold Rhetorical Structure Theory-based discourse tree (RST-DT). However, there is still a large difference between the ROUGE scores of a system with a gold DEP-DT and a system with a DEP-DT obtained from an automatically parsed RST-DT. To improve the ROUGE score, we propose a novel discourse parser that directly generates the DEP-DT. The evaluation results showed that the TKP with our parser outperformed that with the state-of-the-art RST-DT parser, and achieved almost equivalent ROUGE scores to the TKP with the gold DEP-DT.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Discourse structures of documents are believed to be highly beneficial for generating informa- tive and coherent summaries. Several discourse- based summarization methods have been devel- oped, such as <ref type="bibr" target="#b9">(Marcu, 1998;</ref><ref type="bibr" target="#b2">Daumé III and Marcu, 2002;</ref><ref type="bibr" target="#b4">Hirao et al., 2013;</ref><ref type="bibr" target="#b6">Kikuchi et al., 2014</ref>). Moreover, the current best ROUGE score for the summarization benchmark data of the RST- discourse <ref type="bibr">Treebank (Carlson et al., 2002</ref>) has been provided by <ref type="bibr" target="#b4">(Hirao et al., 2013)</ref>, whose method also utilizes discourse trees. Thus, the discourse- based summarization approach is one promising way to obtain high-quality summaries.</p><p>One possible weakness of discourse-based sum- marization techniques is that they rely greatly on the accuracy of the discourse parser they use. For example, the above discourse-based summa- rization methods utilize discourse trees based on the Rhetorical Structure Theory (RST) ( <ref type="bibr" target="#b8">Mann and Thompson, 1988</ref>) for their discourse information. Unfortunately, the current state-of-the-art RST parser, as described in <ref type="bibr" target="#b3">(Hernault et al., 2010)</ref>, is insufficient as an off-the-shelf discourse parser. In fact, there is empirical evidence that the qual- ity (i.e., ROUGE score) of summaries from auto- parsed discourse trees is significantly degraded compared with those generated from gold dis- course trees <ref type="bibr" target="#b9">(Marcu, 1998;</ref><ref type="bibr" target="#b4">Hirao et al., 2013)</ref>.</p><p>From this background, the goal of this paper is to develop an appropriate discourse parser for discourse-based summarization. We first focus on one of the best discourse-based single document summarization methods as proposed in <ref type="bibr" target="#b4">(Hirao et al., 2013)</ref>. Their method formulates a single doc- ument summarization problem as a Tree Knap- sack Problem (TKP) over a dependency-based dis- course tree (DEP-DT). In their method, DEP-DTs are automatically transformed from (auto-parsed) RST-discourse trees (RST-DTs) by heuristic rules. Instead, we develop a DEP-DT parser, that di- rectly provides DEP-DTs for their state-of-the-art discourse-based summarization method. We show that summaries generated by our parser improve the ROUGE scores to almost the same level as those generated by gold DEP-DTs. We also inves- tigate the way in which the parsing accuracy helps to improve the ROUGE scores.</p><p>2 Single-Document Summarization as a Tree Knapsack Problem <ref type="bibr" target="#b4">Hirao et al. (2013)</ref> formulated single-document summarization as a TKP that is run on the DEP- DT. They obtained a summary by trimming the DEP-DT, i.e. the summary is a rooted subtree of the DEP-DT. Suppose that we have N EDUs in a document,  and the i-th EDU e i has l i words. L is the maxi- mum number of words allowed in a summary. In the TKP, if we select e i , we need to select its par- ent EDU in the DEP-DT. We denote parent(i) as the index of the parent of e i in the DEP-DT. x is an N -dimensional binary vector that represents a summary, i.e. x i = 1 denotes that e i is included in the summary. The TKP is defined as the following ILP problem:</p><formula xml:id="formula_0">Root! N! S! N! S! N! S! S! N! N! S! S! N! S! N! N! N! N! S! Elaboration</formula><formula xml:id="formula_1">maximize x ∑ N i=1 F (e i )x i s.t. ∑ N i=1 l i x i ≤ L ∀i : x parent(i) ≥ x i ∀i : x i ∈ {0, 1},</formula><p>where F (e i ) is the score of e i . We define F (e i ) as follows:</p><formula xml:id="formula_2">F (e i ) = ∑ w∈W (e i ) tf(w, D) Depth(e i ) ,</formula><p>where W (e i ) is the set of words contained in e i . tf(w, D) is the term frequency of word w in a doc- ument D. Depth(e i ) is the depth of e i in the DEP- DT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tree Knapsack Problem with</head><p>Dependency-based Discourse Parser</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>In ( <ref type="bibr" target="#b4">Hirao et al., 2013)</ref>, they automatically ob- tain the DEP-DT by transforming from the parsed RST-DT. We simply followed their method for ob- taining the DEP-DTs 1 . The transformation algo- rithm can be found in detail in ( <ref type="bibr" target="#b4">Hirao et al., 2013)</ref>.  <ref type="figure" target="#fig_0">Figure 1</ref>(c). In this example, the parser failed to determine the most salient EDU e 2 , that is the root EDU of the gold DEP-DT. Thus, the summary ex- tracted from this DEP-DT will have a low ROUGE score.</p><p>The results motivated us to design a new dis- course parser fully trained on the DEP-DTs and  </p><formula xml:id="formula_3">DEP=DT Root$ N$ S$ N$ S$ N$ S$ S$ N$ N$ S$ S$ N$ S$ N$ N$ N$ N$ S$ e1$ e2$ e3$ e4$ e5$ e6$ e7$ e8$ e9$ e10$ Elabora7on$ Background$ Elabora7on$ Elabora7on$ Contrast$ Evidence$ Example$ Concession$ An7thesis$ Root$ N$ S$ N$ S$ N$ S$ S$ N$ N$ S$ S$ N$ S$ N$ N$ N$ N$ S$ e1$ e2$ e3$ e4$ e5$ e6$ e7$ e8$ e9$ e10$ Elabora7on$ Background$ Elabora7on$ Elabora7on$ Contrast$ Evidence$ Example$ Concession$ An7thesis$ Root$ N$ S$ N$ S$ N$ S$ S$ N$ N$ S$ S$ N$ S$ N$ N$ N$ N$ S$ e1$ e2$ e3$ e4$ e5$ e6$ e7$ e8$ e9$ e10$ Elabora7on$ Background$ Elabora7on$ Elabora7on$ Contrast$ Evidence$ Example$ Concession$ An7thesis$ RST=DTs Transforma;on)Algorithm (a) e2# e1# e3# e8# e10# e4# e5# e7# e9# e6# Elabora3on# Elabora3on# Elabora3on# Elabora3on# An3thesis# Example# Evidence# Concession# Background# RST$Parser Document Root$ N$ S$ N$ S$ N$ S$ S$ N$ N$ S$ S$ N$ S$ N$ N$ N$ N$ S$ e1$ e2$ e3$ e4$ e5$ e6$ e7$ e8$ e9$ e10$ Elabora7on$ Background$ Elabora7on$ Elabora7on$ Contrast$ Evidence$ Example$ Concession$ An7thesis$ Summary RST$Parser Transforma3on$Algorithm Tree$Knapsack$Problem Parser$Training$Phase Summariza3on$Phase Root$ N$ S$ N$ S$ N$ S$ S$ N$ N$ S$ S$ N$ S$ N$ N$ N$ N$ S$ e1$ e2$ e3$ e4$ e5$ e6$ e7$ e8$ e9$ e10$ Elabora7on$ Background$ Elabora7on$ Elabora7on$ Contrast$ Evidence$ Example$ Concession$ An7thesis$ Root$ N$ S$ N$ S$ N$ S$ S$ N$ N$ S$ S$ N$ S$ N$ N$ N$ N$ S$ e1$ e2$ e3$ e4$ e5$ e6$ e7$ e8$ e9$ e10$ Elabora7on$ Background$ Elabora7on$ Elabora7on$ Contrast$ Evidence$ Example$ Concession$ An7thesis$ Root$ N$ S$ N$ S$ N$ S$ S$ N$ N$ S$ S$ N$ S$ N$ N$ N$ N$ S$ e1$ e2$ e3$ e4$ e5$ e6$ e7$ e8$ e9$ e10$ Elabora7on$ Background$ Elabora7on$ Elabora7on$ Contrast$ Evidence$ Example$ Concession$ An7thesis$ RST&gt;DTs RST&gt;DT DEP&gt;DT (b)</formula><p>Figure 2: (a) Overview of our proposed method. In the parser training phase, the parser is trained on the DEP-DTs, and in the summarization phase, the document is directly parsed into the DEP-DT. (b) Overview of ( <ref type="bibr" target="#b4">Hirao et al., 2013</ref>). In the parser training phase, the parser is trained on RST-DTs, and in the summarization phase, the document is parsed into the RST-DT, and then transformed into the DEP-DT.</p><p>that could directly generate the DEP-DT. <ref type="figure" target="#fig_3">Figure  2</ref>(a) shows an overview of the TKP combined with our DEP-DT parser. In the parser training phase, we transform RST-DTs into DEP-DTs, and di- rectly train our parser with the DEP-DTs. In the summarization phase, our method parses a raw document directly into a DEP-DT, and generates a summary with the TKP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Description of Discourse Dependency Parser</head><p>Our parser is based on the first-order Maximum Spanning Tree (MST) algorithm <ref type="bibr" target="#b11">(McDonald et al., 2005b</ref>). Our parser extracts the features from the EDU e i and the EDU e j . We use almost the fea- tures as those shown in <ref type="bibr" target="#b3">(Hernault et al., 2010)</ref>. Lexical N-gram features use the beginning (or end) lexical N-grams (N ∈ {1, 2, 3}) in e i and e j . We also include POS tags for the beginning (or end) lexical N-grams (N ∈ {1, 2, 3}) in e i and e j . Organizational features include the distance between e i and e j . They also include the num- ber of tokens, and features for identifying whether or not e i and e j belong to the same sentence (or paragraph </p><formula xml:id="formula_4">min w ||w − w (t) || s.t. s(w, y) − s(w, y ) ≥ L(y, y ),<label>(1)</label></formula><p>where w (t) is a weight vector in the t-th iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Redesign of Loss Function for Tree Knapsack Problem</head><p>When we make a summary by solving a TKP, we do not necessarily need a DEP-DT where all of the parent-child relationships are correct. This is be- cause we rarely select the EDUs around the leaves in the DEP-DT. On the other hand, the parent- child relationships around the root EDU in the DEP-DT are important because we often select the EDUs around the root EDU. Incorporating these intuitions enables us to develop a DEP-DT parser optimized for the TKP. To incorporate this infor- mation, we define the following loss function:</p><formula xml:id="formula_5">L Depth (y, y ) = ∑ (i,r,j)∈y [1 − I(y , i, j)] Depth(e i ) ,<label>(2)</label></formula><p>where I(y , i, j) is an indicator function that equals 1 if EDU e j is the parent of EDU e i in the DEP-DT y and 0 otherwise. In Section 4, we re- port results with the original loss function L(·, ·) and with the modified loss function L Depth (·, ·).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Corpus</head><p>We used the RST-DT corpus <ref type="bibr" target="#b0">(Carlson et al., 2002</ref>) for our experimental evaluations. The corpus con- sists of 385 Wall Street Journal articles with RST annotation, and 30 of these documents also have one human-made reference summary. We used these 30 documents as the test documents for the summarization evaluation, and used the remaining 355 RST annotated documents as the training data for the parser. Note that we did not use the 30 test documents for the summarization evaluation when we trained the parser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Summarization Evaluation</head><p>We compared the following three systems that dif- fer in the way they obtain the DEP-DT.</p><p>TKP-GOLD Used a DEP-DT converted from a gold RST-DT.</p><p>TKP-DIS-DEP Used a DEP-DT automatically parsed by our discourse dependency-based parser (DIS-DEP). <ref type="figure" target="#fig_3">Figure 2</ref>(a) shows an overview of this system.</p><p>TKP-DIS-DEP-LOSS Used a DEP-DT automat- ically parsed by our discourse dependency- based parser (DIS-DEP).  <ref type="bibr" target="#b9">(Marcu, 1998)</ref>, a simple knapsack model, a maximum coverage model and LEAD method that simply takes the first L tokens (L = summary length). Thus, we only employed TKP-HILDA as our baseline. We follow the evaluation conditions described in <ref type="figure" target="#fig_0">(Hirao et al., 2013)</ref>. The number of tokens in each summary is determined by the number in the <ref type="table" target="#tab_3">ROUGE-1 ROUGE-2  TKP-GOLD  0.321  0.112  TKP-DIS-DEP  0.319  0.109  TKP-DIS-DEP-LOSS  0.323  0.121  TKP-HILDA  0.284  0.093   Table 1</ref>: ROUGE Recall scores human-annotated reference summary. The aver- age length of the reference summaries corresponds to about 10% of the words in the source document. This is also the commonly used evaluation con- dition for single-document summarization evalu- ation on the RST-DT corpus. We employed the recall of ROUGE-1, 2 as the evaluation measures. <ref type="table">Table 1</ref> shows ROUGE scores on the RST-DT corpus. We can see TKP-DIS-DEP and TKP- DIS-DEP-LOSS outperformed TKP-HILDA, and achieved almost the same ROUGE scores as TKP- GOLD. Wilcoxon's signed rank test in terms of ROUGE rejected the null hypothesis, "there is a difference between TKP-HILDA and TKP- DIS-DEP (or TKP-DIS-DEP-LOSS)" <ref type="bibr" target="#b13">(Wilcoxon, 1945)</ref>. This would be because test documents are relatively small.</p><p>We analyzed the differences between the pro- posed systems (TKP-DIS-DEP and TKP-DIS- DEP-LOSS) and TKP-HILDA. First, we evaluated the overlaps between the EDUs in summaries gen- erated by the system and the EDUs in summaries generated by TKP-GOLD. To see the overlaps, we calculated the average F-value using Recall and Precision defined as follows:</p><formula xml:id="formula_6">Recall = |S s ∩ S g |/|S g |, Precision = |S s ∩ S g |/|S s |,</formula><p>where S s is a set of EDUs in a summary generated by a sys- tem, and S g a set of EDUs in a summary generated by TKP-GOLD. The first line in <ref type="table" target="#tab_3">Table 2</ref> shows the results. TKP-DIS-DEP and TKP-DIS-DEP-LOSS outperformed TKP-HILDA as regards the aver- age F-values. The result revealed that TKP-DIS- DEP and TKP-DIS-DEP-LOSS have more EDUs in common with TKP-GOLD than TKP-HILDA. This result is evidence that TKP-DIS-DEP and TKP-DIS-DEP-LOSS outperformed TKP-HILDA in terms of ROUGE score.</p><p>Second, we evaluated the root accuracy (RA), the rate at which a parser can find the root of DEP- DTs. Since the root of a gold DEP-DT is the most salient EDU in a document, it should be included in the summary. The second line in <ref type="table" target="#tab_3">Table 2</ref> shows that our methods succeeded in extracting the root TKP-DIS-DEP TKP-DIS-DEP-LOSS TKP-HILDA Avg F-value 0.532 0.532 0.415 RA 0.933 0.933 0.733 Avg DAS 0.847 0.843 0.596</p><p>: significantly better than TKP-HILDA (p &lt; .05)  of DEP-DT with high accuracy. Third, to evaluate the coherency of the gener- ated summaries, we compared the average Depen- dency Accuracy in Summary (DAS), which is de- fined as follows:</p><formula xml:id="formula_7">DAS(S) = 1 |S| ∑ e∈S δ(e), δ(e) = { 1 (if parent(e) ∈ S) 0 (otherwise),</formula><p>where S is a set of EDUs contained in the sum- mary and parent(e) returns the parent EDU of e in the gold DEP-DT. DAS(S) measures the rate of the correct parent-child relationships in S. When DAS equals 1, the summary is a rooted subtree of the gold DEP-DT. The third line in <ref type="table" target="#tab_3">Table 2</ref> shows the results. The results demonstrate that the sum- maries generated by TKP-DIS-DEP or TKP-DIS- DEP-LOSS tend to preserve the upper level depen- dency relationships between the EDUs within the gold DEP-DT. <ref type="figure" target="#fig_4">Figure 3</ref> shows summaries of wsj 2317 gener- ated by the three systems. The EDUs correspond- ing to the root of the DEP-DT are used in each system shown in boldface. We can see that the root EDU in the gold DEP-DT is found in the summaries generated by TKP-DIS-DEP and TKP- DIS-DEP-LOSS, but not in the summary gener- ated by TKP-HILDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a novel dependency- based discourse parser for single-document sum- marization. The parser enables us to obtain the DEP-DT without transforming the RST-DT. The evaluation results showed that the TKP with our parser outperformed that with the state-of-the-art RST-DT parser, and achieved almost equivalent ROUGE scores to the TKP with the gold DEP-DT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of RST-DT and DEP-DT. e 1 , · · · , e 10 are EDUs. (a) Example of an RST-DT from (Marcu, 1998). n 1 , · · · , n 19 are the non-terminal nodes. (b) Example of the DEP-DT obtained from the incorrect RST-DT that is made by swapping the Nucleus-Satellite relationship of the node n 2 and the node n 3. (c) The correct DEP-DT obtained from the RST-DT in (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 (</head><label>1</label><figDesc>a) shows an example of the RST-DT. Ac- cording to RST, a document is represented as a tree whose terminal nodes correspond to elementary discourse units (EDUs) and whose non-terminal nodes indicate the role of the contiguous EDUs, namely, 'nucleus (N)' or 'satellite (S)'. Since a nu- cleus is more important than a satellite in terms of the writer's purpose, a satellite is always a child of a nucleus in the RST-DT. Some discourse relations between a nucleus and a satellite or two nuclei are defined. Since the TKP of (Hirao et al., 2013) employs a DEP-DT obtained from an automatically parsed RST-DT, their method strongly relies on the ac- curacy of the RST parser. For example, in Fig- ure 1(a), if the RST-DT parser incorrectly sets the node n 2 as Satellite and the node n 3 as Nu- cleus, we obtain an incorrect DEP-DT in Figure 1(b) because the transformation algorithm uses the Nucleus-Satellite relationships in the RST-DT. The dependency relationships in Figure 1(b) are quite different from that of the correct DEP-DT in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Discourse</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 (</head><label>2</label><figDesc>a) shows an overview of this system. It is trained with the loss function defined in equation (2). TKP-HILDA Used a DEP-DT obtained by trans- forming a RST-DT parsed by HILDA, a state- of-the-art RST-DT parser (Hernault et al., 2010). Figure 2(b) shows an overview of this system. Hirao et al. (2013) proved that TKP-HILDA outperformed other methods including Marcu's method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Summaries of wsj 2317. The sentences shown in bold-face are the root EDUs in each DEP-DT of the summary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Average F-value, Root Accuracy (RA), and average Dependency Accuracy in Summary (DAS). 
Wilcoxon's signed rank test in terms of average F-value, RA and DAS accepted the null hypothesis. 

TKP-GOLD: 
Elcotel Inc. expects fiscal second-quarter earnings to trail 1988 results. Elcotel, a telecommunications company, had net 
income of $272,000, or five cents a share, in its year-earlier second quarter. The lower results, Mr. Pierce said. Elcotel will 
also benefit from moving into other areas. Elcotel has also developed an automatic call processor. Automatic call processors 
will provide that system for virtually any telephone, Mr. Pierce said, not just phones. 

TKP-DIS-DEP, TKP-DIS-DEP-LOSS: 
Elcotel Inc. expects fiscal second-quarter earnings to trail 1988 results. Elcotel, a telecommunications company, had net 
income of $272,000, or five cents a share, in its year-earlier second quarter. George Pierce, chairman and chief executive officer, 
said in an interview. Although Mr. Pierce expects that line of business to strengthen in the next year. Elcotel will also benefit 
from moving into other areas. Elcotel has also developed an automatic call processor. 

TKP-HILDA: 
Elcotel Inc. expects fiscal second-quarter earnings to trail 1988 results. That several new products will lead to a "much 
stronger" performance in its second half. George Pierce, chairman and chief executive officer, said in an interview. Mr. 
Pierce said Elcotel should realize a minimum of $10 of recurring net earnings for each machine each month. Elcotel has also 
developed an automatic call processor. Automatic call processors will provide that system for virtually any telephone. 

</table></figure>

			<note place="foot" n="1"> Li et al. also defined a similar transformation algorithm (Li et al., 2014). In this paper, we follow the transformation algorithm defined in (Hirao et al., 2013).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ellen</forename><surname>Okurowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Rst discourse treebank</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shai Shalev-Shwartz, and Yoram Singer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Keshet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="551" to="585" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Online passive-aggressive algorithms</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A noisychannel model for document compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07-06" />
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hilda: a discourse parser using support vector machine classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Hernault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Prendinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue and Discourse</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhisa</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nishino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norihito</forename><surname>Yasuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Single-document summarization as a tree knapsack problem</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on EMNLP</title>
		<meeting>the 2013 Conference on EMNLP</meeting>
		<imprint>
			<biblScope unit="page" from="1515" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Single document summarization based on nested tree structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="315" to="320" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Text-level discourse dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="25" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rhetorical structure theory: Toward a functional theory of text organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="281" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving summarization through rhetorical parsing tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of The 6th Workshop on VLC</title>
		<meeting>of The 6th Workshop on VLC</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="206" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Online large-margin training of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Non-projective dependency parsing using spanning tree algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiril</forename><surname>Ribarov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing<address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005-01" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sentence level discourse parsing using syntactic and lexical information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Human Language Technology Conference of the North American Chapter</title>
		<meeting>the 2003 Human Language Technology Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Individual Comparisons by Ranking Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wilcoxon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics Bulletin</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="80" to="83" />
			<date type="published" when="1945-12" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
