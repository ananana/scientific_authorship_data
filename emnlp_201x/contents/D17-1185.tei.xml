<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual Tensor Model for Detecting Asymmetric Lexico-Semantic Relations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goran</forename><surname>Glavaš</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Data and Web Science Group</orgName>
								<orgName type="institution">University of Mannheim B6</orgName>
								<address>
									<addrLine>26</addrLine>
									<postCode>DE-68159</postCode>
									<settlement>Mannheim</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Data and Web Science Group</orgName>
								<orgName type="institution">University of Mannheim B6</orgName>
								<address>
									<addrLine>26</addrLine>
									<postCode>DE-68159</postCode>
									<settlement>Mannheim</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dual Tensor Model for Detecting Asymmetric Lexico-Semantic Relations</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1757" to="1767"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Detection of lexico-semantic relations is one of the central tasks of computational semantics. Although some fundamental relations (e.g., hypernymy) are asymmetric, most existing models account for asymmetry only implicitly and use the same concept representations to support detection of symmetric and asymmetric relations alike. In this work, we propose the Dual Tensor model, a neural architecture with which we explicitly model the asymmetry and capture the translation between unspecialized and specialized word embed-dings via a pair of tensors. Although our Dual Tensor model needs only unspecial-ized embeddings as input, our experiments on hypernymy and meronymy detection suggest that it can outperform more complex and resource-intensive models. We further demonstrate that the model can account for polysemy and that it exhibits stable performance across languages.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Specializing Word Embeddings. Unspecialized word embeddings ( <ref type="bibr" target="#b19">Mikolov et al., 2013;</ref><ref type="bibr" target="#b24">Pennington et al., 2014</ref>) capture general semantic proper- ties of words, but are unable to differentiate be- tween different types of semantic relations (e.g., vectors of car and driver might be as similar as vectors of car and vehicle). However, we often need embeddings to be similar only if an exact lexico-semantic relation holds between the words. Numerous methods for specializing word embed- dings for particular relations have been proposed ( <ref type="bibr" target="#b42">Yu and Dredze, 2014;</ref><ref type="bibr" target="#b9">Faruqui et al., 2015;</ref><ref type="bibr" target="#b15">Kiela et al., 2015;</ref><ref type="bibr" target="#b20">Mrkši´Mrkši´c et al., 2016</ref>, inter alia), pri- marily aiming to differentiate synonymic similarity from other types of semantic relatedness.</p><p>Some methods modify the objective or regu- larization of general embedding algorithms like CBOW or skip-gram ( <ref type="bibr" target="#b19">Mikolov et al., 2013</ref>) in or- der to directly train relation-specific embeddings from large corpora. <ref type="bibr" target="#b42">Yu and Dredze (2014)</ref> extend the CBOW objective with synonymy constraints from WordNet and Paraphrase Database (PPDB) ( <ref type="bibr" target="#b11">Ganitkevitch et al., 2013</ref>). Similarly, <ref type="bibr" target="#b15">Kiela et al. (2015)</ref> add synonyms as additional contexts for the skip-gram objective.</p><p>Other models update the whole unspecialized embedding space by moving closer together vec- tors of words standing in a particular relation. Start- ing with unspecialized embeddings of concepts, <ref type="bibr" target="#b9">Faruqui et al. (2015)</ref> run a belief propagation algo- rithm on a graph induced from WordNet or PPDB. <ref type="bibr" target="#b39">Wieting et al. (2015)</ref> couple an objective maximiz- ing the similarity of PPDB pairs with the smart selection of the negative examples. Mrkši´ <ref type="bibr" target="#b20">Mrkši´c et al. (2016)</ref> take this idea further by using antonym pairs from WordNet as negative examples.</p><p>All aforementioned models either directly train specialized embeddings or derive them by updat- ing the unspecialized embeddings. In contrast, via dual tensors, we explicitly capture the function that transforms unspecialized embeddings to special- ized embeddings that are better suited to detect the asymmetric relation of interest.</p><p>Embedding Knowledge Graphs. Recently, vari- ous models for embedding KB concepts and re- lations have been proposed <ref type="bibr" target="#b5">(Bordes et al., 2013;</ref><ref type="bibr" target="#b35">Socher et al., 2013;</ref><ref type="bibr" target="#b40">Yang et al., 2015;</ref><ref type="bibr" target="#b22">Nickel et al., 2016</ref>, inter alia). These models predict existence of relations between entities by arithmetically combin- ing concept vectors and relation matrices or tensors. The scoring functions of KG embedding models combine the concept embeddings via linear prod- uct (i.e., relation tensor multiplies the concatena- tion of concept vectors of the two entities) <ref type="bibr" target="#b6">(Bordes et al., 2011)</ref>, bilinear product (i.e., relation tensor first multiplies the left concept embedding and the result multiplies the embedding of the second con- cept) <ref type="bibr" target="#b40">(Yang et al., 2015)</ref>, or the combination of the two <ref type="bibr" target="#b35">(Socher et al., 2013)</ref>. Both linear and bilinear scoring functions implicitly model asymmetry as they are not commutative with respect to concept embeddings. In this work, we choose to leverage the bilinear product in our model, following the findings of <ref type="bibr" target="#b40">Yang et al. (2015)</ref> who report bilinear product outperforming other scoring combinations.</p><p>KG embedding models employ the same concept embeddings for predicting all relations, symmetric and asymmetric alike. By directly updating concept embeddings in training, they cannot make relation predictions for concepts outside of the training set.</p><p>Hypernymy and Meronymy Detection. Hyper- nymy and meronymy are arguably the two most prominent asymmetric lexico-semantic relations. Methods for their detection can roughly be clas- sified as either distributional or path-based. Path- based methods consider lexico-syntactic paths con-necting pairs of words in their co-occurrence con- texts in large corpus. Early approaches, e.g., Hearst (1992) for hypernymy and <ref type="bibr" target="#b3">Berland and Charniak (1999)</ref> for meronymy, exploited a small set of man- ually created lexico-syntactic patterns that imply a relation of interest (e.g., a such as b). Subsequent approaches looked at ways to eliminate the need for manual compilation of extraction patterns. Pan- tel and <ref type="bibr" target="#b23">Pennacchiotti (2006)</ref> and <ref type="bibr" target="#b12">Girju et al. (2006)</ref> proposed bootstrapping approaches to meronymy detection, starting from a seed set of part-whole pairs. <ref type="bibr" target="#b34">Snow et al. (2004)</ref> provided all dependency paths connecting the concepts in corpus to a logis- tic regression classifier for hypernymy detection.</p><p>Distributional methods detect asymmetric rela- tions using only distributional vectors of words as input. Distributional models come in both unsuper- vised and supervised flavors. Unsupervised metrics for hypernymy detection assume either that the hy- ponym's contexts are included in the hypernym's contexts <ref type="bibr" target="#b38">(Weeds and Weir, 2003;</ref><ref type="bibr" target="#b16">Kotlerman et al., 2010)</ref> or that the linguistics contexts of a hyponym are more informative than the contexts of its hyper- nyms <ref type="bibr" target="#b25">(Rimell, 2014;</ref><ref type="bibr" target="#b29">Santus et al., 2014</ref>). Super- vised hypernymy classifiers represent the pair of words by combining their distributional vectors in different ways -concatenating them ( <ref type="bibr" target="#b0">Baroni et al., 2012</ref>) or subtracting them ( <ref type="bibr" target="#b26">Roller et al., 2014</ref>) - and feeding the resulting vector to a supervised classifier like logistic regression. Most recently,  coupled path-based and dis- tributional information with a recurrent neural net- work (RNN), yielding state-of-the-art hypernymy detection performance. Although our Dual Tensor model is purely distributional, we show that it may outperform such a hybrid model which additionally exploits syntactic information.</p><p>Distributional and path-based models have been used to discriminate between multiple lexico- semantic relations, including hypernymy and meronymy, at once ( . However, as pointed out by <ref type="bibr" target="#b7">(Chersoni et al., 2016)</ref>, distributional vectors and scores based on their comparison fail to discrimi- nate between multiple relation types at once. In this work, we focus on binary classification for a single relation (hypernymy and meronymy) at a time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dual Tensor Model</head><p>The following assumptions and desirable properties guided the design of the Dual Tensor model for detection of asymmetric lexico-semantic relations:</p><p>(1) Unspecialized distributional vectors are not good signals for detecting specific lexico-semantic relations. We thus need to derive specialized rep- resentations that are better suited for detecting the specific asymmetric relation of interest.</p><p>(2) The transformation from unspecialized distribu- tional vectors of words to their relation-specialized embeddings should be captured explicitly, via a well-defined transformation function. Having an explicit embedding specialization function allevi- ates the need to specialize the entire unspecialized embedding space at once, like existing models do <ref type="bibr" target="#b9">(Faruqui et al., 2015;</ref><ref type="bibr" target="#b20">Mrkši´Mrkši´c et al., 2016)</ref>.</p><p>(3) Each concept should have two different relation- specialized embeddings -one for each end of an asymmetric relation. For instance, for hypernymy, the concept's specialized embedding for pairs in which it is considered to be a hyponym (e.g., dog in dog-animal) should differ from its embedding in pairs in which it is tested as a hypernym (e.g., dog in maltese-dog).</p><p>(4) An unspecialized distributional vector of the word might -for each end of the asymmetric re- lation -be transformed into several specialized vectors instead of only one. This way the model may implicitly account for polysemy -i.e., differ- ent specialized vectors might capture asymmetric properties of different senses of polysemous words. E.g., the hyponym properties of bank in the pair bank vs. building may be different from those in the pair bank vs. company). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dual Tensors</head><p>For a given pair of concepts (c 1 , c 2 ), Dual Ten- sor model computes the score s(c 1 , c 2 ) indicating the likelihood that an asymmetric lexico-semantic relation holds between the concepts (e.g., for meronymy, how likely it is that c 1 is a part of c 2 ). The model takes as input the unspecialized embeddings of the two concepts, e 1 and e 2 . For single-word concepts these are simply pre-trained word embeddings, whereas for multi-word con- cepts, similar to ( <ref type="bibr" target="#b35">Socher et al., 2013)</ref>, we average the pre-trained embeddings of constituent words.</p><p>The unspecialized input embeddings are next translated into specialized embeddings, meant to better capture the existence of the asymmetric rela- tion between the concepts, via specialization ten- sors. By introducing dedicated tensors we -unlike existing models, which directly propagate updates to unspecialized embeddings ( <ref type="bibr" target="#b9">Faruqui et al., 2015;</ref><ref type="bibr" target="#b20">Mrkši´Mrkši´c et al., 2016</ref>) -explicitly learn the special- ization function. With an explicit specialization function, we do not have to specialize the whole embedding space at once. Also, unlike KG com- pletion models ( <ref type="bibr" target="#b5">Bordes et al., 2013;</ref><ref type="bibr" target="#b35">Socher et al., 2013)</ref>, we can make predictions for pairs involving concepts unseen in the training data.</p><p>We explicitly model asymmetry by introducing two specialization tensors (hence the model name) that differently specialize the unspecialized input embeddings of concepts. The left tensor, W R ), special- izes the concept embedding when the concept is the second element of the pair:</p><formula xml:id="formula_0">e [1:k] L = tanh e 1 W [1:k] L + b [1:k] L e [1:k] R = tanh e 2 W [1:k] R + b [1:k] R</formula><p>When predicting hypernymy, for example, dual ten- sors ensure that the specialized representation for concept cat in pairs like cat-animal differs from its specialized representation in pairs like birman-cat. Specialization tensors map an unspecialized em- bedding into a set of k specialized embeddings - each slice of the tensor, W i L (W i R ), together with the corresponding bias vector b i L (b i R ), produces one specialized vector e i L (e i R ). By using special- ization tensors with k slices instead of specializa- tion matrices we make the model more general.</p><p>The tensor-based model trivially degrades to the matrix-based model by setting k = 1. We obtain the final specialized representation of a concept by non-linearly transforming (hyperbolic tangent) the product of an unspecialized input embedding and the specialization tensor. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bilinear Product and Scoring</head><p>Using dual tensors, we transform unspecialized embeddings into asymmetrically specialized repre- sentations -sets of specialized vectors -which we next use to predict whether the asymmetric relation holds between the concepts. Our scoring function is based on bilinear products between (1) special- ized vectors e </p><formula xml:id="formula_1">b i = e i L W i B (e i R ) T .</formula><p>The final relation score s(c 1 , c 2 ) for a given pair of concepts is computed by reducing the vector of bilinear product scores b to the mean value (func- tion g in <ref type="figure" target="#fig_0">Figure 1</ref>) 2 and non-linearly bounding the resulting score to the [−1, 1] range:</p><formula xml:id="formula_2">s(c 1 , c 2 ) = tanh 1 k k i=1 b i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimization</head><p>Dual Tensor model is parametrized by the spe- cialization tensors, their corresponding bias vec- tors, and the relation tensor, namely, Ω = {W</p><formula xml:id="formula_3">[1:k] L , W [1:k] R , b [1:k] L , b [1:k] R , W [1:k]</formula><p>B }. Let A be the set of concept pairs in the training set,</p><formula xml:id="formula_4">A = {p i = (c i 1 , c i 2 )} N i=1</formula><p>. We learn model's parameters by minimizing the margin-based objective:</p><formula xml:id="formula_5">J(Ω) = λΩ 2 + p i ∈A max 0, 1 − s(p i ) · y(p i )</formula><p>where s(p i ) is model's prediction for the pair</p><formula xml:id="formula_6">(c i 1 , c i 2 ), y(p i ) ∈ {−1, 1}</formula><p>is the true label of that pair, and λ is the regularization coefficient. In all our experiments, we trained the model in mini- batches, optimizing the parameters with the RM- SProp algorithm <ref type="bibr" target="#b36">(Tieleman and Hinton, 2012)</ref>.</p><p>The model has three hyperparameters: the length of the unspecialized input embeddings l, the num- ber of tensor slices k, and the regularization fac- tor λ. We optimize the hyperparameters (together with the starting learning rate value) via grid- search, by maximizing performance on the vali- dation portion of each dataset. In all our experi- ments, except the multilingual comparison (Sec- tion 5.3), we evaluated variants of the Dual Tensor model using pre-trained English GloVe word em- beddings ( <ref type="bibr" target="#b24">Pennington et al., 2014</ref>) with varying length, l ∈ {50, 100, 200, 300} and tensors with k ∈ {1, . . . , 5} slices. In most experiments, the optimal configuration was l = 300 and k = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We evaluate the Dual Tensor model on several datasets for detecting hypernymy and meronymy, two arguably most prominent asymmetric lexico- semantic relations. In all experiments, we compare the model's performance with state-of-the-art re- sults on respective datasets. Additionally, aiming to quantify the effects that different components of the Dual Tensor model have on prediction perfor- mance, we evaluate two reduced models variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate the Dual Tensor model on the follow- ing hypernymy and meronymy detection datasets:</p><p>HypeNet dataset. Arguing that existing datasets were too small for training their recurrent network,  compiled this dataset for hy- pernymy detection from several external KBs, tak- ing only pairs of concepts in direct relation (i.e., no transitive closure).</p><p>Other hypernymy detection datasets. We ad- ditionally evaluate the Dual Tensor model on four smaller datasets for hypernymy detection: (1) BLESS dataset ( <ref type="bibr" target="#b1">Baroni and Lenci, 2011)</ref> </p><note type="other">and EVALuation dataset (Santus et al., 2015) contain instances of hypernymy and four other relations. BLESS additionally contains random word pairs; (2) Weeds dataset (Weeds et al., 2014) contains hypernymy and co-hyponymy pairs; (3) Benotto dataset (Benotto, 2015) couples hypernymy pairs with synonymy and antonymy pairs. Because these datasets contain at most several thousand pairs, we only use them to evaluate the performance of mod- els trained on larger datasets;</note><p>WN-Hy and WN-Me datasets. We create these datasets by taking concept pairs from WordNet. We take all instances from the transitive closure of hypernymy (all parts of speech) and meronymy (nouns) relations and couple them with all synonym and antonym relations (all parts of speech), as well as lexical entailment relations (verbs).</p><p>For the WN-Hy dataset we designate all hy- pernymy relations (i.e., both direct and indirect) as positive instances and their inverses (i.e., hy- ponymy relations) together with all other rela- tions as negative instances. Finally, we balance the dataset by randomly sampling negative instances to match the number of positive instances. Anal- ogously, we create the WN-Me dataset by taking meronymy relations as positive instances. We com- pile three different WN-Hy datasets: WN-Hy-EN using English WordNet <ref type="bibr">(Fellbaum, 1998)</ref>, WN- Hy-ES using Spanish WordNet ( <ref type="bibr" target="#b13">Gonzalez-Agirre et al., 2012)</ref>, and WN-Hy-FR using French Word- Net ( <ref type="bibr" target="#b27">Sagot and Fišer, 2008)</ref>. To allow for fair com- parison of model's performance across languages, we randomly sample two larger dataset (English and French) to match in size the smallest (Spanish).</p><p>Lexical and Random Splits. <ref type="bibr" target="#b18">Levy et al. (2015)</ref> showed that supervised distributional models for classifying lexico-semantic relations suffer from overfitting in settings with significant lexical over- lap between the training and test set. In such set- tings models tend to learn properties of individual words (e.g., that a word is a prototypical hypernym) instead of relations between words. The reported results on such datasets are thus overly optimistic estimates of models' true performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train</head><p>Val. Test</p><p>HypeNet (rand) 49.5K (20%) 3.5K (19%) 17.7K (20%) HypeNet (lex) 20.3K (20%) 1.4K (20%) 6.6K (20%) BLESS - 2.7K (5%) 23.9K (5%) EVALuation - 1.4K (24%) 12.3K (27%) Weeds - 293 (50%) 2.6K (50%) Benotto - 501 (41%) 4.5K (38%) WN-Hy-EN 103K (50%) 15K (50%) 30K (50%) WN-Hy-EN 103K (50%) 15K (50%) 30K (50%) WN-Hy-FR 103K (50%) 15K (50%) 30K (50%) WN-Me (rand) 13.9K (50%) 2K (50%) 4K (50%) WN-Me (lex) 7.9K (50%) 208 (50%) 318 (50%) <ref type="table">Table 1</ref>: Datasets used in evaluation.</p><p>To eliminate the effect of lexical memorization, <ref type="bibr" target="#b18">Levy et al. (2015)</ref> propose dataset splits with no lexical overlap between the train and test portions. However, model's performance in a lexically-split setting is an overly pessimistic estimate of mod- els' true performance -in a realistic scenario, the model will occasionally make predictions for pairs involving some of the concepts from the training set. Because the true model performance is likely between the performance on a randomly-split and performance on a lexically-split dataset, we report models' performance in both of these settings.</p><p>We show the sizes of all dataset variants used in our experiments in <ref type="table">Table 1</ref>. We additionally report the proportion of positive instances (in brackets), as this percentage directly affects some evaluation metrics (precision, F 1 -score, average precision).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>In addition to specific models yielding best perfor- mance on particular datasets, we compare the Dual Tensor model (DUAL-T) with these baselines:</p><p>Supervised distributional baseline (CONCAT- SVM). We train SVM model with RBF kernel on concatenation of unspecialized concept embed- dings ( <ref type="bibr" target="#b0">Baroni et al., 2012)</ref>, following <ref type="bibr" target="#b18">Levy et al. (2015)</ref>, who report this model outperforming other types of embedding composition;</p><p>Bilinear product (BILIN-PROD). This model is the simple bilinear product between the unspecial- ized concept embeddings, parametrized only by the relation matrix W B . That is, the prediction score for a pair of concepts is given as s(c <ref type="bibr">1</ref>   and DUAL-T, we jointly quantify the effects of <ref type="formula">(1)</ref> explicit modeling of asymmetry and (2) relation- specific embedding specialization;</p><p>Single tensor model (SINGLE-T). This is the reduction of the Dual Tensor model in which we use only one specialization tensor, i.e., W</p><formula xml:id="formula_7">[1:k] L = W [1:k]</formula><p>R . In other words, SINGLE-T model always specializes the unspecialized embedding of a con- cept the same way, regardless of the concept's po- sition in a candidate pair. By comparing the perfor- mance of the DUAL-T model with that of SINGLE- T, we measure the effect of asymmetrically special- izing unspecialized embeddings.</p><p>Same as for the DUAL-T model, we optimize the hyperparameters of the baselines on the validation portions of the datasets used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Classification Experiments</head><p>Binary classification is the most straightforward evaluation setting for relation detection models. For a pair of concepts, we make the binary asymmetric relation prediction r a (c 1 , c 2 ) simply by threshold- ing the model's prediction scores, i.e., r a (c 1 , c 2 ) = I{s(c 1 , c 2 ) &gt; 0}, where I is the indicator function.</p><p>Hypernymy classification. We first evaluate the DUAL-T model and the baselines on the HypeNet dataset ( . We show the perfor- mance of the DUAL-T model in <ref type="table" target="#tab_1">Table 2</ref>, together with the path-based and hybrid (combination of path-based and distributional signal) variants of the the state-of-the-art RNN model of . On the more challenging, lexically-split dataset DUAL-T model significantly 3 outperforms the more complex hybrid HypeNet model ), an RNN model coupling representa- tions of syntactic paths from a large corpus with <ref type="bibr">Lex. split</ref> Rand. split</p><formula xml:id="formula_8">Model P R F1 P R F1</formula><p>CONCAT-SVM 78.6 44.6 56.9 79.9 75.9 77.9 BILIN-PROD 73.3 50.0 59.4 81.0 79.8 80.5 SINGLE-T 77.7 55.5 64.8 85.7 82.6 84.1 DUAL-T 76.5 61.1 67.9 87.7 85.3 86.5 <ref type="table">Table 3</ref>: Meronymy classification performance.</p><p>unspecialized concept embeddings. In both settings DUAL-T outperforms SINGLE-T which, in turn, outperforms BILIN-PROD. This empirically justi- fies both our explicit modeling of asymmetry and relation-specific embedding specialization.</p><p>Meronymy classification. We next evaluate the meronymy classification performance of the mod- els on the WN-Me dataset. The results are shown in <ref type="table">Table 3</ref>. Same as in the case of hypernymy classification, DUAL-T significantly outperforms all three baselines, with SINGLE-T outperforming BILIN-PROD. All distributional models we evalu- ate achieve poorer performance on meronymy than hypernymy detection, especially considering that WN-Me is a balanced dataset, whereas HypeNet is heavily skewed towards negative instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ranking Experiments</head><p>Shwartz et al. <ref type="formula">(2017)</ref> propose ranking as an alter- native evaluation setting for hypernymy detection. The goal is to rank positive relation pairs higher than negative ones. Our DUAL-T model (and as- sociated baselines) rank the concept pairs in de- creasing order of assigned relations scores s(c 1 , c 2 ). Following <ref type="bibr" target="#b33">Shwartz et al. (2017)</ref>, we report perfor- mance in terms of overall average precision (AP) and average precision at rank 100 (AP@100).</p><p>Hypernymy ranking. We evaluate the ranking performance on four small hypernymy test sets: BLESS, EVALuation, Benotto, and Weeds (cf. Ta- ble 1). As these datasets are not big enough to train neural models, we train all models on the HypeNet dataset. For each test set we eliminate the lexical overlap by removing from the HypeNet dataset pairs containing any concept from that test set. <ref type="table" target="#tab_3">Table 4</ref> displays ranking performance for DUAL- T model, the supervised baselines, and the best- performing unsupervised hypernymy detection score (BEST-UNSUP, performance taken from ( <ref type="bibr" target="#b33">Shwartz et al., 2017)</ref>). Hypernymy ranking results depict the effectiveness of the DUAL-T model with respect to supervised baselines even more clearly than hypernymy classification results. All super- vised models outperform the best unsupervised model in terms of AP, but only DUAL-T is consis- tently better when considering only 100 top-ranked pairs (AP@100). This adds to the conclusion that explicit modeling of asymmetry using dual tensors yields crucial performance boost.</p><p>Meronymy ranking. We measure the ranking performance for meronymy detection on the WN- Me dataset, reporting the results for both randomly- and lexically-split variants of the dataset in <ref type="table" target="#tab_4">Table  5</ref>. Meronymy ranking results are in line with per- formance figures for hypernymy ranking. Again, DUAL-T consistently outperforms all three base- lines. Absolute AP scores for meronymy are higher than those we report for hypernymy, but this is merely because WN-Me is a balanced dataset, whereas the hypernymy ranking test sets (with the exception of the Weeds dataset) are substantially skewed in favor of negative concept pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>We perform additional analyses, providing further insights into DUAL-T model's performance. We analyze how model's performance depends on con- cept distance in WordNet and on number of concept senses. We also examine the stability of DUAL-T model's performance across different languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">WordNet Distance</head><p>Unlike the HypeNet dataset ( , which contains only pairs of concepts that exist in a direct relation in some external knowledge base, our WN-Hy and WN-Me datasets (cf. Section 4.1) contain pairs of concepts of varying distance in WordNet, allowing for a more fine-grained analysis of the Dual Tensor model's performance.</p><p>We divide the test sets of WN-Hy-EN and WN- Me into five buckets according to the shortest path distance between concepts in WordNet. <ref type="bibr">4</ref> We show hypernymy and meronymy prediction accuracies for all buckets in <ref type="figure">Figure 2</ref>. For hypernymy, we observe significantly lower accuracy for pairs of concepts appearing close in WordNet hierarchy. Close hyponym-hypernym pairs (e.g., car-vehicle) tend to occur in similar contexts and consequently have similar unspecialized embeddings. Such hy- pernymy instances are difficult to discern from syn-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLESS EVALuation Benotto Weeds</head><p>Model AP AP@100 AP AP@100 AP AP@100 AP AP@100</p><p>BEST-UNSUP ( <ref type="bibr" target="#b33">Shwartz et al., 2017</ref>   onymous pairs (e.g., car-automobile). The same effect is, however, not observed for meronymy - part-whole relations between close concepts are as detectable as between more distant concepts. This is probably because part concepts appear in differ- ent contexts than whole concepts (e.g., wheel-car), resulting in distinct unspecialized embeddings in the first place. For both relations we observe a drop in performance for pairs of very distant concepts. Such pairs typically contain one very abstract con- cept (e.g., object), but embeddings of abstract con- cepts are not superpositions of embeddings of their hyponyms (Rimell, 2014) nor their meronyms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effects of Polysemy</head><p>Given that our Dual Tensor model takes unspecial- ized concept embeddings as input and that unspe- cialized embeddings do not discern between differ- ent senses of words, our Dual Tensor model treats monosemous and polysemous concepts equally. In- tuitively, predicting asymmetric relations for pairs involving polysemous concepts should be more dif- ficult than for pairs of monosemous concepts, be- cause the models in such cases additionally need to learn to discern between different concept senses.</p><p>While designing the Dual Tensor model, we hy- pothesized that different tensor slices might be able to accommodate for asymmetric relations involv- ing different senses of polysemous words. In order to closer examine the effects of polysemy on the performance of the Dual Tensor model, we parti- tioned the test portions of the WN-Hy and WN- Me datasets according to number of senses of the concept pair (we average the number of senses of the two concepts in a candidate pair). We show the Dual Tensor model's performance (k = 3, l = 300) on different number-of-senses buckets, both for hy- pernymy and meronymy prediction, in <ref type="figure" target="#fig_5">Figure 3</ref>.</p><p>For hypernymy, the general trend is as expected: the larger the average number of senses of concepts in the candidate pair, the lower the prediction accu- racy. The exception is the bucket <ref type="bibr">(3,</ref><ref type="bibr">5]</ref> for which the performance is higher than for the previous bucket <ref type="bibr">(1,</ref><ref type="bibr">3]</ref>. The drop in performance is not dras- tic as long as the model is not dealing with highly   polysemous concepts (with more than five senses). These performance figures suggest that, via the mul- tiple tensor slices, the DUAL-T model can, to some extent, alleviate the effects that polysemy has on predicting asymmetric lexico-semantic relations. Somewhat surprisingly, the polysemy seems not to have a clear negative effect for meronymy. Pre- diction accuracy on pairs of highly polysemous concepts seems to be similar to that on monose- mous concept pairs. An instance-level inspection reveals that meronymy detection is more sensitive to the number of senses of the part candidate con- cept than of the whole concept. In other words, if we partition the test set only according to the num- ber of senses of the part concept, then the trends are similar to those observed for hypernymy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Multilingual Comparison</head><p>To examine how the Dual Tensor model performs across languages, we evaluate its performance on equally-sized hypernymy detection datasets in En- glish, Spanish, and French (cf. Section 4.1 and <ref type="table">Table 1</ref>). To increase the comparability of results, for each of the three languages we trained word embeddings using the CBOW algorithm <ref type="bibr" target="#b19">(Mikolov et al., 2013</ref>) on the Wikipedia dump of respective language. Also, for all three models we select the hyperparameter configuration that turned out to be optimal most often in previous experiments - we set the length of unspecialized embeddings to l = 300 and number of tensor slices to k = 3. Hy- pernymy classification performance for different languages is shown in <ref type="table" target="#tab_7">Table 6</ref>. The results sug- gest that Dual Tensor model exhibits stable perfor- mance across languages. The small performance differences between languages may be attributed to different sizes of respective Wikipedia dumps (on which we train unspecialized embeddings) as well as to inherent differences in language complexity (e.g., English being morpho-syntactically simpler).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented a neural model for detecting asymmetric semantic relations. Unlike existing models, which uniformly treat asymmetric and symmetric relations, our Dual Tensor model cap- tures asymmetry explicitly using a pair of special- ization tensors that produce two different embed- ding specializations, depending on the concept's role in the relation. Instead of just updating unspe- cialized embeddings, with specialization tensors we also explicitly capture the mapping function.</p><p>The results from a battery of hypernymy and meronymy experiments show that via asymmetric specialization of concept embeddings the Dual Ten- sor model is able to outperform (1) the supervised model directly using unspecialized embeddings as well as (2) the more complex neural architecture that additionally exploits syntactic information. We have additionally shown that our model can dimin- ish the negative effects of polysemy and that it exhibits stable performance across languages.</p><p>As future work, we plan to develop similar models based on explicit specialization tensors for detecting symmetric relations (e.g., synonymy, antonymy). We will also seek to exploit the Dual Tensor model in different downstream tasks, e.g., hypernymy detection for taxonomy induction <ref type="bibr" target="#b8">(Faralli et al., 2017)</ref> or recognizing textual entailment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 depicts</head><label>1</label><figDesc>Figure 1 depicts the overall architecture of the Dual Tensor model, incorporating all four of abovementioned design guidelines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture of the Dual Tensor model.</figDesc><graphic url="image-1.png" coords="4,72.40,62.80,450.03,160.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(with the corresponding set of bias vectors b [1:k] L ), specializes the concept embedding if the concept is the first element of the pair, whereas the right tensor, W [1:k] R (with bias vectors b [1:k]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>of the first concept, (2) relation tensor W [1:k] B , and (3) specialized vectors e [1:k] R of the second concept. For each pair of specialized vectors e i L and e i R , i ∈ {1, . . . , k}, we compute the bilinear product score, using the corresponding slice W i B of the relation tensor W [1:k] B :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Hypernymy and meronymy performance with respect to concept polysemy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Hypernymy classification performance. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Hypernymy detection, ranking results. 

1 
2 
3-5 
6-10 
&gt;10 
60 

70 

80 

90 

100 

61.9 

84.9 

94.3 
93.5 
87.2 
89.1 
90.5 
90 
87.8 
84.2 

WordNet shortest path distance 

Accuracy (%) 

Hypernymy 
Meronymy 

Figure 2: Hypernymy and meronymy performance with respect to WordNet shortest path distance. 

Lex. split 
Rand. split 

Model 
AP AP@100 AP AP@100 

CONCAT-SVM .686 
.775 
.796 
.865 
BILIN-PROD 
.682 
.832 
.878 
.947 
SINGLE-T 
.772 
.900 
.909 
.979 

DUAL-T 
.840 
.967 
.936 
1.00 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 : Meronymy detection, ranking results.</head><label>5</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Hypernymy classification performance for 
different languages. 

</table></figure>

			<note place="foot" n="1"> Preliminary experiments without applying a non-linear transformation yielded consistently poorer performance. 2 We also experimented with min-and max-reduction, but the reduction to the mean yielded best preliminary results.</note>

			<note place="foot" n="3"> All performance differences were tested using the nonparametric stratified shuffling test (Yeh, 2000) with α = 0.05.</note>

			<note place="foot" n="4"> For any concept with multiple senses, we considered the WordNet synset of its dominant sense.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Entailment above the word level in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Quynh</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Chieh</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How we BLESSed distributional semantic evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Workshop on GEometrical Models of Natural Language Semantics</title>
		<meeting>the 2011 Workshop on GEometrical Models of Natural Language Semantics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Distributional models for semantic relations: A study on hyponymy and antonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Benotto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>University of Pisa</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Finding parts in very large corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Berland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 37th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Annual Conference on Neural Information Processing Systems</title>
		<meeting>the 2013 Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Fifth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="301" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuele</forename><surname>Chersoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Rambelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<idno>abs/1611.01101</idno>
		<title level="m">CogALex-V Shared Task: ROOT18. CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The ContrastMedium algorithm: Taxonomy induction from noisy knowledge graphs with just a few links</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Faralli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="590" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1606" to="1615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">PPDB: The paraphrase database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="758" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic discovery of part-whole relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Badulescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="135" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multilingual central repository version 3.0: upgrading a very large lexical knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egoitz</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Global WordNet Conference</title>
		<meeting>the 6th Global WordNet Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marti A Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th conference on Computational linguistics</title>
		<meeting>the 14th conference on Computational linguistics</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="539" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Specializing word embeddings for similarity or relatedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2044" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Directional distributional similarity for lexical inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Kotlerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan</forename><surname>Zhitomirsky-Geffet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="359" to="389" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An introduction to latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas K Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename><surname>Foltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discourse processes</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="259" to="284" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Do supervised distributional methods really learn lexical inference relations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Remus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Israel</forename><surname>Ramat-Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="970" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Counter-fitting word vectors to linguistic constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="142" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Patty: a taxonomy of relational patterns with semantic types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ndapandula</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1135" to="1145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1955" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Espresso: Leveraging generic patterns for automatically harvesting semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributional lexical entailment by topic coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="511" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Inclusive yet selective: Supervised distributional hypernymy detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics</title>
		<meeting>the 25th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1025" to="1036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Building a free French WordNet from multilingual resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoˆıtbenoˆıt</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darja</forename><surname>Fišer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ontolex</title>
		<meeting>the Ontolex</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The CogALex-V shared task on the corpus-based identification of semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Gladkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Evert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon</title>
		<meeting>the 5th Workshop on Cognitive Aspects of the Lexicon</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Chasing hypernyms in vector spaces with entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="38" to="42" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">EVALution 1.0: an evolving semantic dataset for training and evaluation of distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frances</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Workshop on Linked Data in Linguistics</title>
		<meeting>the 4th Workshop on Linked Data in Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="64" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Cogalex-V shared task: Lexnet-integrated path-based and distributional method for the identification of semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.08694</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving hypernymy detection with an integrated path-based and distributional method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2389" to="2398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hypernyms under siege: Linguistically-motivated artillery for hypernymy detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Schlechtweg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="65" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning syntactic patterns for automatic hypernym discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Annual Conference on Neural Information Processing Systems</title>
		<meeting>the 2004 Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Annual Conference on Neural Information Processing Systems</title>
		<meeting>the 2013 Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to distinguish hypernyms and co-hyponyms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoud</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics</title>
		<meeting>the 25th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2249" to="2259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A general framework for distributional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 conference on Empirical methods in Natural Language Processing</title>
		<meeting>the 2003 conference on Empirical methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">From paraphrase database to compositional paraphrase model and back. Transactions of the Association for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="345" to="358" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 International Conference on Learning Representations</title>
		<meeting>the 2015 International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">More accurate tests for the statistical significance of result differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Computational Linguistics</title>
		<meeting>the 18th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="947" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improving lexical embeddings with semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="545" to="550" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
