<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Computational Cognitive Model of Novel Word Generalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aida</forename><surname>Nematzadeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Grant</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzanne</forename><surname>Stevenson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Computational Cognitive Model of Novel Word Generalization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A key challenge in vocabulary acquisition is learning which of the many possible meanings is appropriate for a word. The word generalization problem refers to how children associate a word such as dog with a meaning at the appropriate category level in a taxonomy of objects, such as Dalma-tians, dogs, or animals. We present the first computational study of word generalization integrated within a word-learning model. The model simulates child and adult patterns of word generalization in a word-learning task. These patterns arise due to the interaction of type and token frequencies in the input data, an influence often observed in people&apos;s generalization of linguistic categories.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning word meanings is a challenging early step in child language acquisition. Imagine a child hears the word dax for the first time while observing a white rabbit jumping around -dax might mean WHITE RABBIT, RABBIT, ANIMAL, CUTE, LOOK, etc. <ref type="bibr" target="#b14">(Quine, 1960)</ref>. How does the child learn the correct meaning of a word from a large pool of potential meanings? A possi- ble explanation is that children infer a word's meaning by identifying the commonalities across the situations in which the word occurs <ref type="bibr" target="#b13">(Pinker, 1989)</ref>. One mechanism for achieving this is cross- situational learning (e.g., <ref type="bibr" target="#b18">Siskind, 1996;</ref><ref type="bibr" target="#b5">Frank et al., 2007;</ref><ref type="bibr" target="#b4">Fazly et al., 2010;</ref><ref type="bibr" target="#b7">Kachergis et al., 2012)</ref>. Recent word learning experiments con- firm that both adults and children infer the cor- rect word-meaning mappings by keeping track of cross-situational statistics across individually am- biguous learning trials ( <ref type="bibr" target="#b21">Yu and Smith, 2007;</ref><ref type="bibr" target="#b19">Smith and Yu, 2008;</ref><ref type="bibr" target="#b22">Yurovsky et al., 2014</ref>).</p><p>Although cross-situational learning is a general mechanism for narrowing down the meaning of a word, it does not explain how children overcome an interesting challenge in word learning: deter- mining the correct level of a hierarchical taxon- omy that a word refers to. For example, children learn that the word dog refers to all kinds of dogs, and not to a specific breed, such as Dalmatians, or to a more general category, such as animals - even though some of these choices (e.g., animals) are compatible with all the cross-situational evi- dence available for dog (because all dogs are also animals). We use the term "word generalization" to refer to this problem of associating a word with the meaning at an appropriate category level, given some sample of experiences with the word.</p><p>Previous research has argued that children use a specific bias or constraint -the basic-level as- sumption -to focus their word generalizations appropriately <ref type="bibr" target="#b10">(Markman, 1991;</ref><ref type="bibr" target="#b6">Golinkoff et al., 1994)</ref>. According to this bias, children prefer to associate a word to a set of objects that form a basic-level category, such as dogs or trucks, and that share a significant number of attributes. It is less preferred to associate a new word to much more specific subordinate categories, such as Dal- matians or bulldozers, or to more general superor- dinate ones, like animals or vehicles, whose mem- bers share fewer attributes <ref type="bibr" target="#b16">(Rosch, 1973;</ref><ref type="bibr" target="#b15">Rosch et al., 1976)</ref>. It remains an important open question of whether a word learner requires such a bias to acquire appropriate mappings. <ref type="bibr" target="#b20">Xu and Tenenbaum (2007)</ref> (X&amp;T henceforth) studied the word generalization problem in a set of experiments in which children and adults were asked to determine which level of a taxonomy a novel word referred to. X&amp;T further examined this behavioral data through computational mod- elling. They proposed a Bayesian model that, given a few exemplars of a novel word, matches human behaviour in how it maps the word to its meanings in a taxonomic category.</p><p>The Bayesian model of X&amp;T is important in providing insight into how people might reason about samples of data that exemplify categories. However, it relies on having complete, built-in knowledge about the taxonomic hierarchy, includ- ing both the detailed composition of categories and the values for between-object similarities, drawn from adult similarity judgments. Further- more, the X&amp;T model does not address the issue of word generalization in the broader context of word learning: While their model reasons over samples of data associated with a word label, it does not develop a meaning representation of the word over time, as a child must do. It is impor- tant to understand how word generalization occurs when embedded in the natural process of learning a word meaning and in the context of more limited category knowledge.</p><p>We address these issues by providing a uni- fied account of word learning and word gener- alization within a computational model of cross- situational learning. Unlike the X&amp;T model, our model is an incremental learner that gradually ac- quires the meaning of words, and uses these devel- oping meanings in determining the appropriate ex- tension of a word to elements of a taxonomy. Our model has general knowledge of category struc- ture without having an elaborated taxonomy en- coding known object similarities. Moreover, in the absence of any bias toward generalization to par- ticular kinds of categories, the model exhibits the observed "basic-level bias" due to general mech- anisms of productivity that have been proposed to apply to many aspects of linguistic knowledge (e.g., <ref type="bibr" target="#b1">Bybee, 1985;</ref><ref type="bibr" target="#b3">Croft and Cruse, 2004)</ref>. <ref type="bibr">1</ref> In what follows, we first describe the human ex- periments of X&amp;T, and then present our computa- tional model and the experiments that simulate the X&amp;T data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Novel Word Generalization in People</head><p>X&amp;T perform a set of empirical studies to inves- tigate how children and adults generalize novel words learned from a few examples to the appro- 1 Computational cognitive models are often categorized with respect to Marr's levels of analysis, i.e., their degree of abstraction <ref type="bibr" target="#b11">(Marr, 1982)</ref>. The model of X&amp;T is at the computational level, providing a Bayesian framework for the problem of word generalization. In contrast, our model in- vestigates more detailed mechanisms and thus lies between the algorithmic and computational levels of analysis.</p><p>priate level of meaning in a taxonomy. In each training trial of an experiment, participants hear a novel word (such as fep) and observe one or more instances exemplifying the word (in the form of pictures for adults and toy objects for children). The conditions vary in that the make-up of the set of training instances is representative of different levels of a taxonomy (e.g., all Dalmatians vs. var- ious kinds of dogs vs. various kinds of animals). In the testing phase, participants are asked to se- lect all objects that they think are feps from a set of test items. Both children and adults make var- ious inferences about what a fep is depending on the levels of the taxonomy from which the training instances are drawn.</p><p>Specifically, X&amp;T use a taxonomy with ani- mals, vehicles, and vegetables, from which in- stances are drawn to produce the training condi- tions in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. For example, in one training condition, participants are shown a Dalmatian, a poodle, and a beagle in three consecutive trials, hearing the word fep to refer to each object. Af- ter training, participants are asked to select all feps from the set of test objects, which includes items from all 3 superordinate categories. As illustrated in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, each test object is assessed as one of the following types of match to the training data:</p><p>• a subordinate match: an object of the same subordinate category as a training ob- ject (e.g., Dalmatians in <ref type="figure" target="#fig_0">Fig. 1)</ref> • a basic-level match: an object of the same basic-level category as a training object (e.g., a dog, but not the same breed as one in train- ing [which would be a subordinate match])</p><p>• a superordinate match: an object of the same superordinate category as the training objects (e.g., another kind of animal, but not one seen in training [which would be a sub- ordinate or basic-level match])</p><p>X&amp;T report the percentage of test objects of each type of match that are selected by participants within each training condition; see <ref type="figure" target="#fig_2">Fig. 2</ref>. (For example, the reported value for "super. match" would be 75% if participants on average chose 3 of the 4 superordinate matches in the test set.) Consider first the data from adults. After see- ing a single object (1-example condition -e.g., a Dalmatian), adults show a strong basic-level bias -i.e., they tend to generalize the word fep to refer to both Dalmatians (subordinate matches) and to  other dogs (basic-level matches), but not to other animals (superordinate matches). But with 3 in- stances of a Dalmatian (3-subordinate condition), this behaviour is attenuated -the number of basic- level matches is much lower. For the 3-basic-level and 3-superordinate conditions, the adults show generalization up to categories consistent with the evidence -i.e., at the basic and superordinate lev- els, respectively. Interestingly, children also show a basic-level bias, but differ from adults in that it is less pro- nounced -e.g., they are less likely than adults to select basic-level matches (other dogs) having seen a single Dalmatian or having seen 3 Dalma- tians. In the other conditions, children's behaviour is similar to adults, but shows somewhat less gen- eralization to unseen types of objects (e.g., other kinds of dogs/animals than those in training).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Word Learning Framework</head><p>Our computational model is based on the cross- situational word learner of <ref type="bibr" target="#b4">Fazly et al. (2010)</ref> (henceforth, FAS), which accounts for a range of observed patterns in child and adult vocabulary ac- quisition. Here we give an overview of the FAS model; the next section explains extensions to han- dle the novel word generalization task.</p><p>A naturalistic language learning scenario con- sists of both linguistic data (what a child a hears) and non-linguistic data (what a child perceives). This input is modeled as a sequence of utterance- scene (U -S) pairs, where an utterance is a group of words and a scene is a set of semantic features representing the meaning of those words:</p><formula xml:id="formula_0">U : { look, a, fep, . . . } S: { PERCEPTION, LOOK, . . . , DALMATIAN, DOG, . . . }</formula><p>Given such input, for each word w, the model of FAS learns a probability distribution over all semantic features, P t (.|w), which represents the word's meaning at time t. Initially, at time t = 0, P 0 (.|w) is a uniform distribution. The word meanings are incrementally learned using an algo- rithm that implements cross-situational learning: for each pair of a word w and a semantic feature f , the model learns P t (f |w) from co-occurrences of w and f across all the utterance-scene pairs seen up to time t, as follows.</p><p>Given an utterance-scene pair U -S at time t, and drawing on its learned knowledge of word meanings up to time t−1, the model of FAS calcu- lates an alignment probability for each w j -f i pair. This probability reflects how strongly the feature f i is associated with w j compared to its associa- tion with other words in U :</p><formula xml:id="formula_1">P t (a ij |U, f i ) = P t−1 (f i |w j ) w ∈U P t−1 (f i |w ) (1)</formula><p>where a ij indicates the mapping between the word w j and the semantic feature f i . These probabilities are incrementally accumu- lated for each w j -f i pair, capturing the overall strength of association of w j and f i at time t:</p><formula xml:id="formula_2">assoc t (f i , w j ) = assoc t−1 (f i , w j ) + P t (a ij |U, f i )<label>(2)</label></formula><p>The (normalized) association scores then serve as the basis for the incremental adjustment of the meaning probabilities of all features f i for each word w j seen in the input at time t:</p><formula xml:id="formula_3">P t (f i |w j ) = assoc t (f i , w j ) + γ fm∈M assoc t (f m , w j ) + k γ (3)</formula><p>Here M is the group of all features that the model has observed, k is the expected number of such features, and γ is a small smoothing parameter, which determines the prior probability of observ- ing a new feature. Smoothing entails that features previously un- seen with a word (all f i such that assoc t (f i , w j ) = 0) have a small but non-zero probability. That is, when f i is unseen with w j , Eqn. (3) reduces to:</p><formula xml:id="formula_4">P u t (f i |w j ) = γ fm∈M assoc t (f m , w j ) + k γ (4)</formula><p>This unseen probability, P u t , reflects the learner's "openness" to the word being associated with new features (Nematzadeh et al., 2011): a higher or lower P u t (f i |w j ) will affect how strongly a pre- viously unseen f i can be associated with w j in the alignment process <ref type="bibr">(Eqn. (1)</ref>). We return to this property of the model below, as it relates to the behaviour of our model in making generalizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Extensions to the Model</head><p>We assume that the representation of meaning can be abstracted to features that correspond to different levels of categorization. For example, a Dalmatian in an input scene is represented as {DALMATIAN, DOG, ANIMAL} and a Bulldog as {BULLDOG, DOG, ANIMAL}, where we use FEA- TURENAME to refer to all the features that are spe- cific to that level of object category. (Note that we could replace each of these features with the ap- propriate "true" set of features, but use the more compact representation for simplicity.) To acquire the meaning of the word Dalmatian, the model must learn a probability distribution in which P (f |Dalmatian) is relatively high for the features DALMATIAN, DOG, and ANIMAL, and low for fea- tures such as BULLDOG, CAT, and VEGETABLE.</p><p>Introducing Feature Groups. In the FAS model, all the features for a word are dependent: increasing the probability of any feature results in decreasing the probability of others. However, this interaction is not always desirable, as many fea- tures regularly co-occur in the world. This is es- pecially an issue for features from a category hi- erarchy, where features of a subordinate category should not compete with features of the parent. That is, while a higher probability of DALMATIAN features (e.g., black spotted coat) may lessen the likelihood of BULLDOG features (e.g., wrinkles), it should not decrease the probability of DOG fea- tures (e.g., having 4 legs).</p><p>To address this, we extend the model by using feature groups that collect together sets of features that sensibly compete. Each feature group is com- prised of all features at the same level of specificity in the category hierarchy, which are therefore mu- tually exclusive, such as DOG, CAT, and BIRD (i.e., different kinds of animals). Instead of learning a single probability distribution over all features as the meaning of a word, the extended model learns a set of probability distributions for a word, one for each feature group (i.e., one per level of the hi- erarchy). Features within a group thereby compete for the probability mass associated with a word, but those from across groups (e.g., DALMATIAN and DOG) can freely co-occur without competing.</p><p>The model does not know a priori all the features in a group, but when presented with a newly observed feature, it can identify the appropriate group for it. In taking this ap- proach, we assume the learner can distinguish the level of specificity of features perceived in the scene.</p><p>For example, in the scene rep- resentations {DALMATIAN, DOG, ANIMAL} and {SIAMESE, CAT, ANIMAL}, the learner can recog- nize that DOG and CAT are at the same level of the hierarchy (kinds of animals) and that DALMA- TIAN and SIAMESE are at the same, more specific level in the hierarchy (finer-grained breeds of an- imals). Our assumption is that children (at this stage in their development) can identify a degree of similarity among concepts that enables them to recognize that Dalmatians and Siamese are distin- guished by similar properties (such as fur color), which differ from more distinguishing properties at higher taxonomic levels (such as number of legs). The model has no other prior knowledge of the category structure. For example, it is not built into the model that DALMATIAN is a type of DOG, only that it is more specific than DOG; any asso- ciation between them would be learned from their pattern of co-occurrence with a word over time.</p><p>Note that, in contrast to the model of X&amp;T, our model does not start with a full taxonomy (it does not know, for example, that Dalmatians and poo- dles are hyponyms of dogs) and it does not have built-in knowledge of similarities among concepts. Still, it encodes some taxonomic knowledge in the feature groups, and an important future direction will be to show that this knowledge is learnable from the input.</p><p>Calculating</p><note type="other">Feature Group Probabilities. To appropriately split the probability mass within a feature group G (but not across feature groups), we use a new formulation of Eqn. (3) to update the meaning probabilities for f i ∈ G as follows:</note><formula xml:id="formula_5">P t (f i |w j ) = assoc t (f i , w j ) + γ G fm∈G assoc t (f m , w j ) + k G γ G<label>(5)</label></formula><p>where k G is the expected number of features in G, and the smoothing factor γ G reflects the prior belief in observing a feature f in G. <ref type="bibr">2</ref> With this new formulation, the probability of a feature f i previously unseen with word w j now re- duces to (cf. Eqn. (4)):</p><formula xml:id="formula_6">P u t (f i |w j ) = γ G fm∈G assoc t (f m , w j ) + k G γ G<label>(6)</label></formula><p>for f i ∈ G. Note that the smoothing factor γ G depends on G, and thus the openness of the word to be associated with new (previously unseen) fea- tures can vary depending on the feature group. This unseen probability is very important to the model's generalization behaviour. Generalization involves the model accepting that a learned word can refer to objects not seen with it before: e.g., in the experiments here, we would expect that the learned meaning for fep after seeing three animals such as a dog, a penguin, and a sheep could also accommodate the meaning of a different animal such as a cat. This ability of the model to asso- ciate new meaning features with a word depends precisely on the unseen probability formulation: the higher the unseen probability for a feature and a word, the more the feature will be acceptable as a generalization of the word.</p><p>Type-Token Effects on Generalization. The unseen probability is sensitive to how many in- stances of features from a group have already been <ref type="bibr">2</ref> Each feature group forms a Categorical distribution with kG categories <ref type="figure" target="#fig_0">(Cat(θ1, ..</ref>., θ k G )), where the θi are drawn from a prior Dirichlet distribution Dir(γ1, ..., γ k G ) at time t = 0, and the θi are updated at time t to be the expected value of the posterior Dirichlet distribution, given in Eqn. <ref type="formula" target="#formula_5">(5)</ref> or Eqn. (6). seen with a word w j : As the model observes more instances (tokens) of features from G with w j , the corresponding assoc t score(s) increase, thereby increasing the denominator in Eqn. (6) and de- creasing P u t . Thus the tendency to generalize w j to more features in G -i.e., to accept additional features as part of the meaning of w j -will de- crease as the model has more evidence of (ob- served) features in that group occurring with w j .</p><p>Generalization of a category to include new kinds of items is typically a function of both token and type frequency (e.g., <ref type="bibr" target="#b1">Bybee, 1985;</ref><ref type="bibr" target="#b3">Croft and Cruse, 2004</ref>): a category with more diverse types is more easily extended to new cases. While the evolving association scores capture the effect of observing more feature tokens, our model as given does not distinguish the number of different types of features seen within a group (e.g., two DOGs vs. one DOG and one CAT).</p><p>We address this issue by having γ G depend on the number of observed types of features in the group:</p><formula xml:id="formula_7">γ t G = γ 0 G × type(G, t) 2<label>(7)</label></formula><p>where type(G, t) is the number of different kinds of features seen in that group (e.g., <ref type="bibr">DOG</ref> and CAT are two different feature types from the same group) up through time t. In this way, the P u t of a feature that occurs in a group with more observed feature types is higher than the P u t in a group with fewer observed types.</p><p>Thus both the type frequency of features in G and their token frequency of co-occurrence with word w j will influence -the first positively and the second negatively -how readily w j can refer to objects with previously unseen features from G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Set-up</head><p>We model X&amp;T's behavioural experiments with our computational word learner as extended above. <ref type="bibr">3</ref> Following X&amp;T, we use a three-tiered cat- egory hierarchy, and the four training conditions and assessment of three types of test matches as described in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Training the model. In each condition, the model processes a sequence of 3 utterance-scene pairs, and updates P t (f i |w j ) after each pair using Eqns. (5) and (6). The utterance-scene pair in each trial consists of the novel word coupled with the scene representation of a training object from the category hierarchy. The object's scene representa- tion is given as a set of four features, each taken from one of four feature groups: one feature cor- responding to each of the subordinate, basic, and superordinate levels of the hierarchy, and a unique "instance" feature, as shown in <ref type="table">Table 1</ref>. (The "in- stance" feature is added to simulate the variations in the different objects of the same subordinate category in the X&amp;T experiments.) <ref type="table">Table 1</ref>: An example of a sequence of utterance-scene pair trials in the 3-super. condition.</p><formula xml:id="formula_8">1 U: { fep } S: { INSTANCE1, DALMATIAN, DOG, ANIMAL } 2 U: { fep } S: { INSTANCE2, TABBY, CAT, ANIMAL } 3 U: { fep } S: { INSTANCE3, POLAR BEAR, BEAR, ANIMAL }</formula><p>Testing the model. After training on a novel word, in order to assess its level of generaliza- tion within the category hierarchy, we compare the model's learned meaning of the word to test objects that constitute various types of matches to the training conditions: i.e., subordinate matches, basic-level matches, and superordinate matches. <ref type="table" target="#tab_2">Table 2</ref> gives an example of each type of match:  To assess whether the model generalizes the learned meaning of a word w to the various types of test matches, we first consider the probability of a test object Y at time t given the learned meaning of w:</p><formula xml:id="formula_9">P t (Y |w) = y i ∈Y P t (y i |w)<label>(8)</label></formula><p>where y i are the features in Y , and P t (y i |w) is calculated using Eqn. (5) for features y i observed with w during training, and using Eqn. (6) for y i not observed with w. (Recall that Eqn. (5) reduces to Eqn. (6) when a feature has not been seen with the word.) From P t (Y |w), we subtract the predic- tive probability of the test object before the model has observed any data, P 0 (Y |w), which gives us its increase in preference attributable to the word learning trials. <ref type="bibr">4</ref> Calculating P t (Y |w) − P 0 (Y |w) is informative about one test object, but we need to measure gen- eralization of the learned word to all the objects of a certain type of match -i.e., subordinate, basic- level, or superordinate. We formulate the proba- bility of generalization to a type of test match as the relative average increase in preference for test items of that type of match, using the Shepard- Luce choice rule <ref type="bibr" target="#b17">(Shepard, 1958;</ref><ref type="bibr" target="#b9">Luce, 1959)</ref>:</p><formula xml:id="formula_10">P gen (m|w) = avg Y ∈m [P t (Y |w)-P 0 (Y |w)] m avg Y ∈m [P t (Y |w)-P 0 (Y |w)]</formula><p>where m is the set of test objects at a certain level of match, and m ranges over subordinate matches, basic-level matches, and superordinate matches. Using P gen (m|w) to communicate our models results has the advantage of using the learned word meanings in a very direct way to assess the pref- erence for the various types of test matches in the X&amp;T experiments. However, the disadvantage is that this measure is not directly comparable to the reported figures from the human data, which are the percentage of test objects selected of a particu- lar type of match. Hence, in presenting our results below, we focus on the general patterns of prefer- ences indicated by the different measures. Parameters. To model children, whom we as- sume to have no bias towards generalization to specific category levels, we equate all parameters k G and γ G across all feature groups, reflecting that all category levels are treated equivalently. Here we use values of k G = 100 and initial values of γ G = 0.5 for all G as the "child" parame- ter settings. <ref type="bibr">5</ref> In contrast, we assume that adults, through word learning experience, have accumu- lated biases that reflect observed differences in feature groups. More specifically, we assume that the probability of observing a new feature for a group G depends on the degree of specificity of that group: That is, over time, it is less likely to ob- serve a completely new kind of animal, e.g., than a new breed of dog. We simulate these biases by us-</p><formula xml:id="formula_11">4 P0(Y |w) = G 1 k G</formula><p>is the prior probability of any object instance, given parameters drawn from the Dirichlet prior, be- cause Eqn. (6) yields the value 1 k G when all assoct scores are 0 -i.e., no features from G have been observed with the word. <ref type="bibr">5</ref> To determine the parameters for the "child" learner, we examined a number of settings with equal parameter values for all the feature groups, and observed similar results in these settings. (We did not perform an exhaustive search over the parameter space.) ing various values for the parameter γ G , which de- termines the prior probability of a word being ob- served with new (previously unseen) features in G (cf. Eqn. <ref type="formula" target="#formula_6">(6)</ref>). We assume that the expected num- ber of features (k G ) is the same across groups. We perform a non-exhaustive search on the parameter space of γ G to select a set of values that yield the patterns of X&amp;T's adult experiments. The "adult" parameter values are given in <ref type="table" target="#tab_3">Table 3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head><p>We present results of the model using both child settings ( <ref type="figure" target="#fig_5">Figure 3b</ref>) and adult settings <ref type="figure" target="#fig_5">(Figure 3a)</ref>. Recall that these values do not correspond to the percentages reported in the human data; to evalu- ate the patterns of generalization, we look at the relative preference for the various types of test match. Note also that since the generalization probabilities sum to 1.0 within each of the 4 train- ing conditions, we can only compare the pattern of generalization across conditions (and not the ac- tual value of the probabilities).</p><p>We discuss each of the child and adult sets of results in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">The Child Learner</head><p>Recall that in the simulations of a child, we use equal values across all feature groups for the k G and initial γ G parameter settings, to reflect that the learner has no bias towards generalization to spe- cific category levels.</p><p>Looking at the results in <ref type="figure" target="#fig_5">Figure 3b</ref>, we can see that the child learner generally replicates the pat- terns of results observed in X&amp;T's experiment on children (cf. <ref type="figure" target="#fig_2">Figure 2b)</ref>. Given multiple training items (the 3-subord., 3-basic, and 3-super. con- ditions), the model, like children, generalizes to the lowest level category in the hierarchy that is consistent with the training items, roughly equally preferring items from that category or lower, with slight preference for the lower categories. In con- trast, after seeing a single training example (the  Each bar is the probability of a type of test match: i.e., sub- ord(inate), basic(-level), or super(ordinate).</p><p>1-ex. condition), the model shows some tendency to generalize to the basic-level, demonstrating a small but notable basic-level bias -e.g., the ten- dency to consider the word as referring to any dogs (but less so to other animals) after seeing just a single example of a particular kind of dog. As in children, the difference in the model between the preference for subordinate vs. basic-level matches is much smaller when trained on 1 instance as op- posed to 3 subordinates. (In <ref type="figure" target="#fig_5">Figure 3b</ref> Interestingly, our child learner exhibits the ob- served basic-level bias in the absence of any dif- ference in the model in how it treats different cat- egory levels. The observed pattern arises from a type/token frequency interaction of the kind often noted to influence generalization of linguistic cate- gories (e.g., <ref type="bibr" target="#b1">Bybee, 1985;</ref><ref type="bibr" target="#b3">Croft and Cruse, 2004</ref>): here, the interaction between the token frequency of word-feature pairs in the input and the type frequency of different features within a group of dependent features. For example, having seen 3 types of animals ("3 super." condition), the model can readily accommodate that fep refers to another kind of animal, in contrast to the "3 basic" con- dition, where it has seen the same number of to- kens but only a single feature type from the feature group at that level (3 dogs). We can also clearly see the inverse impact of token frequencies on gen- eralization: the more examples of a single subor- dinate type are seen, the less the model accepts that fep refers to a different kind of subordinate (the "3-subord." vs. "1-ex." conditions). That is, with only 1 token of DALMATIAN, the model can generalize to other types of dogs more readily than when it has seen 3 tokens of DALMATIAN.</p><p>In general, interactions between the type and to- ken frequencies of the different feature groups in- teract to yield the observed patterns in the model. These results indicate that properties of the input data coupled with the model's handling of feature groups can account for children's word general- ization behaviour, without the need for an explicit basic-level bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">The Adult Learner</head><p>Adult participants in X&amp;T exhibited a stronger tendency than children to generalize to the basic- level category, especially after seeing a single ex- emplar. We explore whether the model can simu- late an adult learner as well. As discussed in Sec- tion 5, by varying the parameters γ G , we can in- corporate biases towards different category levels that we assume an adult has learned. More specif- ically, we set γ G to successively larger values for more specific feature groups G, to ensure succes- sively greater generalization in lower levels of the hierarchy (see <ref type="table" target="#tab_3">Table 3</ref>). As shown in <ref type="figure" target="#fig_5">Figure 3a</ref>, our model (using such settings of the parameters) replicates the patterns of X&amp;T's adult experiments (cf. <ref type="figure" target="#fig_2">Figure 2a)</ref>, including a stronger basic-level bias than that shown by children. That is, in the 1-ex. and 3-subord. conditions, the difference be- tween the 1st bar [subord. match] and 2nd bar [ba- sic match] is smaller for the adult settings of the model <ref type="figure" target="#fig_5">(Figure 3a)</ref> than for the child settings <ref type="figure" target="#fig_5">(Fig- ure 3b)</ref>, mimicking the stronger basic-level bias found in adults.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Variations in Basic-level Generalization</head><p>Research shows that people's degree of basic-level generalization depends on the overall category of the objects. Specifically, <ref type="bibr" target="#b0">Abbott et al. (2012)</ref> per- form the same set of experiments as X&amp;T on adults, exploring three additional superordinate categories (clothing, containers, and seats). Their results are shown in <ref type="figure">Figure 4</ref>; for space reasons, we focus here on the training conditions with 1- example or 3-subordinates, which are the locus of the basic-level effect. The results show that people exhibit no basic-level generalization for containers, moderate generalization for clothing, and strong generalization for seats (compare <ref type="figure">Fig- ures 4a, 4b and 4c)</ref>.</p><p>Interestingly, the computational experiments of <ref type="bibr" target="#b0">Abbott et al. (2012)</ref> also reveal that the Bayesian model of X&amp;T mimics varying levels of basic- level generalization in the 1-example cases, but does not capture the differences that people exhibit across the categories in the 3-subordinate condi- tion (compare "3 subord." in Figures 4 and 5): unlike people, here the X&amp;T model does not ex- hibit basic-level generalization for any of the cate- gories. <ref type="bibr" target="#b0">Abbott et al. (2012)</ref> note that a domain like con- tainers may not follow a "natural taxonomy" in having a clear basic-level category. This sugges- tion is compatible with our view that a basic-level bias arises in response to the particular pattern of co-occurrence of features across the category hierarchy. We looked more closely at the train- ing stimuli of their experiment, and observe that the examples of the category "containers" (with the least basic-level generalization) vary greatly, while those of "clothing" and "seats" are less dif- ferentiated. Examples from "containers" include a cigar box, trash can, and mailbox, whereas "seats" are restricted to different types of chair (such as a dining chair and an armchair; see <ref type="table">Table 1</ref> in <ref type="bibr" target="#b0">Abbott et al. (2012)</ref>).</p><p>Based on this observation, we hypothesize that people generalize less to a basic-level category when their mental representations for that cat- egory's instances have more distinguishing fea- tures. Specifically, we assume that people dif- ferentiate the given instances of the category "containers" more than those for "clothing" and "seats". We model this difference in the granu- larity of representations by varying the number of feature groups used in representing an object. Re- call that in our earlier experiments, each object was represented as a set of features drawn from 4 different feature groups. We take this represen- tation as the least fine-grained representation and use it for the category "seats". We assume that the objects from the categories "clothing" and "con- tainers" (that exhibit less basic-level generaliza- tion) are represented with more feature groups (8 and 12, respectively). on these three categories using the "adult" pa- rameter settings. As expected, the generalization to the basic-level category is high for the least distinguished category "seats", moderate for the category "clothing", and low for the most distin- guished category "containers". <ref type="bibr">7</ref> Our results suggest that the observed varia- tion across categories in basic-level generalization could arise from differences in the granularity of representations of categories. This is particularly interesting since the model of X&amp;T, despite encod- ing an elaborated taxonomy, does not capture the observed behaviour across all training conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>A key challenge faced by children in vocabulary acquisition is learning which of many possible meanings is appropriate for a word, based largely on ambiguous situational evidence. One aspect of this is what we term the "word generalization" problem, which refers to how children associate a word such as dog with a meaning at the appropri- ate category level in a taxonomy of objects, such as Dalmatians, dogs, or animals.</p><p>We present extensions to a cross-situational learner that enable the first computational study of word generalization that is integrated within a word learning model. The model mimics child behavior found by <ref type="bibr" target="#b20">Xu and Tenenbaum (2007)</ref>: it shows a "basic-level" bias -a preference for word meanings that refer to basic-level objects (like dogs), in contrast to higher-level (animals) or lower-level (Dalmatians) categories -and does so <ref type="bibr">7</ref> Similar results obtain using "child" parameter settings, but (as expected) the basic-level generalization is lower.    under parameter settings that treat all levels of cat- egory the same in the model (i.e., with no built-in basic-level bias). Other (unequal) parameter set- tings, which could reflect learned knowledge lead- ing to differential treatment of categories, yield behavior that mimics that of adults, who show a stronger basic-level bias. Moreover, similarly to people <ref type="bibr" target="#b0">(Abbott et al., 2012)</ref>, our model ex- hibits variations in generalization to the basic level for different types of objects, a behavior that the model of <ref type="bibr" target="#b20">Xu and Tenenbaum (2007)</ref> does not fully replicate.</p><p>Overall, the results of our model arise from the interaction of type and token frequencies of fea- tures in the input data, which impact the model's evolving word representations. This mechanism in the model captures the type-token influence of- ten observed to underlie people's generalization of linguistic categories -i.e., their linguistic produc- tivity (e.g., <ref type="bibr" target="#b1">Bybee, 1985;</ref><ref type="bibr" target="#b3">Croft and Cruse, 2004</ref>).</p><p>One shortcoming of the current model is its built-in ability to "detect" in the input that DOG and CAT features are more specific than ANIMAL features. The next step is to consider how the model might learn these relationships from its evolving knowledge of co-occurring features.</p><p>Finally, a similar problem to that of word gen- eralization in humans arises in computational lin- guistics: how to appropriately generalize a set of concepts to an overarching concept that subsumes the set. For example, this problem underlies one way to determine the selectional preferences of a verb: extract the set of nouns that occur as objects of the verb, map them to the concept nodes in a hi- erarchy such as WordNet, and then determine the best overarching WordNet category for capturing the salient properties of the object nouns overall (e.g., <ref type="bibr" target="#b8">Li and Abe, 1998;</ref><ref type="bibr" target="#b2">Clark and Weir, 2001</ref>). An interesting future direction is to explore how an extension of our work can be applied to such problems in computational linguistics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The training and testing conditions of X&amp;T; see text in Section 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: X&amp;T data for (a) adults and (b) children. Each bar is the percentage of chosen test objects of a type of test match: i.e., subord(inate), basic(-level), or super(ordinate).</figDesc><graphic url="image-2.png" coords="3,91.10,551.04,174.60,89.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>: 6 γ</head><label>6</label><figDesc>inst = 1.2 γ subord = 1.0 γ basic = 0.5 γ super = 0.2 k inst = 100 k subord = 100 k basic = 100 k super = 100</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Our model data for (a) adults and (b) children.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>, compare the difference between the 1st bar [subord. match] and 2nd bar [basic match] of the 1-ex. training condition to that of the 3-subord. training condi- tion.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 shows</head><label>6</label><figDesc>Figure 6 shows the results of running our model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Data from our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>An example of each level match from the test ob- jects, given the training condition in Table 1.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : "Adult" parameter settings.</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="3"> Link to our code/data: github.com/eringrant/ word_learning/tree/hypothesis-space.</note>

			<note place="foot" n="6"> For a certain range of such parameter settings-i.e., with gradually decreasing γ G , which determines the prior probability of a word being observed with new (previously unseen) features in G (cf. Eqn. (6)). for feature groups at successively higher levels in the hierarchy-the model produces similar results to the presented adult learner.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Constructing a hypothesis space from the web for large-scale bayesian word learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">T</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">L</forename><surname>Austerweil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual Conference of the Cognitive Science Society</title>
		<meeting>the 34th Annual Conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Morphology: A study of the relation between meaning and form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><forename type="middle">L</forename><surname>Bybee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<pubPlace>Benjamins, Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Class-based probability estimation using a semantic hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies</title>
		<meeting>the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Cognitive linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Cruse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A probabilistic computational model of cross-situational word learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afsaneh</forename><surname>Fazly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afra</forename><surname>Alishahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzanne</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1017" to="1063" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Bayesian framework for cross-situational word-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;07</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Early object labels: The case for a developmental lexical principles framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberta</forename><forename type="middle">M</forename><surname>Golinkoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><forename type="middle">B</forename><surname>Mervis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Hirsh-Pasek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of child language</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="125" to="155" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An associative model of adaptive inference for learning word-referent mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kachergis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">M</forename><surname>Shiffrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Word clustering and disambiguation based on co-occurrence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoki</forename><surname>Abe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Computational Linguistics</title>
		<meeting>the 17th International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="749" to="755" />
		</imprint>
	</monogr>
	<note>COLING &apos;98</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Individual choice behaviour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">D</forename><surname>Luce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1959" />
			<publisher>Wiley</publisher>
			<pubPlace>NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Categorization and naming in children: Problems of induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Markman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>Mit Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Vision: A computational investigation into the human representation and processing of visual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Marr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A computational study of late talking in word-meaning acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aida</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afsaneh</forename><surname>Fazly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzanne</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33th Annual Conference of the Cognitive Science Society</title>
		<meeting>the 33th Annual Conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="705" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learnability and Cognition: The acquisition of Argument Structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Pinker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Word and Object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willard</forename><surname>Van Orman Quine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Basic objects in natural categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleanor</forename><surname>Rosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><forename type="middle">B</forename><surname>Mervis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penny</forename><surname>Boyes-Braem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">On the Internal Structure of Perceptual and Semantic Categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleanor</forename><surname>Rosch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
			<publisher>Academic Press</publisher>
			<biblScope unit="page" from="111" to="144" />
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stimulus and response generalization: Tests of a model relating generalization to distance in psychological space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shepard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">509</biblScope>
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A computational study of cross-situational techniques for learning word-tomeaning mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffery</forename><forename type="middle">M</forename><surname>Siskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="39" to="91" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Infants rapidly learn word-referent mappings via cross-situational statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><forename type="middle">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1558" to="1568" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Word learning as Bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="245" to="272" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rapid word learning under uncertainty via cross-situational statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><forename type="middle">B</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="414" to="420" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The role of partial knowledge in statistical word learning. Psychonomic bulletin &amp; review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yurovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><forename type="middle">C</forename><surname>Fricker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><forename type="middle">B</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
