<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
							<email>w85yang@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Advanced Digital Sciences Center</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
							<email>luwei@sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Advanced Digital Sciences Center</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
							<email>vincent.zheng @adsc.com.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Advanced Digital Sciences Center</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2898" to="2904"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Learning word embeddings has received a significant amount of attention recently. Often, word embeddings are learned in an unsupervised manner from a large collection of text. The genre of the text typically plays an important role in the effectiveness of the resulting embeddings. How to effectively train word embedding models using data from different domains remains a problem that is underexplored. In this paper, we present a simple yet effective method for learning word embeddings based on text from different domains. We demonstrate the effectiveness of our approach through extensive experiments on various downstream NLP tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, the learning of distributed representa- tions for natural language words (or word embed- dings) has received a significant amount of atten- tion ( <ref type="bibr" target="#b19">Mnih and Hinton, 2007;</ref><ref type="bibr" target="#b29">Turian et al., 2010;</ref><ref type="bibr">Mikolov et al., 2013a,b,c;</ref><ref type="bibr" target="#b24">Pennington et al., 2014)</ref>. Such representations were shown to be able to cap- ture syntactic and semantic level information asso- ciated with words ( <ref type="bibr" target="#b15">Mikolov et al., 2013a)</ref>. Word embeddings were shown effective in tasks such as named entity recognition <ref type="bibr" target="#b25">(Sienčnik, 2015)</ref>, sen- timent analysis ( <ref type="bibr" target="#b11">Li and Lu, 2017)</ref> and syntactic parsing <ref type="bibr" target="#b4">(Durrett and Klein, 2015)</ref>. One common assumption made by most of the embedding meth- ods is that, the text corpus is from one single do- main; e.g., articles from bioinformatics. How- ever, in practice, there are often text corpora from multiple domains; e.g., we may have text collec- tions from broadcast news or Web blogs, whose words are not necessarily limited to bioinformat- ics. Can these corpora from different domains help learn better word embeddings, so as to improve the downstream NLP applications in a target domain like bioinformatics? Our answer is yes, because despite the domain differences, these additional domains do introduce more text data converying useful information (i.e., more words, more word co-occurrences), which can be helpful for consoli- dating the word embeddings in the target bioinfor- matics domain.</p><p>In this paper, we propose a simple and easy- to-implement approach for learning cross-domain word embeddings. Our model can be seen as a regularized skip-gram model ( <ref type="bibr">Mikolov et al., 2013a,b)</ref>, where the source domain information is selectively incorporated for learning the target do- main word embeddings in a principled manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Learning a continuous representation for words has been studied for quite a while ( <ref type="bibr" target="#b8">Hinton et al., 1986</ref>). Many earlier word embedding meth- ods employed the computationally expensive neu- ral network architectures <ref type="bibr" target="#b1">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b17">Mikolov et al., 2013c)</ref>. Recently, an ef- ficient method for learning word representations, namely the skip-gram model ( <ref type="bibr" target="#b15">Mikolov et al., 2013a</ref>,b) was proposed and implemented in the widely used word2vec toolkit. It tries to use the current word to predict the surrounding context words, where the prediction is defined over the embeddings of these words. As a result, it learns the word embeddings by maximizing the likeli- hood of predictions.</p><p>Domain adaptation is an important research topic ( <ref type="bibr" target="#b22">Pan et al., 2013)</ref>, and it has been consid- ered in many NLP tasks. For example, domain adaptation is studied for sentiment classification <ref type="bibr" target="#b7">(Glorot et al., 2011</ref>) and parsing <ref type="bibr" target="#b14">(McClosky et al., 2010)</ref>, just to name a few. However, there is very little work on domain adaptation for word embed- ding learning. One major reason preventing peo- ple from using text corpora from different domains for word embedding learning is the lack of guid- ance on which kind of information is worth learn- ing from the source domain(s) for the target do- main. In order to address this problem, some pi- oneering work has looked into this problem. For example, <ref type="bibr" target="#b0">Bollegala et al. (2015)</ref> considered those frequent words in the source domain and the target domain as the "pivots". Then it tried to use the piv- ots to predict the surrounding "non-pivots", mean- while ensuring the pivots to have the same em- bedding across two domains. Embeddings learned from such an approach were shown to be able to improve the performance on a cross-domain sen- timent classification task. However, this model fails to learn embeddings for many words which are neither pivots nor non-pivots, which could be crucial for some downstream tasks such as named entity recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>Let us first state the objective of the skip-gram model ( <ref type="bibr" target="#b15">Mikolov et al., 2013a)</ref> as follows:</p><formula xml:id="formula_0">L D = (w,c)∈D #(w, c) log σ(w · c) + k i=1 E c i ∼P (w) [log σ(−w · c i )]<label>(1)</label></formula><p>where D refers to the complete text corpus from which we learn the word embeddings. The word w is the current word, c is the context word, and #(w, c) is the number of times they co-occur in D. We use w and c to denote the vector represen- tations for w and c, respectively. The function σ(·) is the sigmoid function. The word c i is a "nega- tive sample" sampled from the distribution P (w) - typically chosen as the unigram distribution U (w) raised to the 3/4rd power ( <ref type="bibr" target="#b16">Mikolov et al., 2013b</ref>).</p><p>In our approach, we first learn for each word w an embedding w s from the source domain D s . Next we learn the target domain embeddings as follows:</p><formula xml:id="formula_1">L Dt = L Dt + w∈Dt∩Ds α w · ||w t − w s || 2 (2)</formula><p>where D t refers to the target domain, and w t is the target domain representation for w. Such an regu- larized objective can still be optimized using stan- dard stochastic gradient descent. Note that in the above formula, the regularization term only con- siders words that appear in both source and target domain, ignoring words that only appear in either the source or the target domain only. Our approach is inspired by the recent regularization-based domain adaptation frame- work ( <ref type="bibr" target="#b13">Lu et al., 2016)</ref>. Here, α w measures the amount of transfer across the two domains when learning the representation for word w. If it is large, it means we require the embeddings of word w in the two domains to be similar. We define α w as follows:</p><formula xml:id="formula_2">α w = σ(λ · φ(w)) (3)</formula><p>where λ is a hyper-parameter to decide the scaling factor of the significance function φ(·), which al- lows the user to control the degree of "knowledge transfer" from source domain to target domain. How do we define the significance function φ(w) that controls the amount of transfer for the word w? We first define the frequency of the word w in the dataset D as f D (w), the number of times the word w appears in the domain D. Based on this we can define the normalized frequency for the word w as follows:</p><formula xml:id="formula_3">F D (w) = f D (w) max w ∈D k f D (w )<label>(4)</label></formula><p>where D k ⊂ D consists of all except for the top k most frequent words from D 1 . We define the function φ(·) based on the fol- lowing metric that is motivated by the well-known Sørensen-Dice coefficient <ref type="bibr" target="#b26">(Sørensen, 1948;</ref><ref type="bibr" target="#b3">Dice, 1945)</ref> commonly used for measuring similarities:</p><formula xml:id="formula_4">φ(w) = 2 · F Ds (w) · F Dt (w) F Ds (w) + F Dt (w)<label>(5)</label></formula><p>Why does such a definition make sense? We note that the value of φ(w) would be high only if both both F Ds (w) and F Dt (w) are high -in this case the word w is a frequent word across dif- ferent domains. Intuitively, these are likely those words whose semantics do not change across the two domains, and we should be confident about making their embeddings similar in the two do- mains. On the other hand, domain-specific words  <ref type="table">Table 1</ref>: Statistics for datasets used for embedding learning in all experiments.</p><p>tend to be more frequent in one domain than the other. In this case, the resulting φ(w) will also have a lower score, indicating a smaller amount of transfer across the two domains. While other user- defined significance functions are also possible, in this work we simply adopt such a function based on the above simple observations. We will vali- date our assumptions with experiments in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We present extensive evaluations to assess the ef- fectiveness of our approach. Following recent ad- vice by <ref type="bibr" target="#b20">Nayak et al. (2016)</ref> and <ref type="bibr" target="#b5">Faruqui et al. (2016)</ref>, to assess the quality of the learned word embeddings, we considered employing the learned word embeddings as continuous features in several down-stream NLP tasks, including entity recogni- tion, sentiment classification, and targeted senti- ment analysis.</p><p>We have used various datasets from different domains for learning cross-domain word embed- dings under different tasks. We list the data statis- tics in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baseline Methods</head><p>We consider the following baseline methods when assessing the effectiveness of our approach.</p><p>• DISCRETE: only discrete features (such as bag of words, POS tags, word n-grams and POS tag n-grams, depending on the actual down-stream task) were considered. All fol- lowing systems include both these base fea- tures and the respective additional features.</p><p>• SOURCE: we train word embeddings from the source domain as additional features.</p><p>• TARGET: we train word embeddings from the target domain as additional features.</p><p>• ALL: we combined the data from two do- mains to form a single dataset for learning word embeddings as additional features.</p><p>• CONCAT: we simply concatenate the learned embeddings from both source and target do- mains as additional features.  • DARep: we use the previous approach of <ref type="bibr" target="#b0">Bollegala et al. (2015)</ref> for learning cross- lingual word representations as additional features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Entity Recognition</head><p>Our first experiment was conducted on entity recognition <ref type="bibr" target="#b28">(Tjong Kim Sang and De Meulder, 2003;</ref><ref type="bibr" target="#b6">Florian et al., 2004)</ref>, where the task is to ex- tract semantically meaning entities and their men- tions from the text. For this task, we built a standard entity recogni- tion model using conditional random fields <ref type="bibr" target="#b10">(Lafferty et al., 2001</ref>). We used the standard fea- tures which are commonly used for different meth- ods, including word unigrams and bigrams, bag- of-words features, POS tag window features, POS tag unigrams and bigram features. We conducted two sets of experiments on two different datasets. The first dataset is the GENIA dataset ( <ref type="bibr" target="#b21">Ohta et al., 2002</ref>), a popular dataset used in bioinformatics, and the second is the ACE-2005 dataset ( <ref type="bibr" target="#b30">Walker et al., 2006</ref>), which is a standard dataset used for various information extraction tasks.</p><p>For the GENIA dataset which consists of 10,946 sentences, we used Enwik9 as the source domain and PubMed as the target domain for learning word embeddings. We set the dimension of word representations as 50.</p><p>For the experiments on ACE, we selected the BN subset of ACE2005, which consists of 4,460 CNN headline news and share a similar domain with Gigaword. We used Enwik9 as the source domain and Gigaword as the target domain. We followed a procedure similar to GENIA for exper- iments.</p><p>To tune our hyperparameter λ, we first split the last 10% of the training set as the development  portion. We then trained a model using the re- maining 90% as the training portion and used the development portion for development of the hy- perparameter λ. After development, we re-trained the models using the original training set 2 .</p><p>We report the results in <ref type="table" target="#tab_2">Table 2</ref>. From the re- sults we can observe that the embeddings learned using our algorithm can lead to improved perfor- mance when used in this particular down-stream NLP task. We note that in such a task, many en- tities consist of domain-specific terms, therefore learning good representations for such words can be crucial. As we have discussed earlier, our reg- ularization method enables our model to differen- tiate domain-specific words from words which are more general in the learning process. We believe this mechanism can lead to improved learning of representations for both types of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sentiment Classification</head><p>The second task we consider is sentiment classi- fication, which is essentially a text classification task, where the goal is to assign each text docu- ment a class label indicating its sentiment polarity ( <ref type="bibr" target="#b23">Pang et al., 2002;</ref><ref type="bibr" target="#b12">Liu, 2012)</ref>. This is also the only task presented in the pre- vious DARep work by <ref type="bibr" target="#b0">Bollegala et al. (2015)</ref>. As such, we largely followed <ref type="bibr" target="#b0">Bollegala et al. (2015)</ref> for experiments. Instead of using the dataset they used which only consists of 2,000 reviews, we considered two much larger datasets -IMDB and Yelp 2014 -for such a task, which was previously used in a sentiment classification task ( <ref type="bibr" target="#b27">Tang et al., 2015)</ref>. IMDB dataset ( <ref type="bibr" target="#b2">Diao et al., 2014</ref>) is crawled from the movie review site IMDB 3 which con- sists of 84,919 reviews. Yelp 2014 dataset consists of 231,163 online reviews provided by the Yelp Dataset Challenge <ref type="bibr">4</ref> .</p><p>Following <ref type="bibr" target="#b0">Bollegala et al. (2015)</ref>, for this task we simply learned the word embeddings from the training portion of the review datasets themselves only. No external data was used for learning word embeddings. As <ref type="bibr" target="#b0">Bollegala et al. (2015)</ref> only eval- uated on a small dataset in their paper for such a task, to understand the effect of varying the amount of training data, we also tried to train our model on datasets with different sizes. We con- ducted two sets of experiments: we first used the Yelp dataset as the source domain and IMDB as the target domain, and then we switched these two datasets in our second set of experiments. <ref type="figure" target="#fig_0">Fig- ure 1</ref> shows the F 1 measures for different word embeddings when different amounts of training data were used. We also compared with the pre- vious approach for domain adaptation ( <ref type="bibr" target="#b13">Lu et al., 2016</ref>) which only employs discrete features. We can observe that when the dataset becomes large, our learned word embeddings are shown to be more effective than all other approaches. When the complete training set is used, our model sig- nificantly outperforms DARep (p &lt; 0.05 for both directions with bootstrap resampling test <ref type="bibr" target="#b9">(Koehn, 2004)</ref>). DARep appears to be effective when the training dataset is small. However, as the train- ing set size increases, there is no significant im- provement for such an approach. As we can also observe from the figure, our approach consistently gives better results than baseline approaches (ex- cept for the second experiment when 20% of the data was used). Furthermore, when the amount of training data increases, the differences between our approach and other approaches generally be- come larger.</p><p>Such experiments show that our model works  well when different amounts of data are available, and our approach appears to be more competitive when a large amount of data is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Targeted Sentiment Analysis</head><p>We also conducted experiments on targeted senti- ment analysis ( <ref type="bibr" target="#b18">Mitchell et al., 2013</ref>) -the task of jointly recognizing entities and their sentiment in- formation. We used the state-of-the-art system for targeted sentiment analysis by <ref type="bibr" target="#b11">Li and Lu (2017)</ref> whose code is publicly available 5 , and used the data from <ref type="bibr" target="#b18">(Mitchell et al., 2013</ref>) which consists of 7,105 Spanish tweets and 2,350 English tweets, with named entities and their sentiment informa- tion annotated. Note that the model of <ref type="bibr" target="#b11">Li and Lu (2017)</ref> is a structured prediction model that involves latent variables. The experiments here therefore allow us to assess the effectiveness of our approach on such a setup involving latent vari- ables. We follow <ref type="bibr" target="#b11">Li and Lu (2017)</ref> and report precision (P.), recall (R.) and F1-measure (F 1 ) for such a targeted sentiment analysis task, where the prediction is regarded as correct if and only if both the entity's boundary and its sentiment infor- mation are correct. Also, unlike previous experi- ments, which are conducted on English only, these experiments additionally allow us to assess our ap- proach's effectiveness when a different language other than English is considered. For the English task, we used Enwik9 as the source domain for learning word embeddings, and our crawled English tweets as the target domain. For the Spanish task, we used Eswiki as the source domain, and we also crawled Spanish tweets as the target domain. See <ref type="table">Table 1</ref> for the statistics. Sim- ilar to the experiments conducted for entity recog- nition, we split the first 80% of the data for train- ing, the next 10% for development and the last 10% for evaluation. We tuned the hyper-parameter λ using the development set and re-trained the em- beddings on the dataset combining the training and the development set, which are then used in fi- nal evaluations. Results are reported in <ref type="table" target="#tab_5">Table 3</ref>, which show our approach is able to achieve the best results across two datasets in such a task, and outperforms DARep (p &lt; 0.05). Interestingly, the concatenation approach appears to be competi- tive in this task, especially for the Spanish dataset, which appears to be better than the DARep ap- proach. However, we note such an approach does not capture any information transfer across differ- ent domains in the learning process. In contrast, our approach learns embeddings for the target do- main by capturing useful cross-domain informa- tion and therefore can lead to improved modeling of embeddings that are shown more helpful for this specific down-stream task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we presented a simple yet effec- tive algorithm for learning cross-domain word em- beddings. Motivated by the recent regularization- based domain adaptation framework ( <ref type="bibr" target="#b13">Lu et al., 2016)</ref>, the algorithm performs learning by aug- menting the skip-gram objective with a simple reg- ularization term. Our work can be easily extended to multi-domain scenarios. The method is also flexible, allowing different user-defined metrics to be incorporated for defining the function control- ling the amount of domain transfer.</p><p>Future work includes performing further in- vestigations to better understand and to visual- ize what types of information has been trans- ferred across domains and how such informa- tion influence different types of down-stream NLP tasks.</p><p>It is also important to under- stand how such an approach will work on other types of models such as neural networks based NLP models. Our code is available at http://statnlp.org/research/lr/.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Results on sentiment classification. Left: Yelp (source) to IMDB (target). Right: IMDB (source) to Yelp (target).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Results on entity recognition.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results on targeted sentiment analysis. 

</table></figure>

			<note place="foot" n="1"> In all our experiments, we empirically set k to 20.</note>

			<note place="foot" n="2"> We selected the optimal value for the hyper-parameter λ from the set λ ∈ {0.1, 1, 5, 10, 20, 30, 50} for all experiments in this paper. 3 http://www.imdb.com</note>

			<note place="foot" n="4"> https://www.yelp.com/dataset challenge</note>

			<note place="foot" n="5"> Available at http://statnlp.org/research/st/.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank all the reviewers for their useful feed-back to the earlier draft of this paper. This work was done when the first author was visit-ing Singapore University of Technology and De-sign. We thank the support of Human-centered Cyber-physical Systems Programme at Advanced Digital Sciences Center from Singapores Agency for Science, Technology and Research (A*STAR). This work is supported by MOE Tier 1 grant SUTDT12015008.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised cross-domain word representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Kawarabayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACLIJCNLP</title>
		<meeting>of ACLIJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="730" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Jointly modeling aspects, ratings and sentiments for movie recommendation (jmars)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of KDD</title>
		<meeting>of KDD</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Measures of the amount of ecologic association between species</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee R Dice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="302" />
			<date type="published" when="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.03641</idno>
		<title level="m">Neural crf parsing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02276</idno>
		<title level="m">Problems with evaluation of word embeddings using word similarity tasks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A statistical model for multilingual entity detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nicolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL/HLT</title>
		<meeting>of NAACL/HLT</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Parallel distributed processing: Explorations in the microstructure of cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">chapter Distributed Representations</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="77" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning latent sentiment scopes for entity-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3482" to="3489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis lectures on human language technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A general regularization framework for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><forename type="middle">Leong</forename><surname>Chieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Löfgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="950" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic domain adaptation for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Open domain targeted sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacqueline</forename><surname>Aguilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1643" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Evaluating word embeddings using a representative suite of practical tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neha</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The genia corpus: An annotated research abstract corpus in molecular biology domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuka</forename><surname>Tateisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the second international conference on Human Language Technology Research</title>
		<meeting>of the second international conference on Human Language Technology Research</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="82" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transfer joint embedding for cross-domain named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Toh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Thumbs up?: sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adapting word2vec to named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Scharolta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sienčnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NODALIDA</title>
		<meeting>of NODALIDA</meeting>
		<imprint>
			<publisher>Linköping University Electronic Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="239" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A method of establishing groups of equal amplitude in plant sociology based on similarity of species and its application to analyses of the vegetation on danish commons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorvald</forename><surname>Sørensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Skr</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning semantic representations of users and products for document level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-IJCNLP</title>
		<meeting>of ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1014" to="1023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik F Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT-NAACL</title>
		<meeting>of HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Medero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<title level="m">Ace 2005 multilingual training corpus. Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">57</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
