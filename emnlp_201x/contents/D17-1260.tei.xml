<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Agent-Aware Dropout DQN for Safe and Efficient On-line Dialogue Policy Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Brain Science and Technology Research Center Shanghai</orgName>
								<orgName type="laboratory">Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Eng. SpeechLab</orgName>
								<orgName type="institution">Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Brain Science and Technology Research Center Shanghai</orgName>
								<orgName type="laboratory">Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Eng. SpeechLab</orgName>
								<orgName type="institution">Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Brain Science and Technology Research Center Shanghai</orgName>
								<orgName type="laboratory">Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Eng. SpeechLab</orgName>
								<orgName type="institution">Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhe</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Brain Science and Technology Research Center Shanghai</orgName>
								<orgName type="laboratory">Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Eng. SpeechLab</orgName>
								<orgName type="institution">Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Brain Science and Technology Research Center Shanghai</orgName>
								<orgName type="laboratory">Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Eng. SpeechLab</orgName>
								<orgName type="institution">Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Agent-Aware Dropout DQN for Safe and Efficient On-line Dialogue Policy Learning</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2454" to="2464"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Hand-crafted rules and reinforcement learning (RL) are two popular choices to obtain dialogue policy. The rule-based policy is often reliable within predefined scope but not self-adaptable, whereas RL is evolvable with data but often suffers from a bad initial performance. We employ a companion learning framework to integrate the two approaches for on-line dialogue policy learning, in which a pre-defined rule-based policy acts as a teacher and guides a data-driven RL system by giving example actions as well as additional rewards. A novel agent-aware dropout Deep Q-Network (AAD-DQN) is proposed to address the problem of when to consult the teacher and how to learn from the teacher&apos;s experiences. AAD-DQN, as a data-driven student policy, provides (1) two separate experience memories for student and teacher, (2) an uncertainty estimated by dropout to control the timing of consultation and learning. Simulation experiments showed that the proposed approach can significantly improve both safety and efficiency of on-line policy optimization compared to other companion learning approaches as well as supervised pre-training using static dialogue corpus.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A task-oriented spoken dialogue system (SDS) is a system that can continuously interact with a human to accomplish a predefined task through speech. Dialogue manager, which maintains the dialogue state and decides how to respond, is the core of an SDS. In this paper, we focus on the dia- logue policy.</p><p>At the early research, the spoken dialogue sys- tems assume observable dialogue states. Dialogue policy is simply a set of hand-crafted mapping rules from state to machine action. This is referred to as rule-based policy, which often has acceptable performance but has no ability of self-adaption. Nowadays rule-based policy is popular in com- mercial dialogue systems.</p><p>However, in real world scenarios, unpredictable user behavior, inevitable automatic speech recog- nition, and spoken language understanding errors make it difficult to maintain the true dialogue state and make the decision. Hence, in recent years, there is a research trend towards statistical dia- logue management. A well-founded theory for this is the partially observable Markov decision process (POMDP) ( <ref type="bibr" target="#b13">Kaelbling et al., 1998</ref>), which can provide robustness to errors from the input module and automatic policy optimization by re- inforcement learning. Most POMDP based policy learning research is usually carried out using ei- ther user simulator or employed users <ref type="bibr" target="#b29">(Williams and Young, 2007;</ref>. The trained policy is not guaranteed to work well in real world scenarios. Therefore, on-line policy training has been of great interest <ref type="bibr" target="#b7">(Gaši´Gaši´c et al., 2011</ref>). Re- cently, <ref type="bibr" target="#b1">Chen et al. (2017)</ref> proposed two qualita- tive metrics <ref type="bibr">1</ref> to measure on-line policy learning: safety and efficiency. Safety reflects whether the initial policy can satisfy the quality-of-service re- quirement in real-world scenarios during the on- line policy learning period. Efficiency reflects how long it takes for the on-line policy training algo- rithm to reach a satisfactory performance level.</p><p>Most traditional RL-based policy training suf-fers poor initial performance, i.e. causes the safety problem. In light of above, <ref type="bibr" target="#b1">Chen et al. (2017)</ref> pro- posed a safe and efficient on-line policy optimiza- tion framework, i.e. companion teaching (CT), in which a human teacher is added in the classic POMDP. The teacher has two missions: one is to show example actions, another is to act as a critic to give the student extra reward which can make the learning of policy more efficient. The example actions not only make the learning safer but also can be directly used by the training of the student policy. However, there are costs to the teaching of a human teacher. Based on CT, companion learning (CL) frame- work is proposed to integrate rule-based policy and RL-based policy, resulting in safe and efficient on-line policy learning. Here, the rule-based pol- icy acts as a virtual teacher which replaces the hu- man teacher in CT. There are a few differences be- tween these two kinds of teachers. First, because it has no marginal cost when it's deployed, the rule teacher can be consulted at any time if needed. On the other hand, the rule policy is not as good as the human teacher, therefore it's important to determine when and how much the student pol- icy depends on the rule teacher. Here, we propose an agent-aware dropout Deep Q-Network (AAD- DQN) as the student statistical policy, which pro- vides (1) two separate experience replay pools for student and teacher, (2) an uncertainty estimated by dropout which can be used to control the tim- ing of consultation and learning.</p><p>In summary, our main contributions are three- folds: (1) Companion learning (CL) framework was proposed to integrate rule-based policy and RL-based policy. (2) An agent-aware dropout Deep Q-Network (AAD-DQN) was proposed as the statistical student policy. (3) Compared with other companion teaching approaches <ref type="bibr" target="#b1">(Chen et al., 2017)</ref> as well as supervised pre-training using static dialogue corpus <ref type="bibr" target="#b3">(Fatemi et al., 2016)</ref>, CL with AAD-DQN can achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Most previous studies of on-line policy learn- ing have been focused on the efficiency issue, such as Gaussian Process Reinforcement Learning (GPRL) . In GPRL, the kernel function defines prior correlations of the objective function given different belief states, which can significantly speed up the policy learning <ref type="bibr" target="#b8">(Gaši´Gaši´c and Young, 2014)</ref>. Alternative methods include Kalman temporal difference reinforcement learn- ing <ref type="bibr" target="#b19">(Pietquin et al., 2011</ref>).</p><p>More recently, deep reinforcement learning (DRL) ( <ref type="bibr" target="#b17">Mnih et al., 2015</ref>) is applied in dialogue policy optimization, including deep Q-Network (DQN) <ref type="bibr" target="#b2">(Cuayáhuitl et al., 2015;</ref><ref type="bibr" target="#b3">Fatemi et al., 2016;</ref><ref type="bibr" target="#b33">Zhao and Eskenazi, 2016;</ref><ref type="bibr" target="#b15">Lipton et al., 2016)</ref> and policy gradient (PG) methods, e.g. RE- INFORCE ( <ref type="bibr" target="#b30">Williams and Zweig, 2016;</ref><ref type="bibr" target="#b26">Su et al., 2016;</ref><ref type="bibr" target="#b28">Williams et al., 2017)</ref>, Advantage Actor- Critic (A2C) <ref type="bibr" target="#b3">(Fatemi et al., 2016)</ref>. In order to speed up the learning of DQN, <ref type="bibr" target="#b15">Lipton et al. (2016)</ref> proposed an efficient exploration technique based on Thompson sample from a Bayesian neu- ral network. Furthermore, they showed that using a few successful dialogues generated by a rule- based policy to pre-fill the replay buffer can ben- efit the learning at the beginning. To improve the efficiency of PG methods, policy network is initialized with supervised learning (SL) before RL training <ref type="bibr" target="#b30">(Williams and Zweig, 2016;</ref><ref type="bibr" target="#b28">Williams et al., 2017;</ref><ref type="bibr" target="#b26">Su et al., 2016</ref><ref type="bibr" target="#b25">Su et al., , 2017</ref><ref type="bibr" target="#b3">Fatemi et al., 2016)</ref>, which is similar to the idea in ( <ref type="bibr" target="#b23">Silver et al., 2016)</ref>. However, combining RL with SL for dia- logue policy optimization is not new. <ref type="bibr" target="#b9">Henderson et al. (2008)</ref> were among the first to prove the ben- efits of combining supervised and reinforcement learning. In the experiments, we will compare CL with these pre-training methods.</p><p>Although the improvement of efficiency can benefit the safety of learning process, no matter how efficient the algorithm is, an unsafe on-line learned policy can lead to bad user experience at the beginning of learning period and consequently fail to attract sufficient real users to continuously improve the policy. Therefore, it is important to address the safety issue. There are few works about the safety issue of on-line dialogue policy optimization. <ref type="bibr" target="#b27">Williams (2008)</ref> proposed a method for integrating business rules and POMDPs. The rules act as the action mask, i.e. the rules nomi- nate a set of one or more actions, and the POMDP chooses the optimal action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Companion Learning for On-line Policy Optimization</head><p>In the CL framework, there are two agents: one is the student policy, another is the teacher pol- icy. Here, teacher policy is the extra part com- pared with the classic statistical dialogue manager architecture ( <ref type="bibr" target="#b32">Young et al., 2013)</ref>. The goal of on- line policy training is to optimize the student pol- icy from data via interaction with users in real sce- narios. The teacher guides the policy learning at each turn as a companion of the dialogue policy, hence, referred to as companion learning 2 . The CL framework is described in <ref type="figure" target="#fig_0">Figure 1</ref>(a). At each turn, the input module (ASR and SLU) receives an acoustic input signal from the human user and the dialogue state tracker keeps the di- alogue state up-to-date. The dialogue state is then transmitted to both the student policy and the teacher policy. The student policy first generates a candidate action a stu t and when it needs help from the teacher policy, it sends a stu t with some auxil- iary information which will be transmitted to the teacher. The teacher policy can then help the stu- dent policy with one of the following ways or both:</p><p>• Example Action (EA): The teacher gener- ates an action a tea t instead of a stu t according to its policy. It corresponds to the left switch in <ref type="figure" target="#fig_0">Figure 1</ref>(a).</p><p>• Critic Advice (CA): The teacher will not ex- plicitly show an action. Instead, it gives an extra reward r int t to the student policy. It cor- responds to the right switch in <ref type="figure" target="#fig_0">Figure 1</ref>(a).</p><p>The action from control module is then transmitted to the output module, which generates the nature text and audio. At each turn, an extrinsic reward signal r ext t will be given to the student policy by <ref type="bibr">2</ref> The name companion learning has another potential meaning that the agents can learn from each other, i.e. the rules guide the RL training, and the optimised RL policy can provide some intuition for the revision of rules. We will give some preliminary discussions about this point in section 5.3. the environment, i.e. the user. The extrinsic re- ward r ext t with the extra intrinsic reward r int t will be used to update the policy parameters θ using reinforcement learning algorithms.</p><p>In the CL framework, there are two things that matter: one is when to consult the teacher, another is how to use the teacher's experiences. In this pa- per, an agent-aware dropout DQN (AAD-DQN) is proposed. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b), the certainty information during the interaction is used to define a companion function, which controls how often to sample the teacher's experiences for updating pa- rameters during the training phase (left), and when to use EA or CA teaching method during decision phase (right).</p><p>The rest of this section is organized as fol- lows. The next subsection introduces the agent- aware experience replay in DQN. The definition of certainty in DQN and the companion function are presented in subsection 3.3. The rule-based teacher policy is described in subsection 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Agent-Aware Experience Replay in DQN</head><p>A Deep Q-Network (DQN) is a multi-layer neu- ral network which maps a belief state b t to the Q values of the possible actions a t at that state, Q(b t , a t ; θ), where θ is the weight vector of the neural network. Neural networks for the approxi- mation of value functions have long been investi- gated <ref type="bibr" target="#b14">(Lin, 1993)</ref>. However, these methods were previously quite unstable ( <ref type="bibr">Mnih et al., 2013</ref>). In DQN, <ref type="bibr">Mnih et al. (2013</ref><ref type="bibr" target="#b17">Mnih et al. ( , 2015</ref> proposed two tech- niques to overcome this instability, namely expe- rience replay and the use of a target network.</p><p>At every turn, the transition including the pre- vious belief state b t , previous action a t , corre- sponding reward r t and current belief state b t+1 is put in a finite pool <ref type="bibr" target="#b14">(Lin, 1993)</ref>. In this pa-per, two pools D stu and D tea are used to store the student's experiences and the teacher's experi- ences respectively as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b). When the teaching method EA is used in the t-th turn, a t = a tea t and the transition is put in D tea , other- wise a t = a stu t and the transition is put in D stu . When CA is used, r t = r ext t + r int t , otherwise r t = r ext t . Once any of the pool has reached its predefined maximum size, adding a new transition results in deleting the oldest transition in the pool. During training, a pool is first selected from D tea and</p><formula xml:id="formula_0">D stu . The probability of selecting D tea is p tea , i.e. D ∼ Ber(D tea , D stu ; p tea ) 3 .Then a mini- batch of transitions is uniformly sampled from the selected pool, i.e. (b t , a t , r t , b t+1 ) ∼ U (D).</formula><p>We call this agent-aware experience replay.</p><p>Except for the experience replay, a target net- work with weight vector θ − is used. This target network is similar to the Q-network except that its weights are only copied every K steps from the Q-network, and remain fixed during all the other steps. The loss function for the Q-network at each iteration takes the following form:</p><formula xml:id="formula_1">L(θ) = E D∼Ber(Dtea,Dstu;ptea), (bt,at,rt,b t+1 )∼U (D) r t + γ max a t+1 Q(b t+1 , a t+1 ; θ − ) − Q(b t , a t ; θ) 2</formula><p>(1) where γ ∈ [0, 1] is the discount factor.</p><p>The probability p tea controls how often the stu- dent learns from the teacher's experiences. As the learning goes on, the probability will decrease. More details will be described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Companion Strategy</head><p>It's important for the student to estimate an appro- priate point to end the reliance on the teacher. If the reliance is ended too early, the student itself may not reach an acceptable performance, result- ing in the sharp drop of performance, which is the safety problem. However, if the student always re- lies on the teacher, it's hard to improve its perfor- mance to surpass the teacher's performance, which is the efficiency problem.</p><p>We get some inspirations from the studying pro- cess of a call center service agent. Consider how a new call center service agent gets started. At first, an experienced call center agent tells him some basic rules and the new agent works by of- ten consulting these rules. His confidence about <ref type="bibr">3</ref> Ber is short for Bernoulli. how to make decisions gradually increases during the continuous practice. Eventually, he is so con- fident about his own decisions that he no longer needs any consultation to these rules and even ex- plores some better response ways through inter- action with users which are not initially included in the rules. Similarly, we can use the uncer- tainty/certainty of the Q-network to determine the teaching time.</p><p>There are several methods to estimate the un- certainty/certainty in deep neural networks, e.g. Bayesian neural networks ( <ref type="bibr" target="#b0">Blundell et al., 2015)</ref>, dropout ( <ref type="bibr" target="#b5">Gal and Ghahramani, 2016</ref>), bootstrap <ref type="bibr" target="#b18">(Osband et al., 2016</ref>) . Here we use the dropout to estimate the certainty of Q-Network. We call this Q-network DropoutQNetwork. Dropout is a technique used to avoid over-fitting in neural net- works. It was introduced several years ago by <ref type="bibr" target="#b11">(Hinton et al., 2012)</ref> and studied more extensively in ( <ref type="bibr" target="#b24">Srivastava et al., 2014</ref>). When dropout is used in training, the elements of the output of each hid- den layer h is randomly set to zero with probabil- ity p, i.e. h = h z 4 where z is binary vector and each element z i ∼ Ber(1 − p). h is scaled by 1 1−p and then fed to the next layer. At test time the dropout is disabled, i.e. the output of each hidden layer h is directly fed to the next layer. Although dropout was suggested as an ad-hoc technique, re- cently it was theoretically proven that the dropout training in deep neural networks is an approximate Bayesian inference in deep Gaussian processes ( <ref type="bibr" target="#b5">Gal and Ghahramani, 2016)</ref>. Therefore, a direct result of this theory gives us tools to model un- certainty with dropout neural networks. To obtain the uncertainty, similar with that at train phrase the dropout is enabled at test phrase. For each input instance (i.e. dialogue belief state) b t , performing N stochastic forward passes through the network and averaging the output q i [q i1 , · · · , q iM ] to get the mean and the variance. Generally, the vari- ance can be utilized to measure the uncertainty of output. However, it's not a normalized criteria, and it's hard to set a threshold below which we should be confident with the output.</p><p>Instead, we proposed a novel method to mea- sure the certainty of the decision of student policy at t-th turn. For each stochastic forward passes, the action a ti = arg max j q ij is regarded as a vote. After N passes 5 , there is a committee {a t1 , · · · , a tN } consisting of N votes. The ac- tion a stu t that should be taken in the belief state b t is the one with the largest percentage of the votes, and the corresponding percentage is defined as certainty c t . The process is described in Algo- rithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 The Decision Procedure of Student</head><note type="other">Policy π stu (b t , N ) Require: The repeat times N and the belief state b t 1: Initial the probability vector p = [p 1 , · · · , p M ] with zero vector, where M is the number of actions. 2: for i = 1, N do 3: q i ← DropoutQNetwork(b t ) 4: a ti ← arg max j q ij 5: p[a ti ] ← p[a ti ] + 1/N 6: end for 7: c t ← max j p j 8: a stu t ← arg max j p j 9: return a stu t , c t</note><p>At the end of e-th dialogue, the average cer- tainty of all turns is computed, i.e. C e = 1 Te Te t=0 c t , where T e is the number of turns in e-th dialogue. Generally, the variance of C e be- tween successive dialogues is high. In order to the smooth the estimation, here we use the moving av- erage of C e in previous W dialogues to represent the certainty of student at current dialogue, i.e.</p><formula xml:id="formula_2">C e = 1 W e−1 i=e−W C i .<label>(2)</label></formula><p>As the training goes on, C e grows until it con- verges. If C e in all successive W dialogues are greater then a threshold C th as shown in <ref type="figure" target="#fig_1">Figure  2</ref>, it's assumed that the student reaches a point where it is confident enough with its own decision steadily. Therefore, the teaching, both EA and CA, should be ended from now on. Before the end of the teaching, CA is done in all turns. However, if EA is always done, the dis- appearance of the teacher may cause a dramatic change in the hybrid decision policy, which re- sults in a sharp drop of performance. To deal with this issue, a monotonically increasing func- tion of the relative certainty P tea (∆C e ) is pro- posed to control the frequency of EA teaching. dialogue state can be repeated N times to form a mini-batch, then one forward is executed to get N outputs simultaneously. ∆C e represents the distance between C e and C th , i.e. ∆C e = max(0, C th − C e ). The effect of P tea (∆C e ) is that the closer C e is to C th , the more unlikely EA teaching is executed. Besides controlling how often the student directly consult the teacher, another mission of P tea (∆C e ) is to control how often the teacher's experiences are re- played, i.e. the probability p tea described in sec- tion 3.2. Implementation details of P tea (∆C e ) are described in Appendix C. The full procedure of companion learning with logic rules is described in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Teacher Policy: Logic Rules</head><p>Rule-based policy is popular in commercial dia- logue systems <ref type="bibr" target="#b27">(Williams, 2008)</ref>. The policy, i.e. the dialogue plan/flow, is designed by a domain expert. His knowledge of task domain and busi- ness rules is encoded in the rules. There are many methods to represent the decision rules, e.g. propositional logic, first-order logic, decision tree. Here, we use the ordered propositional logic rules, which can be easily translated into IF-THEN rules. When making the decision, these rules are exe- cuted in pre-defined order. If the conditions of any rule are satisfied, the decision process will be ter- minated and the output is the corresponding ac- tion. In this paper, three hand-crafted logic rules, R1, R2, and R3 , were used as the teacher:</p><p>• R1: confirm the most likely value in slots where the most likely value has probability between 0.1 and 0.6 6 ;</p><p>• R2: offer a restaurant if there is at least one slot in which the belief of most likely value is more than the belief of special value "none";</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Companion Learning with Logic Rules</head><p>Require:</p><p>The number of stochastic forward pass N , the maximal extra reward δ &gt; 0. 1: Initialize the parameters θ of student policy 2: Initialize replay pools D tea and D stu with {}, certainty memory C with {}, teaching with T rue. Set intrinsic reward r int t ← 0 11:</p><p>Get system action and the corresponding certainty, i.e. a stu t , c t ← π stu (b t , N )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>C e ← C e + c t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>Get action from the rule-based policy, i.e.</p><formula xml:id="formula_3">a tea t ← π tea (b t ) 14:</formula><p>EA ∼ Ber(p tea )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15:</head><p>if teaching is T rue and EA is T rue then </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Companion Function Companion(C) Require:</head><p>The average certainty memory C at e-th dia- logue and the moving window size W . 1: Initialize teaching with F alse, p tea with 0 2: for i = 0, W do 3:</p><p>Compute the moving average certainty C e−i in (e-i)-th dialogue with equation (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>if C e−i &lt; C th then </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Metrics of On-line Policy Optimization</head><p>Most previous work on the evaluation of RL-based dialogue policy optimization focuses on the final performance (FP) when the system converges to a steady level. However, for on-line policy op- timization, it's important to measure the learning process. Except for FP, we proposed two quantita- tive metrics: safety loss and efficiency loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Safety Loss</head><p>In the on-line training process, unless the perfor- mance of the system reaches the acceptable perfor- mance S a , the interaction between users and the system will be unsafe and causes trouble to con- tinuing training. So the safety of the system is de- fined to be the system's ability to maintain perfor- mance above the acceptable performance S a . We quantify the safety loss of the system by summing up the performance gap between the acceptable performance and the system perfor- mance S e in every episode during the on-line learning. Suppose there are E dialogues, then</p><formula xml:id="formula_4">L 1 = E e=1 max(0, S a − S e ).</formula><p>The safety loss has an intuitive interpretation as the area of the region be- low the threshold and above training curve. This metric is similar to the integral of absolute error (IAE) <ref type="bibr" target="#b22">(Shinners, 1998</ref>) metric commonly adopted in the evaluation of control systems <ref type="bibr" target="#b4">(Gaing, 2004;</ref><ref type="bibr" target="#b12">Jesus and Tenreiro MacHado, 2008</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Efficiency Loss</head><p>Another important issue of on-line learning is effi- ciency. The efficiency indicates the speed at which the system reaches a specific performance level. In reality, we can tolerate a system to make mistakes at the beginning but it should improve at a signif- icant speed until reaching the ideal performance S i . Therefore, later failures should weight more than early failures to evaluate efficiency. Sim- ilar to the integral of time multiplied by abso- lute error (ITAE) <ref type="bibr" target="#b22">(Shinners, 1998</ref>) metric, we pro- pose a metric efficiency loss. We multiply the performance gap between ideal performance and current performance with the episode index, thus giving later failure greater penalty. Specifically,</p><formula xml:id="formula_5">L 2 = E e=1 max(0, S i − S e )e.</formula><p>More illustrations about safety loss and effi- ciency loss are given in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Our experiments have three objectives: (1) Com- paring our proposed dropout DQN in Algorithm 1 with some baselines when there is no teacher. <ref type="formula" target="#formula_2">(2)</ref> Comparing CL with other two baselines when the teacher gets involved, and investigating the ben- efits of our proposed agent-aware experience re- play. (3) Visually analyzing the differences in be- haviors between the rule-based teacher policy and the optimized student policy.</p><p>An agenda-based user simulator ( <ref type="bibr" target="#b20">Schatzmann et al., 2007a</ref>) with error model ( <ref type="bibr" target="#b21">Schatzmann et al., 2007b</ref>) was implemented to emulate the behav- ior of the human user, and a rule-based policy with 0.695 success rate described in section 3.2 was used as the teacher in our experiments. The purpose of the user's interacting with SDS is to find restaurant information in the Cambridge (UK) area <ref type="bibr" target="#b10">(Henderson and Thomson, 2014)</ref>. This do- main has 7 slots of which 4 can be used by the system to constrain the database search. The sum- mary action space consists of 16 summary actions. More details are described in Appendix A.</p><p>For reward, at each turn, an extrinsic reward of −0.05 is given to the student policy. At the end of the dialogue, a reward of +1 is given for dialogue success. The maximal extra reward δ is 0.05. For each set-up, 10000 dialogues are used for training, the moving dialogue success rate is recorded with a window size of 1000. The final results are the average of 40 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Policy Learning without Teaching</head><p>In this section, four policies without teaching are compared:</p><p>• DQN: A vanilla deep Q-Network ( <ref type="bibr" target="#b17">Mnih et al., 2015</ref>) which has two hidden layers, each with 128 nodes.</p><p>• A2C: An advantage actor-critic policy which consists of an actor network and a critic net- work <ref type="bibr" target="#b3">(Fatemi et al., 2016</ref>).</p><p>• Dropout DQN 1 and Dropout DQN 32: They both have a dropout layer after each hidden layer. The dropout rate is 0.2. Their difference is that the number of stochas- tic forward pass N of Dropout DQN 32 in Algorithm 1 is 32, while that of Dropout DQN 1 is 1. Dropout DQN 1 makes decision according to one output of Q-network similar to that of vanilla DQN. Dropout DQN 1 was first proposed in ( <ref type="bibr" target="#b5">Gal and Ghahramani, 2016)</ref>, and was confirmed that Dropout DQN 1 can obtain more effi- cient exploration. The learning curves are described in <ref type="figure" target="#fig_3">Figure 3</ref> and the evaluation results are described in  dropout can be observed as claimed in ( <ref type="bibr" target="#b5">Gal and Ghahramani, 2016)</ref>. However, Dropout DQN 1 seems to suffer premature and sub-optimal con- vergence, while our proposed Dropout DQN 32, whose decision is based on multi votes (algo- rithm 1), can result in improvement of effi- ciency and better final performance. Moreover, Dropout DQN 32 also performs much better than the policy gradient method A2C.</p><p>For the following experiments, the times of stochastic forward pass N in Algorithm 1 is 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Policy Learning with Teaching</head><p>In this section, four methods of teaching by the rule-based policy are compared:</p><p>• EA: 500 dialogues are taught with EA at the beginning ( <ref type="bibr" target="#b1">Chen et al., 2017</ref>).</p><p>• A2C PreTrain: At the beginning, 500 di- alogue are collected with rule-based policy. These examples are used to pre-train the actor network with supervised learning. After the pre-training, the policy is continuously opti- mized with the A2C algorithm <ref type="bibr" target="#b3">(Fatemi et al., 2016</ref>).</p><p>• CL AAD: Full CL with AAD-DQN described in section 3.</p><p>• CL D: CL without agent-aware experience repay, i.e. the teacher's experiences and stu- dent's experiences are put in one pool and are uniformly sampled for the experience replay in equation (1).</p><p>As can be seen in <ref type="figure" target="#fig_4">Figure 4</ref>, there is a big dip in the performance of A2C PreTrain. One possi- ble explanation is that because the rule-based pol- icy is sub-optimal, the pre-training makes the stu- dent policy reach a local minimum point. The rl- training should first make it escape from the local minimum point, which results in a temporary loss in performance.</p><p>Comparing CL methods (CL D and CL AAD) with EA in <ref type="figure" target="#fig_4">Figure 4</ref> and in <ref type="table" target="#tab_2">Table 1</ref>, we can con- clude that CL can significantly boost the safety of learning process. Moreover, except for safety, CL AAD can boost the efficiency, which benefits from the agent-aware experience replay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison of Optimized Student Policy and Rule-based Teacher Policy</head><p>To interpret what the student has learnt, we fur- ther compare the rules and an optimized student policy with 76.7% success rate. The rule-based policy is used to collect 5000 dialogues, while in each turn the decision made by the student policy is also recorded. <ref type="figure">Figure 5</ref> is a confusion matrix. The x-axis denotes the student's decision and the y-axis denotes the rules' decision. The numbers on the left are the statistics for each action in 5000 dialogues. Each element in the matrix denotes the normalized number of turns when the rule chooses the action in the corresponding line, the student chooses the action in the corresponding column. As is shown in <ref type="figure">Figure 5</ref>, offer and confirm are two action types used most frequently. In more than half of turns when the rule-based pol- <ref type="figure">Figure 5</ref>: Confusion matrix between the decisions of rule-based policy and the decisions of the opti- mised student policy. The x-axis denotes the stu- dent's decision and the y-axis denotes the rules' decision.</p><p>icy chooses offer 1, the student policy will choose a different action. Furthermore, from the element in line offer 1 and column request area, we can find that in this situation the student policy prefers the action request area. Inspired by this disagree- ment, we designed a new rule:</p><p>• R4: request values for slot area when there is only one other slot constraint for the database query.</p><p>Similarly, as can be seen in <ref type="figure">Figure 5</ref>, in a con- siderable proportion of turns when the rule-based policy chooses confirm area, confirm pricerange, or confirm name, the student policy will choose the action offer 2, which may mean that for slots area, pricerange, or name, when there are val- ues for database query, the system should offer a restaurant instead of confirming the slot-value constraints. Therefore, the rule R1 in section 3.2 was revised as follows:</p><p>• R1*: For slot food, confirm the most likely value has the probability between 0.1 and 0.6; For slot area, pricerange and name, confirm the most likely value, the belief of which is smaller than the belief of the special value "none" and is larger than 0.1.  <ref type="table" target="#tab_4">Table 2</ref>: Evaluation results of different ordered rules. As a reference, the performance of opti- mised student policy is success rate 0.767, #turn 5.10 and reward 0.5124.</p><p>while the rule R1* can both boost the success rate and decrease the dialogue length (comparing line 3 with line 1). The combination of R4 and R1* takes respective advantages (comparing line 4 with line 1, line 2 and line 3). The performance of fi- nal order rules is comparable to the performance of optimized student policy. It is worth noting that the primary rules R1, R2, and R3 in section 3.2 don't distinguish between different slots. However, the new rules R4 and R1* are all slot-specific, which it is difficult to de- sign at the beginning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper has proposed a companion learning framework to unify rule-based policy and RL- based policy. Here, the rule-based policy acts as a teacher, which either directly shows example ac- tion or gives an extra reward. Based on the un- certainty estimated using a dropout Q-Network, a companion strategy is proposed to control when the student policy directly consults rules and how often the student policy learns from the teacher's experiences. Simulation experiments showed that our proposed framework can significantly improve both safety and efficiency of on-line policy opti- mization. Additionally, we visually analyzed the differences in behaviors between the rule-based teacher policy and the optimized student policy, which gave us some inspirations to refine the rules.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) RL-based Companion Learning(CL) Framework with Logic Rules in an SDS. (b) AgentAware Dropout DQN (AAD-DQN) for CL.</figDesc><graphic url="image-1.png" coords="3,81.98,62.81,430.88,127.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of average certainty C e and the probability p tea .</figDesc><graphic url="image-2.png" coords="5,307.28,208.18,226.76,133.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>3: for e = 1, E do 4: Update the dialogue belief state b 0 5: Initialize the average certainty C e ← 0 6: if teaching is T rue then 7: teaching, p tea ← Companion(C) 8: end if 9: for t = 0, T e do 10:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of four policies without teaching.</figDesc><graphic url="image-3.png" coords="7,307.28,519.47,226.77,132.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of four methods with teaching by rule-based teacher.</figDesc><graphic url="image-4.png" coords="8,307.28,201.94,226.77,132.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table>Comparing Dropout DQN 1 with DQN in fig-
ure 3, the improvement of efficiency caused by </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 : The quantitative evaluation results of different methods. Here final performance (FP) is</head><label>1</label><figDesc></figDesc><table>the 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 is</head><label>2</label><figDesc></figDesc><table>the evaluation results of different or-
dered rules. The rule R4 can significantly boost 
the success rate (comparing line 2 with line 1), 

</table></figure>

			<note place="foot" n="1"> The quantitative evaluation metrics of safety and efficiency are proposed in section 4.</note>

			<note place="foot" n="4"> Here is the element-wise product. 5 The N forward passes can be done in parallel, e.g. the</note>

			<note place="foot" n="6"> This threshold is the best one we have tried.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the Shanghai Sail-ing Program No. 16YF1405300, the China NSFC projects <ref type="bibr">(No. 61573241 and No. 61603252)</ref> and the Interdisciplinary Program (14JCZ03) of Shanghai Jiao Tong University in China. Exper-iments have been carried out on the PI supercom-puter at Shanghai Jiao Tong University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">1613</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On-line dialogue policy learning with companion teaching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="198" to="204" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Strategic dialogue management via deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heriberto</forename><surname>Cuayáhuitl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Keizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Reinforcement Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Policy networks with two-stage training for dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Layla</forename><forename type="middle">El</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Particle Swarm Optimization Approach for Optimum Design of PID Controller in AVR System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zwe-Lee L</forename><surname>Gaing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Energy Conversion</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="384" to="391" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning (ICML-16)</title>
		<meeting>the 33rd International Conference on Machine Learning (ICML-16)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Gaussian processes for fast policy optimisation of POMDP-based dialogue managers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Jurčíček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Keizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Mairesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">Young</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On-line policy optimisation of spoken dialogue systems via live interaction with human subjects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Jurčíček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="312" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gaussian processes for pomdp-based dialogue manager optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="40" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hybrid reinforcement/supervised learning of dialogue policies from fixed data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kallirroi</forename><surname>Georgila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="487" to="511" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The second dialog state tracking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGDIAL</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">263</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing coadaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fractional control of heat diffusion systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><forename type="middle">S</forename><surname>Jesus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Machado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Dynamics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="263" to="282" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Planning and acting in partially observable stochastic domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><surname>Pack Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony R</forename><surname>Michael L Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cassandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="99" to="134" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Reinforcement learning for robots using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long-Ji</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
		<respStmt>
			<orgName>Fujitsu Laboratories Ltd</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zachary C Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05081</idno>
		<title level="m">Efficient exploration for dialogue policy learning with bbq networks &amp; replay buffer spiking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<title level="m">Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing atari with deep reinforcement learning</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep exploration via bootstrapped dqn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4026" to="4034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sample Efficient On-line Learning of Optimal Dialogue Policies with Kalman Temporal Differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Olivier Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthilkumar</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandramohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1878" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Agenda-based user simulation for bootstrapping a pomdp dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Schatzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Weilhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="149" to="152" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Error simulation for training statistical dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Schatzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition &amp; Understanding, 2007. ASRU. IEEE Workshop on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="526" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Modern control system theory and design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stanley M Shinners</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veda</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sample-efficient actor-critic reinforcement learning with supervised data for dialogue management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawel</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)</title>
		<meeting>the 18th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Rojasbarahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsunghsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02689</idno>
		<title level="m">Continuously learning neural dialogue management</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The best of both worlds: unifying conventional dialog systems and pomdps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1173" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hybrid code networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavosh</forename><surname>Jason D Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Partially observable markov decision processes for spoken dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="393" to="422" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">End-toend LSTM-based dialog control optimized with supervised and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The hidden information state model: A practical framework for pomdp-based spoken dialogue management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Keizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Mairesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Schatzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="150" to="174" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pomdp-based statistical spoken dialog systems: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1160" to="1179" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)</title>
		<meeting>the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
