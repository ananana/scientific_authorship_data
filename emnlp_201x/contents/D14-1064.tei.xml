<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Translate: A Query-Specific Combination Approach for Cross-Lingual Information Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhan</forename><surname>Ture</surname></persName>
							<email>fture@bbn.com</email>
							<affiliation key="aff0">
								<orgName type="institution">BBN Technologies</orgName>
								<address>
									<addrLine>10 Moulton St Cambridge</addrLine>
									<postCode>02138</postCode>
									<settlement>Raytheon, Raytheon</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Boschee</surname></persName>
							<email>eboschee@bbn.com</email>
							<affiliation key="aff1">
								<orgName type="institution">BBN Technologies</orgName>
								<address>
									<addrLine>10 Moulton St Cambridge</addrLine>
									<postCode>02138</postCode>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Translate: A Query-Specific Combination Approach for Cross-Lingual Information Retrieval</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="589" to="599"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>When documents and queries are presented in different languages, the common approach is to translate the query into the document language. While there are a variety of query translation approaches, recent research suggests that combining multiple methods into a single &quot;structured query&quot; is the most effective. In this paper , we introduce a novel approach for producing a unique combination recipe for each query, as it has also been shown that the optimal combination weights differ substantially across queries and other task specifics. Our query-specific combination method generates statistically significant improvements over other combination strategies presented in the literature , such as uniform and task-specific weighting. An in-depth empirical analysis presents insights about the effect of data size, domain differences, labeling and tuning on the end performance of our approach .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cross-lingual information retrieval (CLIR) is a special case of information retrieval (IR) in which documents and queries are presented in different languages. In order to overcome the language barrier, the most commonly adopted method is to translate queries into the document language. Many methods have been introduced for translat- ing queries for CLIR, ranging from word-by-word dictionary lookups ( <ref type="bibr" target="#b33">Xu and Weischedel, 2005;</ref><ref type="bibr" target="#b9">Darwish and Oard, 2003)</ref> to sophisticated use of machine translation (MT) systems ( <ref type="bibr" target="#b18">Magdy and Jones, 2011;</ref><ref type="bibr" target="#b17">Ma et al., 2012)</ref>. Previous research has shown that combining evidence from differ- ent translation approaches is superior to any sin- gle query translation method <ref type="bibr" target="#b5">(Braschler, 2004;</ref><ref type="bibr" target="#b12">Herbert et al., 2011</ref>). While there are numer- ous combination-of-evidence techniques for both mono-lingual and cross-lingual IR, recent work suggests that there is no one-size-fits-all solution. In fact, the optimal combination weights (i.e., weights assigned to each piece of evidence in a linear combination) differ greatly across queries, tasks, languages, and other variants ( <ref type="bibr" target="#b31">Ture et al., 2012;</ref><ref type="bibr" target="#b4">Berger and Savoy, 2007)</ref>.</p><p>In this paper, we introduce a novel method for learning optimal combination weights when build- ing a linear combination of existing query transla- tion approaches. From standard query-document relevance judgments we train a set of classifiers, which produce a unique combination recipe for each query, based on a large set of features ex- tracted from the query and collection. Experi- mental results show that the effectiveness of our method is significantly higher than state-of-the-art query translation methods and other combination strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The earliest approaches to query translation for CLIR used machine-readable bilingual dictio- naries ( <ref type="bibr" target="#b14">Hull and Grefenstette, 1996;</ref><ref type="bibr" target="#b1">Ballesteros and Croft, 1996)</ref>, achieving around up to 60% of monolingual IR effectiveness. <ref type="bibr" target="#b33">Xu and Weischedel (2005)</ref> showed that effectiveness can be increased to around 80% by weighting each translation proportional to its rank in the dictio- nary. The practice of weighting translation candi- dates was later formulated as a "structured query", in which each query term is represented by a prob- ability distribution over its translations in the doc- ument language <ref type="bibr" target="#b25">(Pirkola, 1998;</ref><ref type="bibr" target="#b15">Kwok, 1999;</ref><ref type="bibr" target="#b9">Darwish and Oard, 2003)</ref>. Our approach is based on the structured query formulation.</p><p>Some of the earliest studies in IR discovered that with different underlying models, the re- trieved document set would vary substantially, al-though the effectiveness was similar <ref type="bibr" target="#b19">(McGill et al., 1979)</ref>. Later studies showed that combining different representations of the query and/or doc- ument often produced superior output <ref type="bibr" target="#b27">(Rajashekar and Croft, 1995;</ref><ref type="bibr" target="#b32">Turtle and Croft, 1990;</ref><ref type="bibr" target="#b10">Fox, 1983)</ref>. This intuitive idea was supported theoret- ically by <ref type="bibr" target="#b22">Pearl (1988)</ref>, concluding that multiple pieces of evidence estimates relevance more accu- rately, but that the benefit strongly depends on the quality and independence of each piece. Experi- ments by <ref type="bibr" target="#b2">Belkin et al. (1995)</ref> indicated the need to properly weight each representation with respect to its effectiveness. These so-called "combination- of-evidence" techniques became more powerful with the introduction of Indri, a probabilistic re- trieval framework specifically designed for com- bining multiple query and document representa- tions <ref type="bibr" target="#b20">(Metzler and Croft, 2005</ref>). Croft (2000) pro- vides a detailed summary of earlier query combi- nation approaches in IR, while <ref type="bibr" target="#b24">Peters et al. (2012)</ref> cites more recent related work.</p><p>The benefits of combination-of-evidence trans- fer to the cross-lingual case especially well, since the inherent ambiguity of translation readily pro- vides a diverse set of representations. Most CLIR approaches implement a post-retrieval merging of ranked lists, each generated from different query ( <ref type="bibr" target="#b13">Hiemstra et al., 2001;</ref><ref type="bibr" target="#b29">Savoy, 2001;</ref><ref type="bibr" target="#b11">Gey et al., 2001;</ref><ref type="bibr" target="#b7">Chen and Gey, 2004</ref>) or docu- ment ( <ref type="bibr" target="#b16">Lopez and Romary, 2009</ref>) representations, also called "data fusion". In contrast, we focus on a pre-retrieval combination at the modeling stage, so that a single complex query is used in retrieval, instead of multiple simpler ones. Two advantages of the former are easier implementation (since the approach requires no changes to the modeling side) and the possibly greater diversity that can be achieved by having separate retrieval runs. How- ever, each ranked list needs to be limited in size, which might cause some potentially useful docu- ments not to be considered in the combination at all. Since the focus of this paper is on the model- ing end of retrieval, pre-retrieval combination was a more suitable choice, though we think that the two approaches have complementary benefits.</p><p>The idea of combining query translations before retrieval has been explored previously. <ref type="bibr" target="#b5">Braschler (2004)</ref> combines three translation ap- proaches: output of an MT system, a novel trans- lation approach based on a similarity thesaurus built automatically from a comparable corpus, and a dictionary-based translation. The main reason that this combination does not provide much benefit is due to the lower coverage of the thesaurus-based and dictionary-based trans- lation methods. A similar approach by <ref type="bibr" target="#b12">Herbert et al. (2011)</ref> uses Wikipedia to provide transla- tions of certain phrases and entities, and combin- ing that with the Google Translate MT sys- tem yields statistically significant improvements in English-to-German retrieval. More recently, <ref type="bibr" target="#b31">Ture et al. (2012)</ref> presented a more sophisti- cated translation approach using the internal rep- resentation of an MT system, and reported sta- tistically significant improvements when a pre- retrieval combination was performed.</p><p>All of the previously cited approaches either use uniform weights for combination, or select weights based on collection-level information. However, as stated previously, numerous stud- ies suggest that certain methods work better on certain queries, collections, languages. In fact, when weights are optimized separately on each collection, they differ substantially across differ- ent collections <ref type="bibr" target="#b31">(Ture et al., 2012</ref>). For monolin- gual retrieval, there has been a series of learning- to-rank (LTR) papers that determine weights for query concepts <ref type="bibr" target="#b3">(Bendersky et al., 2011</ref>), such that retrieval effectiveness is maximized. A re- cent study extends this idea to the cross-lingual case, by learning how to weight each translated word for English-Persian CLIR ( <ref type="bibr" target="#b0">Azarbonyad et al., 2013</ref>). In contrast, we extract translated word weights from diverse and sophisticated translation methods, then learn how to weight each trans- lated structured query, We call this "learning-to- translate" (LTT), which can be formulated as a simpler learning problem. In CLIR, both LTR and LTT are under-explored problems, with a common goal of applying machine learning techniques to improve query translation, yet with complemen- tary benefits.</p><p>To our knowledge, there has been one prior LTT approach: a classifier was trained to predict ef- fectiveness of each query translation, using fea- tures based on statistics of the query terms <ref type="bibr" target="#b4">(Berger and Savoy, 2007)</ref>. Instead of weighting, the translations with highest classifier scores were concatenated, yielding statistically significant im- provements over using the single-best translation method. However, the translation methods ex- plored in this paper are all based on one-best MT systems, making it difficult to draw strong conclu- sions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Query Translation</head><p>The primary contribution of this paper is to show how a diverse set of query translation (QT) meth- ods can be combined effectively into a single weighted structured query, with improved retrieval effectiveness. While our approach can applied to any set of translation methods, we focus on three methods that have complementary strengths and that have shown promise in CLIR: word-based probabilistic translation, one-best MT, and n-best probabilistic MT. We briefly present our imple- mentation of each method; more details can be found in earlier work <ref type="bibr" target="#b9">(Darwish and Oard, 2003;</ref><ref type="bibr" target="#b31">Ture et al., 2012)</ref>.</p><p>Each QT method generates a representation of the query in the document language. In the case of word-based and n-best MT approaches, the repre- sentation is a structured query itself, where each query word is represented by a probability distri- bution over translation alternatives. For one-best MT, the query is represented by a bag of translated words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">One-Best MT</head><p>A query translation approach that has become more popular recently is to simply run the query through an MT system, and use the best output as the query:</p><formula xml:id="formula_0">t 1 t 2 . . . t l = MT(s 1 s 2 . . . s k )<label>(1)</label></formula><p>where s = s 1 s 2 . . . s k is the query and t = t 1 t 2 . . . t l is the translated query. Since modern statistical MT systems generate high-quality translations for many language pairs, this one-best strategy works reasonably well for retrieval and provides a competitive baseline. A practical advantage of this approach is the ease of implementation -one can simply use any MT in- terface (e.g., Google Translate) as a black box in their CLIR system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Probabilistic n-best MT</head><p>The top translation might sometimes be incorrect, or might lack some of the alternative representa- tions that are very useful in retrieval. Therefore, considering the n highest scored translations (also referred to as the n-best list in MT literature) has become increasingly popular in CLIR approaches.</p><p>In order to benefit from the diversity amongst the n-best translations, one can simply concate- nate them together, forming a large list of query terms. However, statistical MT systems also assign probabilities to each translation, which can be incorporated into the query representation for better effectiveness, as suggested by <ref type="bibr" target="#b31">Ture et al. (2012)</ref>.</p><p>In this approach, each of the top n transla- tion candidates from the MT system are processed one by one. For each translation candidate, the MT system provides a translation probability, and alignments between words in the query and its translation. As we process each of the n transla- tions, for each query word s i , we accumulate prob- abilities on each translated word t ij aligned to s i . Finally, we normalize the translation probabilities to get Pr nbest (t ij |s i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Word-based</head><p>One of the most widely used approaches in CLIR is based on translating each query word s i in- dependently, with probabilities assigned to each translation candidate t ij . Translations are de- rived automatically from a bilingual corpus using statistical word alignment techniques, which are used as part of the training of statistical MT sys- tems ( <ref type="bibr" target="#b6">Brown et al., 1993</ref>). These probabilities can be exploited for retrieval based on the technique of <ref type="bibr" target="#b9">Darwish and Oard (2003)</ref> for "projecting" text into the document language. After cleaning up the automatically learned translation probabilities (de- tails omitted for space considerations), we end up with the translation probabilities Pr word (t ij |s i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Combination of Evidence</head><p>Once we have multiple ways to represent the query q in the document language (QT i (q), i = 1 . . . m), it is possible to combine these "pieces of evi- dence" into a single representation as follows:</p><formula xml:id="formula_1">QT(q) = m i=1 w i (q)QT i (q)</formula><p>and each combination-of-evidence approach dif- fers by how the combination weights w i are com- puted:</p><p>Uniform In this baseline method, we ignore any information we have about the collection or query and assign equal weights to each method (i.e., w i (q) = 1/m). In our case, this means a weight of 33.3% to each of the one-best, probabilistic n- best, and word-based QT methods.</p><p>Task-specific We can optimize the combination weights by overall effectiveness on a specific re- trieval task. Given a query set and collection, we perform a grid search on combination weights (with a step interval of 0.1) and select the weights that maximize retrieval effectiveness. The training is performed in a leave-one-out manner: weights for test query q are optimized on all queries except for q.</p><p>Query-specific We propose a novel method to compute combination weights specifically for each query, resulting in a more customized op- timization that can take into account how effec- tiveness of each translation method varies across queries.</p><p>In the remainder of this section, we describe the details of our novel query-specific combina- tion method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview of Query-Specific Combination</head><p>We present a novel approach for determining query-specific combination weights by training a classifier for each QT method. Prior to train- ing the classifier, we first run retrieval using each QT method, and evaluate the effectiveness of the retrieved documents. The effectiveness of the i th method on query q (i.e., f i (q)) is then con- verted into a binary label (further described in Section 4.2). Treating each query as a separate instance, a classifier is trained for each method, generating classifiers C 1 , . . . , C m . During re- trieval (i.e., at test time), for each query q, each trained classifier C i is applied to the query, re- sulting in a predicted label l i (q) and the classi- fier's confidence in a positive label, C i (q). 1 These values are then used to determine combination weights w 1 (q), . . . , w m (q) that are custom-fit for the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Labeling</head><p>First of all, we discard queries in which the dif- ference between the best and worst performing methods is small (specifically, the worst perform- ing method scores at least k 1 % of the best per- forming one). For such queries, generating fair training labels is more difficult and therefore more <ref type="bibr">1</ref> The confidence in a negative label is 1 − Ci(q). likely to introduce noise into the process. 2 More- over, these are exactly the queries where choos- ing optimal combination weights is less important (since all methods perform relatively similarly), so it is reasonable to exclude them from training. In fact, a high number of such queries would indi- cate lower potential for combination-of-evidence approaches.</p><p>For each QT method i, we create training in- stances per query, per retrieval task. Since our goal is to select the best among existing methods, the training label should reflect the effectiveness of method i relative to other methods. A strategy that we call best-by-measure assigns a label of 1 if the effectiveness of the i th method (i.e., f i (q)) is at least k 2 % of the maximum effectiveness for that query, and 0 otherwise. While this directly corre- lates with retrieval effectiveness, labels might be distributed in an unbalanced manner, which might affect the training process negatively. A balanced labeling requires sorting all training instances by how much better the i th method is than other meth- ods (max i =i (f i (q)/f i (q))), and then assigning a label of 1 to the lower half and 0 to the higher half. This strategy is called best-by-rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Features</head><p>We introduce a diverse set of features, in order to train a robust classifier for predicting when each QT method performs better and worse than others. We split the feature set into four meaningful cate- gories, so that we can measure the impact of each subset separately:</p><p>Surface features These features do not require a deep analysis of the query: (a) Number of words in query and the translated query, (b) Type of query that we automatically classify based on pre- defined templates (e.g., fact question, cause-effect, etc.), and (c) Number of stop words in the query and the translated query.</p><p>Parse-based features These features are ex- tracted from a deeper syntactic analysis of the query text: (a) Number of related names found in a named entity database, and (b) Existence of syn- tactic constituents in query and its translation (e.g., "is there a VVB in the query parse tree").</p><p>Translation-based features These features consist of statistics computed from the query and its translation: (a) Number of query words that were unaligned in at least half of the n-best query translations, (b) Number of query words that were aligned to multiple target words in at least half of the n-best query translations, (c) Number of query words that were self-aligned (i.e., target word is exactly same string) in at least half of the n-best query translations, Additionally, the target language is a default feature in all of our experiments. For each clas- sification task, we train a separate classifier on each subset of these four feature categories, so that there are 16 different sets (including the empty set). After we select which categories to pull fea- tures from, we optionally perform feature selec- tion to reduce the number of features by a pre- defined percentage.</p><note type="other">(d) Average / Standard deviation / Maximum / Minimum of entropy of Pr nbest of each query word, and (e) Average / Standard deviation / Maximum / Minimum of entropy of Pr word of each query word.</note><p>In our experimentation, we observed that collection-based features were most useful for classifying the one-best method, whereas parse- based features were most discriminative for prob- abilistic 10-best. For the word-based QT method, the translation-based features were most effective in our experiments. We further analyze the effect of various features in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training and Tuning Classifiers</head><p>The scikit-learn package was used for the training pipeline ( <ref type="bibr" target="#b23">Pedregosa et al., 2012)</ref>. Using an established toolkit allowed us to experiment with many options for classification, such as the learner type (support vector machine, maximum entropy, decision tree), feature set (16 subsets of the four categories described earlier) and two fea- ture selection methods (recursive elimination or selection based on univariate statistical tests). In the end, we get 96 different parameter combina- tions while training a classifier for a particular QT method, resulting in the need for tuning -picking the parameters that produce highest accuracy on a representative tuning set.</p><p>Given that we have a set of queries for testing purposes, there are few strategies for selecting a training and tuning set. One approach is to apply a leave-one-out strategy, so that a classifier is trained and tuned on all but one of the test queries, and then applied on the remaining query to predict its label. We call this the fully-open setting.</p><p>In a more realistic scenario, there will not be relevance judgments for the test queries, yet there might be a small amount of labeled data similar to the test task (e.g., different queries on same col- lection) that can be utilized for tuning purposes, and a larger set of training queries from different collections. We call this the half-blind setting.</p><p>If testing in a new domain, queries of similar type are not available for training and tuning pur- poses. This is a more challenging scenario than the previous two, yet it is important for real-world applications. In order to demonstrate the effec- tiveness of the training pipeline in this case, we hold out test queries entirely, then train and tune on queries from a completely different task (i.e., different queries and collection). We call this the fully-blind setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Retrieval</head><p>Once we have classifiers trained for all QT meth- ods, we can apply them to a given query on-the-fly, and compute query-specific combination weights. One approach is hard weighting, putting all weight onto a single method -when there are more than one methods classified with label 1, we can ei- ther pick one randomly or use the classifier con- fidence value as a tie-breaker. An alternative is soft weighting, where the weight of the i th method can be computed either using classifier confidence C i (i.e., how confident the model is that the i th method will perform well), precision on tuning set precision i (i.e., how precise the model is at its pre-dictions for the i th method), or both:</p><formula xml:id="formula_2">w s1 i (q) =C i (q) w s2 i (q) =precision i (1) × l i (q) +(1−precision i (0)) × (1 − l i (q)) w s3 i (q) =precision i (1) × C i (q) +(1−precision i (0)) × (1 − C i (q))</formula><p>The intuition behind all of these weighting schemes is to produce a weight for each QT method, by taking into account the confidence of the classifier, and/or the precision of the classifier on tuning instances.</p><p>The computed weights are normalized before constructing the final query for retrieval:</p><formula xml:id="formula_3">w final i (q) = w i (q)/ m j=1 w j (q)</formula><p>When compared empirically, we noticed that soft weighting is more effective than hard weight- ing, as the latter is more sensitive to classifier er- rors. Among the three soft weighting functions, differences were mostly negligible in our exper- iments. Hence, we decided to use the simplest weighting function w s1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Analytical Model</head><p>It is time-consuming to implement various combination-of-evidence approaches and run re- trieval experiments. Therefore, it is useful to have an analytical model of the process that can pro- vide a rough estimate of how fruitful it would be to spend this effort, given certain details about the task. The model we present in this section esti- mates the effectiveness of combining QT methods 1 . . . m on a query set Q, given (1) the effective- ness of each method on Q and (2) error rate of binary classifiers C 1 . . . C m on Q. Using this for- mulation, one can assess the benefit of combina- tion without running retrieval, based only on er- ror rates -this saves precious time during de- velopment. Moreover, even without trained clas- sifiers, this model can be used to estimate poten- tial benefits by plugging in hypothetical error val- ues. In other words, one can ask the question "If I had classifiers with x% error on this query set, what would be the benefit of using these classi- fiers to combine QT methods?" before developing any combination approach at all.</p><p>The analytical model considers a special case of weighted combination: for each query q, we pick a single QT method i = 1 . . . m, for which the clas- sifier predicts a label of 1. If there are more than one such method, one of them is picked randomly. This simplified version allows us to compute ex- pected effectiveness for q as follows:</p><formula xml:id="formula_4">E[f (q)] = method i Pr(pick i|q)f i (q)</formula><p>While f i (q) is an observed value (the effective- ness of the i th method on query q), Pr(pick i|q) needs to be estimated (the probability of selecting the i th method). Since this depends on the pre- dicted labels, we consider all possible scenarios l = l 1 l 2 . . . l m , where each value is the prediction of a classifier. For instance, "l=010" means that classifiers C 1 and C 3 predicted a label of 0, while C 2 predicted a positive label. Marginalizing over the 2 m possible scenarios gives us the following estimate:</p><formula xml:id="formula_5">Pr(pick i|q) =   1 l 1 =0 . . . 1 lm=0 Pr(l|q)   × Pr(pick i|l, q) =   1 l 1 =0 . . . 1 lm=0 m i=1 Pr(l i |q)   × Pr(i|l, q)</formula><p>In the final step, we assumed that classifiers make predictions independent of each other, which is a desired property for successful combination. Pr(l i |q) can be estimated using classifier error statistics:</p><formula xml:id="formula_6">Pr(l i |q) ∼ count(predicted = l i , true = l q ) count(true = l q )</formula><p>where l q is the true label of q. If l i = l q , this ex- pression becomes the true positive or true negative rate, depending on the value. Similarly, if l i = l q , it is either the false positive or false negative rate. Finally, the probability that the i th method is se- lected in a particular scenario depends solely on the predicted labels, since it is a random selection: Pr(pick i|l) = l i / m j=1 l j This concludes the derivation of the analytical model of query evidence combination, which we use in Section 5.1 to evaluate the effectiveness of labeling approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>We evaluated our approach on four different CLIR tasks: TREC 2002 English-Arabic CLIR, NTCIR- 8 English-Chinese Advanced Cross-Lingual Infor-mation Access (ACLIA), and two forum post re- trieval tasks as part of the DARPA Broad Oper- ational Language Technologies (BOLT) program: English-Arabic (BOLT ar ) and English-Chinese (BOLT ch ). The query language is English in all cases, and we preprocess the queries using BBN's information extraction toolkit SERIF <ref type="bibr" target="#b28">(Ramshaw et al., 2011</ref>). State-of-the-art English-Arabic (En- Ar) and English-Chinese (En-Ch) MT systems were trained on parallel corpora released in NIST OpenMT 2012, in addition to parallel forum data collected as part of the BOLT program (10m En- Ar words; 30m En-Ch words). From these data, word alignments were learned with GIZA++ <ref type="bibr" target="#b21">(Och and Ney, 2003)</ref>, using five iterations of each of IBM Models 1-4 and HMM.</p><p>3-gram Chinese and 5-gram Arabic Kneser-Ney language models were trained from the Gigaword corpus (1b words each) and non-English side of the training corpus. Chinese and English parallel text were preprocessed through the Treebank Tok- enizer, 4 while no special treatment was performed on Arabic.</p><p>For retrieval, we used Indri, a state-of-the- art probabilistic relevance model that supports weighted query representations through operators #combine and #weight <ref type="bibr" target="#b20">(Metzler and Croft, 2005)</ref>. A character-based index was built for Chinese collections, whereas Arabic text was stemmed using Lucene before indexing. <ref type="bibr">5</ref> En- glish text was preprocessed by Indri's imple- mentation of the Porter stemmer <ref type="bibr" target="#b26">(Porter, 1997)</ref>. Statistics for each collection and query set are summarized in <ref type="table">Table 1</ref>.</p><p>Before performing any combination, we first ran the three baseline QT methods individually and evaluated the retrieved documents. Mean average precision (MAP) was used to measure retrieval effectiveness, which is a widely used and stable metric, estimating the area under the precision-recall curve. We set n = 10 for the n-best probabilistic translation method. Baseline scores are reported in <ref type="table" target="#tab_1">Table 2</ref>. The average preci- sion (AP) of each query in these tasks was used to label the query and construct training data accord- ingly.</p><p>In subsequent sections, we evaluate the effect of several variants in the training pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Effect of Labeling</head><p>In Section 4.2, we introduced two ways to label instances. In our evaluation, we set the free pa- rameters k 1 = k 2 = 90, which filters out 33% of queries from the training set of the BOLT ar task; this percentage is 29% in BOLT ch , 44% in TREC, and 27% in NTCIR.</p><p>Labeling determines which query translation method is considered effective or not, which con- sequently determines what the "learning problem" is (since the objective of the classifier is to sep- arate differently labeled instances). As a result, there are two dimensions to consider when com- paring labeling strategies. One is the accuracy of the classifiers on held-out data, and the other is how well the trained classifier reflects this accu- racy when used in retrieval. To clarify the dis- tinction, consider a case where every instance is labeled 1. This generates a trivial learning prob- lem with no test errors, yet this does not entail that using these classifiers in retrieval will be more ef- fective than other labeling strategies. If, even with high classifier accuracy, the retrieval effectiveness is low, that indicates a bad choice for labeling.</p><p>We can theoretically analyze how suitable each labeling method is by applying the analytical model to each CLIR task, setting parameters based on a perfect classifier: true positive/negative rate of 1 and false positive/negative rate of 0 (see Sec- tion 4.6). <ref type="table" target="#tab_1">Table 2</ref> shows these results in the "Per- fect" column, since these scores represent what could be achieved if classifiers were trained to pre- dict labels perfectly (no training or retrieval is ac- tually performed). There are two values in each row of the "Perfect" column, one for each labeling strategy. In each row, we found these two values to be statistically significantly higher than any of the baseline scores. This shows that both labeling ap- proaches have the potential to improve effective- ness significantly.</p><p>We also made an empirical comparison of the two labeling approaches by actually training clas- sifiers with each labeling, and then using the clas- sifiers to combine query translations in retrieval. The "Trained" column in <ref type="table" target="#tab_1">Table 2</ref> shows the MAP we get on each CLIR task (and average classifier accuracies), using either labeling. <ref type="bibr">6</ref> Based on these results, we conclude that best-  by-measure labeling is more useful in practice, supported by typically higher accuracy and effec- tiveness. Best-by-rank yields better results only on TREC, but a closer look reveals that the in- crease in MAP is due to only two outlier queries. For BOLT ar , on the other hand, retrieval with best- by-measure labeling is more effective (statistically significant) than best-by-rank; hence, the former is used in remaining parts of our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effect of Train-Tune Setting</head><p>In Section 4.4, we introduced three major train- tune settings: fully-open, half-blind, and fully- blind. In order to implement these settings, we treat each of the three query sets (BOLT, TREC, NTCIR) as a separate training dataset and experi- ment with a variety of combinations. For simplicity, let us demonstrate the variety of experiments assuming the test collection is BOLT. For the fully-open case, the default training data is all of the BOLT queries (this training set is referred to as b). Additionally, one can include queries from TREC (referred to as t) and NTCIR (referred to as n) into the training data. This gives us four different training datasets for the fully-open case: b, b + n, b + t, b + t + n. Similarly, each of the half-blind and fully-blind settings can be applied to three different training sets: For BOLT, these are t, n, t + n. <ref type="bibr">7</ref> This results in ten different ex- periments run for each task -in each experiment, we train a classifier for each QT method, select the best meta-parameters on the tuning set, and then compute combination weights for retrieval using the classifiers.</p><p>Each cell on the left side of <ref type="table">Table 3</ref> (under col- umn "Query-specific Combination") shows the re- sults of the most effective experiment for a partic- ular task and train-tune setting. Accuracy values for classifiers varied widely across these experi- ments. Still, even when accuracies dropped close to or below 50% (i.e. random baseline), combined retrieval was always more effective than any single QT approach, which emphasizes the robustness of our approach. For instance, in the fully-blind set- ting for the NTCIR task, the individual classifiers had accuracies of only 56%, 49%, and 44% but MAP was 0.163, which is higher than the MAP of any individual method for that collection (0.146, 0.152, or 0.141).</p><p>Another key observation in <ref type="table">Table 3</ref> is that the domain effect (i.e., training and/or tuning on queries similar to test queries) is only noticeable on the two BOLT tasks. For NTCIR and TREC, we do not observe a boost in MAP when queries from the same task are included in training (i.e., fully-open setting). This can be explained by the BOLT-centric nature of our system components: the text analysis tool and MT systems are tuned mainly for forum data, and the collection-based features are extracted from BOLT. Due to this bias, BOLT queries were most useful in our experi- ments, supported by the fact that BOLT is always   <ref type="table">Table 3</ref>: A comparison of query combination approaches. For query-specific combination, MAP and training data are shown for the most effective experiment of each train-tune setting. For each task, the highest MAP achieved with our approach is shown in bold. Superscripts 1, 2, and 3 indicate statisti- cally significant improvements over baseline methods one-best, probabilistic 10-best, and word-based, whereas * indicates improvements over all three. Superscript † indicates results significantly better than uniform and task-specific combination methods.</p><p>included in the train set when testing on TREC or NTCIR (see lowest two rows in <ref type="table">Table 3</ref>). Also, when there is no domain effect (i.e., half-blind and fully-blind ), more data yields higher effectiveness in 6 out of 8 cases (see two right columns on the left side of <ref type="table">Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Retrieval Effectiveness</head><p>In this section, we compare our novel query- specific combination-of-evidence approach to the baseline CLIR approaches, as well as comparable combination methods (uniform and task-specific combination) in terms of retrieval effectiveness. Based on a randomized significance test <ref type="bibr" target="#b30">(Smucker et al., 2007)</ref>, the best query-specific combina- tion method (shown in boldface in <ref type="table">Table 3</ref>) out- performs all baseline QT methods in all tasks with 95% confidence (indicated by superscript * in <ref type="table">Table 3</ref>). This is not the case for uniform or task-specific query combination, which are statis- tically indistinguishable from at least one of the QT methods, depending on the task (indicated by superscripts 1, 2, and 3 for one-best, probabilis- tic 10-best, and word-based QT methods, respec- tively). When we directly compare our query- specific combination approach to other combina- tion methods, the differences are statistically sig- nificant for all tasks but NTCIR (indicated by su- perscript †). For reference, we also computed effectiveness for a hypothetical system (denoted by "Max" in <ref type="table">Table 3</ref>) that could select the best QT method for each query and use only that for retrieval. This is not a strict upper bound, since correctly weight- ing each method can produce better results, but it is still a reasonable target for effectiveness. In our experiments, Arabic retrieval runs were very close to this target with our combination approach, while the gap for Chinese is still substantial, which is worth further exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>In this paper, we introduced a novel combination- of-evidence approach for CLIR, which learns a custom combination recipe for each query. We for- mulate this as a set of binary classification prob- lems, and show that trained classifiers can be used to produce query-specific combination weights ef- fectively. Our deep exploration of many variants (e.g., labeling, training-tuning, weight computa- tion, analytical formulation) and extensive empiri- cal analysis on four different tasks provide insights for future research on the under-studied problem of combining translations for CLIR.</p><p>Our approach advances the state of the art of CLIR, yielding higher effectiveness than three ad- vanced query translation approaches, all based on state-of-the-art MT systems. Furthermore, on three of the four tasks, our combination strategy is statistically significantly better than two compara- ble combination techniques. Experimental results also suggest that even a uniform combination of query translations is consistently better than any individual method. While it is known that com- bining translations helps CLIR, we confirm this on a set of modern CLIR tasks, including two target languages and a variety of text domains.</p><p>Having a simple linear learning problem allows us to train robust models with relatively simpler features. Nevertheless, we are interested in ex- perimenting with more sophisticated learning ap- proaches. In terms of non-linear classifiers, our experience with decision trees in this paper indi- cated a higher tendency to overfit. In terms of combining queries in a non-linear fashion, our fu- ture plans include integrating our approach into a LTR framework, and directly optimize MAP. This will also allow us to explore more complex fea- tures extracted from query and document text, as well as external sources.</p><p>Another possible future endeavor is to extend these ideas to (i) other query translation ap- proaches and (ii) document translation. While the exact same problem can be formulated for learning to translate documents effectively, a more compli- cated infrastructure and longer running times are two challenges that need to be considered.</p><p>Finally, we hope this to be a significant step to- wards more context-dependent and robust CLIR models, by taking advantage of modern translation technologies, as well as machine learning tech- niques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Index-based features These features are based on frequency statistics from a representative col- lection: 3 (a) Average / Standard deviation / Max- imum / Minimum of document frequency (df) of query words and their translations, (b) Average / Standard deviation / Maximum / Minimum of term frequency averaged across query words and their translations, and (c) Sum / Maximum / Minimum of total probability assigned to words that do not appear in the collection (df = 0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Task</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Retrieval effectiveness of baseline QT methods is presented on the left side, and a comparison of 
labeling strategies is provided on the right side. All numbers represent MAP values, except for classifier 
accuracy shown in percentage values (in parantheses). Analytically computed values are shown in italics. 

</table></figure>

			<note place="foot" n="2"> We also experimented with including these queries with a third label (e.g., &quot;same&quot;) and train a ternary classifier. Having more labels requires more training data, which is not easy to obtain for this task. Also, obtaining a balanced label distribution becomes even more difficult with three labels.</note>

			<note place="foot" n="3"> We used the BOLT collection in our experiments.</note>

			<note place="foot" n="4"> http://www.cis.upenn.edu/˜treebank 5 http://lucene.apache.org</note>

			<note place="foot" n="6"> For a fair comparison, we fixed the train-tune setting to fully-open, trained classifiers on the test collection and reported leave-one-out accuracies.</note>

			<note place="foot" n="7"> In the case of half-blind, b is split into two: 20% is used for tuning and the remainder is used for testing.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by DARPA/I2O Con-tract No. HR0011-12-C-0014 under the BOLT program (Approved for Public Release, Distribu-tion Unlimited). The views, opinions, and/or find-ings contained in this article are those of the author and should not be interpreted as representing the official views or policies, either expressed or im-plied, of the Defense Advanced Research Projects Agency or the Department of Defense.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploiting multiple translation resources for english-persian cross language information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hosein</forename><surname>Azarbonyad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azadeh</forename><surname>Shakery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heshaam</forename><surname>Faili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Cross-Language Evaluation Forum on Cross-Language Information Retrieval and Evaluation, CLEF &apos;13</title>
		<meeting>the Cross-Language Evaluation Forum on Cross-Language Information Retrieval and Evaluation, CLEF &apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="93" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dictionary methods for cross-lingual information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W. Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International DEXA Conference on Database and Expert Systems Applications</title>
		<meeting>the 7th International DEXA Conference on Database and Expert Systems Applications</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="791" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining the evidence of multiple query representations for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="431" to="448" />
			<date type="published" when="1995-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Parameterized concept weighting in verbose queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;11</title>
		<meeting>the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="605" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Selecting automatically the best query translations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Large Scale Semantic Access to Content (Text, Image, Video, and Sound), RIAO &apos;07</title>
		<meeting><address><addrLine>Paris, France, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="287" to="300" />
		</imprint>
	</monogr>
	<note>Le Centre de Hautes Etudes Internationales D&apos;Informatique Documentaire</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Combination approaches for multilingual text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Braschler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="183" to="204" />
			<date type="published" when="2004-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multilingual information retrieval using machine translation, relevance feedback and decompounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="149" to="182" />
			<date type="published" when="2004-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Combining approaches to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<editor>W. Bruce Croft</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Probabilistic structured query methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kareem</forename><surname>Darwish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR &apos;03</title>
		<meeting>the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR &apos;03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="338" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Extending the Boolean and Vector Space Models of Information Retrieval with P-norm Queries and Multiple Concept Types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<pubPlace>Ithaca, NY, USA. AAI8328584</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Cornell University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cross-language retrieval for the clef collections-comparing multiple methods of retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivien</forename><surname>Petras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Revised Papers from the Workshop of Cross-Language Evaluation Forum on Cross-Language Information Retrieval and Evaluation, CLEF &apos;00</title>
		<meeting><address><addrLine>London, UK, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="116" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Combining query translation techniques to improve cross-language information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Herbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">György</forename><surname>Szarvas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd European Conference on Advances in Information Retrieval, ECIR&apos;11</title>
		<meeting>the 33rd European Conference on Advances in Information Retrieval, ECIR&apos;11<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="712" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Translation resources, merging strategies, and relevance feedback for cross-language information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djoerd</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wessel</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renée</forename><surname>Pohlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thijs</forename><surname>Westerveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Revised Papers from the Workshop of Cross-Language Evaluation Forum on Cross-Language Information Retrieval and Evaluation, CLEF &apos;00</title>
		<meeting><address><addrLine>London, UK, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="102" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Querying across languages: a dictionary-based approach to multilingual information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Hull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;96</title>
		<meeting>the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;96</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="49" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">English-Chinese crosslanguage retrieval based on a translation package</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui-Lam</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Machine Translation for Cross Language Information Retrieval</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="8" to="13" />
		</imprint>
	</monogr>
	<note>Machine Translation Summit VII</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Patatras: Retrieval model combination and regression models for prior art search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Cross-language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments, CLEF&apos;09</title>
		<meeting>the 10th Cross-language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments, CLEF&apos;09<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="430" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Opening machine translation black box for cross-language information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Retrieval Technology</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="467" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Should MT systems be used as black boxes in CLIR?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Magdy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd European Conference on Information Retrieval, ECIR &apos;11</title>
		<meeting>the 33rd European Conference on Information Retrieval, ECIR &apos;11</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="683" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">An Evaluation of Factors Affecting Document Ranking by Information Retrieval Systems. ERIC reports. School of Information Studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mcgill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Koll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Noreault</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
		<respStmt>
			<orgName>Syracuse University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Markov random field model for term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W. Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;05</title>
		<meeting>the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;05</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Duchesnay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Scikit-learn: Machine learning in python. CoRR, abs/1201.0490</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Multilingual Information Retrieval-From Research To Practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The effects of query structure and dictionary-setups in dictionary-based crosslanguage information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Pirkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;98</title>
		<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;98</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Readings in information retrieval. chapter An Algorithm for Suffix Stripping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<biblScope unit="page" from="313" to="316" />
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Combining automatic and manual index representations in probabilistic retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Rajashekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Soc. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="272" to="283" />
			<date type="published" when="1995-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Boschee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjorie</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Macbride</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Zamanian</surname></persName>
		</author>
		<editor>J. Olive et al.</editor>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="626" to="631" />
		</imprint>
	</monogr>
	<note>Serif language processing-effective trainable language understanding</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Report on CLEF-2001 experiments: Effective combined query-translation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="27" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A comparison of statistical significance tests for information retrieval evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM conference on Conference on Information and Knowledge Management, CIKM &apos;07</title>
		<meeting>the 16th ACM conference on Conference on Information and Knowledge Management, CIKM &apos;07</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="623" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Combining statistical translation techniques for cross-language information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhan</forename><surname>Ture</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Computational Linguistics, COLING &apos;12</title>
		<meeting>the 24th International Conference on Computational Linguistics, COLING &apos;12</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2685" to="2702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Inference networks for document retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W. Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;90</title>
		<meeting>the 13th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;90<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Empirical studies on the impact of lexical resources on CLIR performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="475" to="487" />
			<date type="published" when="2005-05" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
