<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Neural Word Segmentation for Chinese via Segmental Language Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
							<email>zhdeng@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Neural Word Segmentation for Chinese via Segmental Language Modeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4915" to="4920"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4915</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Previous traditional approaches to unsuper-vised Chinese word segmentation (CWS) can be roughly classified into discriminative and generative models. The former uses the carefully designed goodness measures for candidate segmentation, while the latter focuses on finding the optimal segmentation of the highest generative probability. However, while there exists a trivial way to extend the dis-criminative models into neural version by using neural language models, those of genera-tive ones are non-trivial. In this paper, we propose the segmental language models (SLMs) for CWS. Our approach explicitly focuses on the segmental nature of Chinese, as well as preserves several properties of language models. In SLMs, a context encoder encodes the previous context and a segment decoder generates each segment incrementally. As far as we know, we are the first to propose a neu-ral model for unsupervised CWS and achieve competitive performance to the state-of-the-art statistical models on four different datasets from SIGHAN 2005 bakeoff.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unlike English and many other languages, Chi- nese sentences have no explicit word boundaries. Therefore, Chinese Word Segmentation (CWS) is a crucial step for many Chinese Natural Language Processing (NLP) tasks such as syntactic pars- ing, information retrieval and word representation learning ( <ref type="bibr" target="#b8">Grave et al., 2018)</ref>.</p><p>Recently, neural approaches for supervised CWS are attracting huge interest. A great quan- tities of neural models, e.g., tensor neural network ( ), recursive neural network <ref type="bibr" target="#b3">(Chen et al., 2015a</ref>), long-short-term-memory (RNN- LSTM) <ref type="bibr" target="#b4">(Chen et al., 2015b</ref>) and convolutional neural network (CNN) ( <ref type="bibr" target="#b22">Wang and Xu, 2017)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segment Decoder</head><p>Figure 1: A Segmental Language Model (SLM) works on y = y 1 y 2 y 3 y 4 with the candidate segmentation y 1 , y 2:3 and y 4 , where y 0 is an additional start sym- bol which is kept same for all sentences.</p><p>been proposed and given competitive results to the best statistical models <ref type="bibr" target="#b19">(Sun, 2010)</ref>. However, the neural approaches for unsupervised CWS have not been investigated.</p><p>Previous unsupervised approaches to CWS can be roughly classified into discriminative and gen- erative models. The former uses carefully de- signed goodness measures for candidate segmen- tation, while the latter focuses on designing sta- tistical models for Chinese and finds the optimal segmentation of the highest generative probability.</p><p>Popular goodness measures for discriminative models include Mutual Information (MI) <ref type="bibr" target="#b1">(Chang and Lin, 2003)</ref>, normalized Variation of Branch- ing Entropy (nVBE) <ref type="bibr" target="#b14">(Magistry and Sagot, 2012)</ref> and Minimum Description Length (MDL) <ref type="bibr" target="#b15">(Magistry and Sagot, 2013)</ref>. There is a trivial way to extend these statistical discriminative approaches, because we can simply replace the n-gram lan- guage models in these approaches by neural lan- guage models ( <ref type="bibr" target="#b0">Bengio et al., 2003)</ref>. There may exists other more sophisticated neural discrimina- tive approaches, but it is not the focus of this paper.</p><p>For generative approaches, typical statistical models includes Hidden Markov Model (HMM) <ref type="bibr" target="#b2">(Chen et al., 2014</ref>), Hierarchical Dirichlet Pro- cess (HDP) <ref type="bibr" target="#b7">(Goldwater et al., 2009</ref>) and Nested Pitman-Yor Process (NPY) ( <ref type="bibr" target="#b17">Mochihashi et al., 2009)</ref>. However, none of them can be easily ex- tended into a neural model. Therefore, neural gen- erative models for word segmentation are remain- ing to be investigated.</p><p>In this paper, we proposed the Segmental Lan- guage Models (SLMs), a neural generative model that explicitly focuses on the segmental nature of Chinese: SLMs can directly generate segmented sentences and give the corresponding generative probability. We evaluate our methods on four dif- ferent benchmark datasets from <ref type="bibr">SIGHAN 2005</ref><ref type="bibr">bakeoff (Emerson, 2005</ref>, namely PKU, MSR, AS and CityU. To our knowledge, we are the first to propose a neural model for unsupervised Chinese word segmentation and achieve competitive per- formance to the state-of-the-art statistical models on four different datasets. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Segmental Language Models</head><p>In this section, we present our segmental language models (SLMs). Notice that in Chinese NLP, char- acters are the atom elements. Thus in the context of CWS, we use "character" instead of "word" for language modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Language Models</head><p>The goal of language modeling is to learn the joint probability function of sequences of characters in a language. However, This is intrinsically diffi- cult because of the curse of dimensionality. Tradi- tional approaches obtain generalization based on n-grams, while neural approaches introduce a dis- tributed representation for characters to fight the curse of dimensionality.</p><p>A neural Language Model (LM) can give the conditional probability of the next character given the previous ones, and is usually implemented by a Recurrent Neural Network (RNN):</p><formula xml:id="formula_0">h t = f (y t−1 , h t−1 ) (1) p(y t |y 1:t−1 ) = g(h t , y t )<label>(2)</label></formula><p>where y t is the distributed representation for the t th character and h t represents the information of the previous characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Segmental Language Models</head><p>Similar to neural language modeling, the goal of segmental language modeling is to learn the joint probability function of the segmented sequences of characters. Thus, for each segment, we have:</p><formula xml:id="formula_1">ˆ p(y (i) t |y (i) 1:t−1 , y (1:i−1) ) = g(h (i) t , y (i) t )<label>(3)</label></formula><p>where</p><formula xml:id="formula_2">y (i)</formula><p>t is the distributed representation for the t th character in the i th segment and y (1:i−1) is the previous segments. And the concatenation of all segments y</p><formula xml:id="formula_3">(i) 1:T i</formula><p>is exactly the whole sentence y 1:T , where T i is the length of the i th segment y (i) , T is the length of the sentence y.</p><p>Moreover, we introduce a context encoder RNN to process the character sequence y (1:i−1) in order to make y Notice that although we have an encoder and the segment decoder g, SLM is not an encoder- decoder model. Because the content that the de- coder generates is not the same as what the en- coder provides. <ref type="figure">Figure 1</ref> illustrates how SLMs work with a can- didate segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Properties of SLMs</head><p>However, in unsupervised scheme, the given sen- tences are not segmented. Therefore, the probabil- ity for SLMs to generate a given sentence is the joint probability of all possible segmentation:</p><formula xml:id="formula_4">p(y 1:T ) = T 1 ,T 2 ,... i ˆ p(y (i) 1:T i ) = T 1 ,T 2 ,... i T i +1 t=1ˆp t=1ˆ t=1ˆp(y (i) t |y (i) 0:t−1 ) (4)</formula><p>where y Moreover, for sentence generation, SLMs are able to generate arbitrary sentences by generating segments one by one and stopping when gener- ating end of sentence symbol EOS. In addi- tion, the time complexity is linear to the length of the generated sentence, as we can keep the hid- den state of the context encoder RNN and update it when generating new words.</p><p>Last but not least, it is easy to verify that SLMs preserve the probabilistic property of language models:</p><formula xml:id="formula_5">i P (s i ) = 1<label>(5)</label></formula><p>where s i enumerates all possible sentences. In summary, the segmental language models can perfectly substitute vanilla language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training and Decoding</head><p>Similar to language model, the training is achieved by maximizing the training corpus log-likelihood:</p><formula xml:id="formula_6">L = − log p(y 1:T )<label>(6)</label></formula><p>Luckily, we can compute the loss objective function in linear time complexity using dynamic programming, given the initial condition that p(y 1:0 ) = 1:</p><formula xml:id="formula_7">p(y 1:n ) = K k=1 p(y 1:n−k )ˆ p(y n−k+1:n ) (7)</formula><p>where p(·) is the joint probability of all possible segmentation, ˆ p(·) is the probability of one seg- ment and K is the maximal length of the segments.</p><p>We can also find the segmentation with maxi- mal probability (namely, decoding) in linear time using dynamic programming in the similarly way with ¯ p(y 1:0 ) = 1:</p><formula xml:id="formula_8">¯ p(y 1:n ) = K max k=1 ¯ p(y 1:n−k )ˆ p(y n−k+1:n )<label>(8)</label></formula><formula xml:id="formula_9">δ(y 1:n ) = arg K max k=1 ¯ p(y 1:n−k )ˆ p(y n−k+1:n ) (9)</formula><p>where ¯ p is the probability of the best segmenta- tion and δ is used to trace back the decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings and Detail</head><p>We evaluate our models on SIGHAN 2005 bake- off ( <ref type="bibr" target="#b5">Emerson, 2005)</ref>   <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997</ref>) is used as the segment decoder and a 1-layer LSTM is used as the context encoder.</p><note type="other">datasets and replace all the punctuation marks with punc, English charac- ters with eng and Arabic numbers with num F1 score PKU MSR AS CityU HDP 68.7 69.9 - - HDP + HMM 75.3 76.3 - - ESA 77.8 80.1 78.5 76.0 NPY-3 - 80.7 - 81.7 NPY-2 - 80.2 - 82.4 nVBE 80.0 81.3 76.6 76.7 Joint 81.1 81.7 - - SLM-2 80.2 78.5 79.4 78.2 SLM-3 79.8 79.4 80.3 80.5 SLM-4 79.2 79.0 79.8 79.7</note><p>We use stochastic gradient decent with a mini- batch size of 256 and a learning rate of 16.0 to op- timize the model parameters in the first 400 steps, then we use Adam ( <ref type="bibr" target="#b11">Kingma and Ba, 2014</ref>) with a learning rate of 0.005 to further optimize the models. Model parameters are initialized by nor- mal distributions as <ref type="bibr" target="#b6">Glorot and Bengio (2010)</ref> sug- gested. We use a gradient clip = 0.1 and apply a dropout with dropout rate = 0.1 to the character embedding and RNNs to prevent over-fit.</p><p>The standard word precision, recall and F1 mea- sures ( <ref type="bibr" target="#b5">Emerson, 2005</ref>) are used to evaluate seg- mentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results and Analysis</head><p>Our final results are shown in <ref type="table" target="#tab_1">Table 1</ref>, which lists the results of several previous state-of-the- art methods 2 , where we mark the best results in   <ref type="table">Table 2</ref>: Results of SLM-4 incorporating ad hoc guide- lines, where † represents using additional 1024 seg- mented setences for training data and * represents using a rule-based post-processing boldface. We test the proposed SLMs with differ- ent maximal segment length K = 2, 3, 4 and use "SLM-K" to denote the corresponding model. We do not try K &gt; 4 because there are rare words that consist more than 4 characters. As can be seen, it is hard to predict what choice of K will give the best performance. This is be- cause the exact definition of what a word remains hard to reach and different datasets follow differ- ent guidelines. <ref type="bibr" target="#b25">Zhao and Kit (2008)</ref> use cross- training of a supervised segmentation system in order to have an estimation of the consistency be- tween different segmentation guidelines and the average consistency is found to be as low as 85 (f-score). Therefore, this can be regarded as a top line for unsupervised CWS. <ref type="table" target="#tab_1">Table 1</ref> shows that SLMs outperform previous best discriminative and generative models on PKU and AS datasets. This might be due to that the segmentation guideline of our models are closer to these two datasets.</p><p>Moreover, in the experiments, we observe that Chinese particles often attach other words, for ex- ample, "" following adjectives and "" follow- ing verbs. It is hard for our generative models to split them apart. Therefore, we propose a rule- based post-processing module to deal with this problem, where we explicitly split the attached particles from other words. <ref type="bibr">3</ref> The post-processing is applied on the results of "SLM-4". In addi- tion, we also evaluate "SLM-4" using the first 1024 sentences of the segmented training datasets (about 5.4% of PKU, 1.2% of MSR, 0.1% of AS and 1.9% of CityU) for training, in order to teach "SLM-4" the corresponding ad hoc segmentation guidelines. <ref type="table">Table 2</ref> shows the results.</p><p>We can find from the table that only 1024 guideline sentences can improve the performance of "SLM-4" significantly.</p><p>While rule-based <ref type="bibr">3</ref> The rules we use are listed in the appendix at https: //github.com/Edward-Sun/SLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error</head><p>SLM-2 SLM-3 SLM- <ref type="table" target="#tab_1">4  Insertion  7866  4803  3519  Deletion  3855  7518  8851   Table 3</ref>: Statistics of insertion errors and deletion errors that SLM-K produces on PKU dataset post-processing is very effective, "SLM-4 †" can outperform "SLM-4*" on all the four datasets. Moreover, performance drops when applying the rule-based post-processing to "SLM-4 †" on three datasets. These indicate that SLMs can learn the empirical rules for word segmentation given only a small amount of training data. And these guide- line data can improve the performance of SLMs naturally, superior to using explicit rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Effect of the Maximal Segment Length</head><p>The maximal segment length K represents the prior knowledge we have for Chinese word seg- mentation. For example K = 3 represents that there are only unigrams, bigrams and trigrams in the text. While there do exist words that con- tain more than four characters, most of the Chi- nese words are unigram or bigram. Therefore, K denotes a trade-off between the accuracy of short words and long words. Specifically, we investigate two major segmen- tation problems that might affect the accuracy of word segmentation performance, namely, inser- tion errors and deletion errors. An insertion error insert a segment in a word, which split a correct word. And an deletion error delete the segment between two words, which results in a composi- tion error <ref type="bibr" target="#b13">(Li and Yuan, 1998)</ref>. <ref type="table">Table 3</ref> shows the statistics of different errors on PKU of our model with different K. We can observe that insertion er- ror rate decrease with the increase of K, while the deletion error rate increase with the increase of K.</p><p>We also provide some examples in <ref type="table" target="#tab_3">Table 4</ref>, which are taken from the results of our models. It clearly illustrates that different K could result in different errors. For example, there is an insertion error on "" by SML-2, and a deletion error on "" and "" by SLM-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Generative Models for CWS <ref type="bibr" target="#b7">Goldwater et al. (2009)</ref> are the first to proposed a generative model for unsupervised word segmentation. They</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Example SLM-2 SLM-3 SLM-4 Gold  <ref type="formula" target="#formula_0">(2009)</ref> proposed a Bayesian hier- archical language model using Pitman-Yor (PY) process, which can generate sentences hierarchi- cally. <ref type="bibr" target="#b2">Chen et al. (2014)</ref> proposed a Bayesian HMM model for unsupervised CWS inspired by the character-based scheme in supervised CWS task, where the hidden state of charaters are set to {Single, Begin, End, Middle} to represents their corresponding positions in the words. The seg- mental language model is not a neural extension of the above statistical models, as we model the segments directly.</p><p>Segmental Sequence Models Sequence model- ing via segmentations has been well investigated by , where they proposed the Sleep-AWake Network (SWAN) for speech recog- nition. SWAN is similar to SLM. However, SLMs do not have sleep-awake states. And SLMs pre- dict the following segment given the previous con- text while SWAN tries to recover the information in the encoded state. Therefore, the key differ- ence is that SLMs are unsupervised language mod- els while SWANs are supervised seq2seq models. Thereafter,  successfully apply SWAN in their phrase-based machine translation. Another related work in machine translation is the online segment to segment neural transduction ( <ref type="bibr" target="#b24">Yu et al., 2016</ref>), where the model is able to capture un- bounded dependencies in both the input and output sequences. <ref type="bibr" target="#b12">Kong (2017)</ref> also proposed a Segmen- tal Recurrent Neural Network (SRNN) with CTC to solve segmental labeling problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a neural generative model for fully unsupervised Chinese word seg- mentation (CWS). To the best of knowledge, this is the first neural model for CWS. Our segmen- tal language model is an intuitive generalization of vanilla neural language models that directly modeling the segmental nature of Chinese. Ex- perimental results show that our models achieve competitive performance to the previous state-of- the-art statistical models on four datasets from SIGHAN 2005. We also show the improvement of incorporating ad hoc guidelines into our segmen- tal language models. Our future work may include the following two directions.</p><p>• In this work, we only consider the sequential segmental language modeling. In the future, we are interested in build a hierarchical neu- ral language model like the Pitman-Yor pro- cess.</p><p>• Like vanilla language models, the segmental language models can also provide useful in- formation for semi-supervised learning tasks. It would also be interesting to explore our models in the semi-supervised schemes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>T i +1 = eos is the end of segment symbol at the end of each segment, and y (i) 0 is the context representation of y (1:i−1) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>F1</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Main results on SIGHAN 2005 bakeoff 
datasets with previous state-of-the-art models (Chen 
et al., 2014; Wang et al., 2011; Mochihashi et al., 2009; 
Magistry and Sagot, 2012) 

for all text and only consider segment the text be-
tween punctuations. Following Chen et al. (2014) 
, we use both training data and test data for train-
ing and only test data are used for evaluation. In 
order to make a fair comparison with the previous 
works, we do not consider using other larger raw 
corpus. 
We apply word2vec (Mikolov et al., 2013) on 
Chinese Gigaword corpus (LDC2011T13) to get 
pretrained embedding of characters. 
A 2-layer LSTM </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Examples of segmentation with different maximal segment length K 

built a nonparametric Bayesian bigram language 
model based on HDP (Teh et al., 2005). Mochi-
hashi et al. </table></figure>

			<note place="foot" n="1"> Our implementation can be found at https:// github.com/Edward-Sun/SLM</note>

			<note place="foot" n="2"> Magistry and Sagot (2012) evaluated their nVBE on the training data, and the joint model of Chen et al. (2014) combine HDP+HMM and is initialized with nVBE, so in principle these results can not be compared directly.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by the National Train-ing Program of Innovation for Undergraduates (URTP2017PKU001). We would also like to thank the anonymous reviewers for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised word segmentation without dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tracy</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ROCLING 2003 Poster Papers</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="355" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A joint model for unsupervised chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaohong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="854" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gated recursive neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1744" to="1753" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory neural networks for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1197" to="1206" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The second international chinese word segmentation bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Emerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth SIGHAN workshop on Chinese language Processing</title>
		<meeting>the fourth SIGHAN workshop on Chinese language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="123" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A bayesian framework for word segmentation: Exploring the effects of context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="54" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning word vectors for 157 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation (LREC</title>
		<meeting>the International Conference on Language Resources and Evaluation (LREC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Computer science ¿ computation and language towards neural phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sitao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno>arxiv.org/abs/1706.05565</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Neural Representation Learning in Linguistic Structured Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>Google Research</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Pacific Asia Conference on Language, Information and Computation</title>
		<meeting>the 12th Pacific Asia Conference on Language, Information and Computation</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="212" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervized word segmentation: the case for mandarin chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Magistry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoˆıtbenoˆıt</forename><surname>Sagot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="383" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Can mdl improve unsupervised chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Magistry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoˆıtbenoˆıt</forename><surname>Sagot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Joint Conference on Natural Language Processing: Sighan workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bayesian unsupervised word segmentation with nested pitman-yor language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daichi</forename><surname>Mochihashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naonori</forename><surname>Ueda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="100" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Maxmargin tensor neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="293" to="303" />
		</imprint>
	</monogr>
	<note>Baltimore</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Word-based and character-based word segmentation models: Comparison and combination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling 2010: Posters</title>
		<meeting><address><addrLine>Beijing, China. Coling</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1211" to="1219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sharing clusters among related groups: Hierarchical dirichlet processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Yee W Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07463</idno>
		<title level="m">Sequence modeling via segmentations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional Neural Network with Word Embeddings for Chinese Word Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Joint Conference on Natural Language Processing</title>
		<meeting>the 8th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A new unsupervised approach to word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanshi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="421" to="454" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Shiping Tang, and Xiaozhong Fan</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Online segment to segment neural transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08194</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An empirical comparison of goodness measures for unsupervised chinese word segmentation with a unified framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Kit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Joint Conference on Natural Language Processing</title>
		<meeting>the Third International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
