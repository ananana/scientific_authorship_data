<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:13+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Capitalize with Character-Level Recurrent Neural Networks: An Empirical Study</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">Hendy</forename><surname>Susanto</surname></persName>
							<email>{raymond susanto,luwei}@sutd.edu.sg chaileon@dso.org.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Singapore University of Technology and Design ‡ DSO National Laboratories</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><forename type="middle">Leong</forename><surname>Chieu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Singapore University of Technology and Design ‡ DSO National Laboratories</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Singapore University of Technology and Design ‡ DSO National Laboratories</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Capitalize with Character-Level Recurrent Neural Networks: An Empirical Study</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2090" to="2095"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we investigate case restoration for text without case information. Previous such work operates at the word level. We propose an approach using character-level recurrent neural networks (RNN), which performs competitively compared to language model-ing and conditional random fields (CRF) approaches. We further provide quantitative and qualitative analysis on how RNN helps improve truecasing.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural language texts (e.g., automatic speech tran- scripts or social media data) often come in non- standard forms, and normalization would typically improve the performance of downstream natural lan- guage processing (NLP) applications. This paper in- vestigates a particular sub-task in text normalization: case restoration or truecasing. Truecasing refers to the task of restoring case information (uppercase or lowercase) of characters in a text corpus. Case infor- mation is important for certain NLP tasks. For ex- ample, <ref type="bibr" target="#b2">Chieu and Ng (2002)</ref> used unlabeled mixed case text to improve named entity recognition (NER) on uppercase text.</p><p>The task often presents ambiguity: consider the word "apple" in the sentences "he bought an apple" and "he works at apple". While the former refers to a fruit (hence, it should be in lowercase), the latter refers to a company name (hence, it should be cap- italized). Moreover, we often need to recover the case information for words that are previously un- seen by the system.</p><p>In this paper, we propose the use of character- level recurrent neural networks for truecasing. Pre- vious approaches for truecasing are based on word level approaches which assign to each word one of the following labels: all lowercase, all upper- case, initial capital, and mixed case. For mixed case words, an additional effort has to be made to decipher exactly how the case is mixed (e.g., MacKenzie). In our approach, we propose a gen- erative, character-based recurrent neural network (RNN) model, allowing us to predict exactly how cases are mixed in such words.</p><p>Our main contributions are: (i) we show that character-level approaches are viable compared to word-level approaches, (ii) we show that character- level RNN has a competitive performance compared to character-level CRF, and (iii) we provide our quantitative and qualitative analysis on how RNN helps improve truecasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Word-based truecasing The most widely used approach works at the word level. The simplest ap- proach converts each word to its most frequently seen form in the training data. One popular ap- proach uses HMM-based tagging with an N-gram language model, such as in ( <ref type="bibr">Lita et al., 2003;</ref><ref type="bibr" target="#b15">Nebhi et al., 2015)</ref>. Others used a discriminative tagger, such as MEMM ( <ref type="bibr" target="#b1">Chelba and Acero, 2006</ref>) or CRF ( <ref type="bibr" target="#b24">Wang et al., 2006</ref>). Another approach uses statisti- cal machine translation to translate uncased text into a cased one. Interestingly, no previous work oper- ated at the character level. <ref type="bibr" target="#b15">Nebhi et al. (2015)</ref> in- vestigated truecasing in tweets, where truecased cor-pora are less available.</p><p>Recurrent neural networks Recent years have shown a resurgence of interest in RNN, particularly variants with long short-term memory <ref type="bibr" target="#b6">(Hochreiter and Schmidhuber, 1997</ref>) or gated recurrent units ( <ref type="bibr" target="#b3">Cho et al., 2014</ref>). RNN has shown an impressive performance in various NLP tasks, such as machine translation <ref type="bibr" target="#b3">(Cho et al., 2014;</ref><ref type="bibr" target="#b11">Luong et al., 2015)</ref>, language modeling ( <ref type="bibr" target="#b13">Mikolov et al., 2010;</ref><ref type="bibr" target="#b8">Kim et al., 2016)</ref>, and constituency parsing ( <ref type="bibr" target="#b23">Vinyals et al., 2015)</ref>. Nonetheless, understanding the mechanism behind the successful applications of RNN is rarely studied. In this work, we take a closer look at our trained model to interpret its internal mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Truecasing Systems</head><p>In this section, we describe the truecasing systems that we develop for our empirical study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word-Level Approach</head><p>A word-level approach truecases one word at a time. The first system is a tagger based on HMM <ref type="bibr" target="#b20">(Stolcke, 2002</ref>) that translates an uncased sequence of words to a corresponding cased sequence. An N- gram language model trained on a cased corpus is used for scoring candidate sequences. For decoding, the Viterbi algorithm <ref type="bibr" target="#b18">(Rabiner, 1989)</ref> computes the highest scoring sequence.</p><p>The second approach is a discriminative classifier based on linear chain CRF ( <ref type="bibr" target="#b9">Lafferty et al., 2001</ref>). In this approach, truecasing is treated as a sequence labeling task, labelling each word with one of the following labels: all lowercase, all uppercase, initial capital, and mixed case. For our experiments, we used the truecaser in Stanford's NLP pipeline <ref type="bibr" target="#b12">(Manning et al., 2014)</ref>. Their model includes a rich set of features ( <ref type="bibr" target="#b5">Finkel et al., 2005</ref>), such as surrounding words, character N-grams, word shape, etc.</p><p>Dealing with mixed case Both approaches re- quire a separate treatment for mixed case words. In particular, we need a gazetteer that maps each word to its mixed case form -either manually cre- ated or statistically collected from training data. The character-level approach is motivated by this: In- stead of treating them as a special case, we train our model to capitalize a word character by character.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Character-Level Approach</head><p>A character-level approach converts each character to either uppercase or lowercase. In this approach, mixed case forms are naturally taken care of, and moreover, such models would generalize better to unseen words. Our third system is a linear chain CRF that makes character-level predictions. Simi- lar to the word-based CRF, it includes surrounding words and character N-grams as features.</p><p>Finally, we propose a character-level approach us- ing an RNN language model. RNN is particularly useful for modeling sequential data. At each time step t, it takes an input vector x t and previous hid- den state h t−1 , and produces the next hidden state h t . Different recurrence formulations lead to differ- ent RNN models, which we will describe below.</p><p>Long short-term memory (LSTM) is an archi- tecture proposed by <ref type="bibr" target="#b6">Hochreiter and Schmidhuber (1997)</ref>. It augments an RNN with a memory cell vector c t in order to address learning long range dependencies. The content of the memory cell is updated additively, mitigating the vanishing gradi- ent problem in vanilla RNNs ( <ref type="bibr" target="#b0">Bengio et al., 1994)</ref>. Read, write, and reset operations to the memory cell are controlled by input gate i, output gate o, and for- get gate f . The hidden state is computed as:</p><formula xml:id="formula_0">i t = σ(W i h t−1 + U i x t )<label>(1)</label></formula><formula xml:id="formula_1">o t = σ(W o h t−1 + U o x t )<label>(2)</label></formula><formula xml:id="formula_2">f t = σ(W f h t−1 + U f x t ) (3) g t = tanh(W g h t−1 + U g x t )<label>(4)</label></formula><formula xml:id="formula_3">c t = f t c t−1 + i t g t (5) h t = o t tanh(c t )<label>(6)</label></formula><p>where σ and tanh are element-wise sigmoid and hy- perbolic tangent functions, and W j and U j are pa- rameters of the LSTM for j ∈ {i, o, f, g}. Gated recurrent unit (GRU) is a gating mech- anism in RNN that was introduced by <ref type="bibr" target="#b3">Cho et al. (2014)</ref>. They proposed a hidden state computation with reset and update gates, resulting in a simpler LSTM variant:  <ref type="table">Table 2</ref>: Truecasing performance in terms of precision (P), recall (R), and F 1 . All improvements of the best performing character-based systems (bold) over the best performing word-based systems (underlined) are statistically significant using sign test (p &lt; 0.01). All improvements of the best performing RNN systems (italicized) over CRF-CHAR are statistically significant using sign test (p &lt; 0.01).</p><formula xml:id="formula_4">r t = σ(W r h t−1 + U r x t )<label>(7)</label></formula><formula xml:id="formula_5">z t = σ(W z h t−1 + U z x t )<label>(8)</label></formula><formula xml:id="formula_6">h t = tanh(W h (r t h t−1 ) + U h x t )<label>(9)</label></formula><formula xml:id="formula_7">h t = (1 − z t ) h t−1 + z t h t (10) 2091 EN-Wikipedia EN-WSJ EN-Reuters DE-ECI Acc. P R F 1 Acc. P R F 1 Acc. P R F 1 Acc. P R F 1 Word-based Approach LM (N = 3) 94</formula><p>At each time step, the conditional probability dis- tribution over next characters is computed by linear projection of h t followed by a softmax:</p><formula xml:id="formula_8">P (x t = k|x 1:t−1 ) = exp(w k h t ) |V | j=1 exp(w j h t )<label>(11)</label></formula><p>where w k is the k-th row vector of a weight matrix W . The probability of a sequence of characters x 1:T is defined as:</p><formula xml:id="formula_9">P (x 1:T ) = T t=1 P (x t |x 1:t−1 )<label>(12)</label></formula><p>Similar to the N-gram language modeling approach we described previously, we need to maximize Equation 12 in order to decode the most probable cased sequence. Instead of Viterbi decoding, we ap- proximate this using a beam search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Tools</head><p>Our approach is evaluated on English and German datasets. For English, we use a Wikipedia corpus from ( <ref type="bibr" target="#b4">Coster and Kauchak, 2011)</ref> 3.6.0 ( <ref type="bibr" target="#b5">Finkel et al., 2005</ref>). We use a recommended configuration for training the truecaser.We use CRF- Suite version 0.12 <ref type="bibr" target="#b16">(Okazaki, 2007)</ref> to train the character-based CRF model. Our feature set in- cludes character N-grams (N ∈ {1, 2, 3}) and word N-grams (N ∈ {1, 2}) surrounding the current char- acter. We tune the 2 regularization parameter λ us- ing a grid search where λ ∈ {0.01, 0.1, 1, 10}.</p><p>We use an open-source character RNN imple- mentation. <ref type="bibr">2</ref> We train a SMALL model with 2 lay- ers and 300 hidden nodes, and a LARGE model with 3 layers and 700 hidden nodes. We also vary the hidden unit type (LSTM/GRU). The network is trained using truncated backpropagation for 50 time steps. We use a mini-batch stochastic gradient descent with batch size 100 and RMSprop update <ref type="bibr" target="#b21">(Tieleman and Hinton, 2012)</ref>. We use dropout reg- ularization ( <ref type="bibr" target="#b19">Srivastava et al., 2014</ref>) with 0.25 prob- ability. We choose the model with the smallest val- idation loss after 30 epochs. For decoding, we set beam size to 10. The experimental settings are re- ported in more depth in the supplementary materi- als. Our system and code are publicly available at http://statnlp.org/research/ta/.  <ref type="table">Table 2</ref> shows the experiment results in terms of pre- cision, recall, and F 1 . Most previous work did not evaluate their approaches on the same dataset. We compare our work to <ref type="bibr" target="#b1">(Chelba and Acero, 2006</ref>) us- ing the same WSJ sections for training and evalua- tion on 2M word training data. Chelba and Acero only reported error rate, and all our RNN and CRF approaches outperform their results in terms of error rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>First, the word-based CRF approach gives up to 8% relative F 1 increase over the LM approach. Other than WSJ, moving to character level further improves CRF by 1.1-3.7%, most notably on the German dataset. Long compound nouns are com- mon in the German language, which generates many out-of-vocabulary words. Thus, we hypothesize that character-based approach improves generalization. Finally, the best F 1 score for each dataset is achieved by the RNN variants: 93.19% on EN-Wiki, 92.43% on EN-WSJ, 93.79% on EN-Reuters, and 98.01% on DE-ECI.</p><p>We highlight that different features are used in CRF-WORD and CRF-CHAR. CRF-CHAR only includes simple features, namely character and word N-grams and sentence boundary indicators. In con- trast, CRF-WORD contains a richer feature set that is predefined in Stanford's truecaser. For instance, it includes word shape, in addition to neighboring words and character N-grams. It also includes more feature combinations, such as the concatenation of the word shape, current label, and previous label. Nonetheless, CRF-CHAR generally performs better than CRF-WORD. Potentially, CRF-CHAR can be improved further by using larger N-grams. The de- cision to use simple features is for optimizing the training speed. Consequently, we are able to dedi- cate more time for tuning the regularization weight.</p><p>Training a larger RNN model generally improves performance, but it is not always the case due to possible overfitting. LSTM seems to work better than GRU in this task. The GRU models have 25% less parameters. In terms of training time, it took 12 hours to train the largest RNN model on a sin- gle Titan X GPU. For comparison, the longest train- ing time for a single CRF-CHAR model is 16 hours. Training LM and CRF-WORD is much faster: 30 seconds and 5.5 hours, respectively, so there is a speed-accuracy trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Visualizing LSTM Cells</head><p>An interesting component of LSTM is its memory cells, which is supposed to store long range depen- dency information. Many of these memory cells are not human-interpretable, but after introspecting our trained model, we find a few memory cells that are sensitive to case information. In <ref type="figure" target="#fig_0">Figure 1</ref>, we plot the memory cell activations at each time step (i.e., tanh(c t )). We can see that these cells activate differ- ently depending on the case information of a word (towards -1 for uppercase and +1 for lowercase).  In this section, we analyze the system perfor- mance on each case category. First, we report the percentage distribution of the case categories in each test set in <ref type="table" target="#tab_3">Table 3</ref>. For both languages, the most fre- quent case category is lowercase, followed by capi- talization, which generally applies to the first word  in the sentence and proper nouns. The uppercase form, which is often found in abbreviations, occurs more frequently than mixed case for English, but the other way around for German.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Case Category and OOV Performance</head><p>Figure 2 (a) shows system accuracy on mixed case words. We choose the best performing LM and RNN for each dataset. Character-based ap- proaches have a better performance on mixed case words than word-based approaches, and RNN gen- erally performs better than CRF. In CRF-WORD, surface forms are generated after label prediction. This is more rigid compared to LM, where the sur- face forms are considered during decoding.</p><p>In addition, we report system accuracy on capi- talized words (first letter uppercase) and uppercase words in <ref type="figure" target="#fig_1">Figure 2</ref> (b) and (c), respectively. RNN performs the best on capitalized words. On the other hand, CRF-WORD performs the best on uppercase. We believe this is related to the rare occurrences of uppercase words during training, as shown in Ta- ble 3. Although mixed case occurs more rarely in general, there are important clues, such as charac- ter prefix. CRF-CHAR and RNN have comparable performance on uppercase. For instance, there are only 2 uppercase words in WSJ that were predicted differently between CRF-CHAR and RNN. All sys- tems perform equally well (∼99% accuracy) on low- ercase. Overall, RNN has the best performance.</p><p>Last, we present results on out-of-vocabulary (OOV) words with respect to the training set. The statistics of OOV words is given in <ref type="table" target="#tab_3">Table 3</ref>. The sys- tem performance across datasets is reported in <ref type="figure" target="#fig_1">Fig- ure 2 (d)</ref>. We observe that RNN consistently per- forms better than the other systems, which shows that it generalizes better to unseen words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we conduct an empirical investiga- tion of truecasing approaches. We have shown that character-level approaches work well for truecasing, and that RNN performs competitively compared to language modeling and CRF. Future work includes applications in informal texts, such as tweets and short messages <ref type="bibr" target="#b14">(Muis and Lu, 2016</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Cells that are sensitive to lowercased and capitalized words. Text color represents activations (−1 ≤ tanh(ct) ≤ 1): positive is blue, negative is red. Darker color corresponds to greater magnitude.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Accuracy on mixed case (a), capitalized (b), uppercase (c), and OOV words (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : Percentage distribution of the case categories and OOV words</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> https://github.com/karpathy/char-rnn</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would also like to thank the anonymous review-ers for their helpful comments. This work is sup-ported by MOE Tier 1 grant SUTDT12015008.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptation of maximum entropy capitalizer: Little data can help a lot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="382" to="399" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Teaching a weaker classifier: Named entity recognition on upper case text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Leong Chieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="481" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simple English Wikipedia: A new text simplification task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Coster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kauchak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-HLT</title>
		<meeting>ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="665" to="669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by Gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visualizing and understanding recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucian</forename><forename type="middle">Vlad</forename><surname>Lita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
		<title level="m">Salim Roukos, and Nanda Kambhatla. 2003. tRuEcasIng. In Proceedings of ACL</title>
		<imprint>
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Christopher D Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL System Demonstrations</title>
		<meeting>ACL System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`nock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Weak semiMarkov CRFs for noun phrase chunking in informal text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aldrian</forename><surname>Obaja Muis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Restoring capitalization in #tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamel</forename><surname>Nebhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalina</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><surname>Gorrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW Companion</title>
		<meeting>WWW Companion</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1111" to="1115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">CRFsuite: A fast implementation of conditional random fields (CRFs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The design for the Wall Street Journal-based CSR corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><forename type="middle">M</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Speech and Natural Language</title>
		<meeting>the Workshop on Speech and Natural Language</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="357" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A tutorial on hidden Markov models and selected applications in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lawrence R Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="257" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SRILM-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICSLP</title>
		<meeting>ICSLP</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="901" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik F Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2755" to="2763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Capitalizing machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
