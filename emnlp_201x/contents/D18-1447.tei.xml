<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Supervised Learning for Neural Keyphrase Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University Boston</orgName>
								<address>
									<postCode>02115</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
							<email>luwang@ccs.neu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University Boston</orgName>
								<address>
									<postCode>02115</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-Supervised Learning for Neural Keyphrase Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4142" to="4153"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4142</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We study the problem of generating keyphrases that summarize the key points for a given document. While sequence-to-sequence (seq2seq) models have achieved remarkable performance on this task (Meng et al., 2017), model training often relies on large amounts of labeled data, which is only applicable to resource-rich domains. In this paper, we propose semi-supervised keyphrase generation methods by leveraging both labeled data and large-scale unlabeled samples for learning. Two strategies are proposed. First, unlabeled documents are first tagged with synthetic keyphrases obtained from unsupervised keyphrase extraction methods or a self-learning algorithm, and then combined with labeled samples for training. Furthermore, we investigate a multi-task learning framework to jointly learn to generate keyphrases as well as the titles of the articles. Experimental results show that our semi-supervised learning-based methods outperform a state-of-the-art model trained with labeled data only.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Keyphrase extraction concerns the task of se- lecting a set of phrases from a document that can indicate the main ideas expressed in the in- put <ref type="bibr" target="#b31">(Turney, 2000;</ref><ref type="bibr" target="#b12">Hasan and Ng, 2014)</ref>. It is an essential task for document understanding be- cause accurate identification of keyphrases can be beneficial for a wide range of downstream- ing natural language processing and information retrieval applications. For instance, keyphrases can be leveraged to improve text summarization systems ( <ref type="bibr" target="#b39">Zhang et al., 2004</ref>; <ref type="bibr">Liu et al., 2009a</ref>; <ref type="bibr" target="#b33">Wang and Cardie, 2013)</ref>, facilitate sentiment anal- ysis and opinion mining ( <ref type="bibr" target="#b35">Wilson et al., 2005;</ref><ref type="bibr" target="#b0">Berend, 2011)</ref>, and help with document cluster- ing ( <ref type="bibr" target="#b10">Hammouda et al., 2005</ref>). Though relatively * Work was done while visiting Northeastern University.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document:</head><p>In this paper, we consider an enthalpy formulation for a two-phase Stefan problem arising from the solidification of aluminum during process. We solve this free boundary problem in a time varying three- dimensional domain and consider convective heat transfer in the liquid phase. The resulting equations are discretized using a characteristics method in time and a method in space, and we propose a numerical algorithm to solve the obtained nonlinear discretized problem. Finally, numerical results are given which are compared with industrial experimental measurements. easy to implement, extract-based approaches fail to generate keyphrases that do not appear in the source document, which are frequently pro- duced by human annotators as shown in <ref type="figure">Figure  1</ref>. Recently, <ref type="bibr" target="#b21">Meng et al. (2017)</ref> propose to use a sequence-to-sequence model <ref type="bibr" target="#b30">(Sutskever et al., 2014</ref>) with copying mechanism for keyphrase gen- eration, which is able to produce phrases that are not in the input documents.</p><p>While seq2seq model demonstrates good per- formance on keyphrase generation ( <ref type="bibr" target="#b21">Meng et al., 2017)</ref>, it heavily relies on massive amounts of labeled data for model training, which is of- ten unavailable for new domains. To overcome this drawback, in this work, we investigate semi- supervised learning for keyphrase generation, by leveraging abundant unlabeled documents along with limited labeled data. Intuitively, additional documents, though unlabeled, can provide use- ful knowledge on universal linguistic features and discourse structure, such as context infor- mation for keyphrases and that keyphrases are likely to be noun phrases or main verbs. Fur- thermore, learning with unlabeled data can also mitigate the overfitting problem, which is often caused by small size of labeled training data, and thus improve model generalizability and enhance keyphrase generation performance on unseen data.</p><p>Concretely, two major approaches are proposed for leveraging unlabeled data. For the first method, unlabeled documents are first tagged with syn- thetic keyphrases, then mixed with labeled data for model pre-training. Synthetic keyphrases are acquired through existing unsupervised keyphrase extraction methods (e.g., TF-IDF or TextRank ( <ref type="bibr" target="#b23">Mihalcea and Tarau, 2004)</ref>) or a self-learning al- gorithm. The pre-trained model will further be fine-tuned on the labeled data only. For the second approach, we propose a multi-task learning (MTL) framework 1 by jointly learning the main task of keyphrase generation based on labeled samples, and an auxiliary task of title generation <ref type="bibr" target="#b27">(Rush et al., 2015</ref>) on unlabeled documents. Here one encoder is shared among the two tasks. Impor- tantly, we test our proposed methods on a seq2seq framework, however, we believe they can be easily adapted to other encoder-decoder-based systems.</p><p>Extensive experiments are conducted in sci- entific paper domain. Results on five differ- ent datasets show that all of our semi-supervised learning-based models can uniformly significantly outperform a state-of-the-art model ( <ref type="bibr" target="#b21">Meng et al., 2017)</ref> as well as several competitive unsupervised and supervised keyphrase extraction algorithms based on F 1 and recall scores. We further carry out a cross-domain study on generating keyphrases for news articles, where our models yield better F 1 than a model trained on labeled data only. Finally, we also show that training with unlabeled samples can further produce performance gain even when a large amount of labeled data is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Keyphrase Extraction and Generation. Early work mostly focuses on the keyphrase extrac- tion task, and a two-step strategy is typically designed. Specifically, a large pool of candi- date phrases are first extracted according to pre- defined syntactic templates ( <ref type="bibr" target="#b23">Mihalcea and Tarau, 2004;</ref><ref type="bibr" target="#b32">Wan and Xiao, 2008;</ref><ref type="bibr">Liu et al., 2009b</ref><ref type="bibr" target="#b15">Liu et al., , 2011</ref>) or their estimated importance scores <ref type="bibr" target="#b14">(Hulth, 2003)</ref>. In the second step, re-ranking is applied to yield the final keyphrases, based on supervised learning <ref type="bibr" target="#b14">Hulth, 2003;</ref><ref type="bibr" target="#b18">Lopez and Romary, 2010;</ref><ref type="bibr">Kim and Kan, 2009)</ref>, unsupervised graph algorithms <ref type="bibr" target="#b23">(Mihalcea and Tarau, 2004;</ref><ref type="bibr" target="#b32">Wan and Xiao, 2008;</ref><ref type="bibr" target="#b2">Bougouin et al., 2013)</ref>, or topic modelings ( <ref type="bibr" target="#b17">Liu et al., 2009c</ref><ref type="bibr" target="#b16">Liu et al., , 2010</ref>. Keyphrase generation is stud-ied in more recent work. For instance, <ref type="bibr" target="#b15">Liu et al. (2011)</ref> propose to use statistic machine translation model to learn word-alignments between docu- ments and keyphrases, enabling the model to gen- erate keyphrases which do not appear in the input. <ref type="bibr" target="#b21">Meng et al. (2017)</ref> train seq2seq-based generation models <ref type="bibr" target="#b30">(Sutskever et al., 2014</ref>) on large-scale la- beled corpora, which may not be applicable to a new domain with minimal human labels.</p><p>Neural Semi-supervised Learning. As men- tioned above, though significant success has been achieved by seq2seq model in many NLP tasks ( <ref type="bibr" target="#b19">Luong et al., 2015;</ref><ref type="bibr" target="#b28">See et al., 2017;</ref><ref type="bibr" target="#b6">Dong and Lapata, 2016;</ref><ref type="bibr" target="#b34">Wang and Ling, 2016;</ref><ref type="bibr" target="#b37">Ye et al., 2018)</ref>, they often rely on large amounts of labeled data, which are expensive to get. In order to mit- igate the problem, semi-supervised learning has been investigated to incorporate unlabeled data for modeling training <ref type="bibr" target="#b5">(Dai and Le, 2015;</ref><ref type="bibr" target="#b25">Ramachandran et al., 2017)</ref>. For example, neural machine translation community has studied the usage of source-side or target-side monolingual data to im- prove translation quality ( <ref type="bibr">Gülçehre et al., 2015)</ref>, where generating synthetic data ( <ref type="bibr" target="#b29">Sennrich et al., 2016;</ref><ref type="bibr" target="#b38">Zhang and Zong, 2016)</ref>, multi-task learn- ing ( <ref type="bibr" target="#b38">Zhang and Zong, 2016)</ref>, and autoencoder- based methods ( <ref type="bibr" target="#b3">Cheng et al., 2016</ref>) are shown to be effective. Multi-task learning is also ex- amined for sequence labeling tasks <ref type="bibr" target="#b26">(Rei, 2017;</ref><ref type="bibr" target="#b14">Liu et al., 2018)</ref>.</p><p>In our paper, we study semi-supervised learning for keyphrase generation based on seq2seq models, which has not been ex- plored before. Besides, we focus on leveraging source-side unlabeled articles to enhance perfor- mance with synthetic keyphrase construction or multi-task learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Keyphrase Generation Model</head><p>In this section, we describe the neural keyphrase generation model built on a sequence-to-sequence model <ref type="bibr" target="#b30">(Sutskever et al., 2014</ref>) as illustrated in <ref type="figure">Fig- ure 2</ref>. We denote the input source document as a sequence x = x 1 · · · x |x| and its correspond- ing keyphrase set as a = {a i } |a| i=1 , with a i as one keyphrase.</p><p>Keyphrase Sequence Formulation. Different from the setup by <ref type="bibr" target="#b21">Meng et al. (2017)</ref>, where in- put article is paired with each keyphrase a i to consist a training sample, we concatenate the keyphrases in a into a keyphrase sequence y =  <ref type="figure">Figure 2</ref>: Neural keyphrase generation model built on sequence-to-sequence framework. Input is the docu- ment, and output is the keyphrase sequence consisting of phrases present (key i ) or absent (key n i ) in the input.</p><formula xml:id="formula_0">a 1 ♦ a 2 ♦ · · · ♦ a |a| ,</formula><p>where ♦ is a segmenter to separate the keyphrases 2 . With this setup, the seq2seq model is capable to generate all possible keyphrases in one sequence as well as capture the contextual information between the keyphrases from the same sequence.</p><p>Seq2Seq Attentional Model. With source doc- ument x and its keyphrase sequence y, an encoder encodes x into context vectors, from which a de- coder then generates y. We set the encoder as one- layer bi-directional LSTM model and the decoder as another one-layer LSTM model <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber, 1997</ref>). The probability of generat- ing target sequence p(y|x) is formulated as:</p><formula xml:id="formula_1">p(y|x) = |y| t=1 p(y t |y &lt;t , x)<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">y &lt;t = y 1 · · · y t−1 . Let h t = [ − → h t ; ← − h t ]</formula><p>denote the hidden state vec- tor in the encoder at time step t, which is the concatenation of forward hidden vector − → h t and backward hidden vector</p><formula xml:id="formula_3">← − h t . Specifically, − → h t = f LSTMe (x t , − → h t−1 ) and ← − h t = f LSTMe (x t , ← − h t+1 )</formula><p>, where f LSTMe is an LSTM unit in encoder.</p><p>Decoder hidden state is calculated as s t = f LSTM d (y t−1 , s t−1 ), where f LSTM d is an LSTM unit in decoder. We apply global attention mechanism ( <ref type="bibr" target="#b19">Luong et al., 2015)</ref> to calculate the context vec- tor:</p><formula xml:id="formula_4">c t = |x| i=1 α t,i h i α t,i = exp(W att [s t ; h i ]) |x| k=1 exp(W att [s t h k ])<label>(2)</label></formula><p>2 We concatenate keyphrases following the original keyphrase order in the corpora, and we set ♦ as ";" in our implementation. The effect of keyphrase ordering will be studied in the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Keyphrase Ranking</head><p>Input: Generated top R keyphrase sequences S = [y1, · · · , yR] ranked with generation possibility from high to low with beam search Output: Ranked keyphrase set A with importance from high to low function KEY-RANK(S) A ← list() set A as a empty list. Q ← dict() to skip keyword that is already in A. for yi ∈ S do yi ← yi.split("♦"); split yi by "♦" for a ∈ yi do if a has not been merged in A, then keep it.</p><formula xml:id="formula_5">if not Q.has key(a) then A.append(a) Q.update(dict({a : ""}))</formula><p>where α t,i is the attention weight; W att contains learnable parameters. In this paper, we omit the bias variables to save space. The probability to predict y t in the decoder at time step t is factorized as:</p><formula xml:id="formula_6">p vocab (y t |y &lt;t , c t ) = f softmax (W d 1 · tanh(W d 2 [s t ; c t ]))<label>(3)</label></formula><p>where f softmax is the softmax function and W d 1 and W d 2 are learnable parameters.</p><p>Pointer-generator Network. Similar to <ref type="bibr" target="#b21">Meng et al. (2017)</ref>, we utilize copying mechanism via pointer-generator network ( <ref type="bibr" target="#b28">See et al., 2017</ref>) to al- low the decoder to directly copy words from in- put document, thus mitigating out-of-vocabulary (OOV) problem. At time step t, the generation probability p gen is calculated as:</p><formula xml:id="formula_7">p gen = f sigmoid (W c c t + W s s t + W y y t ) (4)</formula><p>where f sigmoid is a sigmoid function; W c , W s and W y are learnable parameters. p gen plays a role of switcher to choose to generate a word from a fixed vocabulary with probability p vocab or directly copy a word from the source document with the atten- tion distribution α t . With a combination of a fixed vocabulary and the extended source document vo- cabulary, the probability to predict y t is:</p><formula xml:id="formula_8">p(y t ) = p gen p vocab (y t |y &lt;t , c t )+(1−p gen ) i:y i =yt α t,i</formula><p>(5) where if y t does not appear in the fixed vocabulary, then the first term will be zero; and if y t is outside source document, the second term will be zero. Decoder 2</p><formula xml:id="formula_9">X, X (1) Y, Y (1) X Y X, X (2) Y Y (2)</formula><p>pre-train fine-tune main task auxiliary task (a) Synthetic Keyphrase Constrction (b) Multi-task Learning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head><p>Figure 3: Our two semi-supervised learning methods which are based on (a) synthetic keyphrase construction, and (b) multi-task learning. (X, Y) represents labeled sample. X (1) and X (2) denotes unlabeled documents. Y (1) refers to synthetic keyphrases of X (1) and Y (2) means the title of X <ref type="bibr">(2)</ref> . For method of synthetic keyphrase construction, model will be pre-trained on the mixture of gold-standards and synthetic data, then fine-tuned on labeled data. For multi-task learning, model parameters of main task and auxiliary task will be jointly updated. Encoder parameters of the two tasks are shared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised Learning. With a labeled dataset</head><formula xml:id="formula_10">D p = {x (i) , y (i) } N i=1</formula><p>, the loss function of seq2seq model is as follows:</p><formula xml:id="formula_11">L(θ) = − N i=1 log p(y (i) |x (i) ; θ)<label>(6)</label></formula><p>where θ contains all model parameters.</p><p>Keyphrase Inference and Ranking Strategy.</p><p>Beam search is utilized for decoding, and the top R keyphrase sequences are leveraged for produc- ing the final keyphrases. Here we use a beam size of 50, and R as 50. We propose a ranking strat- egy to collect the final set of keyphrases. Con- cretely, in sequence we collect unique keyphrases from the top ranked beams to lower ranked beams, and keyphrases in the same sequence are ordered as in the generation process. Intuitively, higher ranked sequences are likely of better quality. As for keyphrases in the same sequence, we find that more salient keyphrases are usually generated first. The ranking method is presented in Algo- rithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Semi-Supervised Learning for Keyphrase Generation</head><p>As illustrated in <ref type="figure">Figure 3</ref>, two methods are pro- posed to leverage abundant unlabeled data. The first is to provide synthetic keyphrases using un- supervised keyphrase extraction methods or self- learning algorithm, then mixed with labeled data for model training, which is described in Sec- tion 4.1. Furthermore, we introduce multi-task learning that jointly generates keyphrases and the title of the document (Section 4.2). We denote the large-scale unlabeled documents as</p><formula xml:id="formula_12">D u = {x (i) } M i=1</formula><p>and labeled data as</p><formula xml:id="formula_13">D p = {x (i) , y (i) } N i=1</formula><p>, where M N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Synthetic Keyphrase Construction</head><p>The first proposed technique is to construct syn- thetic labeled data by assigning keyphrases for unlabeled documents, and then mix the synthetic data with human labeled data for modeling train- ing. Intuitively, adding training samples with syn- thetic keyphrases has two potentially benefits: (1) the encoder is exposed to more documents in train- ing, and (2) the decoder also benefit from addi- tional information from identifying contextual in- formation for keyphrases. We propose and com- pare two methods to extract synthetic keyphrases.</p><p>Unsupervised Learning Methods. Unsuper- vised learning methods on keyphrase extraction have been long studied in previous work <ref type="bibr" target="#b23">(Mihalcea and Tarau, 2004;</ref><ref type="bibr" target="#b32">Wan and Xiao, 2008</ref>). Here we select two effective and widely used meth- ods to select keyphrases on unlabeled dataset D u , which are TF-IDF and TextRank ( <ref type="bibr" target="#b23">Mihalcea and Tarau, 2004</ref>). We combine the two methods into a hybrid approach, in which we first adopt the two methods to separately select top K keyphrases from the document, we then take the union with duplicate removal. To construct the keyphrase se-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Training Procedures for Synthetic Keyphrase Construction</head><p>Input: Dp, Ds, θ Output: θ function PRE-TRAIN(Dp, Ds, θ) Dp+s ← Dp ∪ Ds Shuffle Dp+s randomly Update θ on Dp+s until converge function FINE-TUNE(Dp, θ)</p><p>Set θ as the best parameters from PRE-TRAIN Update θ on Dp until converge quence, we concatenate the terms from TF-IDF and then from TextRank, following the corre- sponding ranking order. We set K as 5 in our ex- periments. randomly, then we pre-train the model on D p+s , in which no network parameters are frozen during the training process. The best performing model is selected based on validation set, then fine-tuned on D p until converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-task Learning with Auxiliary Task</head><p>The second approach to leverage unlabeled doc- uments is to employ a multi-task learning frame- work which combines the main task of keyphrase generation with an auxiliary task through parame- ter sharing strategy. Similar to the model structure in <ref type="bibr" target="#b38">Zhang and Zong (2016)</ref>, our main task and the auxiliary task share an encoder network but have different decoders. Multi-task learning will bene- fit from the source-side information to improve the model generality of encoder.</p><p>In most domains such as scientific papers or news articles, a document usually contains a title that summarizes the core topics or ideas, with a similar spirit as keyphrases. We thus choose ti- tle generation as auxiliary task, which has been studied as a summarization problem ( <ref type="bibr" target="#b27">Rush et al., 2015;</ref><ref type="bibr" target="#b4">Colmenares et al., 2015)</ref>.</p><note type="other">Dataset TRAIN VALID Small-scale Document-Keyphrase 40, 000 5, 000 Document-SyntheticKeyphrase 400, 000 N/A Document-Title 400, 000 15, 000 Large-scale Document-Keyphrase 130, 000 5, 000 Avg. #Tokens in Train LABELED SYN. MTL Small-scale Document 176.3 175.9 165.5 Keyphrase Sequence 23.3 23.5 N/A Title N/A N/A 10.4</note><formula xml:id="formula_14">Let D u = {x (i) , q (i) } M i=1</formula><p>denote the dataset which is as- signed with titles for unlabeled data D u , the loss function of multi-task learning is factorized as:</p><formula xml:id="formula_15">L(θ e , θ d 1 , θ d 2 ) = − N i=1 log p(y (i) |x (i) ; θ e , θ d 1 ) − M i=1 log p(q (i) |x (i) ; θ e , θ d 2 ) (7)</formula><p>where θ e indicates encoder parameters; θ d 1 and θ d 2 are the decoder parameters.</p><p>Training Procedure. We adopt a simple alter- nating training strategy to switch training between the main task and the auxiliary task. Specifically, we first estimate parameters on auxiliary task with D u for one epoch, then train model on the main task with D p (labeled dataset) for T epochs. We follow this training procedure for several times un- til the model of our main task converges. We set T as 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>Our major experiments are conducted on scientific articles which have been studied in previous work <ref type="bibr" target="#b14">(Hulth, 2003;</ref><ref type="bibr" target="#b24">Nguyen and Kan, 2007;</ref><ref type="bibr" target="#b21">Meng et al., 2017)</ref>. We use the dataset from <ref type="bibr" target="#b21">Meng et al. (2017)</ref> which is collected from various online digital li- braries, e.g. ScienceDirect, ACM Digital Library, Wiley, and other portals.</p><p>As indicated in <ref type="table" target="#tab_2">Table 1</ref>, we construct a rel- atively small-scale labeled dataset with 40K document-keyphrase 3 pairs, and a large-scale  dataset of 400K unlabeled documents. Each doc- ument contains an abstract and a title of the pa- per. In multi-task learning setting, the auxiliary task is to generate the title from the abstract.</p><formula xml:id="formula_16">Model INSPEC KRAPIVIN NUS SEMEVAL KP20K F 1 @5 F 1 @10 F 1 @5 F 1 @10 F 1 @5 F 1 @10 F 1 @5 F 1 @10 F 1 @5 F 1 @10 Comparisons TF-IDF 0</formula><p>For the validation set, we collect 5K document- keyphrase pairs for the process of pre-training and fine-tuning for methods based on synthetic data construction. For multi-task learning, we use the same 5K document-keyphrase pairs for the main task training, another 15K document-title pairs for the auxiliary task. We further conduct experiments on a 130K large-scale labeled dataset, which in- cludes the small-scale labeled data. Similar to <ref type="bibr" target="#b21">Meng et al. (2017)</ref>, we test our model on five widely used public datasets from the scientific domain: INSPEC <ref type="bibr" target="#b14">(Hulth, 2003)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Settings</head><p>Data preprocessing is implemented as in ( <ref type="bibr" target="#b21">Meng et al., 2017</ref>). The texts are first tokenized by NLTK ( <ref type="bibr" target="#b1">Bird and Loper, 2004</ref>) and lowercased, then the numbers are replaced with &lt;digit&gt;. We set maxi- mal length of source text as 200, 40 for target text. Encoder and decoder both have a vocabulary size of 50K. The word embedding size is set to 128. Embeddings are randomly initialized and learned during training. The size of hidden vector is 512. Dropout rate is set as 0.3. Maximal gradient nor- malization is 2. Adagrad ( <ref type="bibr" target="#b7">Duchi et al., 2011</ref>) is adopted to train the model with learning rate of 0.15 and the initial accumulator rate is 0.1.</p><p>For synthetic data construction, we first use batch size of 64 for model pre-training and then re- duce to 32 for model fine-tuning. For both training stages, after 8 epochs, learning rate be decreased with a rate of 0.5. For multi-task learning, batch size is set to 32 and learning rate is reduced to half after 20 training epochs. To build baseline seq2seq model, we set batch size as 32 and decrease learn- ing rate after 8 epochs. For self-learning algo- rithm, beam size is set to 10 to generate target se- quences for unlabeled data and the top one is re- tained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparisons with Baselines</head><p>Evaluation Metrics. Following ( <ref type="bibr" target="#b15">Liu et al., 2011;</ref><ref type="bibr" target="#b21">Meng et al., 2017)</ref>, we adopt top-N macro- averaged precision, recall and F-measure (F 1 ) for model evaluation. Precision means how many top- N extracted keywords are correct and recall means how many target keyphrases are extracted in top- N candidates. Porter Stemmer is applied before comparisons.</p><p>Baselines. We train a baseline seq2seq model on the small-scale labeled dataset. We further com- pare with four unsupervised learning methods: TF-IDF, TextRank ( <ref type="bibr" target="#b23">Mihalcea and Tarau, 2004</ref>), SingleRank ( <ref type="bibr" target="#b32">Wan and Xiao, 2008)</ref>, ExpandRank ( <ref type="bibr" target="#b32">Wan and Xiao, 2008)</ref>, and two supervised learn- ing methods of Maui ( <ref type="bibr" target="#b20">Medelyan et al., 2009</ref>) and KEA (   labeled corpora, our baseline seq2seq model with copying mechanism still achieves better F 1 @5 scores, compared to other baselines. This demon- strates that a baseline seq2seq model has learned the mapping patterns from source text to target keyphrases to some degree. However, small-scale labeled data still hinders the potential of seq2seq model according to the poor performance of F 1 @10. By leveraging large-scale unlabeled data, our semi-supervised learning methods achieve sig- inifcant improvement over seq2seq baseline, as well as exhibit the best performance in both F 1 @5 and F 1 @10 on almost all datasets. We further compare seq2seq based models on the task of generating keyphrases beyond input article vocabulary. Illustrated by <ref type="table" target="#tab_6">Table 3</ref>, semi- supervised learning significantly improves the ab- sent generation performance, compared to the baseline seq2seq. Among our models, the multi- task learning method is more effective at gen- erating absent keyphrases than the two methods by leveraging synthetic data. The main reason may lie in that synthetic keyphrases potentially introduce noisy annotations, while the decoder in multi-task learning setting focuses on learn- ing from gold-standard keyphrases. We can also see that the overall performances by all models are low, due to the intrinsic difficulty of absent keyphrase generation ( <ref type="bibr" target="#b21">Meng et al., 2017)</ref>. More- over, we only employ 40K labeled data for train- ing, which is rather limited for training. Besides, we believe better evaluation methods should be used instead of exact match, e.g., by considering paraphrases. This will be studied in the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Effect of Synthetic Keyphrase Quality</head><p>In this section, we conduct experiments to further study the effect of synthetic keyphrase quality on model performance. Two sets of experiments are undertaken, one for evaluating unsupervised learn- ing and one for self-learning algorithm.</p><p>For self-learning algorithm, we further generate  <ref type="figure">Figure 4</ref>: Pre-training curves with perplexity on vali- dation set with various options for synthetic keyphrases construction. Left is for options for self-learning algo- rithm and right is for unsupervised learning methods.</p><p>synthetic keyphrases using following options:</p><p>• Beam-size-3: Based on our baseline model trained with labeled data, we use beam search with a smaller beam size of 3 to generate syn- thetic data <ref type="bibr">5</ref> .</p><p>• Trained-model: We adopt the model which has been trained with self-learning algorithm on 40K labeled data and 400K unlabeled data, to generate the top one keyphrase se- quence with beam size of 10.</p><p>For unsupervised learning method, we origi- nally merge top-K (K = 5) keyphrases from TF- IDF and TextRank, here we use options where K is set as 1 or 10 to extract keyphrases:</p><p>• Top@1: Using TF-IDF or TextRank, we only keep top 1 extraction from each, then take the union of the two.</p><p>• Top@10: Similarly, we keep top 10 extracted terms from TF-IDF or TextRank, then take the union.</p><p>As illustrated in <ref type="figure">Figure 4</ref>, when models are pre- trained with synthetic keyphrases of better quality, results by "Trained-model" consistently produce better performance (i.e., lower perplexity). Sim- ilar phenomenon can be observed when "top@5" and "top@10" are applied for extraction in un- supervised learning setting. Furthermore, after models are pre-trained and then fine-tuned, the re- sults in <ref type="figure">Figure 5</ref> show that the difference among baselines becomes insignificant-the quality of synthetic keyphrases have limited effect on final scores. The reason might be that though synthetic  <ref type="figure">Figure 5</ref>: Effect of synthetic data quality on present keyphrase generation (models are pre-trained and fine- tuned) based on F 1 @5 (left three columns) and F 1 @10 (right three columns), on five datasets. The upper is for self-learning algorithm and the bottom is for unsuper- vised learning method. keyphrases potentially introduce noisy informa- tion for decoder training, the encoder is still well trained. In addition, after fine-tuning on labeled data, the decoder acquires additional knowledge, thus leading to better performance and minimal difference among the options.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Effect of Amount of Unlabeled Data</head><p>In this section, we further evaluate whether vary- ing the amount of unlabeled data will affect model performance. We conduct experiments based on methods of synthetic data construction with un- supervised learning and multi-task learning. We further carry out experiments with randomly se- lected 50K <ref type="formula">(</ref> Multi-task(50K) Multi-task(100K) Multi-task(200K) Multi-task(300K) Multi-task(400K) <ref type="figure">Figure 7</ref>: Perplexity on validation set with varying amounts of unlabeled data for training. Left is for fine-tuning procedure based on models trained with synthetic data constructed with unsupervised learning. Right is for multi-task learning procedure with perfor- mance on the main task.  the model performances. N is set as 10 or 50.</p><p>The present keyphrase generation results are shown in <ref type="figure" target="#fig_3">Figure 6</ref>, from which we can see that when increasing the amount of unlabeled data, model performance is further improved. This is because additional unlabeled data can provide with more evidence on linguistic or context fea- tures and thus make the model, especially the en- coder, have better generalizability. This finding echoes with the training procedure illustrated in <ref type="figure">Figure 7</ref>, where more unlabeled data uniformly leads to better performance. Therefore, we believe that leveraging more unlabeled data for model training can boost model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">A Pilot Study for Cross-Domain Test</head><p>Up to now, we have demonstrated the effectiveness of leveraging unlabeled data for in-domain exper- iments, but is it still effective when being tested on a different domain? We thus carry out a pi- lot cross-domain test on news articles. The widely used DUC dataset ( <ref type="bibr" target="#b32">Wan and Xiao, 2008</ref>) is uti- lized, consisting of 308 articles with 2, 048 labeled keyphrases.</p><p>The experimental results are shown in <ref type="table" target="#tab_11">Table 4</ref> which indicate that: 1) though trained on scientific papers, our models still have the ability to gener-  ate keyphrases for news articles, illustrating that our models have learned some universal features between the two domains; and 2) semi-supervised learning by leveraging unlabeled data improves the generation performances more, indicating that our proposed method is reasonably effective when being tested on cross-domain data. Though un- supervised methods are still superior, for future work, we can leverage unlabeled out-of-domain corpora to improve cross-domain keyphrase gen- eration performance, which could be a promising direction for domain adaption or transfer learning.  </p><formula xml:id="formula_17">Model INSPEC KRAPIVIN NUS SEMEVAL KP20K F 1 @5 F 1 @10 F 1 @5 F 1 @10 F 1 @5 F 1 @10 F 1 @5 F 1 @10 F 1 @5 F 1 @10 SEQ2SEQ-COPY 0.34</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-supervised</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Training on Large-scale Labeled Data</head><p>Finally, it would be interesting to study whether unlabeled data can still improve performance when the model is trained on a larger scaled la- beled data. We conduct experiments on a larger labeled dataset with 130K pairs, along with the 400K unlabeled data. Here the baseline seq2seq model is built on the 130K dataset. From the present keyphrase generation results in <ref type="table" target="#tab_13">Table 5</ref>, it can be seen that unlabeled data is still helpful for model training on a large-scale la- beled dataset. This implies that we can also lever- age unlabeled data to enhance generation perfor- mance even in a resource-rich setting. Referring to the absent keyphrase generation results shown in <ref type="table" target="#tab_15">Table 6</ref>, semi-supervised learning also boosts the scores. From <ref type="table" target="#tab_15">Table 6</ref>, training on large-scale labeled data, absent generation is significantly im- proved, compared to being trained on a small-scale labeled data (see <ref type="table" target="#tab_6">Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we presented a semi-supervised learning framework that leverages unlabeled data for keyphrase generation built upon seq2seq mod- els. We introduced synthetic keyphrases con- struction algorithm and multi-task learning to ef- fectively leverage abundant unlabeled documents. Extensive experiments demonstrated the effective- ness of our methods, even in scenario where large- scale labeled data is available.</p><p>For future work, we will 1) leverage unlabeled data to study domain adaptation or transfer learn- ing for keyphrase generation; and 2) investigate novel models to improve absent keyphrase gener- ation when limited labeled data is available based on semi-supervised learning. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: Sample document with labeled keyphrases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, NUS (Nguyen and Kan, 2007), KRAPIVIN (Krapivin et al., 2009), SEMEVAL-2010 (Kim et al., 2010) and KP20K (Meng et al., 2017).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Effect of various amounts of unlabeled data for training on present keyphrase generation with F 1 @10. Upper is for synthetic data construction method with unsupervised learning. Bottom is for multi-task learning algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Su</head><label></label><figDesc>Nam Kim and Min-Yen Kan. 2009. Re-examining automatic keyphrase extraction approaches in sci- entific articles. In Proceedings of the Work- shop on Multiword Expressions: Identification, Interpretation, Disambiguation and Applications, MWE@IJCNLP 2009, Singapore, August 6, 2009, pages 9-16. Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timothy Baldwin. 2010. Semeval-2010 task 5 : Au- tomatic keyphrase extraction from scientific articles. In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval@ACL 2010, Upp- sala University, Uppsala, Sweden, July 15-16, 2010, pages 21-26. Mikalai Krapivin, Aliaksandr Autaeu, and Maurizio Marchese. 2009. Large dataset for keyphrases ex- traction. Technical report, University of Trento. Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu. 2009a. Unsupervised approaches for automatic key- word extraction using meeting transcripts. In Pro- ceedings of human language technologies: The 2009 annual conference of the North American chapter of the association for computational linguistics, pages 620-628. Association for Computational Linguis- tics. Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu. 2009b. Unsupervised approaches for automatic key- word extraction using meeting transcripts. In Hu- man Language Technologies: Conference of the North American Chapter of the Association of Com- putational Linguistics, Proceedings, May 31 -June 5, 2009, Boulder, Colorado, USA, pages 620-628.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Statistics of datasets used in our experiments. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results of present keyphrase generation with metrics F 1 @5 and F 1 @10.  *  marks numbers that are 
significantly better than SEQ2SEQ-COPY (p &lt; 0.01, F -test). Due to the high time perplexity, no result is reported 
by ExpandRank on KP20K, as done in Meng et al. (2017). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>). We follow Meng et al. (2017) for baselines setups. Results. Here we show results for present and absent keyphrase generation separately 4 . From the results of present keyphrase generation as shown in Table 2, although trained on small-scale</figDesc><table>Semi-supervised 
Dataset 
SEQ. 
SYN.UN. 
SYN.SELF. 
MULTI. 
INSPEC 
0.012 
0.018 
0.013 
0.022 
KRAPIVIN 
0.01 
0.008 
0.018 
0.021 
NUS 
0.002 
0.011 
0.003 
0.013 
SEMEVAL 
0.001 
0.008 
0.003 
0.006 
KP20K 
0.01 
0.016 
0.013 
0.021 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results of absent kephrase generation based 
on Recall@10. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results of keyphrase generation for news from 
DUC dataset with F 1 . Results of unsupervised learning 
methods are adopted from Hasan and Ng (2010). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Results of present keyphrase generation on large-scale labeled data with F 1 @5 and F 1 @10.  *  indicates 
significant better performance than SEQ2SEQ-COPY with p &lt; 0.01 (F -test). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Performance on absent keyphrase generation 
by Recall@10 with large-scale labeled training data. 

</table></figure>

			<note place="foot" n="1"> We use &quot;semi-supervised learning&quot; as a broad term to refer to the two methods proposed in this paper.</note>

			<note place="foot" n="3"> Here keyphrase refers to the keyphrase sequence.</note>

			<note place="foot" n="4"> Recall that present means the keyphrase appears in the document, otherwise, it is absent.</note>

			<note place="foot" n="5"> We also experiment with greedy search (i.e. beam size of 1), however, unknown words are frequently generated.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research is based upon work supported in part by National Science Foundation through Grants IIS-1566382 and IIS-1813341, and by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract # FA8650-17-C-9116. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, ei-ther expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copy-right annotation therein. We thank three anony-mous reviewers for their insightful suggestions on various aspects of this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Opinion expression mining by exploiting keyphrase extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Berend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth International Joint Conference on Natural Language Processing</title>
		<meeting><address><addrLine>Chiang Mai, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11-08" />
			<biblScope unit="page" from="1162" to="1170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">NLTK: the natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Poster and Demonstration</publisher>
			<date type="published" when="2004-07-21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Topicrank: Graph-based topic ranking for keyphrase extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bougouin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Boudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Béatrice</forename><surname>Daille</surname></persName>
		</author>
		<idno>IJC- NLP 2013</idno>
	</analytic>
	<monogr>
		<title level="m">Sixth International Joint Conference on Natural Language Processing</title>
		<meeting><address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10-14" />
			<biblScope unit="page" from="543" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semisupervised learning for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">HEADS: headline generation as sequence prediction using an abstract feature-rich space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">A</forename><surname>Colmenares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Litvak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Mantrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Silvestri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-31" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2015</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semisupervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Language to logical form with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nevill-Manning. 1999. Domain-specific keyphrase extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><forename type="middle">W</forename><surname>Paynter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Gutwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, IJCAI 99</title>
		<meeting>the Sixteenth International Joint Conference on Artificial Intelligence, IJCAI 99<address><addrLine>Stockholm</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1450" />
			<biblScope unit="page" from="668" to="673" />
		</imprint>
	</monogr>
<note type="report_type">Sweden</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">On using monolingual corpora in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huei-Chi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1503.03535</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Corephrase: Keyphrase extraction for document clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khaled</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><forename type="middle">N</forename><surname>Hammouda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">S</forename><surname>Matute</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Machine Learning and Data Mining in Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="265" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Conundrums in unsupervised keyphrase extraction: Making sense of the state-of-the-art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saidul</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2010, 23rd International Conference on Computational Linguistics, Posters Volume</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-08" />
			<biblScope unit="page" from="365" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic keyphrase extraction: A survey of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saidul</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014-06-22" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1262" to="1273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved automatic keyword extraction given more linguistic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anette</forename><surname>Hulth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. ; Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">Fangzheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Japan; New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-07-11" />
		</imprint>
	</monogr>
	<note>Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic keyphrase extraction by bridging vocabulary gap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fifteenth Conference on Computational Natural Language Learning<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06-23" />
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic keyphrase extraction via topic decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2010</biblScope>
			<biblScope unit="page" from="366" to="376" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clustering to find exemplar terms for keyphrase extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-08-07" />
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">HUMB: automatic key term extraction from scientific articles in GROBID</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07-15" />
			<biblScope unit="page" from="248" to="251" />
		</imprint>
		<respStmt>
			<orgName>Uppsala University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Human-competitive tagging using automatic keyphrase extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Olena Medelyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-08-07" />
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep keyphrase generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Brusilovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th</title>
		<meeting>the 55th</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="582" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Textrank: Bringing order into text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL, held in conjunction with ACL</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Keyphrase extraction in scientific publications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thuy</forename><forename type="middle">Dung</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Digital Libraries. Looking Back 10 Years and Forging New Frontiers, 10th International Conference on Asian Digital Libraries</title>
		<meeting><address><addrLine>Hanoi, Vietnam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-12-10" />
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised pretraining for sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-09" />
			<biblScope unit="page" from="383" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised multitask learning for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2121" to="2130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-813" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning algorithms for keyphrase extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="336" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Single document keyphrase extraction using neighborhood knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence, AAAI</title>
		<meeting>the Twenty-Third AAAI Conference on Artificial Intelligence, AAAI<address><addrLine>Chicago, Illinois, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-07-13" />
			<biblScope unit="page" from="855" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">DomainIndependent Abstract Generation for Focused Meeting Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1395" to="1405" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural networkbased abstract generation for opinions and arguments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1217" />
			<biblScope unit="page" from="47" to="57" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recognizing contextual polarity in phraselevel sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on human language technology and empirical methods in natural language processing</title>
		<meeting>the conference on human language technology and empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">KEA: practical automatic keyphrase extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><forename type="middle">W</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Paynter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><forename type="middle">G</forename><surname>Gutwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nevill-Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth ACM conference on Digital Libraries</title>
		<meeting>the Fourth ACM conference on Digital Libraries<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-08-11" />
			<biblScope unit="page" from="254" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Interpretable charge predictions for criminal cases: Learning to generate court views from fact descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhunchen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018<address><addrLine>Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1854" to="1864" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exploiting source-side monolingual data in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">World wide web site summarization. Web Intelligence and Agent Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nur Zincir-Heywood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><forename type="middle">E</forename><surname>Milios</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="39" to="53" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
