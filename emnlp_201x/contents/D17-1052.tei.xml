<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sentiment Lexicon Construction with Representation Learning Based on Hierarchical Sentiment Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyi</forename><surname>Wang</surname></persName>
							<email>leyiwang.cn@gmail.com, rxia@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Nanjing University of Science &amp; Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Nanjing University of Science &amp; Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sentiment Lexicon Construction with Representation Learning Based on Hierarchical Sentiment Supervision</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="502" to="510"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Sentiment lexicon is an important tool for identifying the sentiment polarity of words and texts. How to automatically construct sentiment lexicons has become a research topic in the field of sentiment analysis and opinion mining. Recently there were some attempts to employ representation learning algorithms to construct a sentiment lexicon with sentiment-aware word embedding. However, these methods were normally trained under document-level sentiment supervision. In this paper, we develop a neural architecture to train a sentiment-aware word embedding by integrating the sentiment supervision at both document and word levels, to enhance the quality of word embedding as well as the sentiment lexicon. Experiments on the SemEval 2013-2016 datasets indicate that the sentiment lexicon generated by our approach achieves the state-of-the-art performance in both supervised and unsuper-vised sentiment classification, in comparison with several strong sentiment lexicon construction methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment lexicon is a set of words (or phrases) each of which is assigned with a sentiment polar- ity score. Sentiment lexicon plays an important role in many practical sentiment analysis and opin- ion mining tasks. There were some manually an- notated universal sentiment lexicons such as Gen- eral Inquireer (GI) and HowNet. However, due to the ubiquitous domain diversity and absence of domain prior knowledge, the automatic construc- tion technique for domain-specific sentiment lex- *</p><p>The corresponding author of this paper.</p><p>icons has become a challenging research topic in the field of sentiment analysis and opinion mining ( <ref type="bibr" target="#b18">Wang and Xia, 2016)</ref>.</p><p>The early work employed unsupervised learn- ing for sentiment lexicon construction. They nor- mally labelled a set of seed words at first, and then learned the polarity of each candidate word, based on either word conjunction relations (e.g., constel- lation and transition in texts) <ref type="bibr" target="#b4">(Hatzivassiloglou and McKeown, 1997)</ref>, or the word co-occurrence information (such as pointwise mutual informa- tion, PMI) <ref type="bibr" target="#b15">(Turney, 2002)</ref>, between the candidate word and the seed words. However, the unsuper- vised manner showed limited effect in sentiment prediction, and the performance greatly depends on the quality of the seed words.</p><p>To fully exploit the sentiment labeling informa- tion in texts, a series of supervised learning meth- ods was further proposed to learn the sentiment lexicons. For example, <ref type="bibr" target="#b11">Mohammad et al. (2013)</ref> proposed to construct sentiment lexicons by cal- culating PMI between the word and the distant- ly supervised sentiment labels (such as emoticon- s) in tweets and the word's sentiment orientation (SO). The resulting lexicons obtained the best re- sults in SemEval 2013. More advanced repre- sentation learning models were also utilized, with the aim to construct the sentiment lexicons with efficient word embeddings ( <ref type="bibr" target="#b13">Tang et al., 2014a;</ref><ref type="bibr" target="#b3">Hamilton et al., 2016;</ref><ref type="bibr" target="#b17">Vo and Zhang, 2016)</ref>. The traditional representation learning framework such as Word2Vec only captures the syntactic informa- tion in the texts, but ignores the sentiment rela- tions between words. Therefore, some researcher- s attempted to add sentiment supervision into the network structure, in order to train a sentiment- aware word embedding. For example, <ref type="bibr" target="#b13">Tang et al. (2014a)</ref> exploited a dedicated neural architecture to integrate document-level sentiment supervision and the syntactic knowledge for representation learning. The sentiment-aware word embedding is then used to construct a sentiment lexicon. <ref type="bibr" target="#b17">Vo and Zhang (2016)</ref> proposed to learn a two-dimensional sentiment representation based on a simple neu- ral network. The sentiment lexicons generated by their approach obtained better performance to pre- dict the tweet sentiment labels, in comparison with the PMI-based method <ref type="bibr" target="#b11">(Mohammad et al., 2013)</ref>.</p><p>Although these supervised learning method- s can to some extent exploit the sentiment la- beling information in the texts and can learn a sentiment-aware word embedding, the manner of using document-level sentiment supervision suf- fers from some complex linguistic phenomena such as negation, transition and comparative de- gree, and hence unable to capture the fine-grained sentiment information in the text. For example, in the following tweet "Four more fake people added me. Is this why people don't like Twitter? :( ", the document-level sentiment label is negative, but there is a positive word "like" in the text. In representation learning, the embeddings of word- s are summed up to represent the document, and the word "like" will be falsely associated with the negative sentiment label. Such linguistic phenom- ena occur frequently in review texts, and makes sentiment-aware word representation learning less effective.</p><p>To address this problem, in this paper, we pro- pose a new representation learning framework called HSSWE, to learn sentiment-aware word embeddings based on hierarchical sentiment su- pervision. In HSSWE, the learning algorithm is supervised under both document-level sentiment labels and word-level sentiment annotations (e.g., labeling "like" as a positive word). By leverag- ing the sentiment supervision at both document and word level, our approach can avoid the sen- timent learning flaws caused by coarse-grained document-level supervision by incorporating fine- grained word-level supervision, and improve the quality of sentiment-aware word embedding. Fi- nally, following <ref type="bibr" target="#b13">Tang et al. (2014a)</ref>, a simple classifier was constructed to obtain the domain- specific sentiment lexicon by using word embed- dings as inputs.</p><p>The main contributions of this work are as fol- lows:</p><p>1. To the best of our knowledge, this is the first work that learns the sentiment-aware word representation under supervision at both doc- ument and word levels.</p><p>2. Our approach supports several kinds of word- level sentiment annotations such as 1) pre- defined sentiment lexicon; 2) PMI-SO lexi- con with hard sentiment annotation; 3) PMI- SO lexicon with soft sentiment annotation. By using PMI-SO dictionary as word-level sentiment annotation, our approach is totally corpus-based, without any external resource.</p><p>3. Our approach obtains the state-of-the-art per- formance in comparison with several strong sentiment lexicon construction methods, on the benchmark SemEval 2013-2016 datasets for twitter sentiment classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In general, sentiment lexicons construction can be classified into two categories, dictionary-based methods and corpus-based methods. Dictionary-based methods generally integrate predefined resources, such as WordNet, to con- struct sentiment lexicons. <ref type="bibr" target="#b6">Hu and Liu (2004)</ref> ex- ploited WordNet for sentiment lexicon construc- tion. They first labelled two sets of seed word- s by polarities, then extended the sets by adding the synonyms for each word to the same set and antonyms to the other. For a given new word, <ref type="bibr" target="#b8">Kim and Hovy (2004)</ref> introduced a Naive Bayes model to predict the polarities with .the synonym set obtained from WordNet as features. <ref type="bibr" target="#b7">Kamps et al. (2004)</ref> investigated a graph-theoretic model of WordNet's synonymy relation and measured the sentiment orientation by distance between each candidate word and the seed words with differ- ent polarities. <ref type="bibr" target="#b5">Heerschop et al. (2011)</ref> proposed a method to propagate the sentiment of seed set words through semantic relations of WordNet.</p><p>Corpus-based approaches originate from the la- tent relation hypothesis: "Pairs of words that co- occur in similar patterns tend to have similar se- mantic and sentiment relations" <ref type="bibr" target="#b16">(Turney, 2008)</ref>.</p><p>The primary corpus-based method made the use of PMI. Turney (2002) built a sentiment lexicon by calculating PMI between the candidate word and seed words. The difference of the PMI score between positive and negative seed words is final- ly used as the sentiment orientation (SO) of each candidate word <ref type="bibr" target="#b15">(Turney, 2002</ref>). Many variants of PMI were proposed afterwards, for example, pos- itive pointwise mutual information (PPMI), sec- ond order co-occurrence PMI (SOC-PMI), etc. <ref type="bibr" target="#b3">Hamilton et al. (2016)</ref> proposed to build a senti- ment lexicon by a propagation method. The key of this method is to build a lexical graph by calcu- lating the PPMI between words. Instead of calcu- lating the PMI between words, <ref type="bibr" target="#b11">Mohammad et al. (2013)</ref> proposed to use emoticons as distant super- vision and calculate the PMI between words and the distant class labels, and obtained sound perfor- mance for tweet sentiment classification.</p><p>The latest corpus-based approaches normally utilize the up-to-date machine learning models (e.g. neural networks) to first learn a sentiment- aware distributed representation of words, based on which the sentiment lexicon is then construct- ed. There were many word representation learn- ing methods such as NNLM ( <ref type="bibr" target="#b0">Bengio et al., 2003)</ref> and <ref type="bibr">Word2Vec (Mikolov et al., 2013)</ref>. Howev- er, they mainly consider the syntactic relation of words in the context but ignore the sentiment in- formation. Some work were later proposed to deal with this problem by incorporating the sentimen- t information during representation learning. For example, <ref type="bibr" target="#b13">Tang et al. (2014a)</ref> adapted a variant of skip-gram model, which can learn the sentiment information based on distant supervision. Further- more, <ref type="bibr" target="#b14">Tang et al. (2014b)</ref> proposed a new neural network approach called SSWE to train sentiment- aware word representation. <ref type="bibr" target="#b17">Vo and Zhang (2016)</ref> exploited a simple and fast neural network to train a 2-dimensional representation. Each dimension is explicitly associated with a sentiment polarity.</p><p>The sentiment-aware word representation in these methods was normally trained based on on- ly document-level sentiment supervision. In con- trast, the learning algorithm in our approach is supervised under both document-level and word- level sentiment supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>Our approach is comprised of three base mod- ules: (1) Word-level sentiment learning and an- notation; (2) Sentiment-aware word embedding learning; (3) Sentiment lexicon construction.</p><p>Our approach depends on document-level sen- timent labels. The tweet corpus provides a cheap way to get document-level sentiment annotation, owing to the distant sentiment supervision. But it should be noted that our approach is feasible for any corpus provided with document-level sen- timent labels (not merely tweets).</p><p>The first module of our method aims to learn the pseudo sentiment distribution for each word and use it as word-level sentiment annotations to supervise word embedding learning.</p><p>In the second module, we learn the sentiment- aware embeddings for each word in corpus, based on hierarchical sentiment supervision.</p><p>In the last module, we construct a sentiment lex- icon by using the sentiment-aware word embed- dings as the basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning Word-Level Sentiment Supervision</head><p>In addition to use a pre-defined sentiment lexi- con for word-level annotations, we also propose to learn the word-level sentiment supervision, based on PMI and SO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(1) PMI and SO</head><p>Given a corpus with document-level class label- s. We first compute the PMI score between each word t and two class labels</p><formula xml:id="formula_0">P M I(t, +) = log p(+|t) p(+) ,<label>(1)</label></formula><formula xml:id="formula_1">P M I(t, −) = log p(−|t) p(−) ,<label>(2)</label></formula><p>where + and − denote the positive and negative document-level class labels, respectively.</p><p>Second, we compute the SO score for each word t:</p><formula xml:id="formula_2">SO(t) = P M I(t, +) − P M I(t, −). (3)</formula><p>We call {t, SO(t)} as PMI-SO dictionary. The PMI-SO dictionary was widely used as a corpus- based sentiment lexicon for sentiment classifica- tion. By contrast, in our approach, it is the first step to learn the sentiment-aware word represen- tation. Our approach supports two kinds of word- level sentiment annotations: 1) PMI-SO dictionary with hard sentiment annotation; 2) PMI-SO dictio- nary with soft sentiment annotation. (2) PMI-SO lexicon with hard sentiment anno- tation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notations Description et</head><p>The embedding of word t de</p><p>The document representation of d bt</p><p>The bias of word-level softmax layer b d</p><p>The bias of document-level softmax layer θt Weight of word-level softmax layer θ d</p><p>Weight of document-level softmax layer p(c|et)</p><p>The sentiment distribution of word t predict- ed by our model p(c|de)</p><p>The sentiment distribution of document d predicted by our modeîmodeî p(c|t)</p><p>The word-level sentiment annotation of word t with respect to class c ˆ p(c|d)</p><p>The document-level sentiment annotation of document d with respect to class c <ref type="table">Table 1</ref>: The parameters used in our neural net- work.</p><p>"Hard sentiment annotation" indicates that</p><formula xml:id="formula_3">[ˆ p(−|t), ˆ p(+|t)]</formula><p>is a two-dimensional one-hot representation, where the annotation of words is given by the class labels:</p><formula xml:id="formula_4">[ˆ p(−|t), ˆ p(+|t)] =      [0, 1], if SO(t) &gt; 0 [1, 0], if SO(t) &lt; 0 random{[1, 0]or[0, 1]}, otherwise .<label>(4)</label></formula><p>(3) PMI-SO lexicon with soft sentiment annota- tion "Soft sentiment annotation" means that the anno- tation is given by the probability of two sentiment polarities, rather than the class label. We first use the sigmoid function to map the SO score to the range of a probability, and then define</p><formula xml:id="formula_5">[ˆ p(−|t), ˆ p(+|t)] = [1 − σ(SO(t)), σ(SO(t))]<label>(5)</label></formula><p>as the PMI-SO soft sentiment distribution of the word t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning Sentiment-aware Word Representation under Hierarchical Sentiment Supervision</head><p>Till now we have obtained both document and word-level sentiment annotations, in the next step, we propose a neural network framework to learn the sentiment-aware word representation by inte- grating the sentiment supervision at both word and document granularities. We call it "hierarchical sentiment supervision". The architecture of our model is shown in <ref type="figure">Figure 1</ref>. We denote the corpus as D = {d 1 , d 2 , ..., d N } where N is the size of the corpus. Suppose d k is k-th document in D, and t i represents the i-th word in a document d. The pa- rameters used in our neural network are described in <ref type="table">Table 1</ref>.</p><p>We construct a embedding matrix C ∈ R V ×M , of which each row represents the embedding of a word in the vocabulary, where V is the size of the vocabulary and M is the dimension of word em- bedding. We randomly initialize each element of matrix C with a normal distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(1) Word-Level Sentiment Supervision</head><p>We use the word-level sentiment annotation [ˆ p(−|t), ˆ p(+|t)] provided in Section 3.1 to super- vise word representation learning at the word lev- el.</p><p>For each word in document d, we map it to a continuous representation as e ∈ C and feed e into our model to predict the sentiment distribution of the input word:</p><formula xml:id="formula_6">p(c|e) = sof tmax(θ t · e + b t ).<label>(6)</label></formula><p>The cost function is defined as the average cross entropy that measures the difference between the sentiment distribution predicted in our model and the sentiment annotations at the word level:</p><formula xml:id="formula_7">f word = − 1 T N k=1 t∈d k c∈{+,−} ˆ p(c|t) log p(c|e t )<label>(7)</label></formula><p>where T is the number of words in corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(2) Document-Level Sentiment Supervision</head><p>We use the document-level sentiment annotation- s to supervise word representation learning at the document level.</p><p>In order to obtain a continuous representation of a document d, we simply use the average embed- ding of words in d as de:</p><formula xml:id="formula_8">de = 1 |d| t∈d e t .<label>(8)</label></formula><p>We feed de into our model to predict the sentiment probability: Similarly, the cost function is defined as aver- age cross entropy that measures the difference be- tween the sentiment distribution predicted in our model and the sentiment annotation at the docu- ment level:</p><formula xml:id="formula_9">p(c|de) = sof tmax(θ d · de + b d ).<label>(9)</label></formula><formula xml:id="formula_10">f doc = − 1 N N k=1 c∈{+,−} ˆ p(c|d k ) log p(c|de k ) (10) wherê p(c|d k ) is the sentiment annotation of doc- ument d k . ˆ p(c|d k ) = 1 denotes the class label of d k is positive, otherwisê p(c|d k ) = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(3) Word and Document-Level Joint Learning</head><p>In order to learn the sentiment-aware word repre- sentation at both word and document levels, we in- tegrate the cost function of two levels in a weight- ed combination way. The final cost function is de- fined as follows:</p><formula xml:id="formula_11">f = αf word + (1 − α)f doc (11)</formula><p>where α is a tradeoff parameter(0 ≤ α ≤ 1). The weight of f word can be increased by choosing a lager value of α.</p><p>We train our neural model with stochastic gradi- ent descent and use <ref type="bibr">AdaGrad (Duchi et al., 2011)</ref> to update the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">From Sentiment Representation to Sentiment Lexicon</head><p>In this part, we follow the method proposed by <ref type="bibr" target="#b13">Tang et al. (2014a)</ref> to build a classifier to convert the sentiment-aware word representation learned in Section 3.2 to a sentiment lexicon. The word representation is the input of the classifier and word sentiment polarity is the output.</p><p>Firstly, we utilize the embedding of 125 positive and 109 negative seed words manually labelled by <ref type="bibr" target="#b13">Tang et al. (2014a)</ref> as training data 1 .</p><p>Secondly, a variant-KNN classifier is also ap- plied to extending the seed words on a web dictio- nary called Urban Dictionary. Unlike (Tang et al.,  <ref type="table" target="#tab_2">SemEval2013-train  3632  1449  5081  SemEval2013-dev  482  282  764  SemEval2013-test  1474  559  2033  SemEval2014-test  982  202  1184  SemEval2015-test  1038  365  1403  SemEval2016-test  7059  3231  10290   Table 2</ref>: Statistics of Evaluation Datasets 2014a), we did not extend the neutral words.</p><p>Thirdly, a traditional logistic regression classi- fier is trained by using the embeddings of extend- ed sentiment words as the inputs. The sentiment score of a word is the difference between its posi- tive and negative probabilities.</p><p>Finally, the sentiment lexicon can be collected by using the classifier to predict the other words' sentiment score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Settings</head><p>We utilize the public distant-supervision corpus 2 ( <ref type="bibr" target="#b2">Go et al., 2009</ref>) to learn our lexicons. We set M , the dimension of embedding, as 50. The learning rate is 0.3 for stochastic gradient descent optimiz- er. We tune the hyper-parameter α in the training process.</p><p>We evaluate the sentiment lexicons in both su- pervised and unsupervised sentiment classification tasks, on the SemEval 2013-2016 datasets. The s- tatistics of evaluation datasets are shown in <ref type="table">Table  2</ref>.</p><p>Supervised Sentiment Classification Evalua- tion: To evaluate the effect of the sentiment lexi- con in supervised sentiment classification, we re- port the supervised sentiment classification perfor- mance by using some pre-defined lexicon features. We follow ( <ref type="bibr" target="#b11">Mohammad et al., 2013)</ref> to extract the lexicon features as follows:</p><p>• Total count of words in the tweet score of which is greater than 0;</p><p>• Total count of words in the tweet score of which is less than 0;</p><p>• The sum of scores for all word great than 0;</p><p>2 http://help.sentiment140.com/for-students</p><p>• The sum of scores for all word less than 0;</p><p>• The max score greater than 0;</p><p>• The min score less than 0;</p><p>• Non-zero score of the last positive word in the tweet;</p><p>• Non-zero score of the last negative word in the tweet.</p><p>We report the performance of SVM by using these lexicon features. The LIBSVM 3 toolkit is used with a linear kernel and the penalty parameter is set as the default value. The metric is F 1 score.</p><p>Unsupervised Sentiment Classification Eval- uation: For unsupervised sentiment classification, we sum up the scores of all sentiment words in the document, according to the sentiment lexicon. If the sum is greater than 0, the document will be considered as positive, otherwise negative. The unsupervised learning evaluation metric is accu- racy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">(External) Comparison with Public Lexicons</head><p>We compare our HSSWE method with four senti- ment lexicons generated by the related work pro- posed in recent years:</p><p>• Sentiment140 was constructed by <ref type="bibr" target="#b11">Mohammad et al. (2013)</ref> on tweet corpus based on PMI between each word and the emoticons.</p><p>• HIT was constructed by <ref type="bibr" target="#b13">Tang et al. (2014a)</ref> with a representation learning approach.</p><p>• NN was constructed by Vo and Zhang (2016) with a neural network method.</p><p>• ETSL refers to SemEval-2015 English Twit- ter Sentiment Lexicon 4 ( <ref type="bibr" target="#b12">Rosenthal et al., 2015;</ref><ref type="bibr" target="#b9">Kiritchenko et al., 2014</ref>), which is done using Best-Worst Scaling.</p><p>Note that <ref type="bibr" target="#b13">Tang et al. (2014a)</ref>, <ref type="bibr" target="#b17">Vo and Zhang (2016)</ref>     <ref type="table" target="#tab_3">Table 4</ref>. In can be seen that HSSWE obtains the best performance on Semeval 2013-2015. On the Semeval 2016 dataset, it is s- lightly lower than ETSL. Across four datasets, the average accuracy of HSSWE is 6.6, 3.1, 9.6 and 0.94 higher than Sentiment140, HIT, NN and ET- SL, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">(Internal) Comparison within the Model</head><p>In order to further verify the effectiveness of our method and analyze which part of our model con- tributes the most, we carried out the internal com- parison within our model. We design the follow- ing two simplified versions of our model for com- parison:</p><p>• PMI-SO denotes a PMI-SO based senti- ment lexicon with soft sentiment annotation learned in Section 3.1.</p><p>• Doc-Sup denotes the neural network system with only document-level sentiment supervi- sion. It equals to HSSWE when α = 0.</p><p>Actually, HSSWE can be viewed as a "combi- nation" of PMI-SO and Doc-Sup. In <ref type="table" target="#tab_5">Tables 5 and  6</ref>, we report the comparison results on supervised and unsupervised sentiment classification respec- tively.</p><p>Supervised Sentiment Classification: As is shown in <ref type="table" target="#tab_5">Table 5</ref>, two basic models PMI-SO and Doc-Sup show similar performance. They have distinct superiority across different datasets. But both are significantly lower than HSSWE. It shows that by combing the supervision at both document and word levels, it can indeed improve the quality of sentiment-aware word embedding and the sub- sequent sentiment lexicon.</p><p>Unsupervised Sentiment Classification: As is shown in <ref type="table" target="#tab_6">Table 6</ref>, the conclusions are similar with that in supervised sentiment classification: HSS- WE achieves the significantly better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Word-level Sentimnt Annotation: Hard vs. Soft</head><p>In Section 3.1, we introduce two kinds of word- level sentiment annotation, i.e., soft and hard sen- timent annotation. We now compare two meth- ods. The results are reported in <ref type="table" target="#tab_5">Tables 5 and  6</ref>. It can be seen that for supervised evaluation, HSSWE (soft) and HSSWE (hard) yield compar- ative performance. HSSWE (hard) has slight su- periority over HSSWE (hard) in <ref type="bibr">Semeval 2013</ref><ref type="bibr" target="#b9">, 2014</ref> is better on Semeval2015. In contrast, for unsupervised eval- uation, HSSWE (soft) is significantly better than    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Tuning the Parameter α</head><p>In this section, we discuss the tradeoff between t- wo parts of supervisions by turning the tradeoff parameter α. When α is 0, HSSWE only ben- efits from the document-level sentiment supervi- sion and when α is 1, HSSWE benefits from only word-level sentiment supervision. We observe that HSSWE performs better when α is in the range of <ref type="bibr">[0.45,0.55]</ref>. By integrating two component parts of sentiment supervision, HSSWE has significant superiority over that learned from either one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Lexicon Analysis</head><p>In order to gain more insight of our model and observe the effectiveness of the sentiment lexi- con, in <ref type="table" target="#tab_8">Table 7</ref> we extract the positive sentimen-  t score of some representative words learned by different methods. The positive scores are sup- posed to be: best&gt;better&gt;well. HSSWE captures such comparative sentiment strength but PMI-SO does not. We further observe that in many cases where the results of PMI-SO and Doc-Sup are in- consistent (e.g., Doc-Sup incorrectly predicts "un- reasonable", "boreddddd" and "sickkk" as pos- itive words, but PMI-SO predicts them correct- ly; PMI-SO incorrectly predicts "fit" but Doc-Sup predicts it correctly.), HSSWE often yield the cor- rect results. It shows the advantages of hierarchi- cal sentiment supervision. HSSWE can also cor- rect the sentiment prediction where both PMI-SO and Doc-Sup are inefficient (e.g., "overplayed").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed to construct sentiment lexicons based on a sentiment-aware word repre- sentation learning approach. In contrast to tradi- tional methods normally learned based on only the document-level sentiment supervision. We pro- posed word representation learning via hierarchi- cal sentiment supervision, i.e., under the supervi-sion at both word and document levels. The word- level supervision can be provided based on either predefined sentiment lexicons or the learned PMI- SO based sentiment annotation of words. A wide range of experiments were conducted on several benchmark sentiment classification datasets. The results indicate that our method is quite effective for sentiment-aware word representation, and the sentiment lexicon generated by our approach beats the state-of-the-art sentiment lexicon construction approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The word-level sentiment annotation is repre- sented as [ˆ p(−|t), ˆ p(+|t)]. We employ the follow- ing two ways to obtain [ˆ p(−|t), ˆ p(+|t)].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: HSSWE (soft) with different α</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>used incomplete dataset of SemEval2013 in their papers. For fair comparison, we conduct</figDesc><table>Lexicon 

Semeval2013 Semeval2014 Semeval2015 Semeval2016 Average 
Sentiment140 
0.7317 
0.7271 
0.6917 
0.6809 
0.7079 
HIT 
0.7181 
0.6947 
0.6797 
0.6928 
0.6963 
NN 
0.7225 
0.7115 
0.6970 
0.6887 
0.7049 
ETSL 
0.7104 
0.7090 
0.6650 
0.6862 
0.6926 
HSSWE 
0.7550 
0.7424 
0.6921 
0.7097 
0.7248 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 : Supervised Evaluation for External Comparison (F 1 Score)</head><label>3</label><figDesc></figDesc><table>Lexicon 
Semeval2013 Semeval2014 Semeval2015 Semeval2016 Average 
Sentiment140 
0.7208 
0.7416 
0.6935 
0.6928 
0.7122 
HIT 
0.7566 
0.7922 
0.7128 
0.7282 
0.7474 
NN 
0.6903 
0.7280 
0.6507 
0.6585 
0.6819 
ETSL 
0.7675 
0.8226 
0.7505 
0.7365 
0.7693 
HSSWE 
0.7734 
0.8539 
0.7669 
0.7206 
0.7787 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Unsupervised Evaluation for External Comparison (Accuracy) 

all the comparison experiments on the complete 
benchmark datasets. 

Supervised Sentiment Classification: We first 
report the supervised sentiment classification F 1 
score of five compared methods on the Semeval 
2013-2016 datasets in Table 3. It can be seen that 
our HSSWE method gets the best result on all four 
datasets. It outperforms Sentiment140, HIT, NN 
and ETSL 1.7, 2.8, 1.9, and 3.2 percentages on the 
average of four datasets. The improvements are 
significant according to the paired t-test. 

Unsupervised Sentiment Classification: We 
then report the unsupervised sentiment classifi-
cation accuracy of five methods on the Semeval 
2013-2016 datasets in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 : Supervised Evaluation for Internal Comparison (F 1 Score), where HSSWE (hard) and HSSWE (soft) utilize the PMI-SO lexicon with hard sentiment annotation and soft sentiment annotation at the word level, respectively.</head><label>5</label><figDesc></figDesc><table>Lexicon 
Semeval2013 Semeval2014 Semeval2015 Semeval2016 Average 
Doc-Sup 
0.7252 
0.8294 
0.7391 
0.6859 
0.7449 
HSSWE (soft) 
0.7734 
0.8539 
0.7669 
0.7206 
0.7787 
HSSWE (hard) 
0.7418 
0.8395 
0.7633 
0.7011 
0.7614 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Unsupervised Evaluation for Internal Comparison (Accuracy) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Sentiment Lexicon Analysis, where s-
core with (F) means falsely predicted polarity or 
strength. 

</table></figure>

			<note place="foot" n="1"> http://ir.hit.edu.cn/ dytang/paper/14coling/data.zip</note>

			<note place="foot" n="3"> http://www.csie.ntu.edu.tw/ cjlin/libsvm 4 http://www.saifmohammad.com/WebDocs/lexiconstoreleas eonsclpage/SemEval2015-English-Twitter-Lexicon.zip</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The work was supported by the Natural Science Foundation of China (No. 61672288), and the Natural Science Foundation of Jiangsu Province for Excellent Young Scholars (No. BK20160085).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Twitter sentiment classification using distant supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richa</forename><surname>Bhayani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<pubPlace>Stanford</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">CS224N Project Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Inducing domain-specific sentiment lexicons from unlabeled corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="595" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predicting the semantic orientation of adjectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kathleen R Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics</title>
		<meeting>the eighth conference on European chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="174" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sentiment lexicon creation from lexical resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bas</forename><surname>Heerschop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hogenboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flavius</forename><surname>Frasincar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Business Information Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="185" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the Tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using wordnet to measure semantic orientations of adjectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Marx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Robert J Mokken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Language Resources and Evaluation</title>
		<meeting>the Fourth International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1115" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Determining the sentiment of opinions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Soo-</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">1367</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sentiment analysis of short informal texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="723" to="762" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop at 1st International Conference on Learning Representations (ICLR2013)</title>
		<meeting>Workshop at 1st International Conference on Learning Representations (ICLR2013)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nrc-canada: Building the state-ofthe-art in sentiment analysis of tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh international workshop on Semantic Evaluation Exercises (SemEval-2013)</title>
		<meeting>the seventh international workshop on Semantic Evaluation Exercises (SemEval-2013)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="321" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semeval-2015 task 10: Sentiment analysis in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval@ NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="451" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Building large-scale twitter-specific sentiment lexicon : A representation learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics (COLING 2014)</title>
		<meeting>the 25th International Conference on Computational Linguistics (COLING 2014)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="172" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning sentimentspecific word embedding for twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter D Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The latent relation mapping engine: Algorithm and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter D Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="615" to="655" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dont count, predict! an automatic approach to learning sentiment lexicons for short text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tin</forename><surname>Duy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="219" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A survey on automatical construction methods of sentiment lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Automatica Sinica</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="495" to="511" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
