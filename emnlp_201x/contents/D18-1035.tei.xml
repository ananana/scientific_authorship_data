<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Stable and Effective Learning Strategy for Trainable Greedy Decoding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">New York University</orgName>
								<address>
									<settlement>CIFAR Global Scholar</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
							<email>bowman@nyu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">New York University</orgName>
								<address>
									<settlement>CIFAR Global Scholar</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Stable and Effective Learning Strategy for Trainable Greedy Decoding</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="380" to="390"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>380</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Beam search is a widely used approximate search strategy for neural network decoders, and it generally outperforms simple greedy decoding on tasks like machine translation. However, this improvement comes at substantial computational cost. In this paper, we propose a flexible new method that allows us to reap nearly the full benefits of beam search with nearly no additional computational cost. The method revolves around a small neural network actor that is trained to observe and manipulate the hidden state of a previously-trained decoder. To train this actor network, we introduce the use of a pseudo-parallel corpus built using the output of beam search on a base model, ranked by a target quality metric like BLEU. Our method is inspired by earlier work on this problem, but requires no reinforcement learning, and can be trained reliably on a range of models. Experiments on three parallel corpora and three architectures show that the method yields substantial improvements in translation quality and speed over each base system.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural network sequence decoders yield state- of-the-art results for many text generation tasks, including machine translation ( <ref type="bibr" target="#b3">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b27">Luong et al., 2015;</ref><ref type="bibr" target="#b15">Gehring et al., 2017;</ref><ref type="bibr" target="#b39">Vaswani et al., 2017;</ref><ref type="bibr" target="#b11">Dehghani et al., 2018</ref>), text summarization ( <ref type="bibr" target="#b32">Rush et al., 2015;</ref><ref type="bibr" target="#b31">Ranzato et al., 2015;</ref><ref type="bibr" target="#b33">See et al., 2017;</ref><ref type="bibr" target="#b30">Paulus et al., 2017</ref>) and image captioning ( <ref type="bibr" target="#b42">Xu et al., 2015</ref>). These decoders generate tokens from left to right, at each step giving a distribution over possi- ble next tokens, conditioned on both the input and all the tokens generated so far. However, since the space of all possible output sequences is infinite and grows exponentially with sequence length, heuristic search methods such as greedy decod- ing or beam search <ref type="bibr" target="#b16">(Graves, 2012;</ref><ref type="bibr">BoulangerLewandowski et al., 2013)</ref> must be used at de- coding time to select high-probability output se- quences. Unlike greedy decoding, which selects the token of the highest probability at each step, beam search expands all possible next tokens at each step, and maintains the k most likely pre- fixes, where k is the beam size. Greedy decoding is very fast-requiring only a single run of the un- derlying decoder-while beam search requires an equivalent of k such runs, as well as substantial additional overhead for data management. How- ever, beam search often leads to substantial im- provement over greedy decoding. For example, <ref type="bibr" target="#b31">Ranzato et al. (2015)</ref> report that beam search (with k = 10) gives a 2.2 BLEU improvement in trans- lation and a 3.5 ROUGE-2 improvement in sum- marization over greedy decoding.</p><p>Various approaches have been explored recently to improve beam search by improving the method by which candidate sequences are scored ( <ref type="bibr" target="#b25">Li et al., 2016;</ref><ref type="bibr" target="#b36">Shu and Nakayama, 2017)</ref>, the termination criterion ( <ref type="bibr" target="#b20">Huang et al., 2017)</ref>, or the search func- tion itself ( . In contrast, <ref type="bibr" target="#b18">Gu et al. (2017)</ref> have tried to directly improve greedy de- coding to decode for an arbitrary decoding objec- tive. They add a small actor network to the de- coder and train it with a version of policy gradient to optimize sequence objectives like BLEU. How- ever, they report that they are seriously limited by the instability of this approach to training.</p><p>In this paper, we propose a procedure to mod- ify a trained decoder to allow it to generate text greedily with the level of quality (according to metrics like BLEU) that would otherwise require the relatively expensive use of beam search. To do so, we follow Cho (2016) and <ref type="bibr" target="#b18">Gu et al. (2017)</ref> in our use of an actor network which manipulates the decoder's hidden state, but introduce a stable and effective procedure to train this actor. In our training procedure, the actor is trained with ordi- nary backpropagation on a model-specific artifi- cial parallel corpus. This corpus is generated by running the un-augmented model on the training set with large-beam beam search, and selecting outputs from the resulting k-best list which score highly on our target metric.</p><p>Our method can be trained quickly and reli- ably, is effective, and can be straightforwardly em- ployed with a variety of decoders. We demon- strate this for neural machine translation on three state-of-the-art architectures: RNN-based ( <ref type="bibr" target="#b27">Luong et al., 2015</ref>), <ref type="bibr">ConvS2S (Gehring et al., 2017)</ref> and <ref type="bibr">Transformer (Vaswani et al., 2017)</ref>, and three corpora: IWSLT16 German-English, 1 WMT15 Finnish-English 2 and WMT14 German-English. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Machine Translation</head><p>In sequence-to-sequence learning, we are given a set of source-target sentence pairs and tasked with learning to generate each target sentence (as a se- quence of words or word-parts) from its source sentence. We first use an encoding model such as a recurrent neural network to transform a source se- quence into an encoded representation, then gen- erates the target sequence using a neural decoder.</p><p>Given a source sentence x = {x 1 , ..., x Ts }, a neural machine translation system models the distribution over possible output sentences y = {y 1 , ..., y T } as:</p><formula xml:id="formula_0">P (y|x; θ) = T t=1 P (y t |y &lt;t , x; θ),<label>(1)</label></formula><p>where θ is the set of model parameters. Given a parallel corpus D x,y of source-target sentence pairs, the neural machine translation model can be trained by maximizing the log- likelihood:</p><formula xml:id="formula_1">ˆ θ = argmax θ x,y∈Dx,y log P (y|x; θ) .<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Decoding</head><p>Given estimated model parametersˆθparametersˆ parametersˆθ, the decision rule for finding the translation with the highest 1 https://wit3.fbk.eu/ 2 http://www.statmt.org/wmt15/translation-task.html 3 http://www.statmt.org/wmt14/translation-task probability for a source sentence x is given byˆy byˆ byˆy = argmax</p><formula xml:id="formula_2">y P (y|x; ˆ θ) .<label>(3)</label></formula><p>However, since such exact inference requires the intractable enumeration of large and potentially infinite set of candidate sequences, we resort to approximate decoding algorithms such as greedy decoding, beam search, noisy parallel decoding (NPAD; Cho, 2016), or trainable greedy decoding ( <ref type="bibr" target="#b18">Gu et al., 2017)</ref>.</p><p>Greedy Decoding In this algorithm, we gener- ate a single sequence from left to right, by choos- ing the token that is most likely at each step. The outputˆyoutputˆ outputˆy = {ˆy{ˆy 1 , ..., ˆ y T } can be represented asˆy asˆ asˆy t = argmax</p><formula xml:id="formula_3">y P (y|ˆyy|ˆy &lt;t , x; ˆ θ) .<label>(4)</label></formula><p>Despite its low computational complexity of O(|V | × T ), the translations selected by this method may be far from optimal under the over- all distribution given by the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Beam Search</head><p>Beam search decodes from left to right, and maintains k &gt; 1 hypotheses at each step. At each step t, beam search considers all pos- sible next tokens conditioned on the current hy- potheses, and picks the k with the overall highest scores t t =1 P (y t |y &lt;t , x; ˆ θ). When all the hy- potheses are complete (they end in an end-of-the- sentence symbol or reach a predetermined length limit), it returns the hypothesis with the highest likelihood. Tuning to find a roughly optimal beam size k can yield improvements in performance with sizes as high as <ref type="bibr">30 (Koehn and Knowles, 2017;</ref><ref type="bibr" target="#b5">Britz et al., 2017)</ref>. However, the complexity of beam search grows linearly in beam size, with high constant terms, making it undesirable in some applications where latency is important, such as in on-device real-time translation.</p><p>NPAD Noisy parallel approximate decoding (NPAD; Cho, 2016) is a parallel decoding algo- rithm that can be used to improve greedy decod- ing or beam search. The main idea is that a better translation with a higher probability may be found by injecting unstructured random noise into the hidden state of the decoder network. Positive re- sults with NPAD suggest that small manipulations to the decoder hidden state can correspond to sub- stantial but still reasonable changes to the output sequence. Trainable Greedy Decoding Approximate de- coding algorithms generally approximate the maximum-a-posteriori inference described in Equation 3. This is not necessarily the optimal ba- sis on which to generate text, since (i) the condi- tional log-probability assigned by a trained NMT model does not necessarily correspond well to translation quality ( <ref type="bibr" target="#b38">Tu et al., 2017)</ref>, and (ii) dif- ferent application scenarios may demand differ- ent decoding objectives ( <ref type="bibr" target="#b18">Gu et al., 2017</ref>). To solve this, <ref type="bibr" target="#b18">Gu et al. (2017)</ref> extend NPAD by re- placing the unstructured noise with a small feed- forward actor neural network. This network is trained using a variant of policy gradient rein- forcement learning to optimize for a target qual- ity metric like BLEU under greedy decoding, and is then used to guide greedy decoding at test time by modifying the decoder's hidden states. Despite showing gains over the equivalent actorless model, their attempt to directly optimize the quality met- ric makes training unstable, and makes the model nearly impossible to optimize fully. This paper of- fers a stable and effective alternative approach to training such an actor, and further develops the ar- chitecture of the actor network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>We propose a method for training a small actor neural network, following the trainable greedy de- coding approach of <ref type="bibr" target="#b18">Gu et al. (2017)</ref>. This actor takes as input the current decoder state h t , an at- tentional context vector e t for the source sentence, and optionally the previous hidden state s t−1 of the actor, and produces a vector-valued action a t which is used to update the decoder hidden state. The actor function can take on a variety of forms, and we explore four: a feedforward network with one hidden layer (ff ), feedforward network with two hidden layers (ff2), a GRU recurrent network (rnn; <ref type="bibr" target="#b9">Cho et al., 2014)</ref>, and gated feedforward net- work (gate).</p><p>The feedforward ff actor function is computed as</p><formula xml:id="formula_4">z t = σ([h t , e t ]W i + b i ), a t = tanh(z t W o + b o ),<label>(5)</label></formula><p>the ff2 actor is computed as</p><formula xml:id="formula_5">z 1 t = σ([h t , e t ]W i + b i ), z 2 t = σ(z 1 t W z + b z ), a t = tanh(z 2 t W o + b o ),<label>(6)</label></formula><p>the rnn actor is computed as</p><formula xml:id="formula_6">z t = σ([h t , e t ]U z + s t−1 W z ), r t = σ([h t , e t ]U r + s t−1 W r ), ˜ s t = tanh [h t , e t ]U h + (s t−1 • r t )W h , s t = (1 − z t ) • ˜ s t + z t • s t−1 , a t = s t U,<label>(7)</label></formula><p>and the gate actor is computed as</p><formula xml:id="formula_7">z t = σ([h t , e t ]U z ), a t = z t • tanh([h t , e t ]U ).<label>(8)</label></formula><p>Once the action a t has been computed, the hid- den state h t is simply replaced with the updated state˜hstate˜ state˜h t : <ref type="figure" target="#fig_0">Figure 1</ref> shows a single step of the actor inter- acting with the underlying neural decoder of each of the three NMT architectures we use: the RNN- based model of <ref type="bibr" target="#b27">Luong et al. (2015)</ref>, <ref type="bibr">ConvS2S (Gehring et al., 2017)</ref>, and Transformer ( <ref type="bibr" target="#b39">Vaswani et al., 2017)</ref>. We add the actor at the decoder layer immediately after the computation of the atten- tional context vector. For the RNN-based NMT, we add the actor network only to the last decoder layer, the only place attention is used. Here, it takes as input the hidden state of the last decoder layer h L t and the source context vector e t , and out- puts the action a t , which is added back to the at- tention vector˜hvector˜ vector˜h L t . For ConvS2S and Transformer, we add an actor network to each decoder layer. This actor is added to the sublayer which performs multi-head or multi-step attention over the output of the encoder stack. It takes as input the decoder state h l t and the source context vector e l t , and out- puts an action a l t which is added back to get˜hget˜ get˜h l t . Training To overcome the severe instability re- ported by <ref type="bibr" target="#b18">Gu et al. (2017)</ref>, we introduce the use of a pseudo-parallel corpus generated from the un- derlying NMT model ( <ref type="bibr" target="#b14">Gao and He, 2013;</ref><ref type="bibr" target="#b1">Auli and Gao, 2014;</ref><ref type="bibr" target="#b21">Kim and Rush, 2016;</ref><ref type="bibr" target="#b13">Freitag et al., 2017;</ref><ref type="bibr" target="#b43">Zhang et al., 2017</ref>) for actor training. This corpus includes pairs that both (i) have a high model likelihood, so that we can coerce the model to generate them without much additional training or many new parameters and, (ii) represent high-quality translations, measured according to a target metric like BLEU. We do this by generating sentences from the original unaug- mented model with large-beam beam search and selecting the best sentence from the resulting k- best list according to the decoding objective.</p><formula xml:id="formula_8">˜ h t = f (h t , e t ) + a t .<label>(9)</label></formula><p>More specifically, let x, y be a sentence pair in the training data and Z = {z 1 , ..., z k } be the k- best list from beam search on the pretrained NMT model, where k is the beam size. We define the objective score of the translation z w.r.t. the gold- standard translation y according to a target met- ric such as BLEU ( <ref type="bibr" target="#b29">Papineni et al., 2002</ref>), NIST <ref type="bibr" target="#b12">(Doddington, 2002</ref>), negative TER ( <ref type="bibr" target="#b37">Snover et al., 2006</ref>), or METEOR ( <ref type="bibr" target="#b24">Lavie and Denkowski, 2009)</ref> as O(z, y). Then we choose the sentence˜zsentence˜ sentence˜z that has the highest score to become our new target sentence:</p><formula xml:id="formula_9">˜ z = argmax z=z 1 ,..,z k O(z, y).<label>(10)</label></formula><p>Once we obtain the pseudo-corpus</p><formula xml:id="formula_10">D x,z = {{x i , ˜ z i } n i=1</formula><p>}, we keep the underlying model fixed and train the actor by maximizing the log- likelihood of the actor parameters with these pairs:</p><formula xml:id="formula_11">ˆ θ a = argmax θa x,z∈Dx,z log P (z|x; ˆ θ, θ a )<label>(11)</label></formula><p>In this way, the actor network is trained to manip- ulate the neural decoder's hidden state at decoding time to induce it to produce better-scoring outputs under greedy or small-beam decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setting</head><p>We evaluate our approach on IWSLT16 German- English, WMT15 Finnish-English, and WMT14 De-En translation in both directions with three strong translation model architectures. For IWSLT16, we use tst2013 and tst2014 for validation and testing, respectively. For WMT15, we use newstest2013 and newstest2015 for vali- dation and testing, respectively. For WMT14, we use newstest2013 and newstest2014 for validation and testing, respectively. All the data are tok- enized and segmented into subword symbols using byte-pair encoding (BPE; <ref type="bibr" target="#b34">Sennrich et al., 2016)</ref> to restrict the size of the vocabulary. Our pri- mary evaluations use tokenized and cased BLEU. For METEOR and TER evaluations, we use mul- teval <ref type="bibr">4</ref> with tokenized and case-insensitive scor- ing. All the underlying models are trained from scratch, except for ConvS2S WMT14 English- German translation, for which we use the trained model (as well as training data) provided by <ref type="bibr" target="#b15">Gehring et al. (2017)</ref>. <ref type="bibr">5</ref>   <ref type="table">Table 2</ref>: Generation quality (BLEU↑) using the pro- posed trainable greedy decoder without and with beam search (k = 4). Results without beam search (tg) are also appeared in <ref type="table">Table 1.</ref> RNN We use OpenNMT-py ( <ref type="bibr" target="#b22">Klein et al., 2017)</ref> 6 to implement our model. It is com- posed of an encoder with two-layer bidirec- tional RNN, and a decoder with another two- layer RNN. We refer to OpenNMT's default setting (rnn size = 500, word vec size = 500) and the setting in ConvS2S We implement our model based on fairseq-py. <ref type="bibr">7</ref> We follow the settings in fconv iwslt de en and fconv wmt en de for IWSLT16 and WMT, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLEU↑</head><note type="other">tok/s↑ BLEU↑ tok/s↑ greedy beam4 tg greedy beam4 tg greedy beam4 tg greedy beam4 tg IWSLT16 De → En En →</note><p>Transformer We implement our model based on the code from <ref type="bibr" target="#b17">Gu et al. (2018)</ref>. <ref type="bibr">8</ref> We follow their hyperparameter settings for all experiments.</p><p>In the results below, we focus on the gate ac- tor and pseudo-parallel corpora constructed by choosing the sentence with the best BLEU score from the k-best list produced by beam search with k = 35. Experiments motivating these choices are shown later in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Analysis</head><p>The results <ref type="table">(Table 1)</ref> show that the use of the ac- tor makes it practical to replace beam search with greedy decoding in most cases: We lose little or no performance, and doing so yields an increase in decoding efficiency, even accounting for the small overhead added by the actor. Among the three ar- chitectures, ConvS2S-the one with the most and largest layers-performs best. We conjecture that this gives the decoder more flexibility with which to guide decoding. In cases where model through- put is less important, our method can also be com- bined with beam search at test time to yield re- sults somewhat better than either could achieve alone. <ref type="table">Table 2</ref> shows the result when combining our method with beam search. Am Vormittag wollte auch die Arbeitsgruppe Migration und Integration ihre Beratungen fortsetzen . ref During the morning , the Migration and Integration working group also sought to continue its discussions .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>greedy</head><p>The morning also wanted to continue its discussions on migration and integration . beam4</p><p>In the morning , the working group on migration and integration also wanted to continue its discussions . beam35</p><p>In the morning , the migration and integration working group also wanted to continue its discussions .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>tg</head><p>The morning , the Migration and Integration Working Group wanted to continue its discussions . tg+beam4 In the morning , the Migration and Integration Working Group wanted to continue its discussions .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>src</head><p>Die meisten Mails werden unterwegs mehrfach von Software-Robotern gelesen . ref</p><p>The majority of e-mails are read several times by software robots en route to the recipient . greedy Most mails are read by software robots on the go . beam4</p><p>Most mails are read by software robots on the go . beam35</p><p>Most e-mails are read several times by software robots on the road .   <ref type="table">Table 4</ref>: Word-level likelihood (%) averaged by sentence for the IWSLT16 and WMT14 De-En test sets with Transformer. Each row represents the model used to evaluate word-level likelihood, and each column represents a different source of translations, including the reference (ref), greedy decoding on the base model (greedy), beam search with k = 35 on the base model and the BLEU scorer (k35 ), and trainable greedy decoder (tg).</p><p>Examples <ref type="table" target="#tab_2">Table 3</ref> shows a few selected transla- tions from the WMT14 German-English test set. In manual inspection of these examples and oth- ers, we find that the actor encourages models to recover missing tokens, optimize word order, and correct prepositions.</p><p>Likelihood We also compare word-level likeli- hood for different decoding results assigned by the base model and the actor-augmented model. For a sentence pair x, y, word-level likelihood is de- fined as <ref type="table">Table 4</ref> shows the word-level likelihood aver- aged over the test set for IWSLT16 and WMT14 German to English translation with Transformer.</p><formula xml:id="formula_12">P w = 1 T T t=1 P (y t |y &lt;t , x; θ).<label>(12)</label></formula><p>Our trainable greedy decoder learns a much more peaked distribution and assigns a much higher probability mass to its greedy decoding result than the base model. When evaluated under the base model, the translations from trainable greedy de- coding have smaller likelihood than the transla- tions from greedy decoding using the base model for both datasets. This indicates that the trainable greedy decoder is able to find a sequence that is not highly scored by the underlying model, but that corresponds to a high value of the target metric.</p><p>Magnitude of Action Vector We also record the L 2 norm of the action, decoder hidden state, and attentional source context vectors on the validation set. <ref type="figure" target="#fig_3">Figure 2</ref> shows these values over the course of training on the IWSLT16 De-En validation set with Transformer. The norm of the action starts small, increases rapidly early in training, and con- verges to a value well below that of the decoder hidden state. This suggests that the action adjusts the decoders hidden state only slightly, rather than overwriting it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effects of Model Settings</head><p>Actor Architecture <ref type="figure" target="#fig_4">Figure 3</ref> shows the train- able greedy decoding result on IWSLT16 De-En validation set with different actor architectures. We observe that our approach is stable across dif- ferent actor architectures and is relatively insen- sitive to the hyperparameters of the actor. For the same type of actor, the performance increases gradually with the hidden layer size. The use of a recurrent connection within the actor does not meaningfully improve performance, possibly since all actors can use the recurrent connections of the underlying decoder. Since the gate actor contains no additional hyperparameters and was observed to learn quickly and reliably, we use it in all other experiments. Here, we also explore a simple alternative to the use of the actor: creating a pseudo-parallel corpus with each model, and then training each model, unmodified and entirety, directly on this new corpus. This experiment (cont. in <ref type="figure" target="#fig_4">Figure 3</ref>) yields results that are comparable to, but not bet- ter than, the results seen with the actors. How- ever, this comes with substantially greater com- putational complexity at training time, and, if the same trained model is to be optimized for multiple target metrics, greater storage costs as well. Beam Size <ref type="figure" target="#fig_5">Figure 4a</ref> shows the effect of the beam size used to generate the pseudo-parallel corpus on the IWSLT16 De-En validation set with Transformer. Trainable greedy decoding improves over greedy decoding even when we set k = 1, namely, running greedy decoding on the unaug- mented model to construct the new training cor- pus. With increased beam size k, the BLEU score consistently increases, but we observe diminish- ing returns beyond roughly k = 35, and we use that value elsewhere.</p><p>Training Corpus Construction There are a va- riety of ways one might use the output of beam search to construct a pseudo-parallel corpus: We could use the single highest-scoring output (by BLEU, or our target metric) for each input (top1), use all 35 beam search outputs (full), use all those outputs that score higher than the threshold, namely the base model's greedy decoding output (thd), or combine the top1 results with the gold- standard translations (comb.). We show the effect of training corpus construction in <ref type="figure" target="#fig_5">Figure 4b</ref>. para denotes the baseline approach of training the actor with the original parallel corpus used to train the underlying NMT model. Among the four novel approaches, full obtains the worst performance, since the beam search outputs contain translations that are far from the gold-standard translation. We choose the best performing top1 strategy.  Decoding Objectives As our approach is capa- ble of using an arbitrary decoding objective, we investigate the effect of different objectives on BLEU, METEOR (MTR) and TER scores with Transformer for IWSLT16 De-En translation. <ref type="table" target="#tab_4">Ta- ble 5</ref> shows the final result on the test set. When trained with one objective, our model yields rela- tively good performance on that objective. For ex- ample, negative sentence-level TER (i.e., -sTER) leads to -3.0 TER improvement over greedy de- coding and -0.5 TER improvement over beam search. However, since these objectives are all well correlated with each other, training with dif- ferent objectives do not differ dramatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Data Distillation Our work is directly inspired by work on knowledge distillation, which uses a similar pseudo-parallel corpus strategy, but aims at training a compact model to approximate the function learned by a larger model or an ensem- ble of models ( <ref type="bibr" target="#b19">Hinton et al., 2015)</ref>. <ref type="bibr" target="#b21">Kim and Rush (2016)</ref> introduce knowledge distillation in the context of NMT, and show that a smaller stu- dent network can be trained to achieve similar performance to a teacher model by learning from pseudo-corpus generated by the teacher model. <ref type="bibr" target="#b43">Zhang et al. (2017)</ref> propose a new strategy to generate a pseudo-corpus, namely, fast sequence- interpolation based on the greedy output of the teacher model and the parallel corpus. <ref type="bibr" target="#b13">Freitag et al. (2017)</ref> extend knowledge distillation on an ensemble and oracle BLEU teacher model. How- ever, all these approaches require the expensive procedure of retraining the full student network.</p><p>Pseudo-Parallel Corpora in Statistical MT Pseudo-parallel corpora generated from beam search have been previously used in statistical machine translation (SMT) <ref type="bibr" target="#b7">(Chiang, 2012;</ref><ref type="bibr" target="#b14">Gao and He, 2013;</ref><ref type="bibr" target="#b1">Auli and Gao, 2014;</ref><ref type="bibr" target="#b10">Dakwale and Monz, 2016)</ref>. <ref type="bibr" target="#b14">Gao and He (2013)</ref> integrate a re- current neural network language model as an ad- ditional feature into a trained phrase-based SMT system and train it by maximizing the expected BLEU on k-best list from the underlying model. Our work revisits a similar idea in the context trainable greedy decoding for neural MT.</p><p>Decoding for Multiple Objectives Several works have proposed to incorporate different de- coding objectives into training. <ref type="bibr" target="#b31">Ranzato et al. (2015)</ref> and <ref type="bibr" target="#b2">Bahdanau et al. (2016)</ref> use reinforce- ment learning to achieve this goal. <ref type="bibr" target="#b35">Shen et al. (2016)</ref> and <ref type="bibr" target="#b28">Norouzi et al. (2016)</ref> train the model by defining an objective-dependent loss func- tion. <ref type="bibr" target="#b41">Wiseman and Rush (2016)</ref> propose a learn- ing algorithm tailored for beam search. Unlike these works that optimize the entire model,  introduce an additional network that predicts an arbitrary decoding objective given a source sentence and a prefix of translation. This prediction is used as an auxiliary score in beam search. All of these methods focus primarily on improving beam search results, rather than those with greedy decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper introduces a novel method, based on an automatically-generated pseudo-parallel cor- pus, for training an actor-augmented decoder to optimize for greedy decoding. Experiments on three models and three datasets show that the train- ing strategy makes it possible to substantially im- prove the performance of an arbitrary neural se- quence decoder on any reasonable translation met- ric in either greedy or beam-search decoding, all with only a few trained parameters and minimal additional training time.</p><p>As our model is agnostic to both the model ar- chitecture and the target metric, we see the explo- ration of more diverse and ambitious model-target metric pairs as a clear avenue for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A single step of a generic actor interacting with a decoder of each of three types. The dashed arrows denote an optional recurrent connection in the actor network.</figDesc><graphic url="image-1.png" coords="3,94.68,62.80,408.18,227.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Artetxe et al. (2018) (rnn size = 600, word vec size = 300), and choose similar hyper-parameters: rnn size = 500, word vec size = 300 for IWSLT16 and rnn size = 600, word vec size = 500 for WMT. We use the input-feeding decoder and global atten- tion with the general alignment function (Luong et al., 2015). 6 https://github.com/OpenNMT/OpenNMT-py</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>src</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The norms of the three activation vectors on the IWSLT16 De-En validation set with Transformer. Action, Context and State represent the norm of the action, attentional source context vector and decoder hidden state, respectively.</figDesc><graphic url="image-2.png" coords="7,77.46,62.80,207.35,144.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The effect of the actor architecture and hidden state size on trainable greedy decoding results over the IWSLT16 De-En validation set with Transformer (BLEU↑), shown with a baseline (cont.) in which the underlying model, rather than the actor, is trained on the pseudo-parallel corpus. The Y-axis starts from 1.0. w.o. indicates an actor with no hidden layer. 0.0 corresponds to 33.04 BLEU.</figDesc><graphic url="image-3.png" coords="7,318.19,62.81,196.39,147.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) The effect of beam size on the IWSLT16 De-En validation with Transformer and (b) the effect of the training corpus composition in the same setting. para: parallel corpus; full: all 35 beam search outputs; thd: beam search outputs that score higher than the base model's greedy decoding output; top1: beam search output with the highest bleu score; comb.: top1+para. 0.0 corresponds to 33.04 BLEU.</figDesc><graphic url="image-4.png" coords="8,312.73,62.81,207.34,199.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>tg Most mails are read several times by software robots on the road . tg+beam4 Most mails are read several times by software robots on the road . src Ich suche schon seit einiger Zeit eine neue Wohnung fr meinen Mann und mich . ref I have been looking for a new home for my husband and myself for some time now . greedy I have been looking for a new apartment for some time for my husband and myself . beam4 I have been looking for a new apartment for some time for my husband and myself . beam35 I have been looking for a new apartment for my husband and myself for some time now . tg I have been looking for a new apartment for my husband and myself for some time now . tg+beam4 I have been looking for a new apartment for my husband and myself for some time now .</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Translation examples from the WMT14 De-En test set with Transformer. We show translations generated by the underlying transformer using greedy decoding, beam search with k = 4, and beam search with k = 35 and the oracle BLEU scorer (). We also show the translations using our trainable greedy decoder both without and with beam search. Phrases of interest are underlined.</figDesc><table>IWSLT16 De-En 
WMT14 De-En 
ref 
greedy k35 
tg 
ref 
greedy k35 
tg 

Base Model 
20.4 
65.3 
61.5 64.2 23.5 
65.2 
63.8 65.1 
+Trainable Greedy Decoder 19.1 
70.4 
65.3 75.1 18.9 
76.0 
72.6 82.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Results when trained with different decoding 
objectives on IWSLT16 De-En translation using Trans-
former. MTR denotes METEOR. We report greedy de-
coding and beam search (k = 4) results using the orig-
inal model, and results with trainable greedy decoding 
(lower half). 

</table></figure>

			<note place="foot" n="4"> https://github.com/jhclark/multeval 5 https://s3.amazonaws.com/fairseqpy/models/wmt14.v2.en-de.fconv-py.tar.bz2</note>

			<note place="foot" n="7"> https://github.com/facebookresearch/fairseq-py 8 https://github.com/salesforce/nonauto-nmt</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partly supported by Samsung Ad-vanced Institute of Technology (Next Generation Deep Learning: from pattern recognition to AI), Samsung Electronics (Improving Deep Learning using Latent Structure) and the Facebook Low Re-source Neural Machine Translation Award. KC thanks support by eBay, TenCent, NVIDIA and CIFAR. This project has also benefited from finan-cial support to SB by <ref type="bibr">Google and Tencent Holdings.</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Decoder integration and expected bleu training for recurrent neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="136" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An actor-critic algorithm for sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philemon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">Joseph</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07086</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Audio chord recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Society for Music Information Retrieval Conference</title>
		<meeting>the 14th International Society for Music Information Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Massive exploration of neural machine translation architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1442" to="1451" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A teacher-student framework for zeroresource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1925" to="1935" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hope and fear for discriminative training of statistical translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1159" to="1187" />
			<date type="published" when="2012-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Noisy parallel approximate decoding for conditional recurrent language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.03835</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving statistical machine translation performance by oracle-bleu model re-estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Dakwale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="38" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Universal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03819</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic evaluation of machine translation quality using n-gram cooccurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Doddington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second international conference on Human Language Technology Research</title>
		<meeting>the second international conference on Human Language Technology Research</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="138" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Ensemble distillation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baskaran</forename><surname>Sankaran</surname></persName>
		</author>
		<idno>arXiv:702.01802</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Training mrfbased phrase translation models using gradient ascent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="450" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Sequence transduction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno>arXiv:211.3711</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nonautoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Trainable greedy decoding for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1968" to="1978" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">When to finish? optimal beam search for neural text generation (modulo beam size)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2134" to="2139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequencelevel knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1317" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Opennmt: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017, System Demonstrations</title>
		<meeting>ACL 2017, System Demonstrations<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="67" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Six challenges for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Knowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Neural Machine Translation</title>
		<meeting>the First Workshop on Neural Machine Translation<address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The meteor metric for automatic evaluation of machine translation. Machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Denkowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="105" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A simple, fast diverse decoding algorithm for neural generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08562</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning to decode for future success</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06549</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reward augmented maximum likelihood for neural structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Zhifeng Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1723" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04304</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06732</idno>
		<title level="m">Sequence level training with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Minimum risk training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1683" to="1692" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Later-stage minimum bayes-risk decoding for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Nakayama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03169</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of association for machine translation in the Americas</title>
		<meeting>association for machine translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">200</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Neural machine translation with reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence learning as beam-search optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1296" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Towards compact and fast neural machine translation using a combined method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1475" to="1481" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
