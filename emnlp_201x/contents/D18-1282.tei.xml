<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Grounding Semantic Roles in Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 2616</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><surname>Silberer</surname></persName>
							<email>carina.silberer@upf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational Linguistics</orgName>
								<orgName type="institution">Saarland University</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universitat Pompeu Fabra Barcelona</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
							<email>pinkal@coli.uni-saarland.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational Linguistics</orgName>
								<orgName type="institution">Saarland University</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Grounding Semantic Roles in Images</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2616" to="2626"/>
							<date type="published">October 31-November 4, 2018. 2018. 2616</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We address the task of visual semantic role labeling (vSRL), the identification of the participants of a situation or event in a visual scene, and their labeling with their semantic relations to the event or situation. We render candidate participants as image regions of objects, and train a model which learns to ground roles in the regions which depict the corresponding participant. Experimental results demonstrate that we can train a vSRL model without reliance on prohibitive image-based role annotations , by utilizing noisy data which we extract automatically from image captions using a linguistic SRL system. Furthermore, our model induces frame-semantic visual representations , and their comparison to previous work on supervised visual verb sense disam-biguation yields overall better results.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Images of everyday scenes can be interpreted and described in many ways, depending on the per- ceiver and the context in which the image is pre- sented. The latter may be natural language data or a visual sequence. As an example, consider the two scenes in <ref type="figure" target="#fig_0">Figure 1</ref> and the question What is the man doing? The interpretation of the first target image (left) in isolation would allow many answers. Tak- ing into account the visual context, however, may disprove many of those answers (e.g., He is ques- tioning the women.). For the target image on the right, the reason for Why there is so much food on the table? can be inferred from its textual context.</p><p>As the examples illustrate, the interpretation of a (visual) scene is related to the determina- tion of its events, their participants and the roles they play therein (i.e., distill who did what to whom, where, why and how), and this may re- quire a joint processing or reasoning with possi- bly multiple (extra-)linguistic information sources (e.g., text, images). In NLP, the well-established and studied task of semantic role labeling (SRL) aims to extract such knowledge in the form of shal- low semantic structures from natural language texts (e.g., questioning(Agent:man, Theme:women) ); see, e.g., <ref type="bibr" target="#b5">Gildea and Jurafsky (2002)</ref>; <ref type="bibr" target="#b11">Palmer et al. (2010)</ref>, for an overview). It is considered an essen- tial task towards text understanding, and was shown to be beneficial for applications such as informa- tion extraction (see <ref type="bibr" target="#b14">Roth and Lapata (2016)</ref> and the references therein) and question answering <ref type="bibr" target="#b16">(Shen and Lapata, 2007)</ref>. In computer vision research, recent efforts have been made on visual SRL or sit- uation recognition, a task coined by transferring the use of semantic roles to produce similar structured meaning descriptions for visual scenes (e.g., <ref type="bibr" target="#b19">Yang et al. (2016)</ref>; <ref type="bibr" target="#b21">Yatskar et al. (2016)</ref>).</p><p>To facili- tate the endeavor of joint processing over multiple sources, it is desirable to induce representations of texts and visual scenes which do encode this kind of information, and in, essentially, a congruent and generic way. The latter would furthermore support the induction of a desired level of abstraction as needed.</p><p>In this paper we propose an approach towards this goal: We address the task of visual SRL (vSRL) and learn frame-semantic representations of images. Specifically, we present a model that learns to ground the semantic roles of a seman- tic frame in image regions, which may be crucial for, e.g., human-robot interaction or surveillance (e.g., Who/Where is the robber?). For example, the image shown in <ref type="figure" target="#fig_1">Figure 2</ref> evokes the ARREST frame, and its semantic roles Authorities, Suspect, and Place are grounded in the image regions (delin- eated by bounding boxes) which depict their corre- sponding fillers. While being trained on this task, our model learns distributed situation representa- tions (for images and frames), and participant rep- resentations (for image regions and roles) which Well, the fridge broke, so I had to eat everything. capture the visual-frame-semantic features of situ- ations and participants, respectively. We train our model on data that we automati- cally extract by running a linguistic SRL system on image captions-human produced data that is abundant and requires less time and expertise than frame-semantic annotations. Supervised SRL has suffered from data sparsity since it relies on labor- intensive human annotations. Analogous issues on manually annotated images have been addressed by <ref type="bibr" target="#b20">Yatskar et al. (2017)</ref>. By leveraging existing efforts made in NLP, we explore whether we can alleviate the supervision bottleneck in visual SRL Our experiments yield promising results, and our models are even able to make correct predictions for erroneous data points. Furthermore, we eval- uate the induced situation representations on the task of supervised visual verb sense disambigua- tion, where it outperforms or is comparable to pre- vious work (on motion or non-motion verbs, re- spectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Yatskar et al. (2016) introduced the ImSitu dataset for the task of situation recognition, i.e., the prob- lem of, given an image, predicting a structured out- put which specifies the depicted activity (e.g., jump- ing) and its associated semantic roles paired with their nominal fillers (e.g., {(agent, bear), (obsta- cle, water) }. To address the task, <ref type="bibr" target="#b21">Yatskar et al. (2016</ref><ref type="bibr" target="#b20">Yatskar et al. ( , 2017</ref> train conditional random field (CRF) models on ImSitu ( <ref type="bibr" target="#b21">Yatskar et al., 2016</ref>) and on additional training data for rarely occurring noun-role combinations which they source from the web ( <ref type="bibr" target="#b20">Yatskar et al., 2017)</ref>. <ref type="bibr" target="#b8">Mallya and Lazebnik (2017)</ref> assume that the roles associated with each activity are in a fixed order, and treat the above task as one of recognizing activities and generating a sequence of nouns, for which they use a recurrent neural network. They show how hereby learned features can be transferred to tackle image caption generation. <ref type="bibr" target="#b7">Li et al. (2017)</ref> explicitly model role de- pendencies through a gated graph neural network. Given an image, they instantiate a fully connected graph with a verb and its roles as nodes. Each node's hidden state vector is initialized with image features from two CNNs, which were pre-trained for the prediction of verbs and nouns, respectively. Using a softmax layer augmented with hidden state vectors, they predict the verb and the nominal fillers of its roles.</p><p>In contrast to above works on ImSitu, we do not link the roles of a verb to their lexical fillers. We address the related task of explicitly grounding roles in the corresponding image regions, since our focus is on the relation between semantic roles and the typical visual features of their fillers (e.g., a Body part is typically not a bike but arms). <ref type="bibr" target="#b6">Gupta and Malik (2015)</ref> introduced this task as visual se- mantic role labeling. Similarly, <ref type="bibr" target="#b19">Yang et al. (2016)</ref> formulate a CRF that jointly processes a cooking video and its natural language descriptions in or- der to ground the semantic roles associated with the verbs in corresponding object tracks. Both of these studies are limited to a small number of activ- ities performed by people and a few semantic roles (26 and 11 verbs, 3 and 6 roles, respectively).</p><p>Unlike related work, our approach does not rely on manual role annotations of images, but exploits a linguistic SRL system for data creation. With more than 1k frame-specific roles, our data is of a larger scope than <ref type="bibr" target="#b6">Gupta and Malik (2015)</ref> and <ref type="bibr" target="#b19">Yang et al. (2016)</ref>. Further, unlike the CRF-based approaches, our model induces frame-semantic rep- resentations during training.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Grounding Semantic Roles in Images</head><p>We first define the task of vSRL and then present our model and our approach for data creation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Definition: vSRL</head><p>Our approach is based on the linguistic theory of frame-semantics <ref type="bibr" target="#b3">(Fillmore, 1982)</ref>, which underlies the idea that words evoke semantic frames. Frames describe prototypical situations or events and con- tain semantic roles. For example, in the sentence They arrested him for assault, the argument they fills the Authorities role, him is the Suspect, and assault the Charges of the ARREST frame, which was evoked by the verb arrest.</p><p>Let F be a set of frames, E be the set of all semantic role labels, and E f be the inventory of roles associated with the frame f (e.g., E ARREST ={Authorities, Suspect, Charges, Offense, Place}) 1 . As- sume we are given an image i, which evokes a frame f , and a set of image regions R i , which ren- der one or several objects in i. The task of vSRL is to link each role e ∈ E f to the object r ∈ R i that fills role e in the situation or event which f describes. We call a role e to be realized in an image, if it can be grounded in an image (re- gion). The object r shown in the image region is called the filler or realization of e. The struc- ture A f = {(r, e)|r ∈ R i , e ∈ E f } overall repre-sents the frame f in the image i.</p><p>In SRL, the task of identifying the frame which a predicate evokes is a prerequisite, but it is usually treated as a subtask of SRL. We follow this ap- proach and consider the identification of the frames evoked by an image as a subtask of vSRL. We for- mulate two further subtasks for vSRL, namely role prediction-determining the correct role for a rele- vant image region, and role grounding-linking a realized role to its filler.</p><p>Note that not all roles of a frame may be realized in an image, and not all objects may play a role in an evoked frame. <ref type="figure" target="#fig_1">Figure 2</ref>, for instance, shows an image with some of its objects delineated by six bounding boxes R i = {r 1 , r 2 , r 3 , r 4 , r 5 , r 6 }. The target outputs (bottom, <ref type="figure" target="#fig_1">Fig. 2</ref>) are the frames AR- REST and PLACING, as well as their realized roles which are aligned with their fillers (marked by col- ors). The FrameNet roles Charges and Offense are not realized in the image, i.e., they cannot be grounded. The vehicle, box r 4 , in turn, does not participate in the ARREST frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model: Visual-Frame-Semantic Embedder</head><p>Our model, illustrated in <ref type="figure">Figure 3</ref>, is formulated as a neural network architecture. Its input is a tuple q = (i, r, f, e) ∈ Q of an image i, an ob- ject which is delineated by bounding box r, a frame f ∈ F , and a role label e ∈ E f (e.g., q = (img 1 , r 5 , ARREST, Suspect); cf. <ref type="figure" target="#fig_1">Fig. 2</ref>). The model output is a score s(q) ∈ [−1, 1] which quan- tifies the visual-frame-semantic correspondence between the box r and the role e of f <ref type="figure">(Fig. 3, right)</ref>. More specifically, the model maps visual en- codings of i and r (e.g., vectors of a pre-trained CNN), and frame-semantic representations of f and e (randomly initialized embeddings) to com- mon visual-frame-semantic spaces (cross-modal layers in <ref type="figure">Fig. 3</ref>).</p><p>We assume that images capture different frame- semantic features than image regions-an image encodes the whole scene and its participants and thus evokes a frame, while individual image regions of participants capture the participant-specific fea- tures of the semantic roles they fill. We there- fore distinguish between two different cross-modal spaces: a situation space for images and frames, and a participant space for regions and roles. Us- ing the respective representations in these spaces, the model then estimates the situation similar- <ref type="figure">Figure 3</ref>: The ImgObjLoc model which scores the correspondence between a semantic role and its frame, respec- tively, and a candidate role filler (an image region) and the whole image, respectively. ity, sim s (i, f ), between the image and the frame, and the participant-role similarity, sim p (r, e), be- tween the box and the role. Finally, the overall frame-semantic score s(q) is the aggregation of sim s and sim p :</p><formula xml:id="formula_0">s(q) = b f sim s (i, f ) + (1 − b f ) sim p (r, e), (1)</formula><p>where parameter b f ∈ θ weights the contribution of the situation and participant scores to the overall score and is learned along all model parameters θ.</p><p>By definition of the output function s (Equ. 1), each role-object pair is scored independently of the decisions made for the other roles and regions of the same frame and image, respectively. Techni- cally, this allows for the use of partially labeled training data, where not every realized role of a frame has been linked to its filler, as we will ex- plain in Section 3.3.</p><p>Below we describe how we use our model to ad- dress the subtasks of role prediction and grounding (Section 3.1), respectively, for which we will report experimental results in Section 5. <ref type="bibr">2</ref> In any case, the method is based on the visual-frame-semantic correspondence s(q) (Equ. 1), where we discard all candidates of role-filler pairings with a score less than zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Role Prediction</head><p>Given an image i, we formulate the role prediction problem as a mapping L:</p><formula xml:id="formula_1">L :{i} × R i → F × E L(i, r) = arg max (f,e),f ∈F,e∈E f s(i, r, f, e) (2)</formula><p>That is, the predicted role (and the frame it is as- sociated with) which an image region r ∈ R i of i fills is that e ∈ E to which r is most similar in the visual-frame-semantic space.</p><p>Role Grounding is the equivalent to linguis- tic semantic role labeling. 3 Given a frame f realized in i, we ground each role e ∈ E f in the region r ∈ R i with the highest visual-frame- semantic similarity to e:</p><formula xml:id="formula_2">G : {i} × {f } × E f → R i G(i, f, e) = arg max r∈R i s(i, r, f, e) (3)</formula><p>Training We train the model by using a rank- ing criterion designed to give higher scores to true cross-modal frame-semantic combina- tions (i, r, f, e) than to mismatches, by a mar- gin M . To this end, for each positive exam- ple q = (i, r, f, e) of a training set Q, we sample K negative examples q k = (i, r, f , e ) of a frame f and role e ∈ E f not true for image i and box r, 4 and learn model parameters θ by minimizing the maximum margin hinge loss function on the tu- ples (q, q ) (Equ. 4) . Ideally, using this loss func- tion would guide the parameter learning towards mapping images and the frames they evoke, and regions and the roles they fill, respectively, nearby each other in the cross-modal spaces. (2a) PLACING (Theme:r5/A man, Goal:r4/a police car, Agent:r1/a uniformed officer ) (2b) ARREST (Authorities:r1,r2/The police, Suspect:r5/someone, Place:r3/on a busy city street ) (2c) ARREST ( Suspect:r5/A young guy )   <ref type="formula">2006)</ref>, in our case), including the therein defined frame and role labels, could facilitate cross-modal interactions- advances in vSRL can help to improve SRL and vice versa, or jointly draw inferences from both modalities (e.g., a text and its illustration). Our data creation approach is to use a (linguistic) SRL system to extract frame-semantic annotations from a corpus of images paired with captions. We use the Flickr30k Entities dataset (Plummer et al., 2015) 5 which contains 30k images and five captions per image. We chose this dataset since its captions are augmented with entity mention annotations, associating them with the 276k man- ually annotated bounding boxes (i.e., entities are grounded in the image). To create the set</p><formula xml:id="formula_3">θ = arg min θ q∈Q 1 K K k=1 max(0, M −s(q)+s(q k ))<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Using Linguistic Knowledge for Data Creation</head><formula xml:id="formula_4">Q = {(i (j) , r (k j ) , f (l j ) , e (l j ,k j ) )|j ∈ {1, . . . , 30k}}</formula><p>of training instances, we run PathLSTM <ref type="bibr" target="#b13">(Roth, 2016;</ref><ref type="bibr" target="#b14">Roth and Lapata, 2016</ref>) on all captions, and extract all semantic frame annotations whose roles are filled by a grounded entity. As a result, our training corpus comprises images, the frames they evoke, and the associated semantic roles paired with their grounded fillers (i.e., bounding boxes).</p><p>Sentences (1a)-(1c) in <ref type="figure" target="#fig_2">Figure 4</ref> (left), for ex- <ref type="bibr">5</ref> See web.engr.illinois.edu/ ˜ bplumme2/ Flickr30kEntities ample, are three human produced captions for the image in <ref type="figure" target="#fig_1">Figure 2</ref>, in which entity mentions are linked to their image regions (indicated by colors). Using PathLSTM, we extract the grounded frame- semantic annotations (2a)-(2c) <ref type="figure" target="#fig_2">(Fig. 4, right)</ref>, which results in the following six instances of our corpus Q:</p><formula xml:id="formula_5">(img1, r5, PLACING, Theme) (img1, r1, PLACING, Agent) (img1, r4, PLACING, Goal) (img1, r1 r2, ARREST, Authorities) (img1, r5, ARREST, Suspect) (img1, r3, ARREST, Place)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data</head><p>Training Data We adopt the training, valida- tion and test splits provided in the Flickr30k En- tities dataset <ref type="bibr" target="#b12">(Plummer et al., 2015</ref>) and create our dataset Q with the method described above. Some verbs and the frame types which they evoke occur very frequently in the set of annotations (e.g., BEING LOCATED) and therefore allow the in- duction of a finer-grained frame inventory. Specifi- cally, we transform each frame which is evoked by an individual verb (e.g., stand or sit) for at least 100 images (as obtained from the cap- tions) in the Flickr30k Entities training split to a finer-grained frame type by concatenating it with the verb (e.g., BEING LOCATED-sit ). Finally, we keep all frame types (fine-grained or coarse) which had been assigned to at least 100 differ- ent images. This amounts to an inventory of 252 frame types (102 coarse types, e.g., STATE- MENT), 1, 409 frame-specific role types (e.g., STATE- MENT.speaker), 169 role labels (e.g., Speaker) and 76, 939 training instances. We derive our validation and test splits from the original splits on the basis of above modifications. See <ref type="table">Table 1</ref> for the quanti- tative details on the dataset, which we henceforth call Flickr30k Roles. # types frame.role <ref type="table" target="#tab_4">frame.role  frames roles  fine coarse fine coarse  train  76,939 1,409 426 252 102 169  val  7,171  755 421 239 102 143  test  7,229  756 426 242 102 146   Table 1</ref>: Overview of our Flickr30K Roles dataset.</p><p>Reference Data Flickr30k Roles may contain false instances due to its creation on the basis of automatic frame-semantic annotations. Im- Situ ( <ref type="bibr" target="#b21">Yatskar et al., 2016</ref>) is, to the best of our knowledge, the only existing benchmark dataset for vSRL. As explained in Section 2, however, it is image-based, and does not provide explicit links between roles and the regions which depict their fillers. It cannot be used for the evaluation of role prediction and grounding without additional anno- tations.</p><p>We therefore created a set of reference instances by presenting a subset of the Flickr30k Roles test data to two human subjects (both students of com- putational linguistics) for annotation. We chose all instances which agree in their frame label with instances extracted from at least two other cap- tions of the underlying image. This amounts to 201 images and 715 instances. The annotators were presented with an image with relevant ob- jects rendered by bounding boxes, along with the automatically grounded semantic frame annota- tions. <ref type="figure" target="#fig_5">Figure 5</ref> gives an example image along with the 4 automatically obtained instances. They were asked to judge the correctness of the frame (e.g., <ref type="bibr">INGESTION,</ref>  <ref type="figure" target="#fig_5">Fig. 5</ref>), the verb (in the case of a fine-grained frame type; e.g., eat) and each of the role-filler links (e.g., Ingestor-226403). They further linked wrong role assignments to their cor- rect fillers when possible. We created the reference set as the intersection of all correct instances of the two annotators (frame and role-filler linkings), which amounts to 554 instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual</head><p>Representations We use high- dimensional distributed vectors to represent images and regions (bounding boxes), and repre- sent the latter by additional contextual features. These encode a region's relative location and size with respect to the whole image (cf. ( <ref type="bibr" target="#b9">Mao et al., 2016)</ref>):</p><formula xml:id="formula_6">x tl W , y tl H , x br W , y br H , w · h W · H ,<label>(5)</label></formula><p>where (x tl , y tl ) and (x br , y br ) are the coordinates of the top left and bottom right corners of the  . We hypothe- size that the relative position and size of an object can be likewise informative for the roles it can(not) realize. For example, an object that is located at the bottom of an image is probably rather the Patient of a KICKing event than the Agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We first evaluate our model in terms of different aspects related to visual SRL on the two subtasks role prediction and grounding (see Section 3). Our second experiment assesses the usefulness of the learned frame-semantic image representa- tions on the task of visual verb disambiguation: given an image and a verb, assign the correct sense of the verb, i.e., the one that describes the action de- picted in the image (e.g., play an instrument; play sport). This task is different from visual SRL, but forms a prerequisite for it, since in frame semantics, roles are defined on the basis of frames evoked by verb senses.</p><p>Model Details For each bounding box and im- age, we use the VGG16 network <ref type="bibr" target="#b17">(Simonyan and Zisserman, 2014</ref>), trained on ImageNet ( <ref type="bibr" target="#b0">Deng et al., 2009)</ref>, to extract a 4, 096-dimensional feature vec-Fine-grained frame types Coarse frame types top-1-pred. top-5 preds. gt fr. top-1-pred. top-5 preds. frame <ref type="table">fr.role role frame fr.role role role  frame fr.role</ref>   tor from the fully connected fc7 layer. To transform the feature vectors into the visual-frame-semantic embedding space, we use two two-layer networks which are composed of a layer with rectified lin- ear activation units (relu) followed by a layer with tanh activations (see <ref type="figure">Fig. 3</ref>, top left). We further- more concatenate the first hidden layer (relu layer) of each image region (i.e., box) with a vector of contextual features (relative box size and location, Equ. 5).</p><p>Frames and roles, in turn, are encoded as one- hot vectors and mapped to randomly initialized embedding layers, which are then transformed into the visual-frame-semantic representations using tanh activation layers <ref type="figure">(Fig. 3, bottom)</ref>. We use the cosine similarity to quantify visual-frame-semantic correspondences in the cross-modal space (Equ. 1).</p><p>Throughout our experiments we compare our model (ImgObjLoc), which takes into account the contextual features (Equ. 5), to a model that does not use contextual box features (ImgObject), and one that only uses the image as visual input (Image- only). Image-only derives its cross-modal role representation by augmenting both, the image and the box input layers with the image's fc7 feature vector.</p><p>The network parameters were optimized using AdaGrad <ref type="bibr" target="#b1">(Duchi et al., 2011</ref>) with a learning rate of 0.003. We monitored the role prediction perfor- mance on the validation set of Flickr30k Roles and kept the best performing model. See Appendix A.1 for further details on the model hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Exp.1: Semantic Role Prediction and Local Grounding</head><p>In the role prediction evaluation, the model is given an image and a bounding box, which represents a candidate role filler, and needs to predict the frame and role which the entity (or entities) in the box fills.</p><p>In the grounding experiment, the model is given an image, a frame and an associated role which is realized in the image, and needs to determine the correct role filler from a list of boxes. We report results on using ground truth boxes as well as box proposals, extracted with selective search <ref type="bibr" target="#b18">(Uijlings et al., 2013)</ref>. Regarding the latter, we apply the intersection over union (IoU) metric (e.g., Ever- ingham et al. <ref type="formula">(2010)</ref>), and consider a role to be grounded in the correct box proposal˜rproposal˜ proposal˜r if the area of overlap betweeñ r and the reference box, divided by the area of their union, exceeds 50%.</p><p>Results We report top-1 and top-k accuracy (i.e., the frame and role is among the top-k scored predictions) on the Flickr30k Roles test and refer- ence sets for both subtasks (recall that Flickr30k Entities provides ground truth alignments between entity mentions and objects). <ref type="table" target="#tab_4">Table 2</ref> gives the results on role prediction with ground truth bounding boxes (i.e., for all entities which fill at least one semantic role). We report the accuracy for predicting the correct frame and role (columns fr.role), for predicting the correct frame (columns frame), and the correct role regardless of its frame (columns role; e.g., a prediction of STATEMENT.Speaker would be considered correct even if the reference was SPEAK ON TOPIC.Speaker). We further give results for the coarse frame types, where verbs are stripped off the frame labels (i.e., STATEMENT-speak is STATEMENT). Since the role prediction performance is equal for both frame types, we report the results for the fine-grained frames only.</p><p>As <ref type="table" target="#tab_4">Table 2</ref> shows, the models which use partic- ipant representations extracted from the relevant image regions (ImgObject and ImgObjLoc) per- form better than Image-only which considers the Fine-grained frame types Fine-grained frame types top-1 pred. filler top-3 pred. fillers top-1 pred. filler top-3 pred. fillers frame fr <ref type="table">.role role frame fr.role role  frame fr.role role frame fr.role role   test</ref>    global image only, except for the top-1 frame pre- diction. This indicates that the two models are able to learn useful role-specific visual representations. Contextual features in the form of the relative size and location of a region (cf. Equ. 5) seems to be also beneficial, due to ImgObjLoc yielding the overall best results. These features are furthermore beneficial for role grounding in automatically selected bounding boxes: When using automatically selected boxes, ImgObjLoc is significantly more effective than ImgObject in all settings (rows props, right block in <ref type="table" target="#tab_6">Table 3</ref>). The Random baseline, which assigns each role randomly to a box in the image, performs unsurprisingly worst.</p><p>Interestingly, the models perform substantially better on the reference set than on the noisy test set (top and bottom blocks in <ref type="table" target="#tab_4">Tables 2,3</ref>). <ref type="bibr">6</ref> This indi- cates that they were able to generalize over wrong role-filler pairs in the training data, and are able to make correct predictions even for erroneous in- stances (see the qualitative analysis below). When assuming that the correct frame has been identi- fied (columns gt fr.), the best role prediction ac- <ref type="bibr">6</ref> The accuracy scores on the uncorrected instances in the reference set yield comparable or worse accuracy scores than those on the test set, except for the top-5 predicted frames. curacy reaches 70.3% on the reference set, and grounding accuracy with box proposals is at 35.5% <ref type="figure" target="#fig_1">(ImgObjLoc, Tables 2,3, respectively)</ref>.</p><p>Finally, frame prediction proves to be a diffi- cult task, especially for fine-grained frame types (e.g., BEING LOCATED-sit ; left block in <ref type="table" target="#tab_4">Table 2</ref>).</p><p>Qualitative Analysis Notably, our analysis re- vealed that ImgObjLoc could correctly predict roles for cases in which PathLSTM failed, espe- cially for highly visual entities (e.g., performance vs. location, goal vs. path). Overall, ImgOb- jLoc was often able to identify location roles which PathLSTM had missed, but may confuse the spe- cific labels (e.g., area vs. path or location) for reasons discussed below. See <ref type="figure">Figure 6</ref> for the recall of ImgObjLoc on the reference set for individual roles <ref type="bibr">(top-20)</ref>.</p><p>In an error analysis of the predictions of Im- gObjLoc we identified several classes of errors. Typical errors in role prediction were in cases in which an image region contained multiple ob- jects, and the system predicted a label for an ob- ject which was occluded by the target or vice versa (e.g., ingestibles vs. source; clothing vs. wearer or body part; path vs. area). We found that this error was propagated from noise in the training data. <ref type="table" target="#tab_7">Table 4</ref> shows the roles which were most difficult to predict by ImgObjLoc, and which the textual SRL system (PathLSTM) could predict with a high precision (top; as calculated from the human annotations, cf. Section 4), or with a low precision (bottom), respectively. As may be expected, among these are also highly non-visual roles, such as manner and purpose.</p><p>Other noise propagated from the training data was caused by wrong frame predictions of PathLSTM (e.g., TRAVERSE-pass instead of BRINGING-carry; CONTAINING-hold .contents vs. IN- <ref type="figure">Figure 6</ref>: Prediction recall of ImgObjLoc on the refer- ence set for the top-20 roles, ordered by their frequency. <ref type="bibr">GESTION.ingestibles)</ref>. Frequent patterns of in- correct frame predictions were furthermore a fail- ure of the system to distinguish between fine- grained frames (e.g., BEING LOCATED-sit vs. -lie or SELF MOTION-walk vs. -run), or between motion and non-motion actions (e.g., POSTURE vs. SELF MOTION).</p><p>Finally, we observed that often the reference did not contain an actually valid frame which had been predicted by the system for an im- age, due to different levels of frame speci- ficity, i.e., the output of ImgObjLoc was more specific (e.g., ASSISTANCE-help.helper vs. WEAR- ING.wearer; OPERATE VEHICLE-ride.vehicle vs. PER- CEPTION ACTIVE-look .location of perceiver) or it was more general (e.g., WEARING.wearer vs. IMPACT- hit.impactor).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Exp.2: Visual Verb Sense Disambiguation</head><p>We evaluate the effectiveness of the frame-semantic image representations that can be extracted with our ImgObjLoc model on the VerSe (visual Verb Sense disambiguation) dataset <ref type="bibr" target="#b4">(Gella et al., 2018)</ref>. It covers 90 verbs and 163 senses used to annotate 3, 510 images. We follow the supervised method applied in <ref type="bibr" target="#b4">(Gella et al., 2018)</ref>, divide VerSe into training and test data, and train logistic regression classifiers for sense prediction on 19 motion verbs and 19 non-motion verbs (those which have at least 20 images and at least 2 senses). Input to the sense classifiers are the frame-semantic image represen- tations (second top cross-modal layer in <ref type="figure">Fig. 3</ref>) of the VerSe images, which we extract with the ImgObjLoc model, trained on Flickr30k Roles. <ref type="table" target="#tab_8">Table 5</ref> gives the mean accuracy obtained on the test data (of 100 runs). Our ImgObjLoc vectors outperform all comparison models on motion verbs, including CNN-based image features and the best-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features</head><p>Motion Non-motion Random 76.7 ± 0.86 78.5 ± 0.39 MFS + 76.1 80.0 CNN + 82.3 80.0 Gella-CNN+O + 83.0 80.0 Gella-CNN+C + 82.3 80.3 CNN (reproduced) 83.1 79.8 ± 0.53 ImgObjLoc 84.8 ± 0.69 80.4 ± 0.57 performing models of <ref type="bibr" target="#b4">(Gella et al., 2018)</ref>, namely Gella-CNN+O and Gella-CNN+C (CNN features concatenated with predicted object labels and im- age captions, respectively). On non-motion verbs, the best models, including our own, perform only comparably to the most frequent sense heuristic. Note that we examine the simplest representation ImgObjLoc can yield, i.e., frame-semantic repre- sentations for individual images. More complex representations are left for future work. See Ap- pendix A.3 for examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We addressed the task of grounding semantic roles of frames which an image evokes in the correspond- ing image regions of its fillers. We found that our model can be trained without the need of manual role annotations of image data, and that the frame- semantic image representations it learns can be used for related tasks. Encouraged by our find- ings, future work includes the exploration of the model and its learned frame-semantic representa- tions for tasks such as the interpretation of multi- modal scenes and stories and referring expressions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example images along with their visual (left) or textual (right) contexts.</figDesc><graphic url="image-3.png" coords="2,200.61,131.01,104.88,65.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example image (from Flickr30k Entities) augmented with frame-semantic annotations. Top: Image with objects rendered by bounding boxes. Bottom: Annotations which show the frames which the image evokes, and their roles, linked to their filler objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Flickr30k captions for the image in Fig. 2. Left: Flickr30k Entities annotations of the mentioned objects with unique entity ids. Right: Frame-semantic annotations of the sentences, output by PathLSTM (Roth, 2016; Roth and Lapata, 2016).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>SRL systems in NLP research use training data which have been carefully created by linguistic ex- perts (e.g., Ruppenhofer et al. (2006); Palmer et al. (2005)) for many years. To train our model on the visual SRL task, we build upon the annotation efforts made in NLP. The exploitation of existing resources which were developed for the analogous goal means to get around the time-consuming and costly annotation effort involved in the creation of training data. Moreover, adopting an established framework in NLP for shallow semantic representa- tions (FrameNet, Ruppenhofer et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Automatically derived instances in Flickr30k Roles (colored, left columns) and the human correctness judgments of the frame, verb, and role fillers (right-most column; 1 is correct, 0 wrong). The object names were presented to facilitate the annotation, but are not part of the instance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Role prediction accuracy on the Flickr30k Roles test data and on its human corrected subset. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>set</head><label></label><figDesc></figDesc><table>Random 
gt 

37.7 23.6 25.3 70.8 56.5 59.4 
props 
5.5 3.7 4.1 15.7 10.6 11.6 
ImgObject 
55.9 55.1 58.0 83.2 84.0 78.7 
10.5 11.3 11.7 21.8 21.4 21.2 
ImgObjLoc 
56.6 56.6 59.4 83.1 85.1 79.7 
11.5 12.8 13.3 22.3 22.6 22.5 
reference 
Random 
gt 

54.7 25.7 25.7 91.7 65.5 65.5 
props 
8.1 3.8 3.8 22.9 11.8 11.8 
ImgObject 
78.9 62.1 62.1 95.8 88.2 83.6 
13.7 12.8 12.8 39.6 30.9 28.2 
ImgObjLoc 
80.8 63.9 63.9 97.9 91.8 86.4 
18.6 16.9 16.9 43.8 35.5 34.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Role grounding accuracy on the Flickr30k Roles test data and on its human corrected subset. Instances 
with less than 2 (for top-1) or 3 (for top-3) gt filler candidates were discarded. 

PathLSTM Roles 
high prec. 

Ingestor 
Source 

Carrier 
Speaker 

low prec. 

Body part Activity Seller 

Buyer 
Manner 
Purpose 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Roles which were most difficult to predict by 
ImgObjLoc, in the order of their total frequency in the 
reference set (top left to bottom right), distinguished by 
the prediction precision of PathLSTM. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Sense prediction accuracy for motion (left) 
and non-motion verbs (right) using different image 
representations. + marks results taken from Gella et 
al. (2018). MFS is the most frequent sense heuristic. 

</table></figure>

			<note place="foot" n="1"> We use FrameNet 1.5 (Ruppenhofer et al., 2006).</note>

			<note place="foot" n="2"> We refer to Appendix A.2 in the supplemental material for the production of the structure A i,f of all role-filler pairs for a frame f evoked by image i.</note>

			<note place="foot" n="3"> More formally, the task of SRL is the determination of the arguments and their semantic roles of a predicate in a sentence. 4 We could extend the model to also sample a negative image and box for f and e, and a negative role r for f that is filled by another box in the image. We refrain from this since we create our training data from automatically labeled data, which hence could contain erroneous role-filler pairs.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers, Leonie Harter, Christine Schäfer, Michael Roth, Gemma Boleda, Anna Rohrbach and Bernt Schiele. This research was supported by the German Research Founda-tion (DFG EXC 285), by the European Research Council (ERC Horizon 2020 grant agreement No 715154), and the Spanish Ramon y Cajal pro-gramme <ref type="bibr">(grant RYC-2015-18907)</ref>. This paper re-flects the authors' view only, and the EU is not responsible for any use that may be made of the information it contains.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplemental Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Model Parameters</head><p>We optimized the network parameters using Ada- Grad ( <ref type="bibr" target="#b1">Duchi et al., 2011</ref>) with a learning rate of 0.003. Dropout with rate 0.25 was added on top of the visual-frame-semantic layers. We monitored the role prediction performance on the validation Algorithm 1 vSRL algorithm which grounds each semantic role e of a frame f * in at most one region r ∈ R i of image i. s(.) denotes the visual-frame-semantic correspondence score (Equation (1)).</p><p>set of Flickr30k Roles and kept the best performing model, which was obtained after about 20 epochs for each model. For all models, the first visual hidden layer has 1000 dimensions, and all other layers have 250 units. The margin M (Equation 4 in the main paper) was set to 0.3, and K = 10 neg- ative frame-role examples were sampled for each training instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Model: vSRL</head><p>The full vSRL task requires, given an im- age i, the computation of the set A i,f of role-object pairs which comprises the seman- tic roles of a frame f grounded to their fillers, i.e., A i,f = {(r, e)|r ∈ R i , e ∈ E f }. Using our model, we first determine the frame f * which im- age i evokes on the basis of all role-filler predic- tions, i.e.,</p><p>We then apply a simple algorithm (Algorithm 1) which chooses the filler-role pairs with maxi- mum similarity from the set S i,f * of all scored frame-specific filler-role pairings for image i given frame f * (cf. Equation 1 in the main paper), such that every role is grounded in at most one region, and every region fills at most one role (line 6, Al- gorithm 1). <ref type="figure">Figure 7</ref> shows example images of non-motion verbs for which ImgObjLoc achieved a high (serve, 95%) and a low accuracy (reach, 50%), their serve (95%): dish out, hand out something, often food sys: serve (95%): put a ball into play sys: reach (50%) extend physically/by influence sys: reach (50%) extend physically/by influence sys: reach (50%): gt: pass or transfer something sys: attain/arrive at a state, real or abstract reach (50%): gt: pass or transfer something sys: extend physi- cally/by influence ground truth senses (gt) and the predictions of Im- gObjLoc (sys).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Examples for VSD</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet: A Large-scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Pascal Visual Object Classes (VOC) Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<publisher>Frame Semantics. Hanshin Publishing Co</publisher>
			<pubPlace>Seoul, South Korea</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<title level="m">Disambiguating Visual Verbs. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In press</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic Labeling of Semantic Roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="288" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Visual Semantic Role Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno>abs/1505.04474</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Situation Recognition With Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06233</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">Recurrent Models for Situation Recognition. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generation and Comprehension of Unambiguous Object Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Proposition Bank: An Annotated Corpus of Semantic Roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="106" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semantic Role Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
		<idno>abs/1505.04870</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving Frame Semantic Parsing via Dependency Path Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Book of Abstracts of the 9th International Conference on Construction Grammar</title>
		<meeting><address><addrLine>Juiz de Fora, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="165" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural Semantic Role Labeling with Dependency Path Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1192" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">FrameNet II: Extended Theory and Practice. International Computer Science Institute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Ruppenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ellsworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><forename type="middle">R L</forename><surname>Petruck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>Berkeley, California</pubPlace>
		</imprint>
	</monogr>
	<note>Distributed with the FrameNet data</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using Semantic Roles to Improve Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Selective Search for Object Recognition. International Journal of Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="154" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Grounded Semantic Role Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="149" to="159" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Commonly Uncommon: Semantic Sparsity in Situation Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Situation Recognition: Visual Semantic Role Labeling for Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
