<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2979" to="2984"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2979</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Continuous word representations learned separately on distinct languages can be aligned so that their words become comparable in a common space. Existing works typically solve a quadratic problem to learn a orthogonal matrix aligning a bilingual lexicon, and use a retrieval criterion for inference. In this paper, we propose an unified formulation that directly optimizes a retrieval criterion in an end-to-end fashion. Our experiments on standard benchmarks show that our approach outperforms the state of the art on word translation, with the biggest improvements observed for distant language pairs such as English-Chinese.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Previous work has proposed to learn a linear map- ping between continuous representations of words by employing a small bilingual lexicon as supervi- sion. The transformation generalizes well to words that are not observed during training, making it possible to extend the lexicon. Another applica- tion is to transfer predictive models between lan- guages ( <ref type="bibr" target="#b12">Klementiev et al., 2012</ref>).</p><p>The first simple method proposed by <ref type="bibr" target="#b15">Mikolov et al. (2013b)</ref> has been subsequently improved by changing the problem parametrization. One successful suggestion is to 2 -normalize the word vectors and to constrain the linear mapping to be orthogonal ( <ref type="bibr" target="#b17">Xing et al., 2015</ref>). An alignment is then efficiently found using orthogonal Pro- crustes ( <ref type="bibr" target="#b0">Artetxe et al., 2016;</ref><ref type="bibr" target="#b16">Smith et al., 2017)</ref>, improving the accuracy on standard benchmarks.</p><p>Yet, the resulting models suffer from the so- called "hubness problem": some word vectors tend to be the nearest neighbors of an abnormally high number of other words. This limitation is now ad- dressed by applying a corrective metric at inference time, such as the inverted softmax (ISF) ( <ref type="bibr" target="#b16">Smith et al., 2017)</ref> or the cross-domain similarity local scaling (CSLS) ( <ref type="bibr" target="#b6">Conneau et al., 2017</ref>). This is not fully satisfactory because the loss used for in- ference is not consistent with that employed for training. This observation suggests that the square loss is suboptimal and could advantageously be replaced by a loss adapted to retrieval.</p><p>In this paper, we propose a training objective inspired by the CSLS retrieval criterion. We in- troduce convex relaxations of the corresponding objective function, which are efficiently optimized with projected subgradient descent. This loss can advantageously include unsupervised information and therefore leverage the representations of words not occurring in the training lexicon.</p><p>Our contributions are as follows. First we in- troduce our approach and empirically evaluate it on standard benchmarks for word translation. We obtain state-of-the-art bilingual mappings for more than 25 language pairs. Second, we specifically show the benefit of our alternative loss function and of leveraging unsupervised information. Fi- nally, we show that with our end-to-end formu- lation, a non-orthogonal mapping achieves better results. The code for our approach is a part of the fastText library 1 and the aligned vectors are available on https://fasttext.cc/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries on bilingual mappings</head><p>This section introduces pre-requisites and prior works to learn a mapping between two languages, using a small bilingual lexicon as supervision.</p><p>We start from two sets of continuous representa- tions in two languages, each learned on monolin- gual data. Let us introduce some notation. Each word i ∈ {1, . . . , N } in the source language (re- spectively target language) is associated with a vec- tor x i ∈ R d (respectively y i ∈ R d ). For simplicity, we assume that our initial lexicon, or seeds, cor- responds to the first n pairs (x i , y i ) i∈{1,...,n} . The goal is to extend the lexicon to all source words i ∈ {n + 1, . . . , N } that are not seeds. <ref type="bibr" target="#b15">Mikolov et al. (2013b)</ref> learn a linear mapping W ∈ R d×d be- tween the word vectors of the seed lexicon that min- imizes a measure of discrepancy between mapped word vectors of the source language and word vec- tors of the target language:</p><formula xml:id="formula_0">min W∈R d×d 1 n n i=1 (Wx i , y i ),<label>(1)</label></formula><p>where is a loss function, typically the square loss 2 (x, y) = x − y 2 2 . This leads to a least squares problem, which is solved in closed form.</p><p>Orthogonality. The linear mapping W is con- strained to be orthogonal, i.e. such that W W = I d , where I d is the d-dimensional identity ma- trix. This choice preserves distances between word vectors, and likewise word similarities. Previous works ( <ref type="bibr" target="#b17">Xing et al., 2015;</ref><ref type="bibr" target="#b0">Artetxe et al., 2016;</ref><ref type="bibr" target="#b16">Smith et al., 2017)</ref> experimentally observed that constrain- ing the mapping in such a way improves the quality of the inferred lexicon. With the square loss and by enforcing an orthogonal mapping W, Eq. (1) admits a closed form solution <ref type="bibr" target="#b9">(Gower and Dijksterhuis, 2004</ref>): W * = UV , where UDV is the singular value decomposition of the matrix Y X.</p><p>Inference. Once a mapping W is learned, one can infer word correspondences for words that are not in the initial lexicon. The translation t(i) of a source word i is obtained as</p><formula xml:id="formula_1">t(i) ∈ arg min j∈{1,...,N } (Wx i , y j ).<label>(2)</label></formula><p>When the squared loss is used, this amounts to com- puting Wx i and to performing a nearest neighbor search with respect to the Euclidean distance:</p><formula xml:id="formula_2">t(i) ∈ arg min j∈{1,...,N } Wx i − y j 2 2 . (3)</formula><p>Hubness. A common observation is that near- est neighbor search for bilingual lexicon inference suffers from the "hubness problem" ( <ref type="bibr" target="#b8">Doddington et al., 1998;</ref><ref type="bibr" target="#b7">Dinu et al., 2014</ref>). Hubs are words that appear too frequently in the neighborhoods of other words. To mitigate this effect, a simple solution is to replace, at inference time, the square 2 -norm in Eq. <ref type="formula">(3)</ref>  This solution, both with ISF and CSLS criteria, is applied with a transformation W learned using the square loss. However, replacing the loss in Eq. <ref type="formula">(3)</ref> creates a discrepancy between the learning of the translation model and the inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Word translation as a retrieval task</head><p>In this section, we propose to directly include the CSLS criterion in the model in order to make learn- ing and inference consistent. We also show how to incorporate unsupervised information..</p><p>The CSLS criterion is a similarity measure be- tween the vectors x and y defined as:</p><formula xml:id="formula_3">CSLS(x, y) = −2 cos(x, y) + 1 k y ∈N Y (x) cos(x, y )+ 1 k x ∈N X (y) cos(x , y),</formula><p>where N Y (x) is the set of k nearest neighbors of the point x in the set of target word vec- tors Y = {y 1 , . . . , y N }, and cos is the cosine sim- ilarity. Note, the second term in the expression of the CSLS loss does not change the neighbors of x. However, it gives a loss function that is symmet- rical with respect to its two arguments, which is a desirable property for training.</p><p>Objective function. Let us now write the opti- mization problem for learning the bilingual map- ping with CSLS. At this stage, we follow previ- ous work and constrain the linear mapping W to belong to the set of orthogonal matrices O d .</p><p>Here, we also assume that word vectors are 2 - normalized. Under these assumptions, we have</p><formula xml:id="formula_4">cos(Wx i , y i ) = x i W y i . Similarly, we have y j − Wx i 2 2 = 2 − 2x i W y j .</formula><p>Therefore, find- ing the k nearest neighbors of Wx i among the ele- ments of Y is equivalent to finding the k elements of Y which have the largest dot product with Wx i . We adopt this equivalent formulation because it leads to a convex formulation when relaxing the orthogonality constraint on W. In summary, our optimization problem with the Relaxed CSLS loss (RCSLS) is written as:</p><formula xml:id="formula_5">min W∈O d 1 n n i=1 −2x i W y i + 1 k y j ∈N Y (Wx i ) x i W y j + 1 k Wx j ∈N X (y i ) x j W y i .<label>(4)</label></formula><p>Convex relaxation. Eq. (4) involves the min- imization of a non-smooth cost function over the manifold of orthogonal matrices O d . As such, it can be solved using manifold optimiza- tion tools ( <ref type="bibr" target="#b4">Boumal et al., 2014)</ref>. In this work, we consider as an alternative to the set O d , its convex hull C d , i.e., the unit ball of the spectral norm. We refer to this projection as the "Spectral" model. We also consider the case where these constraints on the alignment matrix are simply removed. Having a convex domain allows us to reason about the convexity of the cost function. We ob- serve that the second and third terms in the CSLS loss can be rewritten as follows:</p><formula xml:id="formula_6">y j ∈N k (Wx i ) x i W y j = max S∈S k (n) j∈S x i W y j ,</formula><p>where S k (n) denotes the set of all subsets of {1, . . . , n} of size k. This term, seen as a func- tion of W, is a maximum of linear functions of W, which is convex ( <ref type="bibr" target="#b5">Boyd and Vandenberghe, 2004</ref>). This shows that our objective function is convex with respect to the mapping W and piecewise lin- ear (hence non-smooth). Note, our approach could be generalized to other loss functions by replac- ing the term x i W y j by any function convex in W. We minimize this objective function over the convex set C d by using the projected subgradient descent algorithm.</p><p>The projection onto the set C d is solved by tak- ing the singular value decomposition (SVD) of the matrix, and thresholding the singular values to one.</p><p>Extended Normalization. Usually, the number of word pairs in the seed lexicon n is small with respect to the size of the dictionaries N . To benefit from unlabeled data, it is common to add an itera- tive "refinement procedure" ( <ref type="bibr" target="#b1">Artetxe et al., 2017)</ref> when learning the translation model W. Given a model W t , this procedure iterates over two steps. First it augments the training lexicon by keeping the best-inferred translation in Eq. (3). Second it learns a new mapping W t+1 by solving the prob- lem in Eq. (1). This strategy is similar to standard semi-supervised approaches where the training set is augmented over time. In this work, we propose to use the unpaired words in the dictionaries as "negatives" in the RCSLS loss: instead of comput- ing the k-nearest neighbors N Y (Wx i ) amongst the annotated words {y 1 , . . . , y n }, we do it over the whole dictionary {y 1 , . . . , y N }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>This section reports the main results obtained with our method. We provide complementary results and an ablation study in the appendix. We refer to our method without constraints as RCSLS and as RCSLS+spectral if the spectral constraints are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation details</head><p>We choose a learning rate in {1, 10, 25, 50} and a number of epochs in {10, 20} on the validation set. For the unconstrained RCSLS, a small 2 regu- larization can be added to prevent the norm of W to diverge. In practice, we do not use any regular- ization. For the English-Chinese pairs (en-zh), we center the word vectors. The number of nearest neighbors in the CSLS loss is 10. We use the 2 - normalized fastText word vectors by <ref type="bibr" target="#b3">Bojanowski et al. (2017)</ref> trained on Wikipedia. <ref type="table">Table 1</ref> reports the comparison of RCSLS with stan- dard supervised and unsupervised approaches on 5 language pairs (in both directions) of the MUSE benchmark ( <ref type="bibr" target="#b6">Conneau et al., 2017)</ref>. Every approach uses the Wikipedia fastText vectors and supervi- sion comes in the form of a lexicon composed of 5k words and their translations. Regardless of the relaxation, RCSLS outperforms the state of the art by, on average, 3 to 4% in accuracy. This shows the importance of using the same criterion during training and inference. Note that the refinement step ("refine") also uses CSLS to finetune the align- ments but leads to a marginal gain for supervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The MUSE benchmark</head><p>Interestingly, RCSLS achieves a better perfor- mance without constraints (+0.8%) for all pairs. Contrary to observations made in previous works, this result suggests that preserving the distance be- tween word vectors is not essential for word trans- lation. Indeed, previous works used a 2 loss where, indeed, orthogonal constraints lead to an improve- ment of +5.3% (Procrustes versus Least Square Error). This suggests that a linear mapping W with no constraints works well only if it is learned with a proper criterion.</p><p>Impact of extended normalization. <ref type="table" target="#tab_1">Table 2</ref> re- ports the gain brought by including words not in the lexicon (unannotated words) to the performance of RCSLS. Extending the dictionary significantly improves the performance on all language pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>en-es es-en en-fr fr-en en-de de-en en-ru ru-en en-zh zh-en avg.</p><p>Adversarial + refine 81. en-es en-fr en-de en-ru avg.</p><p>Train 80.7 82.3 74.8 51.9 72.4 Ext.</p><p>84.1 83.3 79.1 57.9 76.1  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The WaCky dataset</head><p>Dinu et al. <ref type="formula" target="#formula_0">(2014)</ref> introduce a setting where word vectors are learned on the WaCky datasets ( <ref type="bibr" target="#b2">Baroni et al., 2009</ref>) and aligned with a noisy bilingual lexicon. We select the number of epochs within {1, 2, 5, 10} on a validation set. Avg. 51.9 62.0 57.3 52.8 58.5 58.5 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with existing aligned vectors</head><p>Recently, word vectors based on fastText have been aligned and released by <ref type="bibr">Smith et al. (2017, BabylonPartners, BP)</ref> and <ref type="bibr">Conneau et al. (2017, MUSE)</ref>. Both use a variation of Procrustes to align word vectors in the same space. We compare these methods to RCSLS and re- port results in <ref type="table" target="#tab_6">Table 5</ref>. RCSLS improves the perfor- mance by +3.5% over MUSE vectors when trained with the same lexicon (Original). Training RSCSL on the full training lexicon (Full) brings an addi- tional improvement of +2.9% on average with a CSLS criterion, and +6.1% with a NN criterion. For reference, the performance of Procrustes only improves by +1.4% with CSLS and even degrades with a NN criterion. RCSLS benefits more from additional supervision than Procrustes. Finally,  the gap between RCSLS and the other methods is higher with a NN criterion, suggesting that RCSLS imports some of the properties of CSLS to the dot product between aligned vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Impact on word vectors</head><p>Non-orthogonal mapping of word vectors changes their dot products. We evaluate the impact of this mapping on word analogy tasks (Mikolov et al.,  2013a). In <ref type="table" target="#tab_4">Table 4</ref>, we report the accuracy on analo- gies for raw word vectors and our vectors mapped to English with an alignement trained on the full MUSE training set. Regardless of the source lan- guage, the mapping does not negatively impact the word vectors. Similarly, our alignement has also lit- tle impact on word similarity, as shown in <ref type="table" target="#tab_8">Table 6</ref>. We confirm this observation by running the re- verse mapping, i.e., by mapping the English word vectors of <ref type="bibr" target="#b14">Mikolov et al. (2018)</ref> to Spanish. It leads to an improvement of 1% both for vectors trained on Common Crawl (85% to 86%) and Wikipedia + News (87% to 88%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper shows that minimizing a convex relax- ation of the CSLS loss significantly improves the quality of bilingual word vector alignment. We use a reformulation of CSLS that generalizes to convex functions beyond dot products and provides a sin- gle end-to-end training that is consistent with the inference stage. Finally, we show that removing the orthogonality constraint does not degrade the quality of the aligned vectors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>by another criterion, such as ISF (Smith et al., 2017) or CSLS (Conneau et al., 2017).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Accuracy with and without an extended nor-
malization for RCSLS. "Ext." uses the full 200k vocab-
ulary and "Train" only uses the pairs from the training 
lexicon. 

en-it it-en 

Adversarial + refine + CSLS 45.1 38.3 

Mikolov et al. (2013b) 
33.8 24.9 
Dinu et al. (2014) 
38.5 24.6 
Artetxe et al. (2016) 
39.7 33.8 
Smith et al. (2017) 
43.1 38.0 
Procrustes + CSLS 
44.9 38.5 

RCSLS 
45.5 38.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Accuracy on English and Italian with the set-
ting of Dinu et al. (2014). "Adversarial" is an unsuper-
vised technique. The adversarial and Procrustes results 
are from Conneau et al. (2017). We use a CSLS crite-
rion for retrieval. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 shows</head><label>3</label><figDesc></figDesc><table>that 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Performance on word analogies for differ-
ent languages. We compare the original embeddings to 
their mapping to English. The mappings are learned 
using the full MUSE bilingual lexicons. We use the 
fastText vectors of Bojanowski et al. (2017). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison with publicly available aligned 
vectors over 28 languages. All use supervision. Aligne-
ments are learned either on the "Original" or "Full" 
MUSE training. We report the detailed performance 
with a CSLS criterion and the average for both NN and 
CSLS criteria. 
 *  BP uses a different training set of comparable size. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Performance on word similarities for differ-
ent languages. We compare the original embeddings 
to their mapping to English. The mappings are learned 
with the full MUSE bilingual lexicons over the fastText 
vectors of Bojanowski et al. (2017). 

</table></figure>

			<note place="foot" n="1"> https://github.com/facebookresearch/ fastText/tree/master/alignment/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning principled bilingual mappings of word embeddings while preserving monolingual invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2289" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning bilingual word embeddings with (almost) no bilingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The wacky wide web: a collection of very large linguistically processed webcrawled corpora. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><surname>Ferraresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eros</forename><surname>Zanchetta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="209" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<ptr target="https://fasttext.cc/docs/en/pretrained-vectors.html" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Manopt, a Matlab toolbox for optimization on manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Boumal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Bamdev Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Absil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sepulchre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1455" to="1459" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lieven</forename><surname>Vandenberghe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04087</idno>
		<ptr target="http://github.com/facebookresearch/MUSE" />
		<title level="m">Word translation without parallel data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improving zero-shot learning by mitigating the hubness problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6568</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Sheep, goats, lambs and wolves: A statistical analysis of speaker performance in the NIST 1998 speaker recognition evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Liggett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Reynolds</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Procrustes problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garmt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dijksterhuis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Oxford University Press on Demand</publisher>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unsupervised alignment of embeddings with wasserstein procrustes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Berthet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11222</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An iterative closest point method for unsupervised word translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06126</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inducing crosslingual distributed representations of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Bhattarai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012</title>
		<meeting>COLING 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1459" to="1474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Advances in pre-training distributed word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Offline bilingual word vectors, orthogonal transformations and the inverted softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Turban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">Y</forename><surname>Hamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hammerla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03859</idno>
		<ptr target="https://github.com/Babylonpartners/fastText_multilingual" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Normalized word embedding and orthogonal transform for bilingual word translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiye</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1006" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
