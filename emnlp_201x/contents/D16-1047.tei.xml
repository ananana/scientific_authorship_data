<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Morphological Priors for Probabilistic Neural Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parminder</forename><surname>Bhatia</surname></persName>
							<email>parminder@yikyakapp.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Interactive Computing Georgia Institute of Technology Atlanta</orgName>
								<orgName type="institution">Yik Yak, Inc</orgName>
								<address>
									<addrLine>3525 Piedmont Rd NE, Building 6, Suite 500 Atlanta</addrLine>
									<postCode>30312</postCode>
									<region>GA, GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Guthrie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Interactive Computing Georgia Institute of Technology Atlanta</orgName>
								<orgName type="institution">Yik Yak, Inc</orgName>
								<address>
									<addrLine>3525 Piedmont Rd NE, Building 6, Suite 500 Atlanta</addrLine>
									<postCode>30312</postCode>
									<region>GA, GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Interactive Computing Georgia Institute of Technology Atlanta</orgName>
								<orgName type="institution">Yik Yak, Inc</orgName>
								<address>
									<addrLine>3525 Piedmont Rd NE, Building 6, Suite 500 Atlanta</addrLine>
									<postCode>30312</postCode>
									<region>GA, GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Morphological Priors for Probabilistic Neural Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="490" to="500"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Word embeddings allow natural language processing systems to share statistical information across related words. These embeddings are typically based on distributional statistics, making it difficult for them to generalize to rare or unseen words. We propose to improve word embeddings by incorporating morphological information, capturing shared sub-word features. Unlike previous work that constructs word embeddings directly from morphemes, we combine morphological and distributional information in a unified probabilistic framework , in which the word embedding is a latent variable. The morphological information provides a prior distribution on the latent word em-beddings, which in turn condition a likelihood function over an observed corpus. This approach yields improvements on intrinsic word similarity evaluations, and also in the downstream task of part-of-speech tagging.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word embeddings have been shown to improve many natural language processing applications, from lan- guage models ( <ref type="bibr">Mikolov et al., 2010</ref>) to information extraction <ref type="bibr" target="#b9">(Collobert and Weston, 2008)</ref>, and from parsing ( <ref type="bibr" target="#b5">Chen and Manning, 2014</ref>) to machine trans- lation ( <ref type="bibr">Cho et al., 2014</ref>). Word embeddings leverage a classical idea in natural language processing: use distributional statistics from large amounts of unla- beled data to learn representations that allow sharing * The first two authors contributed equally.</p><p>Code is available at https://github.com/rguthrie3/ MorphologicalPriorsForWordEmbeddings. across related words <ref type="bibr" target="#b4">(Brown et al., 1992)</ref>. While this approach is undeniably effective, the long-tail nature of linguistic data ensures that there will always be words that are not observed in even the largest cor- pus <ref type="bibr" target="#b43">(Zipf, 1949)</ref>. There will be many other words which are observed only a handful of times, making the distributional statistics too sparse to accurately estimate the 100-or 1000-dimensional dense vectors that are typically used for word embeddings. These problems are particularly acute in morphologically rich languages like German and Turkish, where each word may have dozens of possible inflections.</p><p>Recent work has proposed to address this issue by replacing word-level embeddings with embeddings based on subword units: morphemes ( <ref type="bibr" target="#b27">Luong et al., 2013;</ref><ref type="bibr" target="#b3">Botha and Blunsom, 2014</ref>) or individual char- acters ( <ref type="bibr" target="#b35">Santos and Zadrozny, 2014;</ref><ref type="bibr" target="#b23">Kim et al., 2016</ref>). Such models leverage the fact that word meaning is often compositional, arising from subword components. By learning representations of subword units, it is possible to generalize to rare and unseen words.</p><p>But while morphology and orthography are some- times a signal of semantics, there are also many cases similar spellings do not imply similar meanings: bet- ter-batter, melon-felon, dessert-desert, etc. If each word's embedding is constrained to be a determinis- tic function of its characters, as in prior work, then it will be difficult to learn appropriately distinct em- beddings for such pairs. Automated morphological analysis may be incorrect: for example, really may be segmented into re+ally, incorrectly suggesting a similarity to revise and review. Even correct morpho- logical segmentation may be misleading. Consider that incredible and inflammable share a prefix in-, which exerts the opposite effect in these two cases. <ref type="bibr">1</ref> Overall, a word's observed internal structure gives evidence about its meaning, but it must be possible to override this evidence when the distributional facts point in another direction.</p><p>We formalize this idea using the machinery of probabilistic graphical models. We treat word em- beddings as latent variables <ref type="bibr" target="#b39">(Vilnis and McCallum, 2014)</ref>, which are conditioned on a prior distribution that is based on word morphology. We then maximize a variational approximation to the expected likeli- hood of an observed corpus of text, fitting variational parameters over latent binary word embeddings. For common words, the expected word embeddings are largely determined by the expected corpus likelihood, and thus, by the distributional statistics. For rare words, the prior plays a larger role. Since the prior distribution is a function of the morphology, it is pos- sible to impute embeddings for unseen words after training the model. We model word embeddings as latent binary vec- tors. This choice is based on linguistic theories of lexical semantics and morphology. Morphemes are viewed as adding morphosyntactic features to words: for example, in English, un-adds a negation feature (unbelievable), -s adds a plural feature, and -ed adds a past tense feature <ref type="bibr" target="#b19">(Halle and Marantz, 1993)</ref>. Sim- ilarly, the lexicon is often viewed as organized in terms of features: for example, the word bachelor carries the features HUMAN, MALE, and UNMAR- RIED ( <ref type="bibr" target="#b22">Katz and Fodor, 1963)</ref>. Each word's semantic role within a sentence can also be characterized in terms of binary features <ref type="bibr" target="#b14">(Dowty, 1991;</ref><ref type="bibr" target="#b34">Reisinger et al., 2015)</ref>. Our approach is more amenable to such theoretical models than traditional distributed word embeddings. However, we can also work with the ex- pected word embeddings, which are vectors of prob- abilities, and can therefore be expected to hold the advantages of dense distributed representations <ref type="bibr" target="#b1">(Bengio et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>The modeling framework is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, focusing on the word sesquipedalianism. This word is rare, but its morphology indicates several of its properties: the -ism suffix suggests that the word is a noun, likely describing some abstract property; the sesqui-prefix refers to one and a half, and so on. If the word is unknown, we must lean heavily on these intuitions, but if the word is well attested then we can rely instead on its examples in use.</p><p>It is this reasoning that our modeling framework aims to formalize. We treat word embeddings as la- tent variables in a joint probabilistic model. The prior distribution over a word's embedding is conditioned on its morphological structure. The embedding it- self then participates, as a latent variable, in a neural sequence model over a corpus, contributing to the overall corpus likelihood. If the word appears fre- quently, then the corpus likelihood dominates the prior -which is equivalent to relying on the word's distributional properties. If the word appears rarely, then the prior distribution steps in, and gives a best guess as to the word's meaning.</p><p>Before describing these component pieces in detail, we first introduce some notation. The representation of word w is a latent binary vector b w ∈ {0, 1} k , where k is the size of each word embedding. As noted in the introduction, this binary representation is motivated by feature-based theories of lexical se- mantics ( <ref type="bibr" target="#b22">Katz and Fodor, 1963)</ref>. Each word w is constructed from a set of M w observed morphemes, M w = (m w,1 , m w,2 , . . . , m w,Mw ). Each morpheme is in turn drawn from a finite vocabulary of size v m , so that m w,i ∈ {1, 2, . . . , v m }. Morphemes are obtained from an unsupervised morphological segmenter, which is treated as a black box. Fi- nally, we are given a corpus, which is a sequence of words, x = (x 1 , x 2 , . . . , x N ), where each word x t ∈ {1, 2, . . . , v w }, with v w equal to the size of the vocabulary, including the token UNK for unknown words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Prior distribution</head><p>The key differentiating property of this model is that rather than estimating word embeddings directly, we treat them as a latent variable, with a prior distri- bution reflecting the word's morphological proper-plagued by sesquipedalianism . . . ties. To characterize this prior distribution, each mor- pheme m is associated with an embedding of its own, u m ∈ R k , where k is again the embedding size. Then for position i of the word embedding b w , we have the following prior,</p><formula xml:id="formula_0">h 1 h 2 h 3 b plagued b by b sesquipedalianism u plague u by u sesqui u ed u pedal u ian u ism</formula><formula xml:id="formula_1">b w,i ∼ Bernoulli σ( m∈Mw u m,i ) ,<label>(1)</label></formula><p>where σ(·) indicates the sigmoid function. The prior log-likelihood for a set of word embeddings is,</p><formula xml:id="formula_2">logP (b; M, u) (2) = Vw w log P (b w ; M w , u) (3) = Vw w k i log P (b w,i ; M w , u) (4) = Vw w k i b w,i log σ m∈Mw u m,i<label>(5)</label></formula><formula xml:id="formula_3">+ (1 − b w,i ) log 1 − σ m∈Mw u m,i</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Expected likelihood</head><p>The corpus likelihood is computed via a recurrent neural network language model ( <ref type="bibr">Mikolov et al., 2010, RNNLM)</ref>, which is a generative model of sequences of tokens. In the RNNLM, the probability of each word is conditioned on all preceding words through a recurrently updated state vector. This state vector in turn depends on the embeddings of the previous words, through the following update equations:</p><formula xml:id="formula_4">h t =f (b xt , h t−1 ) (6) x t+1 ∼Multinomial (Softmax [Vh t ]) .<label>(7)</label></formula><p>The function f (·) is a recurrent update equation; in the RNN, it corresponds to σ(Θh t−1 + b xt ), where σ(·) is the elementwise sigmoid function. The matrix V ∈ R v×k contains the "output embeddings" of each word in the vocabulary. We can then define the condi- tional log-likelihood of a corpus x = (x 1 , x 2 , . . . x N ) as,</p><formula xml:id="formula_5">log P (x | b) = N t log P (x t | h t−1 , b). (8)</formula><p>Since h t−1 is deterministically computed from x 1:t−1 (conditioned on b), we can equivalently write the log-likelihood as,</p><formula xml:id="formula_6">log P (x | b) = t log P (x t | x 1:t−1 , b).<label>(9)</label></formula><p>This same notation can be applied to compute the likelihood under a long-short term memory (LSTM) language model ( <ref type="bibr" target="#b36">Sundermeyer et al., 2012</ref>). The only difference is that the recurrence function f (·) from Equation 6 now becomes more complex, including the input, output, and forget gates, and the recurrent state h t now includes the memory cell. As the LSTM update equations are well known, we focus on the more concise RNN notation, but we employ LSTMs in all experiments due to their better ability to capture long-range dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Variational approximation</head><p>Inference on the marginal likelihood P (x 1:N ) = P (x 1:N , b)db is intractable. We address this is- sue by making a variational approximation,</p><formula xml:id="formula_7">log P (x) = log b P (x | b)P (b) (10) = log b Q(b) Q(b) P (x | b)P (b) (11) = log E q P (x | b) P (b) Q(b) (12) ≥E q [log P (x | b) + log P (b) − log Q(b)]<label>(13)</label></formula><p>The variational distribution Q(b) is defined using a fully factorized mean field approximation,</p><formula xml:id="formula_8">Q(b; γ) = vw w k i q(b w,i ; γ w,i ).<label>(14)</label></formula><p>The variational distribution is a product of Bernoullis, with parameters γ w,j ∈ [0, 1]. In the evaluations that follow, we use the expected word embeddings q(b w ), which are dense vectors in <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> k . We can then use Q(·) to place a variational lower bound on the expected conditional likelihood, Even with this variational approximation, the ex- pected log-likelihood is still intractable to compute. In recurrent neural network language models, each word x t is conditioned on the entire prior history, x 1:t−1 -indeed, this is one of the key advantages over fixed-length n-gram models. However, this means that the individual expected log probabilities involve not just the word embedding of x t and its immediate predecessor, but rather, the embeddings of all words in the sequence x 1:t :</p><formula xml:id="formula_9">E q [log P (x | b)]<label>(15)</label></formula><formula xml:id="formula_10">= N t E q [log P (x t | x 1:t−1 , b)]<label>(16)</label></formula><formula xml:id="formula_11">= N t {bw:w∈x 1:t } Q({b w : w ∈ x 1:t }) × log P (x t | x 1:t−1 , b).<label>(17)</label></formula><p>We therefore make a further approximation by tak- ing a local expectation over the recurrent state,</p><formula xml:id="formula_12">E q [h t ] ≈ f (E q [b xt ] , E q [h t−1 ]) (18) E q [log P (x t | x 1:t−1 , b)] ≈ log Softmax (VE q [h t ]) .<label>(19)</label></formula><p>This approximation means that we do not propa- gate uncertainty about h t through the recurrent up- date or through the likelihood function, but rather, we use local point estimates. Alternative methods such as variational autoencoders <ref type="bibr" target="#b8">(Chung et al., 2015</ref>) or sequential Monte Carlo (de <ref type="bibr" target="#b13">Freitas et al., 2000</ref>) might provide better and more principled approximations, but this direction is left for future work.</p><p>Variational bounds in the form of Equation 13 can generally be expressed as a difference between an expected log-likelihood term and a term for the <ref type="bibr">Kullback-Leibler (KL)</ref> divergence between the prior distribution P (b) and the variational distribution Q(b) ( <ref type="bibr" target="#b40">Wainwright and Jordan, 2008)</ref>. Incorporat- ing the approximation in Equation 19, the resulting objective is,</p><formula xml:id="formula_13">L = N t log P (x t | x 1:t−1 ; E q [b]) − D KL (Q(b) P (b)).<label>(20)</label></formula><p>The KL divergence is equal to,</p><formula xml:id="formula_14">D KL (Q(b) P (b))<label>(21)</label></formula><formula xml:id="formula_15">= vw w k i D KL (q(b w,i ) P (b w,i ))<label>(22)</label></formula><formula xml:id="formula_16">= vw w k i γ w,i log σ( m∈Mw u m,i ) + (1 − γ w,i ) log(1 − σ( m∈Mw u m,i )) − γ w,i log γ w,i − (1 − γ w,i ) log(1 − γ w,i ).<label>(23)</label></formula><p>Each term in the variational bound can be easily constructed in a computation graph, enabling auto- matic differentiation and the application of standard stochastic optimization techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Implementation</head><p>The objective function is given by the variational lower bound in Equation 20, using the approxima- tion to the conditional likelihood described in Equa- tion 19. This function is optimized in terms of several parameters:</p><p>• the morpheme embeddings, {u m } m∈1...vm ;</p><p>• the variational parameters on the word embed- dings, {γ} w∈1...vw ;</p><p>• the output word embeddings V;</p><p>• the parameter of the recurrence function, Θ.</p><p>Each of these parameters is updated via the RMSProp online learning algorithm <ref type="bibr">(Tieleman and Hinton, 2012</ref>). The model and baseline (described be- low) are implemented in blocks <ref type="bibr">(van Merriënboer et al., 2015</ref>). In the remainder of the paper, we refer to our model as VAREMBED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data and preprocessing</head><p>All embeddings are trained on 22 million tokens from the the North American News Text (NANT) corpus <ref type="bibr" target="#b18">(Graff, 1995)</ref>. We use an initial vocabu- lary of 50,000 words, with a special UNK token for words that are not among the 50,000 most com- mon. We then perform downcasing and convert all numeric tokens to a special NUM token. After these steps, the vocabulary size decreases to 48,986. Note that the method can impute word embeddings for out-of-vocabulary words under the prior distribution P (b; M, u); however, it is still necessary to decide on a vocabulary size to determine the number of variational parameters γ and output embeddings to estimate. Unsupervised morphological segmentation is per- formed using Morfessor ( <ref type="bibr" target="#b12">Creutz and Lagus, 2002</ref>), with a maximum of sixteen morphemes per word. This results in a total of 14,000 morphemes, which includes stems for monomorphemic words. We do not rely on any labeled information about morpho- logical structure, although the incorporation of gold morphological analysis is a promising topic for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning details</head><p>The LSTM parameters are initialized uniformly in the range [−0.08, 0.08]. The word embeddings are initialized using pre-trained word2vec embeddings. We train the model for 15 epochs, with an initial learning rate of 0.01, a decay of 0.97 per epoch, and minibatches of size 25. We clip the norm of the gradients (normalized by minibatch size) at 1, using the default settings in the RMSprop implementation in blocks. These choices are motivated by prior work ( <ref type="bibr">Zaremba et al., 2014</ref>). After each iteration, we compute the objective function on the development set; when the objective does not improve beyond a small threshold, we halve the learning rate.</p><p>Training takes roughly one hour per iteration us- ing an NVIDIA 670 GTX, which is a commodity graphics processing unit (GPU) for gaming. This is nearly identical to the training time required for our reimplementation of the algorithm of Botha and Blunsom (2014), described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baseline</head><p>The most comparable approach is that of <ref type="bibr" target="#b3">Botha and Blunsom (2014)</ref>. In their work, embeddings are es- timated for each morpheme, as well as for each in- vocabulary word. The final embedding for a word is then the sum of these embeddings, e.g., greenhouse = greenhouse + green + house, <ref type="bibr">(24)</ref> where the italicized elements represent learned em- beddings.</p><p>We build a baseline that is closely inspired by this approach, which we call SUMEMBED. The key differ- ence is that while <ref type="bibr" target="#b3">Botha and Blunsom (2014)</ref> build on the log-bilinear language model <ref type="bibr" target="#b32">(Mnih and Hinton, 2007)</ref>, we use the same LSTM-based architecture as in our own model implementation. This enables our evaluation to focus on the critical difference between the two approaches: the use of latent variables rather than summation to model the word embeddings. As with our method, we used pre-trained word2vec embeddings to initialize the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Number of parameters</head><p>The dominant terms in the overall number of parame- ters are the (expected) word embeddings themselves. The variational parameters of the input word em- beddings, γ, are of size k × v w . The output word embeddings are of size #|h| × v w . The morpheme embeddings are of size k × v m , with v m v w . In our main experiments, we set v w = 48, 896 (see above), k = 128, and #|h| = 128. After including the character embeddings and the parameters of the recurrent models, the total number of parameters is roughly 12.8 million. This is identical to number of parameters in the SUMEMBED baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>Our evaluation compares the following embeddings: <ref type="bibr">WORD2VEC</ref> We train the popular word2vec CBOW (continuous bag of words) model ( <ref type="bibr" target="#b30">Mikolov et al., 2013)</ref>, using the gensim implementation.</p><p>SUMEMBED We compare against the baseline de- scribed in § 3.3, which can be viewed as a reimplementation of the compositional model of <ref type="bibr" target="#b3">Botha and Blunsom (2014)</ref>. VAREMBED For our model, we take the expected embeddings E q <ref type="bibr">[b]</ref>, and then pass them through an inverse sigmoid function to obtain values over the entire real line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Word similarity</head><p>Our first evaluation is based on two classical word similarity datasets: Wordsim353 ( <ref type="bibr" target="#b17">Finkelstein et al., 2001</ref>) and the Stanford "rare words" (rw) dataset <ref type="bibr" target="#b27">(Luong et al., 2013</ref>). We report Spearmann's ρ, a mea- sure of rank correlation, evaluating on both the entire vocabulary as well as the subset of in-vocabulary words.</p><p>As shown in <ref type="table">Table 1</ref>, VAREMBED consistently outperforms SUMEMBED on both datasets. On the subset of in-vocabulary words, <ref type="bibr">WORD2VEC</ref> gives slightly better results on the wordsim words that are in the NANT vocabulary, but is not applicable to the complete dataset. On the rare words dataset, WORD2VEC performs considerably worse than both morphology-based models, matching the findings of <ref type="bibr" target="#b27">Luong et al. (2013)</ref> and <ref type="bibr" target="#b3">Botha and Blunsom (2014)</ref> regarding the importance of morphology for doing well on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Alignment with lexical semantic features</head><p>Recent work questions whether these word similar- ity metrics are predictive of performance on down- stream tasks <ref type="bibr" target="#b16">(Faruqui et al., 2016</ref>). The QVEC statis- tic is another intrinsic evaluation method, which has been shown to be better correlated with downstream tasks ( <ref type="bibr" target="#b38">Tsvetkov et al., 2015)</ref>. This metric measures the alignment between word embeddings and a set of lexical semantic features. Specifically, we use the semcor noun verb supersenses oracle provided at the qvec github repository. <ref type="bibr">2</ref> As shown in <ref type="table" target="#tab_1">Table 2</ref>, VAREMBED outperforms SUMEMBED on the full lexicon, and gives simi- lar performance to WORD2VEC on the set of in- vocabulary words. We also consider the morpheme embeddings alone. For SUMEMBED, this means that we construct the word embedding from the sum of the embeddings for its morphemes, without the ad- ditional embedding per word. For VAREMBED, we use the expected embedding under the prior distribu- tion E[b | c]. The results for these representations are shown in the bottom half of Table 2, revealing that VAREMBED learns much more meaningful em- beddings at the morpheme level, while much of the power of SUMEMBED seems to come from the word embeddings. <ref type="bibr">WORD2VEC</ref>   <ref type="formula" target="#formula_2">(353)</ref> n/a 42.9 48.8 in-vocab <ref type="formula">(348)</ref> 51.4  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUMEMBED VAREMBED</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Part-of-speech tagging</head><p>Our final evaluation is on the downstream task of part-of-speech tagging, using the Penn Treebank. We build a simple classification-based tagger, us- ing a feedforward neural network. (This is not in- tended as an alternative to state-of-the-art tagging algorithms, but as a comparison of the syntactic utility of the information encoded in the word em- beddings.) The inputs to the network are the con- catenated embeddings of the five word neighbor- hood (x t−2 , x t−1 , x t , x t+1 , x t+2 ); as in all evalua- tions, 128-dimensional embeddings are used, so the total size of the input is 640. This input is fed into a network with two hidden layers of size 625, and a softmax output layer over all tags. We train using RMSProp <ref type="bibr">(Tieleman and Hinton, 2012)</ref>. Results are shown in    at p &lt; .05. <ref type="figure" target="#fig_3">Figure 2</ref> breaks down the errors by word frequency. As shown in the figure, the tagger based on WORD2VEC performs poorly for rare words, which is expected because these embeddings are estimated from sparse distributional statistics. SUMEMBED is slightly better on the rarest words (the 0 − 100 group accounts for roughly 10% of all tokens). In this case, it appears that this simple additive model is better, since the distributional statistics are too sparse to offer much improvement. The probabilistic VAREMBED embeddings are best for all other frequency groups, showing that it effectively combines morphology and distributional statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>Adding side information to word embeddings An alternative approach to incorporating additional information into word embeddings is to constrain the embeddings of semantically-related words to be sim- ilar. Such work typically draws on existing lexical semantic resources such as WordNet. For example, Yu and Dredze (2014) define a joint training objec- tive, in which the word embedding must predict not only neighboring word tokens in a corpus, but also related word types in a semantic resource; a similar approach is taken by <ref type="bibr" target="#b2">Bian et al. (2014)</ref>. Alternatively,  propose to "retrofit" pre-trained word embeddings over a semantic network. Both retrofitting and our own approach treat the true word embeddings as latent variables, from which the pre- trained word embeddings are stochastically emitted. However, a key difference from our approach is that the underlying representation in these prior works is relational, and not generative. These methods can capture similarity between words in a relational lex- icon such as WordNet, but they do not offer a gen- erative account of how (approximate) meaning is constructed from orthography or morphology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word embeddings and morphology</head><p>The SUMEMBED baseline is based on the work of <ref type="bibr" target="#b3">Botha and Blunsom (2014)</ref>, in which words are segmented into morphemes using MORFESSOR ( <ref type="bibr" target="#b12">Creutz and Lagus, 2002</ref>), and then word representations are computed through addition of morpheme represen- tations. A key modeling difference from this prior work is that rather than computing word embeddings directly and deterministically from subcomponent embeddings (morphemes or characters, as in ( <ref type="bibr" target="#b23">Kim et al., 2016)</ref>), we use these subcomponents to define a prior distribution, which can be overridden by distributional statistics for common words. Other work exploits morphology by training word embeddings to optimize a joint objective over distributional statistics and rich, morphologically-augmented part of speech tags <ref type="bibr" target="#b10">(Cotterell and Schütze, 2015)</ref>. This can yield better word embeddings, but does not provide a way to compute embeddings for unseen words, as our approach does. Recent work by <ref type="bibr" target="#b11">Cotterell et al. (2016)</ref> extends the idea of retrofitting, which was based on semantic similarity, to a morphological framework. In this model, embeddings are learned for morphemes as well as for words, and each word embedding is con- ditioned on the sum of the morpheme embeddings, using a multivariate Gaussian. The covariance of this Gaussian prior is set to the inverse of the number of examples in the training corpus, which has the effect of letting the morphology play a larger role for rare or unseen words. Like all retrofitting approaches, this method is applied in a pipeline fashion after training word embeddings on a large corpus; in contrast, our approach is a joint model over the morphology and corpus. Another practical difference is that <ref type="bibr" target="#b11">Cotterell et al. (2016)</ref> use gold morphological features, while we use an automated morphological segmentation.</p><p>Latent word embeddings Word embeddings are typically treated as a parameter, and are optimized through point estimation ( <ref type="bibr" target="#b0">Bengio et al., 2003;</ref><ref type="bibr" target="#b9">Collobert and Weston, 2008;</ref><ref type="bibr">Mikolov et al., 2010)</ref>. Cur- rent models use word embeddings with hundreds or even thousands of parameters per word, yet many words are observed only a handful of times. It is therefore natural to consider whether it might be beneficial to model uncertainty over word embed- dings. <ref type="bibr" target="#b39">Vilnis and McCallum (2014)</ref> propose to model Gaussian densities over dense vector word embed- dings. They estimate the parameters of the Gaussian directly, and, unlike our work, do not consider us- ing orthographic information as a prior distribution. This is easy to do in the latent binary framework proposed here, which is also a better fit for some the- oretical models of lexical semantics <ref type="bibr" target="#b22">(Katz and Fodor, 1963;</ref><ref type="bibr" target="#b34">Reisinger et al., 2015)</ref>. This view is shared by <ref type="bibr" target="#b25">Kruszewski et al. (2015)</ref>, who induce binary word representations using labeled data of lexical seman- tic entailment relations, and by <ref type="bibr" target="#b20">Henderson and Popa (2016)</ref>, who take a mean field approximation over binary representations of lexical semantic features to induce hyponymy relations.</p><p>More broadly, our work is inspired by recent ef- forts to combine directed graphical models with dis- criminatively trained "deep learning" architectures. The variational autoencoder <ref type="bibr" target="#b24">(Kingma and Welling, 2014)</ref>, neural variational inference ( <ref type="bibr" target="#b31">Mnih and Gregor, 2014;</ref><ref type="bibr" target="#b28">Miao et al., 2016)</ref>, and black box variational inference ( <ref type="bibr" target="#b33">Ranganath et al., 2014</ref>) all propose to use a neural network to compute the variational approx- imation. These ideas are employed by <ref type="bibr" target="#b8">Chung et al. (2015)</ref> in the variational recurrent neural network, which places a latent continuous variable at each time step. In contrast, we have a dictionary of latent vari-ables -the word embeddings -which introduce uncertainty over the hidden state h t in a standard re- current neural network or LSTM. We train this model by employing a mean field approximation, but these more recent techniques for neural variational infer- ence may also be applicable. We plan to explore this possibility in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and future work</head><p>We present a model that unifies compositional and distributional perspectives on lexical semantics, through the machinery of Bayesian latent variable models. In this framework, our prior expectations of word meaning are based on internal structure, but these expectations can be overridden by distributional statistics. The model is based on the very successful long-short term memory (LSTM) for sequence mod- eling, and while it employs a Bayesian justification, its inference and estimation are little more compli- cated than a standard LSTM. This demonstrates the advantages of reasoning about uncertainty even when working in a "neural" paradigm.</p><p>This work represents a first step, and we see many possibilities for improving performance by extending it. Clearly we would expect this model to be more ef- fective in languages with richer morphological struc- ture than English, and we plan to explore this possi- bility in future work. From a modeling perspective, our prior distribution merely sums the morpheme em- beddings, but a more accurate model might account for sequential or combinatorial structure, through a recurrent ( ), recursive <ref type="bibr" target="#b27">(Luong et al., 2013)</ref>, or convolutional architecture ( <ref type="bibr" target="#b23">Kim et al., 2016</ref>). There appears to be no technical obstacle to imposing such structure in the prior distribution. Furthermore, while we build the prior distribution from morphemes, it is natural to ask whether char- acters might be a better underlying representation: character-based models may generalize well to non- word tokens such as names and abbreviations, they do not require morphological segmentation, and they require a much smaller number of underlying em- beddings. On the other hand, morphemes encode rich regularities across words, which may make a morphologically-informed prior easier to learn than a prior which works directly at the character level. It is possible that this tradeoff could be transcended by combining characters and morphemes in a single model.</p><p>Another advantage of latent variable models is that they admit partial supervision. If we follow <ref type="bibr" target="#b38">Tsvetkov et al. (2015)</ref> in the argument that word embeddings should correspond to lexical semantic features, then an inventory of such features could be used as a source of partial supervision, thus locking dimen- sions of the word embeddings to specific semantic properties. This would complement the graph-based "retrofitting" supervision proposed by , by instead placing supervision at the level of individual words.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Model architecture, applied to the example sequence. .. plagued by sesquipedalianism. .. . Blue solid arrows indicate direct computation, red dashed arrows indicate probabilistic dependency. For simplicity, we present our models as recurrent neural networks rather than long short-term memories (LSTMs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Error rates by word frequency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Alignment with lexical semantic features, as measured 

by QVEC. Higher scores are better, with a maximum possible 

score of 100. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>Both 
morphologically-informed 
embeddings 
are 
significantly better to WORD2VEC (p &lt; .01, 
two-tailed binomial test), but the difference between 
SUMEMBED and VAREMBED is not significant 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : Part-of-speech tagging accuracies</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> The confusion is resolved by morphologically analyzing the second example as (in+flame)+able, but this requires hierarchical morphological parsing, not just segmentation.</note>

			<note place="foot" n="2"> https://github.com/ytsvetko/qvec</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Thanks to Erica Briscoe, Martin Hyatt, Yangfeng Ji, Bryan Leslie Lee, and Yi Yang for helpful discussion of this work. Thanks also the EMNLP reviewers for constructive feedback. This research is supported by the Defense Threat Reduction Agency under award HDTRA1-15-1-0019.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Knowledgepowered deep learning for word embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="132" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Compositional morphology for word representations and language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Classbased n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer C</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods for Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods for Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<imprint>
			<publisher>Holger Schwenk</publisher>
			<biblScope unit="page">498</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods for Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods for Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<meeting><address><addrLine>Montréal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Morphological word-embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics (NAACL)<address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Morphological smoothing and extrapolation of word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of morphemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Creutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><surname>Lagus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 workshop on Morphological and phonological learning</title>
		<meeting>the ACL-02 workshop on Morphological and phonological learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sequential monte carlo methods to train neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesan</forename><surname>João Fg De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">H</forename><surname>Niranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Gee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doucet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="955" to="993" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Thematic proto-roles and argument selection. Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dowty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="547" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sujay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics (NAACL)<address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Problems with evaluation of word embeddings using word similarity tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno>arxiv, 1605.02276</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
	<note>North american news text corpus</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed morphology and the pieces of inflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morris</forename><surname>Halle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Marantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The view from building 20</title>
		<editor>Kenneth L. Hale and Samuel J. Keyser</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A vector space for distributional semantics for entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><forename type="middle">Nicoleta</forename><surname>Popa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The structure of a semantic theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jerrold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><forename type="middle">A</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fodor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language</title>
		<imprint>
			<biblScope unit="page" from="170" to="210" />
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the National Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Autoencoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deriving boolean structures from distributional vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="375" to="388" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramón</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods for Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods for Natural Language Processing (EMNLP)<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural variational inference for text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cernock`nock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural variational inference and learning in belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1791" to="1799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Black box variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Seventeenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="814" to="822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic proto-roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Rawlins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="475" to="488" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cicero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1818" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">LSTM neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijman</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rmsprop. Technical report, Coursera Neural Networks for Machine Learning</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Evaluation of word vector representations by subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno>abs/1506.00619</idno>
	</analytic>
	<monogr>
		<title level="m">Chorowski, and Yoshua Bengio. 2015. Blocks and fuel: Frameworks for deep learning. CoRR</title>
		<meeting><address><addrLine>Lisbon; David Warde-Farley</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
	<note>Proceedings of Empirical Methods for Natural Language Processing (EMNLP)</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Word representations via gaussian embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno>abs/1412.6623</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improving lexical embeddings with semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)<address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="545" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Human behavior and the principle of least effort</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kingsley Zipf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1949" />
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
