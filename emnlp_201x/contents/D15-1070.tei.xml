<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image-Mediated Learning for Zero-Shot Cross-Lingual Document Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruka</forename><surname>Funaki</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="laboratory">Machine Perception Group Graduate</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Nakayama</surname></persName>
							<email>{funaki,nakayama}@nlab.ci.i.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="laboratory">Machine Perception Group Graduate</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Image-Mediated Learning for Zero-Shot Cross-Lingual Document Retrieval</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose an image-mediated learning approach for cross-lingual document retrieval where no or only a few parallel corpora are available. Using the images in image-text documents of each language as the hub, we derive a common semantic subspace bridging two languages by means of generalized canonical correlation analysis. For the purpose of evaluation , we create and release a new document dataset consisting of three types of data (English text, Japanese text, and images). Our approach substantially enhances retrieval accuracy in zero-shot and few-shot scenarios where text-to-text examples are scarce.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cross-lingual document retrieval (CLDR) is the task of finding relevant documents in one lan- guage given a query document in another lan- guage. While sufficiently large-scale corpora are critical for parallel corpus-based learning meth- ods, manually creating corpora requires huge hu- man effort and is unrealistic in many cases.</p><p>A straightforward approach is to crawl bilin- gual documents from the Web for use as train- ing data. However, because most documents on the Web are written in one language, it is not al- ways easy to collect a sufficient number of multi- lingual documents, especially those involving mi- nor languages. Let us consider the multimedia information in documents. We can, for exam- ple, find abundant pairings of text and images, e.g., text with the ALT property of &lt;IMG&gt; tags in HTML, text with photos posted to social network- ing sites, and articles on Web news posted with images. Unlike text, an image is a universal rep- resentation; we can easily understand the semantic Our idea is to learn the relation between two lan- guages indirectly by using images attached to text. If two documents written in different languages include images with similar image features, it is likely that the texts contained in the two docu- ments are similar. Based on this idea, we seek the relation of texts written in different languages me- diated by the similarity between images.</p><p>content of images regardless of our mother tongue. Motivated by this observation, we expect that we can learn the relation of two languages indirectly through images, even if we do not have sufficient bilingual text pairs <ref type="figure" target="#fig_0">(Figure 1</ref>).</p><p>Generally, traditional image recognition tech- niques (or image features) are very poor com- pared with those in the natural language process- ing field. In recent years, however, deep learning has resulted in a breakthrough in visual recogni- tion and dramatically improved image recognition accuracy in generic domains, which is rapidly ap- proaching human recognition levels <ref type="bibr" target="#b3">(Fang et al., 2015)</ref>. We expect that these state-of-the-art im- age recognition technologies can effectively assist CLDR tasks.</p><p>We show that hub images enable zero-shot training of CLDR systems and improve retrieval accuracy given only a few parallel text samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multimodal Learning for CLDR</head><p>Multimodal learning, defined as a framework for machine learning using inputs from multiple me- dia or sensors, has played a key role in various cross-modal applications. The most widely used standard method for multimodal learning is canon- ical correlation analysis (CCA) <ref type="bibr" target="#b6">(Hotelling, 1936)</ref>, which projects multimodal data into a shared rep- resentation. For example, CCA has been success- fully used in image retrieval (tag to images) and image annotation (image to tags) ( <ref type="bibr" target="#b5">Hardoon et al., 2004;</ref><ref type="bibr" target="#b15">Rasiwasia et al., 2010;</ref><ref type="bibr" target="#b4">Gong et al., 2014</ref>). In the context of CLDR, each language's texts con- stitute one modality. CCA has also commonly been used for cross-lingual information retrieval ( <ref type="bibr" target="#b21">Vinokourov et al., 2002;</ref><ref type="bibr" target="#b11">Li and Shawe-Taylor, 2004;</ref><ref type="bibr" target="#b19">Udupa and Khapra, 2010)</ref>. Whereas CCA can handle only two modalities, we need to con- sider relations between three modalities because we use images in addition to the two languages. Therefore, we focus on an extension of CCA, gen- eralized canonical correlation analysis (GCCA), to handle more than two inputs <ref type="bibr" target="#b8">(Kettenring, 1971)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Zero-Shot Learning for CLDR</head><p>Our core idea is to use another modality (image) as a hub to indirectly learn the relevance between two different languages. The work by Rupnik et al. is probably the closest to ours <ref type="bibr" target="#b16">(Rupnik et al., 2012)</ref>. In their study, they used a popular language (e.g., English) with enough bilingual documents shared with other languages as a hub to enhance CLDR for minor languages with few direct bilin- gual texts available. Nevertheless, this method assumes that parallel corpora of the hub and tar- get languages exist and therefore, its application is limited to specific domains where manual transla- tions are readily available, such as Wikipedia and news sites. Contrarily, because we use images as the hub, we can use documents closed with respect to each language for training. Considering that current generic Web documents are mostly closed with respect to one language, yet equipped with rich multimedia data, our setup is assumed to be more reasonable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Division</head><p>English Images Japanese  </p><formula xml:id="formula_0">1 [train-E/I] E 1 I 1 - 2 [train-I/J] - I 2 J 2 3 [train-E/J] E 3 - J 3 4 [test-E/J] E 4 - J 4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of Image-Mediated Learning</head><p>We use the following notations for specifying each non-overlapping data division.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">[train-E/I]: Training documents consisting of</head><p>English text and images. 2.</p><p>[train-I/J]: Training documents consisting of images and Japanese text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">[train-E/J]: Training documents consisting of</head><p>English and Japanese text. 4. [test-E/J]: Test documents consisting of En- glish and Japanese text. We define IDs for each modality in each division as given in <ref type="table" target="#tab_0">Table 1</ref> An overview of our system is depicted in <ref type="figure">Fig- ure</ref> 2. We compress features by principal compo- nent analysis (PCA) and train them by GCCA. For testing, we compress features by PCA, project fea- tures by GCCA, then, search the nearest neighbors from Japanese to English in the joint space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Extraction</head><p>A convolutional neural network (CNN) is one of the most successful deep learning methods for vi- sual recognition. It is known that we can ob- tain very good image features by taking activation of hidden neurons in a network pre-trained by a sufficiently large dataset ( <ref type="bibr" target="#b2">Donahue et al., 2013)</ref>. We apply the CNN model pre-trained using the ILSVRC2012 dataset ( <ref type="bibr" target="#b17">Russakovsky et al., 2015)</ref> provided by Caffe ( <ref type="bibr" target="#b7">Jia et al., 2014</ref>), a standard deep learning software package in the field of vi- sual recognition.</p><p>As the text feature for both English and Japanese, we use the bag-of-words (BoW) rep- resentation and term frequency-inverse document frequency (TF-IDF) weighting.</p><p>The MeCab ( <ref type="bibr" target="#b10">Kudo et al., 2004</ref>) library is used to divide Japanese text into words by morphological analy- sis. No preprocessing approaches like eliminating stop words and stemming, are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GCCA</head><p>GCCA is a generalization of CCA for any m modalities (m = 3 in our case). Although sev- eral slightly different versions of GCCA have been proposed <ref type="bibr" target="#b0">(Carroll, 1968;</ref><ref type="bibr" target="#b16">Rupnik et al., 2012;</ref><ref type="bibr" target="#b20">Velden and Takane, 2012)</ref>, we implement the sim- plest one <ref type="bibr" target="#b8">(Kettenring, 1971)</ref> because GCCA itself is not the main focus of this study.</p><p>Let E, I, and J denote English, images, and Japanese, respectively. For feature vector x k , ∀k ∈ {E, I, J}, let z k = (x k − x k )h k de- note its canonical variables. Σ ij denotes a covari- ance matrix of modalities i and j where i, j ∈ {E, I, J}. Projection vectors h k are computed such that they maximize the sum of correlations between each pair of modalities obtained by solv- ing the following generalized eigenvalue problem:</p><formula xml:id="formula_1">1 2   0 Σ EI Σ EJ Σ IE 0 Σ IJ Σ JE Σ JI 0   h = ρ   Σ EE 0 0 0 Σ II 0 0 0 Σ JJ   h<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">h = (h T E , h T I , h T J ) T .</formula><p>The canon- ical axises h are normalized such that</p><formula xml:id="formula_3">1 3 ∑ 3 k∈{E,I,J} h T k Σ kk h k = 1.</formula><p>Additionally, we add a regularization term to the self covariance matrices to prevent over-fitting; that is, we set Σ kk → Σ kk + αI, where α is a parameter to avoid the singularity issue.</p><p>Despite our training datasets having only two of the three modalities as given in <ref type="table" target="#tab_0">Table 1</ref>, we can handle this situation naturally by computing covariance matrices from the available data only. For example, in the few-shot learning scenario, we compute Σ EI using E 1 and I 1 , and Σ EE us- ing E 1 and E 3 . In the zero-shot learning scenario, because [train-E/J] is not available, we compute Σ EE using E 1 only and use a zero matrix for Σ EJ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Nearest Neighbor Search in Joint Space</head><p>We can find relevant documents in another lan- guage by computing the distances from the query documents using the coupled canonical subspaces. Having set Japanese as the query language, we retrieve documents written in English. Nearest neighbors are obtained as follows:</p><formula xml:id="formula_4">ˆ j := arg min j d(z i J , z j E ),<label>(2)</label></formula><p>where z i J , z j E are projected feature vectors of the query and target documents, respectively, and d(·) is a distance function, which in our case, is the Euclidean distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pascal Sentence Dataset with Japanese Translation</head><p>The UIUC Pascal Sentence Dataset (Rashtchian et al., 2010) contains 1000 images, each of which is annotated with five English sentences describ- ing its content. This dataset was originally created for the study of sentence generation from images, which is one of the current hot topics in computer vision. To establish a new benchmark dataset for image-mediated CLDR, we included a Japanese translation for each English sentence provided by professional translators 1 , as shown in <ref type="figure" target="#fig_6">Figure 3</ref>. In this experiment, we bundled the five sentences at- tached to each image for use as one text document. Therefore, in our setup, each of the 1000 docu- ments in the dataset consists of three items: an im- age, and the corresponding English and Japanese text.</p><p>-A family on a boat with a cross on a river -A happy couple with a young child wearing a life preserver sitting on a boat. -A man, a woman, and a child sit on boat with a large cross on it. -A man, women and small child sitting on top of a boat moving along the river. -Family of three sitting on deck, child wearing red vest, brush and shoes are seen in the foreground.</p><formula xml:id="formula_5">-川で十字架のあるボートに乗っている家族。 -ボートに座っている、救命具を着た幼い子ど もと幸せなカップル。 -男性、女性と子どもが大きな十字架のある ボートに座っています。 -川に沿って動いているボートの上部に座って いる男性、女性と小さな子ども。 -ブラシと靴が前景に写されている、子どもが 赤いベストを着て、デッキに座っている三人 の家族。 ︙</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English Texts</head><p>Images Japanese Texts ︙ ︙ -A black and white cow in a grassy field stares at the camera. -A black and white cow standing in a grassy field. -A black and white cow stands on grass against a partly cloudy blue sky. -a cow is gazing over the grass he is about to graze -Black and white cow standing in grassy field. <ref type="figure" target="#fig_6">Figure 3</ref>: Examples from the Pascal Sentence Dataset with Japanese translations: each image has about five sentences describing it from different perspectives.</p><formula xml:id="formula_6">-草地の黒と白の雌牛がカメラをじっと見てい ます。 -草地に立っている黒と白の雌牛。 -一部曇った青空を背に黒と白の雌牛が草地に 立っています。 -雌牛が食べようとしている草をじっと眺めて います。 -草地に立っている黒と白の雌牛。</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>We randomly sampled data from the dataset for each division in <ref type="table" target="#tab_0">Table 1</ref> without any overlap; we ignored the modality of each document that was not available in each data division (e.g., Japanese text in [train-E/I]). We ran experiments with vary- ing sample sizes for [train-E/I] and [train-I/J], that is, 100, 200, 300, and 400. Furthermore, we grad- ually increased the number of [train-E/J] samples from 0 to 100 to emulate the few-shot learning sce- nario. The size of the test data [test-E/J] was fixed at 100. Following this setup, we performed image- mediated CLDR based on GCCA, and compared the results with those obtained by standard CLDR using only [train-E/J] data with CCA. We eval- uated the performance with respect to the top-1 Japanese to English retrieval accuracy in the test data. Given that we used 100 test samples, the chance rate was 1%. For each run, we conducted 50 trials randomly replacing data and used the av- erage score. All features were compressed into 100 dimensions via PCA and α was set to 0.01.</p><p>The experimental results, illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>, clearly show that better accuracy is obtained with a greater number of text-image data in both zero- shot and few-shot scenarios. We can expect even better zero-shot accuracy with more text-image data, although, we cannot increase [train-E/I] and [train-I/J] more than 400 each in the current setup because of the restricted dataset size. We sum- marized results in zero-shot scenario in <ref type="table" target="#tab_1">Table 2</ref> in several cases. Although both GCCA and CCA show improved performance as the sample size of [train-E/J] increases, not surprisingly, GCCA is gradually overtaken by CCA when we have enough samples to learn the relevance between English and Japanese texts directly. However, ac- curacies of image-mediated learning in the cases when [train-E/J] is scarce are higher than CCA baseline. Hence, we confirmed that the image- mediated model is also effective in the few-shot learning scenario.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effect of Image Features</head><p>We also verified the effect of the performance of image features in our framework (see <ref type="figure" target="#fig_5">Figure 5</ref> and <ref type="table">Table 3</ref>). CNN has improved dramatically over the last few years, and many new powerful pre-trained networks are currently available. We compared three different features extracted from GoogLeNet ( <ref type="bibr" target="#b18">Szegedy et al., 2014</ref>), VGG 16 lay- ers ( <ref type="bibr" target="#b1">Chatfield et al., 2014</ref>), and CaffeNet ( <ref type="bibr" target="#b7">Jia et al., 2014;</ref><ref type="bibr" target="#b9">Krizhevsky et al., 2012</ref>). Additionally, we tested the Fisher Vector ( <ref type="bibr" target="#b13">Perronnin et al., 2010)</ref>, which was the standard hand-crafted image fea- ture before deep learning. We extracted features from the pool5/7x7 s1 layer in GoogLeNet, fc6 layer in VGG, and fc6 layer in CaffeNet. For the Fisher Vector, following the standard implemen- tation, we compressed SIFT descriptors (Lowe,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature</head><p>Accuracy(%) CNN(GoogLeNet), BoW 37.4 ± 3.8 CNN(VGG), BoW 31.3 ± 3.5 CNN(CaffeNet), BoW 25.1 ± 3.4 FisherVector, BoW 10.8 ± 2.7 CNN(GoogLeNet), TF-IDF 42.0 ± 4.6 CNN(VGG), TF-IDF 37.8 ± 2.9 CNN(CaffeNet), TF-IDF 29.7 ± 4.4 FisherVector, TF-IDF 12.6 ± 2.7  1999) into 64 dimensions by PCA, and used a Gaussian mixture model with 64 components. We used four spatial grids for the final feature extrac- tion. Overall, the order of performance of features corresponds to that known in the image classifica- tion domain ( <ref type="bibr" target="#b17">Russakovsky et al., 2015)</ref>. This re- sult indicates that when more powerful image fea- tures are used, better performance can be achieved in image-mediated CLDR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed an image-mediated learning ap- proach to realize zero-shot or few-shot CLDR. For evaluation, we created and released a new dataset consisting of Japanese, English, and image triplets, based on the widely used Pascal Sentence Dataset. We showed that state-of-the-art CNN- based image features can substantially improve zero-shot CLDR performance. Considering that image features have continued to improve rapidly since the deep learning breakthrough and the uni- versality of images in Web documents, this ap- proach could become even more important in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Concept of image-mediated learning. Our idea is to learn the relation between two languages indirectly by using images attached to text. If two documents written in different languages include images with similar image features, it is likely that the texts contained in the two documents are similar. Based on this idea, we seek the relation of texts written in different languages mediated by the similarity between images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: System overview</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>.</head><label></label><figDesc>For example, E 1 represents features of English text in the [train-E/I] division. Typical CLDR based on parallel corpora uses only [train-E/J] for training and [test-E/J] for evalua- tion. In the zero-shot learning scenario without any [train-E/J] data, we use only [train-E/I] and [train-I/J] for training. In the few-shot learning scenario, we also use a small number of [train-E/J] samples. We call this approach image-mediated learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Retrieval accuracy varying the number of [train-E/J] data. Each colored line shows the performance of our method with a different sample size of [train-E/I] and [train-I/J] data (e.g., GCCA-400 denotes respective 400 samples of [train-E/I] and [train-I/J] for GCCA). We used image features extracted from GoogLeNet and text features represented as bags-of-words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Retrieval accuracy using different image features in image-mediated CLDR. The sample size of both [train-E/I] and [train-I/J] is 400. Text features are based on the bag-of-words model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>3 :</head><label>3</label><figDesc>Accuracy of zero-shot learning in multi- ple image features. The sample size of both [train- E/I] and [train-I/J] is 400. Both the bag-of-words (BoW) and the TF-IDF model are used as text fea- tures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Division of training and test data. Each 
division of training dataset is missing one of the 
three modalities. 

PCA 
projection 

English Features 
PCA 

PCA 

PCA 

GCCA 
Image Features 

Japanese Features 

English Features 
GCCA 
projection 

Japanese Features 

Joint Space 

Training 

Test 

GCCA 
projection 

English Features 

Japanese Features 

Nearest 
Neighbors 
Search 

PCA 
projection 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Accuracy of zero-shot learning. Image 
features are extracted from GoogLeNet and both 
the bag-of-words (BoW) and the TF-IDF model 
are used as text features. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Dataset is available at: http://www.nlab.ci.i.u-tokyo.ac.jp/ dataset/pascal_sentence_jp/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by JST CREST, JSPS KAKENHI Grant Number 26730085. We thank the three anonymous reviewers for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generalization of canonical correlation analysis to three or more sets of variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">References</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 76th Annual Convention of the American Psychological Association</title>
		<meeting>the 76th Annual Convention of the American Psychological Association</meeting>
		<imprint>
			<date type="published" when="1968" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="227" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Return of the Devil in the Details: Delving Deep into Convolutional Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">From Captions to Visual Concepts and Back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Hao Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Rupesh</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Multiview Embedding Space for Modeling Internet Images, Tags, and their Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifa</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="233" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Canonical Correlation Analysis: an Overview with Application to Learning Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandor</forename><surname>David R Hardoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shawetaylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2639" to="2664" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Relations between Two Sets of Variants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="321" to="377" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Caffe : Convolutional Architecture for Fast Feature Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Canonical Analysis of Several Sets of Variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">Robers</forename><surname>Kettenring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="433" to="451" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note>Hinton</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Applying Conditional Random Fields to Japanese Morphological Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaoru</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="230" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using KCCA for Japanese-English Cross-language Information Retrieval and Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Methods for Text Understanding and Mining Workshop</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="117" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David G Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh IEEE International Conference on Computer Vision</title>
		<meeting>the Seventh IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving the Fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Collecting Image Annotations Using Amazon s Mechanical Turk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ave</forename><surname>North Goodwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="139" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A New Approach to Cross-modal Multimedia Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Rasiwasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">Costa</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Coviello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gert</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Multimedia</title>
		<meeting>the International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="251" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cross-Lingual Document Retrieval through Hub Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Rupnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Muhič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Primoškrabaprimoˇprimoškraba</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>CoRR abs/1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving the Multilingual User Experience of Wikipedia Using Cross-Language Name Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghavendra</forename><surname>Udupa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khapra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="492" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generalized Canonical Correlation Analysis with Missing Values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Velden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshio</forename><surname>Takane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="551" to="571" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Vinokourov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1473" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
