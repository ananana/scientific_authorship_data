<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Supervised Learning of Sequence Models with the Method of Moments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zita</forename><surname>Marinho</surname></persName>
							<email>zmarinho@cmu.edu, andre.martins@unbabel.com, scohen@inf.ed.ac.uk, nasmith@cs.washington.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><forename type="middle">F T</forename><surname>Martins</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Instituto de TelecomunicaçTelecomunicaç˜Telecomunicações</orgName>
								<orgName type="institution" key="instit2">Instituto Superior Técnico</orgName>
								<address>
									<postCode>1049-001</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Unbabel Lda</orgName>
								<address>
									<addrLine>Rua Visconde de Santarém, 67-B</addrLine>
									<postCode>1000-286</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Priberam Labs</orgName>
								<orgName type="institution">Alameda D. Afonso Henriques</orgName>
								<address>
									<addrLine>41, 2 o</addrLine>
									<postCode>1000-123</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98195</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Instituto de Sistemas e Robótica</orgName>
								<orgName type="institution">Instituto Superior Técnico</orgName>
								<address>
									<postCode>1049-001</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-Supervised Learning of Sequence Models with the Method of Moments</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="287" to="296"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a fast and scalable method for semi-supervised learning of sequence models, based on anchor words and moment matching. Our method can handle hidden Markov models with feature-based log-linear emissions. Unlike other semi-supervised methods, no decoding passes are necessary on the unlabeled data and no graph needs to be constructed-only one pass is necessary to collect moment statistics. The model parameters are estimated by solving a small quadratic program for each feature. Experiments on part-of-speech (POS) tagging for Twitter and for a low-resource language (Malagasy) show that our method can learn from very few annotated sentences.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Statistical learning of NLP models is often lim- ited by the scarcity of annotated data. Weakly su- pervised methods have been proposed as an alter- native to laborious manual annotation, combining large amounts of unlabeled data with limited re- sources, such as tag dictionaries or small annotated datasets <ref type="bibr" target="#b17">(Merialdo, 1994;</ref><ref type="bibr" target="#b26">Smith and Eisner, 2005;</ref><ref type="bibr" target="#b10">Garrette et al., 2013)</ref>. Unfortunately, most semi- supervised learning algorithms for the structured problems found in NLP are computationally expen- sive, requiring multiple decoding passes through the unlabeled data, or expensive similarity graphs. More scalable learning algorithms are in demand.</p><p>In this paper, we propose a moment-matching method for semi-supervised learning of sequence models. Spectral learning and moment-matching approaches have recently proved a viable alternative to expectation-maximization (EM) for unsupervised learning ( <ref type="bibr" target="#b12">Hsu et al., 2012;</ref><ref type="bibr" target="#b2">Balle and Mohri, 2012;</ref><ref type="bibr" target="#b1">Bailly et al., 2013)</ref>, supervised learning with latent variables <ref type="bibr" target="#b6">(Cohen and Collins, 2014;</ref><ref type="bibr" target="#b24">Quattoni et al., 2014;</ref>) and topic modeling ( <ref type="bibr" target="#b0">Arora et al., 2013;</ref><ref type="bibr" target="#b18">Nguyen et al., 2015)</ref>. These methods have learnability guarantees, do not suffer from lo- cal optima, and are computationally less demanding.</p><p>Unlike spectral methods, ours does not require an orthogonal decomposition of any matrix or tensor. Instead, it considers a more restricted form of super- vision: words that have unambiguous annotations, so-called anchor words ( <ref type="bibr" target="#b0">Arora et al., 2013)</ref>. Rather than identifying anchor words from unlabeled data ( <ref type="bibr" target="#b28">Stratos et al., 2016)</ref>, we extract them from a small labeled dataset or from a dictionary. Given the an- chor words, the estimation of the model parameters can be made efficient by collecting moment statistics from unlabeled data, then solving a small quadratic program for each word.</p><p>Our contributions are as follows:</p><p>• We adapt anchor methods to semi-supervised learning of generative sequence models.</p><p>• We show how our method can also handle log- linear feature-based emissions.</p><p>• We apply this model to POS tagging. Our ex- periments on the Twitter dataset introduced by <ref type="bibr" target="#b11">Gimpel et al. (2011)</ref> and on the dataset in- troduced by <ref type="bibr" target="#b10">Garrette et al. (2013)</ref> for Mala- gasy, a low-resource language, show that our method does particularly well with very little la- beled data, outperforming semi-supervised EM and self-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>287</head><p>In this paper, we address the problem of sequence labeling. Let x 1:L = x 1 , . . . , x L be a sequence of L input observations (for example, words in a sen- tence). The goal is to predict a sequence of labels h 1:L = h 1 , . . . , h L , where each h i is a label for the observation x i (for example, the word's POS tag). We start by describing two generative sequence models: hidden Markov models (HMMs, §2.1), and their generalization with emission features ( §2.2). Later, we propose a weakly-supervised method for estimating these models' parameters ( §3- §4) based only on observed statistics of words and contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hidden Markov Models</head><p>We define random variables X := X 1 , . . . , X L and H := H 1 , . . . , H L , corresponding to observa- tions and labels, respectively. Each X i is a random variable over a set X (the vocabulary), and each H i ranges over H (a finite set of "states" or "labels"). We denote the vocabulary size by V = |X |, and the number of labels by K = |H|. A first-order HMM has the following generative scheme:</p><formula xml:id="formula_0">p(X = x 1:L , H = h 1:L ) := (1) L =1 p(X =x | H =h ) L =0 p(H +1 =h +1 | H =h ),</formula><p>where we have defined h 0 = START and h L+1 = STOP. We adopt the following notation for the pa- rameters:</p><formula xml:id="formula_1">• The emission matrix O ∈ R V ×K , defined as O x,h := p(X = x | H = h), ∀h ∈ H, x ∈ X . • The transition matrix T ∈ R (K+2)×(K+2) , de- fined as T h,h := p(H +1 = h | H = h ), for every h, h ∈ H ∪ {START, STOP}. This matrix satisfies T 1 = 1. 1</formula><p>Throughout the rest of the paper we will adopt X ≡ X and H ≡ H to simplify notation, when- ever the index is clear from the context. Under this generative process, predicting the most proba- ble label sequence h 1:L given observations x 1:L is 1 That is, it satisfies</p><formula xml:id="formula_2">K h=1 p(H +1 = h | H = h ) + p(H +1 = STOP | H = h ) = 1; and also K h=1 p(H1 = h | H0 = START) = 1.</formula><p>accomplished with the Viterbi algorithm in O(LK 2 ) time.</p><p>If labeled data are available, the model param- eters O and T can be estimated with the maxi- mum likelihood principle, which boils down to a simple counting of events and normalization. If we only have unlabeled data, the traditional ap- proach is the expectation-maximization (EM) algo- rithm, which alternately decodes the unlabeled ex- amples and updates the model parameters, requiring multiple passes over the data. The same algorithm can be used in semi-supervised learning when la- beled and unlabeled data are combined, by initial- izing the model parameters with the supervised esti- mates and interpolating the estimates in the M-step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature-Based Hidden Markov Models</head><p>Sequence models with log-linear emissions have been considered by <ref type="bibr" target="#b26">Smith and Eisner (2005)</ref>, in a discriminative setting, and by <ref type="bibr" target="#b3">Berg-Kirkpatrick et al. (2010)</ref>, as generative models for POS induc- tion. Feature-based HMMs (FHMMs) define a fea- ture function for words, φ(X) ∈ R W , which can be discrete or continuous. This allows, for example, to indicate whether an observation, corresponding to a word, starts with an uppercase letter, contains digits or has specific affixes. More generally, it helps with the treatment of out-of-vocabulary words. The emis- sion probabilities are modeled as K conditional dis- tributions parametrized by a log-linear model, where the θ h ∈ R W represent feature weights:</p><formula xml:id="formula_3">p(X = x | H = h) := exp(θ h φ(x))/Z(θ h ). (2) Above, Z(θ h ) := x ∈X exp(θ h φ(x )</formula><p>) is a nor- malization factor. We will show in §4 how our moment-based semi-supervised method can also be used to learn the feature weights θ h .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Semi-Supervised Learning via Moments</head><p>We now describe our moment-based semi- supervised learning method for HMMs. Through- out, we assume the availability of a small labeled dataset D L and a large unlabeled dataset D U .</p><p>The full roadmap of our method is shown as Al- gorithm 1. Key to our method is the decomposition of a context-word moment matrix Q ∈ R C×V , which counts co-occurrences of words and contexts,  <ref type="figure">Figure 1</ref>: HMM, context (green) conditionally indepen- dent of present (red) w given state h . and will be formally defined in §3.1. Such co- occurrence matrices are often collected in NLP, for various problems, ranging from dimensionality re- duction of documents using latent semantic index- ing <ref type="bibr" target="#b8">(Deerwester et al., 1990;</ref><ref type="bibr" target="#b13">Landauer et al., 1998)</ref>, distributional semantics <ref type="bibr" target="#b25">(Schütze, 1998;</ref><ref type="bibr" target="#b14">Levy et al., 2015)</ref> and word embedding generation ( <ref type="bibr" target="#b9">Dhillon et al., 2015;</ref><ref type="bibr" target="#b20">Osborne et al., 2016</ref>). We can build such a moment matrix entirely from the unlabeled data D U . The same unlabeled data is used to build an estimate of a context-label moment matrix R ∈ R C×K , as explained in §3.3. This is done by first identifying words that are unambiguously associated with each label h, called anchor words, with the aid of a few labeled data; this is outlined in §3.2. Finally, given empirical estimates of Q and R, we estimate the emission matrix O by solving a small optimization problem independently per word ( §3.4). The transi- tion matrix T is obtained directly from the labeled dataset D L by maximizing the likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Moments of Contexts and Words</head><p>To formalize the notion of "context," we introduce the shorthand Z := X 1:(−1) , X (+1):L . Impor- tantly, the HMM in Eq. 1 entails the following con- ditional independence assumption: X is condition- ally independent of the surrounding context Z given the hidden state H . This is illustrated in <ref type="figure">Figure 1</ref>, using POS tagging as an example task.</p><p>We introduce a vector of context features ψ(Z ) ∈ R C , which may look arbitrarily within the context Z (left or right), but not at X itself. These features could be "one-hot" representations or other reduced-dimensionality embeddings (as de- scribed later in §5). Consider the word w ∈ X an instance of X ≡ X . A pivotal matrix in our formu- lation is the matrix Q ∈ R C×V , defined as:</p><formula xml:id="formula_4">Q c,w := E[ψ c (Z) | X = w].<label>(3)</label></formula><p>Expectations here are taken with respect to the prob- abilistic model in Eq. 1 that generates the data. The following quantities will also be necessary:</p><formula xml:id="formula_5">q c := E[ψ c (Z)], p w := p(X = w).<label>(4)</label></formula><p>Since all the variables in Eqs. 3-4 are observed, we can easily obtain empirical estimates by taking ex- pectations over the unlabeled data:</p><formula xml:id="formula_6">Q c,w = x,z∈D U ψ c (z)1(x = w) x,z∈D U 1(x = w) , (5) q c = x,z∈D U ψ c (z) |D U |, (6) p w = x,z∈D U 1(x = w) |D U |. (7)</formula><p>where we take 1(x = w) to be the indicator for word w. Note that, under our modeling assumptions, Q decomposes in terms of its hidden states:</p><formula xml:id="formula_7">E[ψ c (Z) | X = w] = (8) h∈H p(H = h | X = w)E[ψ c (Z) | H = h]</formula><p>The reason why this holds is that, as stated above, Z and X are conditionally independent given H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Anchor Words</head><p>Following <ref type="bibr" target="#b0">Arora et al. (2013)</ref> and <ref type="bibr" target="#b6">Cohen and Collins (2014)</ref>, we identify anchor words whose hidden state is assumed to be deterministic, regardless of context. In this work, we generalize this notion to more than one anchor word per label, for improved context estimates. This allows for more flexible forms of anchors with weak supervision. For each state h ∈ H, let its set of anchor words be</p><formula xml:id="formula_8">A(h)= {w ∈ X : p(H = h | X = w) = 1} (9) = w ∈ X : O w,h &gt;0 ∧ O w,h =0, ∀h =h .</formula><p>That is, A(h) is the set of unambiguous words that always take the label h. This can be estimated from the labeled dataset D L by collecting the most fre- quent unambiguous words for each label.</p><p>Algorithms for identifying A(h) from unlabeled data alone were proposed by <ref type="bibr" target="#b0">Arora et al. (2013)</ref> and <ref type="bibr" target="#b32">Zhou et al. (2014)</ref>, with application to topic models. Our work differs in which we do not aim to discover anchor words from pure unlabeled data, but rather exploit the fact that small amounts of labeled data are commonly available in many NLP tasks-better anchors can be extracted easily from such small la- beled datasets. In §5 we give a more detailed de- scription of the selection process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Moments of Contexts and Labels</head><p>We define the matrix R ∈ R C×K as follows:</p><formula xml:id="formula_9">R c,h := E[ψ c (Z) | H = h].<label>(10)</label></formula><p>Since the expectation in Eq. 10 is conditioned on the (unobserved) label h, we cannot directly estimate it using moments of observed variables, as we do for Q. However, if we have identified sets of anchor words for each label h ∈ H, we have:</p><formula xml:id="formula_10">E[ψ c (Z) | X ∈ A(h)] = = h E[ψ c (Z) | H = h ] p(H = h | X ∈ A(h)) =1(h =h) = R c,h .<label>(11)</label></formula><p>Therefore, given the set of anchor words A(h), the hth column of R can be estimated in a single pass over the unlabeled data, as follows:</p><formula xml:id="formula_11">R c,h = x,z∈D U ψ c (z)1(x ∈ A(h)) x,z∈D U 1(x ∈ A(h))<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Emission Distributions</head><p>We can now put all the ingredients above together to estimate the emission probability matrix O. The procedure we propose here is computationally very efficient, since only one pass is required over the un- labeled data, to collect the co-occurrence statistics Q and R. The emissions will be estimated from these moments by solving a small problem independently for each word. Unlike EM and self-training, no de- coding is necessary, only counting and normalizing; and unlike label propagation methods, there is re- quirement to build a graph with the unlabeled data.</p><p>The crux of our method is the decomposition in Eq. 8, which is combined with the one-to-one cor- respondence between labels h and anchor words A(h). We can rewrite Eq. 8 as:</p><formula xml:id="formula_12">Q c,w = h R c,h p(H = h | X = w).<label>(13)</label></formula><p>In matrix notation, we have Q = RΓ, where</p><formula xml:id="formula_13">Γ ∈ R K×V is defined as Γ h,w := p(H = h | X = w).</formula><p>If we had infinite unlabeled data, our moment es- timates Q and R would be perfect and we could solve the system of equations in Eq. 13 to obtain Γ exactly. Since we have finite data, we resort to a least squares solution. This corresponds to solv- ing a simple quadratic program (QP) per word, in- dependent from all the other words, as follows. De- note by q w := E[ψ(Z) | X = w] ∈ R C and by γ w := p(H = · | X = w) ∈ R K the wth columns of Q and Γ, respectively. We estimate the latter dis- tribution following <ref type="bibr" target="#b0">Arora et al. (2013)</ref>:</p><formula xml:id="formula_14">γ w = arg min γ w q w − Rγ w 2 2 s.t. 1 γ w = 1, γ w ≥ 0.<label>(14)</label></formula><p>Note that this QP is very small-it has only K variables-hence, we can solve it very quickly (1.7 ms on average, in Gurobi, with K = 12). Given the probability tables for p(H = h | X = w), we can estimate the emission probabilities O by direct application of Bayes rule:</p><formula xml:id="formula_15">O w,h = p(H = h | X = w) × p(X = w) p(H = h)<label>(15)</label></formula><formula xml:id="formula_16">= γ w,c × Eq. 7 p w w γ w ,c × p w .<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>290</head><p>These parameters are guaranteed to lie in the prob- ability simplex, avoiding the need of heuristics for dealing with "negative" and "unnormalized" prob- abilities required by prior work in spectral learn- ing ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Transition Distributions</head><p>It remains to estimate the transition matrix T. For the problems tackled in this paper, the number of labels K is small, compared to the vocabulary size V . The transition matrix has only O(K 2 ) degrees of freedom, and we found it effective to estimate it us- ing the labeled sequences in D L alone, without any refinement. This was done by smoothed maximum likelihood estimation on the labeled data, which boils down to counting occurrences of consecutive labels, applying add-one smoothing to avoid zero probabilities for unobserved transitions, and normal- izing.</p><p>For problems with numerous labels, a possible al- ternative is the composite likelihood method <ref type="bibr" target="#b5">(Chaganty and Liang, 2014</ref>). Given O, the maximization of the composite log-likelihood function leads to a convex optimization problem that can be efficiently optimized with an EM algorithm. A similar proce- dure was carried out by <ref type="bibr" target="#b6">Cohen and Collins (2014)</ref>. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Feature-Based Emissions</head><p>Next, we extend our method to estimate the param- eters of the FHMM in §2.2. Other than contextual features ψ(Z) ∈ R C , we also assume a feature encoding function for words, φ(X) ∈ R W . Our framework, illustrated in Algorithm 2, allows for both discrete and continuous word and context fea- tures. Lines 2-5 are the same as in Algorithm 1, replacing word occurrences with expected values of word features (we redefine Q and Γ to cope with features instead of words). The main difference with respect to Algorithm 1 is that we do not es- timate emission probabilities; rather, we first esti- mate the mean parameters (feature expectations E[φ(X) | H = h]), by solving one QP for each <ref type="bibr">2</ref> In preliminary experiments, the compositional likelihood method was not competitive with estimating the transition ma- trices directly from the labeled data, on the datasets described in §6; results are omitted due to lack of space. However, this may be a viable alternative if there is no labeled data and the anchors are extracted from gazetteers or a dictionary. emission feature; and then we solve a convex op- timization problem, for each label h, to recover the log-linear weights over emission features (called canonical parameters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Semi-Supervised Learning of Feature- Based HMMs with Moments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Estimation of Mean Parameters</head><p>First of all, we replace word probabilities by expec- tations over word features. We redefine the matrix Γ ∈ R K×W as follows:</p><formula xml:id="formula_17">Γ h,j := p(H = h) × E[φ j (X) | H = h] E[φ j (X)] .<label>(17)</label></formula><p>Note that, with one-hot word features, we have</p><formula xml:id="formula_18">E[φ w (X) | H = h] = P (X = w | H = h), E[φ w (X)] = p(X = w)</formula><p>, and therefore Γ h,w = p(H = h | X = w), so this can be regarded as a generalization of the framework in §3.4. Second, we redefine the context-word moment matrix Q as the following matrix in R C×W :</p><formula xml:id="formula_19">Q c,j = E [ψ c (Z) × φ j (X)]/ E[φ j (X)].<label>(18)</label></formula><p>Again, note that we recover the previous Q if we use one-hot word features. We then have the following generalization of Eq. 13:</p><formula xml:id="formula_20">E [ψ c (Z) × φ j (X)]/E[φ j (X)] = (19) h E [ψ c (Z) | H = h] or, in matrix notation, Q = RΓ.</formula><p>Again, matrices Q and R can be estimated from data by collecting empirical feature expectations over unlabeled sequences of observations. For R use Eq. 12 with no change; for Q replace Eq. 5 by</p><formula xml:id="formula_21">Q c,j = x,z∈D U ψ c (z)φ j (x) x,z∈D U φ j (x). (20)</formula><p>Let q j ∈ R C and γ j ∈ R K be columns of Q and Γ, respectively. Note that we must have</p><formula xml:id="formula_22">1 γ j = h P (H=h)E[φ j (X)|H=h] E[φ j (X)] = 1,<label>(21)</label></formula><formula xml:id="formula_23">since E[φ j (X)] = h P (H = h)E [φ j (X) | H = h].</formula><p>We rewrite the QP to minimize the squared difference for each dimension j independently:</p><formula xml:id="formula_24">γ j = arg min γ j q j − Rγ j 2 2 s.t. 1 γ j = 1.</formula><p>(22) Note that, if φ(x) ≥ 0 for all x ∈ X , then we must have γ j ≥ 0, and therefore we may impose this in- equality as an additional constraint.</p><p>Let ¯ γ ∈ R K be the vector of state probabilities, with entries ¯ γ h := p(H = h) for h ∈ H. This vec- tor can also be recovered from the unlabeled dataset and the set of anchors, by solving another QP that aggregates information for all words:</p><formula xml:id="formula_25">¯ γ = arg min ¯ γ ¯ q − R¯ γ 2 2 s.t. 1 ¯ γ = 1, ¯ γ ≥ 0.<label>(23)</label></formula><p>where ¯ q := E[ψ(Z)] ∈ R C is the vector whose entries are defined in Eq. 6.</p><p>Let µ h := E[φ(X) | H = h] ∈ R W be the mean parameters of the distribution for each state h. These parameters are computed by solving W inde- pendent QPs (Eq. 22), yielding the matrix Γ defined in Eq. 17, and then applying the formula:</p><formula xml:id="formula_26">µ h,j = Γ j,h × E[φ j (X)]/ ¯ γ h ,<label>(24)</label></formula><p>with ¯ γ h = p(H = h) estimated as in Eq. 23.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Estimation of Canonical Parameters</head><p>To compute a mapping from mean parameters µ h to canonical parameters θ h , we use the well-known Fenchel-Legendre duality between the entropy and the log-partition function <ref type="bibr" target="#b29">(Wainwright and Jordan, 2008)</ref>. Namely, we need to solve the following con- vex optimization problem:</p><formula xml:id="formula_27">θ h = arg max θ h θ h µ h − log Z(θ h ) + θ h , (25)</formula><p>where is a regularization constant. <ref type="bibr">3</ref> In practice, this regularization is important, since it prevents θ h from growing unbounded whenever µ h falls outside the marginal polytope of possible mean parameters. We solve Eq. 25 with the limited-memory BFGS al- gorithm ( <ref type="bibr" target="#b15">Liu and Nocedal, 1989)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Method Improvements</head><p>In this section we detail three improvements to our moment-based method that had a practical impact.</p><p>Supervised Regularization. We add a supervised penalty term to Eq. 22 to keep the label posteriors γ j close to the label posteriors estimated in the labeled set, γ j , for every feature j ∈ <ref type="bibr">[W ]</ref>. The regularized least-squares problem becomes:</p><formula xml:id="formula_28">min γ j (1 − λ)q j − Rγ j 2 + λγ j −γ j 2 s.t. 1 γ j = 1.<label>(26)</label></formula><p>CCA Projections. A one-hot feature representa- tion of words and contexts has the disadvantage that it grows with the vocabulary size, making the mo- ment matrix Q too sparse. The number of contex- tual features and words can grow rapidly on large text corpora. Similarly to <ref type="bibr" target="#b6">Cohen and Collins (2014)</ref> and <ref type="bibr" target="#b9">Dhillon et al. (2015)</ref>, we use canonical correla- tion analysis (CCA) to reduce the dimension of these vectors. We use CCA to form low-dimensional pro- jection matrices for features of words P W ∈ R W ×D and features of contexts P C ∈ R C×D , with D min{W, C}. We use these projections on the origi- nal feature vectors and replace the these vectors with their projections.</p><p>Selecting Anchors. We collect counts of each word-label pair, and select up to 500 anchors with high conditional probability on the anchoring state p(h | w). We tuned the probability threshold to select the anchors on the validation set, using steps of 0.1 in the unit interval, and making sure that all tags have at least one anchor. We also considered a frequency threshold, constraining anchors to oc- cur more than 500 times in the unlabeled corpus, and four times in the labeled corpus. Note that past work used a single anchor word per state (i.e., |A(h)| = 1). We found that much better results are obtained when |A(h)| 1, as choosing more an- chors increases the number of samples used to esti- mate the context-label moment matrix R, reducing noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We evaluated our method on two tasks: POS tagging of Twitter text (in English), and POS tagging for a low-resource language (Malagasy). For all the ex- periments, we used the universal POS tagset ( <ref type="bibr" target="#b23">Petrov et al., 2012)</ref>, which consists of K = 12 tags. We compared our method against supervised base- lines (HMM and FHMM), which use the labeled data only, and two semi-supervised baselines that exploit the unlabeled data: self-training and EM. For the Twitter experiments, we also evaluated a stacked architecture in which we derived features from our model's predictions to improve a state-of- the-art POS tagger (MEMM). <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Twitter POS Tagging</head><p>For the Twitter experiment, we used the Oct27 dataset of <ref type="bibr" target="#b11">Gimpel et al. (2011)</ref>, with the provided partitions (1,000 tweets for training and 328 for val- idation), and tested on the Daily547 dataset (547 tweets). Anchor words were selected from the train- ing partition as described in §5. We used 2.7M unlabeled tweets <ref type="bibr" target="#b19">(O'Connor et al., 2010</ref>) to train the semi-supervised methods, filtering the English tweets as in <ref type="bibr">Lui and Baldwin (2012)</ref>, tokenizing them as in <ref type="bibr" target="#b21">Owoputi et al. (2013)</ref>, and normalizing at-mentions, URLs, and emoticons.</p><p>We used as word features φ(X) the word iself, as well as binary features for capitalization, titles, and digits <ref type="bibr" target="#b3">(Berg-Kirkpatrick et al., 2010)</ref>, the word shape, and the Unicode class of each character. Sim- ilarly to <ref type="bibr" target="#b21">Owoputi et al. (2013)</ref>, we also used suf- fixes and prefixes (up to length 3), and Twitter- specific features: whether the word starts with @, #, or http://. As contextual features ψ(Z), we de- rive analogous features for the preceding and fol- lowing words, before reducing dimensionality with CCA. We collect feature expectations for words and contexts that occur more than 20 times in the un- labeled corpus. We tuned hyperparameters on the development set: the supervised interpolation co- efficient in Eq. 26, λ ∈ {0, 0.1, . . . , 1.0}, and, for all systems, the regularization coefficient ∈ {0.0001, 0.001, 0.01, 0.1, 1, 10}. (Underlines indi- cate selected values.) The former controls how much we rely on the supervised vs. unsupervised es- timates. For λ = 1.0 we used supervised estimates only for words that occur in the labeled corpus, all the remaining words rely solely on unsupervised es- timates.</p><p>Varying supervision. <ref type="figure">Figure 2</ref> compares the learning curves of our anchor-word method for the FHMM with the supervised baselines. We show the performance of the anchor methods without in- terpolation (λ = 0), and with supervised interpo- lation coefficient (λ = 1). When the amount of supervision is small, our method with and without interpolation outperforms all the supervised base- lines. This improvement is gradually attenuated when more labeled sequences are used, with the su- pervised FHMM catching up when the full labeled dataset is used. The best model λ = 1.0 relies on supervised estimates for words that occur in the la- beled corpus, and on anchor estimates for words that occur only in the unlabeled corpus. The unregular-  <ref type="table">Table 1</ref>: Tagging accuracies on Twitter. Shown are the supervised and semi-supervised baselines, and our moment-based method, trained with 150 training labeled sequences, and the full labeled corpus (1000 sequences).</p><p>ized model λ = 0.0 relies solely on unsupervised estimates given the set of anchors.</p><p>Semi-supervised comparison. Next, we compare our method to two other semi-supervised baselines, using both HMMs and FHMMs: EM and self- training. EM requires decoding and counting in multiple passes over the full unlabeled corpus. We initialized the parameters with the supervised esti- mates, and selected the iteration with the best ac- curacy on the development set. <ref type="bibr">5</ref> The self-training baseline uses the supervised system to tag the unla- beled data, and then retrains on all the data. Results are shown in <ref type="table">Table 1</ref>. We observe that, for small amounts of labeled data (150 tweets), our method outperforms all the supervised and semi- supervised baselines, yielding accuracies 6.1 points above the best semi-supervised baseline for a simple HMM, and 1.9 points above for the FHMM. With more labeled data (1,000 instances), our method out- performs all the baselines for the HMM, but not with the more sophisticated FHMM, in which our accura- cies are 0.3 points below the self-training method. <ref type="bibr">6</ref> These results suggest that our method is more effec- tive when the amount of labeled data is small. <ref type="bibr">5</ref> The FHMM with EM did not perform better than the su- pervised baseline, so we consider the initial value as the best accuracy under this model. <ref type="bibr">6</ref> According to a word-level paired Kolmogorov-Smirnov test, for the FHMM with 1,000 tweets, the self-training method outperforms the other methods with statistical significance at p &lt; 0.01; and for the FHMM with 150 tweets the anchor-based and self-training methods outperform the other baselines with the same p-value. Our best HMM outperforms the other base- lines at a significance level of p &lt; 0.01 for 150 and 1000 se- quences.  Stacking features. We also evaluated a stacked ar- chitecture in which we use our model's predictions as an additional feature to improve the state-of-the- art Twitter POS tagger of <ref type="bibr" target="#b21">Owoputi et al. (2013)</ref>. This system is based on a semi-supervised discriminative model with Brown cluster features ( <ref type="bibr" target="#b4">Brown et al., 1992)</ref>. We provide results using their full set of fea- tures (all), and using the same set of features in our anchor model (same). We compare tagging accuracy on a model with these features plus Brown clusters (+clusters) against a model that also incorporates the posteriors from the anchor method as an addi- tional feature in the MEMM (+clusters+posteriors).</p><p>The results in <ref type="table" target="#tab_4">Table 2</ref> show that using our model's posteriors are beneficial in the small labeled case, but not if the entire labeled data is used.</p><p>Runtime comparison. The training time of an- chor FHMM is 3.8h (hours), for self-training HMM 10.3h, for EM HMM 14.9h and for Twitter MEMM (all+clusters) 42h. As such, the anchor method is much more efficient than all the baselines because it requires a single pass over the corpus to collect the moment statistics, followed by the QPs, with- out the need to decode the unlabeled data. EM and the Brown clustering method (the latter used to ex- tract features for the Twitter MEMM) require several passes over the data; and the self-training method in- volves decoding the full unlabeled corpus, which is expensive when the corpus is large. Our analysis adds to previous evidence that spectral methods are more scalable than learning algorithms that require inference ( <ref type="bibr" target="#b22">Parikh et al., 2012;</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Malagasy POS Tagging</head><p>For the Malagasy experiment, we used the small la- beled dataset from <ref type="bibr" target="#b10">Garrette et al. (2013)</ref>, which con- sists of 176 sentences and 4,230 tokens. We also make use of their tag dictionaries with 2,773 types Models Accuracies supervised FHMM 90.5 EM FHMM 90.5 self-training FHMM 88.7 anchors FHMM (token), λ=1.0 89.4 anchors FHMM (type+token), λ=1.0 90.9 and 23 tags, and their unlabeled data (43.6K se- quences, 777K tokens). We converted all the orig- inal POS tags to universal tags using the mapping proposed in <ref type="bibr" target="#b10">Garrette et al. (2013)</ref>. <ref type="table" target="#tab_5">Table 3</ref> compares our method with semi- supervised EM and self-training, for the FHMM.We tested two supervision settings: token only, and type+token annotations, analogous to <ref type="bibr" target="#b10">Garrette et al. (2013)</ref>. The anchor method outperformed the base- lines when both type and token annotations were used to build the set of anchor words. <ref type="bibr">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We proposed an efficient semi-supervised sequence labeling method using a generative log-linear model. We use contextual information from a set of an- chor observations to disambiguate state, and build a weakly supervised method from this set. Our method outperforms other supervised and semi- supervised methods, with small supervision in POS- tagging for Malagasy, a scarcely annotated lan- guage, and for Twitter. Our anchor method is most competitive for learning with large amounts of un- labeled data, under weak supervision, while training an order of magnitude faster than any of the base- lines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>4</head><label></label><figDesc>Figure 2: POS tagging accuracy in the Twitter data versus the number of labeled training sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 : Tagging accuracy for the MEMM POS tagger of</head><label>2</label><figDesc></figDesc><table>Owoputi et al. (2013) with additional features from our 
model's posteriors. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 : Tagging accuracies for the Malagasy dataset.</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot">P (H=h)E[φ j (X)|H=h] E[φ j (X)] ,</note>

			<note place="foot" n="3"> As shown by Xiaojin Zhu (1999) and Yasemin Altun (2006), this regularization is equivalent, in the dual, to a &quot;soft&quot; constraint E θ h [φ(X) | H = h] − µ h 2 ≤ , as opposed to a strict equality.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Support for this research was provided by FundaçFundaç˜Fundação para a Ciência e Tecnologia (FCT) through the CMU Portugal Program under grant SFRH/BD/52015/2012. This work has also been partially supported by the European Union under H2020 project SUMMA, grant 688139, and by <ref type="bibr">7</ref> Note that the accuracies are not directly comparable to <ref type="bibr" target="#b10">Garrette et al. (2013)</ref>, who use a different tag set. However, our supervised baseline trained on those tags is already superior to the best semi-supervised system in <ref type="bibr" target="#b10">Garrette et al. (2013)</ref>, as we get 86.9% against the 81.2% reported in <ref type="bibr" target="#b10">Garrette et al. (2013)</ref> using their tagset.</p><p>FCT, through contracts UID/EEA/50008/2013, through the LearnBig project (PTDC/EEI-SII/7092/2014), and the GoLocal project (grant CMUPERI/TIC/0046/2014).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A practical algorithm for topic modeling with provable guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoni</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference of Machine Learning</title>
		<meeting>of International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised spectral learning of WCFG as low-rank matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphaël</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariadna</forename><surname>Luque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quattoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Empirical Methods in Natural Language Processing</title>
		<meeting>of Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="624" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spectral learning of general weighted automata via constrained matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borja</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2168" to="2176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Painless unsupervised learning with features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: Conference of the North American Association of Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Classbased n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer</forename><forename type="middle">C</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Estimating latent-variable graphical models using moments and likelihoods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Arun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Chaganty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Machine Learning</title>
		<meeting>of International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A provably correct learning algorithm for latent-variable PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Association for Computational Linguistics</title>
		<meeting>of Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Experiments with spectral learning of latent-variable PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><forename type="middle">P</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of North American Association of Computational Linguistics</title>
		<meeting>of North American Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paramveer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><forename type="middle">P</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><forename type="middle">H</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
		<title level="m">Eigenwords: Spectral word embeddings. Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="3035" to="3078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Real-world semi-supervised learning of POS-taggers 295 for low-resource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Mielens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Association for Computational Linguistics</title>
		<meeting>of Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Partof-speech tagging for twitter: Annotation, features, and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flanigan</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smith</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Association of Computational Linguistics</title>
		<meeting>of Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A spectral algorithm for learning hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1460" to="1480" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An introduction to latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Foltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename><surname>Laham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discourse Processes</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="259" to="284" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the limited memory bfgs method for large scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="503" to="528" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">2012. langid.py: An off-the-shelf language identification tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Association of Computational Linguistics System Demonstrations</title>
		<meeting>of Association of Computational Linguistics System Demonstrations</meeting>
		<imprint>
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tagging english text with a probabilistic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Merialdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="171" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Is your anchor going up or down? Fast and accurate supervised topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Seppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Ringger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of North American Association for Computational Linguistics</title>
		<meeting>of North American Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">TweetMotif: Exploratory search and topic summarization for Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI Conference on Weblogs and Social Media</title>
		<meeting>of AAAI Conference on Weblogs and Social Media</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Encoding prior knowledge with eigenword embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Cohen</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved part-of-speech tagging for online conversational text with word clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olutobi</forename><surname>Owoputi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of North American Association for Computational Linguistics</title>
		<meeting>of North American Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A spectral algorithm for latent junction trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariya</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabi</forename><surname>Ishteva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Teodoru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Uncertainty in Artificial Intelligence</title>
		<meeting>of Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>of International Conference on Language Resources and Evaluation (LREC)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spectral regularization for max-margin sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariadna</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borja</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference of Machine Learning</title>
		<meeting>of International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1710" to="1718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic word sense discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="123" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Contrastive estimation: Training log-linear models on unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Association for Computational Linguistics</title>
		<meeting>of Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="354" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spectral learning of refinement hmms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computational Natural Language Learning</title>
		<meeting>of Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised part-of-speech tagging with anchor hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="245" to="257" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Linguistic features for whole sentence maximum entropy language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roni</forename><surname>Rosenfeld Xiaojin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><forename type="middle">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Speech Communication and Technology</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unifying divergence minimization and statistical inference via convex duality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola Yasemin Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Conference on Learning Theory</title>
		<meeting>of Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Divide-and-conquer learning by anchoring a conical hull</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">A</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
