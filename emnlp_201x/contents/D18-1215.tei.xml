<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Probabilistic Logic: A Unifying Framework for Indirect Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
							<email>haiwang@ttic.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
								<address>
									<settlement>Chicago</settlement>
									<region>Illinois</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
							<email>hoifung@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Probabilistic Logic: A Unifying Framework for Indirect Supervision</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1891" to="1902"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1891</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Deep learning has emerged as a versatile tool for a wide range of NLP tasks, due to its superior capacity in representation learning. But its applicability is limited by the reliance on annotated examples, which are difficult to produce at scale. Indirect supervision has emerged as a promising direction to address this bottleneck, either by introducing labeling functions to automatically generate noisy examples from un-labeled text, or by imposing constraints over interdependent label decisions. A plethora of methods have been proposed, each with respective strengths and limitations. Probabilis-tic logic offers a unifying language to represent indirect supervision, but end-to-end mod-eling with probabilistic logic is often infea-sible due to intractable inference and learning. In this paper, we propose deep proba-bilistic logic (DPL) as a general framework for indirect supervision, by composing prob-abilistic logic with deep learning. DPL models label decisions as latent variables, represents prior knowledge on their relations using weighted first-order logical formulas, and alternates between learning a deep neural network for the end task and refining uncertain formula weights for indirect supervision, using variational EM. This framework subsumes prior indirect supervision methods as special cases, and enables novel combination via infusion of rich domain and linguistic knowledge. Experiments on biomedical machine reading demonstrate the promise of this approach.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning has proven successful in a wide range of NLP tasks ( <ref type="bibr" target="#b1">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b2">Bengio et al., 2003;</ref><ref type="bibr" target="#b5">Clark and Manning, 2016;</ref><ref type="bibr" target="#b11">Hermann et al., 2015;</ref><ref type="bibr" target="#b43">Sutskever et al., 2014</ref>). The versatility stems from its capacity of learning a compact rep- resentation of complex input patterns (Goodfellow et al., 2016). However, success of deep learning is bounded by its reliance on labeled examples, which are expensive and time-consuming to produce. In- direct supervision has emerged as a promising di- rection for breaching the annotation bottleneck. A powerful paradigm is joint inference <ref type="bibr" target="#b4">(Chang et al., 2007;</ref><ref type="bibr" target="#b33">Poon and Domingos, 2008;</ref><ref type="bibr" target="#b8">Druck et al., 2008;</ref><ref type="bibr" target="#b9">Ganchev et al., 2010)</ref>, which leverages linguistic and domain knowledge to impose con- straints over interdependent label decisions. More recently, another powerful paradigm, often loosely called weak supervision, has gained in popularity.</p><p>The key idea is to introduce labeling functions to automatically generate (noisy) training examples from unlabeled text. Distant supervision is a promi- nent example that used existing knowledge bases for this purpose <ref type="bibr" target="#b6">(Craven and Kumlien, 1999;</ref><ref type="bibr" target="#b26">Mintz et al., 2009)</ref>. Data programming went further by soliciting labeling functions from domain experts <ref type="bibr" target="#b39">(Ratner et al., 2016;</ref><ref type="bibr" target="#b0">Bach et al., 2017</ref>).</p><p>Indirect-supervision methods have achieved re- markable successes in a number of NLP tasks, but they also exhibit serious limitations. Distant su- pervision often produces incorrect labels, whereas labeling functions from data programming vary in</p><p>The deletion mutation on exon-19 of EGFR gene was present in 16 patients, while the L858E point mutation on exon-21 was noted in 10.</p><p>All patients were treated with gefitinib and showed a partial response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TREAT(Gefitinib, EGFR, L858E)</head><p>Figure 2: Example of cross-sentence relation extrac- tion for precision cancer treatment.</p><p>quality and coverage, and may contradict with each other on individual instances. Joint inference incurs greater modeling complexity and often requires specialized learning and inference procedures.</p><p>Since these methods draw on diverse and often orthogonal sources of indirect supervision, com- bining them may help address their limitations and amplify their strengths. Probabilistic logic offers an expressive language for such an integration, and is well suited for resolving noisy and contradictory in- formation ( <ref type="bibr" target="#b41">Richardson and Domingos, 2006</ref>). Un- fortunately, probabilistic logic generally incurs in- tractable learning and inference, often rendering end-to-end modeling infeasible.</p><p>In this paper, we propose deep probabilistic logic (DPL) as a unifying framework for indirect supervision ( <ref type="figure" target="#fig_0">Figure 1</ref>). Specifically, we made four contributions. First, we introduce a modular design to compose probabilistic logic with deep learning, with a supervision module that represents indirect supervision using probabilistic logic, and a predic- tion module that performs the end task using a deep neural network. Label decisions are modeled as latent variables and serve as the interface between the two modules.</p><p>Second, we show that all popular forms of indi- rect supervision can be represented in DPL by gen- eralizing virtual evidence <ref type="bibr" target="#b42">(Subramanya and Bilmes, 2007;</ref><ref type="bibr" target="#b30">Pearl, 2014)</ref>. Consequently, these diverse methods can be easily combined within a single framework for mutual amplification.</p><p>Third, we show that our problem formulation yields a well-defined learning objective (maximiz- ing conditional likelihood of virtual evidence). We proposed a modular learning approach by decom- posing the optimization over the supervision and prediction modules, using variational EM, which enables us to apply state-of-the-art methods for probabilistic logic and deep learning.</p><p>Finally, we applied DPL to biomedical machine reading <ref type="bibr" target="#b31">Peng et al., 2017)</ref>. Biomedicine offers a particularly attractive appli- cation domain for exploring indirect supervision. Biomedical literature grows by over one million each year 1 , making it imperative to develop ma- chine reading methods for automating knowledge curation ( <ref type="figure">Figure 2</ref>). While crowd sourcing is hardly applicable, there are rich domain knowledge and structured resources to exploit for indirect supervi- sion. Using cross-sentence relation extraction and entity linking as case studies, we show that distant supervision, data programming, and joint inference can be seamlessly combined in DPL to substan- tially improve machine reading accuracy, without requiring any manually labeled examples. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Distant supervision This paradigm was first in- troduced for binary relation extraction <ref type="bibr" target="#b6">(Craven and Kumlien, 1999;</ref><ref type="bibr" target="#b26">Mintz et al., 2009</ref>). In its simplest form, distant supervision generates a positive exam- ple if an entity pair with a known relation co-occurs in a sentence, and samples negative examples from co-occurring entity pairs not known to have the given relation. It has recently been extended to cross-sentence relation extraction <ref type="bibr" target="#b31">Peng et al., 2017)</ref>. In principle, one simply looks beyond single sentences for co-occurring en- tity pairs. However, this can introduce many false positives and prior work used a small sliding win- dow and filtering (minimal-span) to mitigate train- ing noise. Even so, accuracy is relatively low. Both  and <ref type="bibr" target="#b31">Peng et al. (2017)</ref> used ontology-based string matching for entity linking, which also incurs many false positives, as biomed- ical entities are highly ambiguous (e.g., PDF and AAAS are gene names). Distant supervision for entity linking is relatively underexplored, and prior work generally focuses on Freebase entities, where links to the corresponding Wikipedia articles are available for learning <ref type="bibr" target="#b15">(Huang et al., 2015)</ref>.</p><p>Data Programming Instead of annotated exam- ples, domain experts are asked to produce labeling functions, each of which assigns a label to an in- stance if the input satisfies certain conditions, of- ten specified by simple rules <ref type="bibr" target="#b39">(Ratner et al., 2016)</ref>. This paradigm is useful for semantic tasks, as high- precision text-based rules are often easy to come by. However, there is no guarantee on broad cover- age, and labeling functions are still noisy and may contradict with each other. The common denois- ing strategy assumes that labeling functions make random mistakes, and focuses on estimating their accuracy and correlation <ref type="bibr" target="#b39">(Ratner et al., 2016;</ref><ref type="bibr" target="#b0">Bach et al., 2017)</ref>. A more sophisticated strategy also models instance-level labels and uses instance em- bedding to estimate instance-level weight for each labeling function ( <ref type="bibr" target="#b22">Liu et al., 2017)</ref>.</p><p>Joint Inference Distant supervision and data pro- gramming focus on infusing weak supervision on individual labels. Additionally, there is rich linguis- tic and domain knowledge that does not specify values for individual labels, but imposes hard or soft constraints on their joint distribution. For ex- ample, if two mentions are coreferent, they should agree on entity properties <ref type="bibr" target="#b33">(Poon and Domingos, 2008)</ref>. There is a rich literature on joint infer- ence for NLP applications. Notable methodologies include constraint-driven learning ( <ref type="bibr" target="#b4">Chang et al., 2007)</ref>, general expectation ( <ref type="bibr" target="#b8">Druck et al., 2008)</ref>, posterior regularization ( <ref type="bibr" target="#b9">Ganchev et al., 2010)</ref>, and probabilistic logic <ref type="bibr" target="#b33">(Poon and Domingos, 2008)</ref>. Constraints can be imposed on relational instances or on model expectations. Learning and inference are often tailor-made for each approach, including beam search, primal-dual optimization, weighted satisfiability solvers, etc. Recently, joint inference has also been used in denoising distant supervi- sion. Instead of labeling all co-occurrences of an entity pair with a known relation as positive exam- ples, one only assumes that at least one instance is positive ( <ref type="bibr" target="#b12">Hoffmann et al., 2011;</ref><ref type="bibr" target="#b21">Lin et al., 2016</ref>).</p><p>Probabilistic Logic Probabilistic logic com- bines logic's expressive power with graphical model's capability in handling uncertainty. A repre- sentative example is Markov logic ( <ref type="bibr" target="#b41">Richardson and Domingos, 2006</ref>), which define a probability distri- bution using weighted first-order logical formulas as templates for a Markov model. Probabilistic logic has been applied to incorporating indirect su- pervision for various NLP tasks <ref type="bibr">Domingos, 2007, 2008;</ref><ref type="bibr" target="#b35">Poon and Vanderwende, 2010)</ref>, but its expressive power comes at a price: learning and inference are generally intractable, and end-to- end modeling often requires heavy approximation ( <ref type="bibr" target="#b19">Kimmig et al., 2012</ref>). In DPL, we limit the use of probabilistic logic to modeling indirect supervi- sion in the supervision module, leaving end-to-end modeling to deep neural network in the prediction module. This alleviates the computational chal- lenges in probabilistic logic, while leveraging the strength of deep learning in distilling complex pat- terns from high-dimension data.</p><p>Knowledge-Rich Deep Learning Infusing knowledge in neural network training is a long- standing challenge in deep learning <ref type="bibr" target="#b44">(Towell and Shavlik, 1994)</ref>. <ref type="bibr">Hu et al. (2016a,b)</ref> first used logical rules to help train a convolutional neural network for sentiment analysis. DPL draws inspiration from their approach, but is more general and theoretically well-founded. <ref type="bibr">Hu et al. (2016a,b)</ref> focused on supervised learning and the logical rules were introduced to augment labeled examples via posterior regularization ( <ref type="bibr" target="#b9">Ganchev et al., 2010)</ref>. DPL can incorporate both direct and indirect supervision, including posterior regularization and other forms of indirect supervision. Like DPL, <ref type="bibr" target="#b14">Hu et al. (2016b)</ref> also refined uncertain weights of logical rules, but they did it in a heuristic way by appealing to symmetry with standard posterior regularization. We provide a novel problem formulation using generalized virtual evidence, which shows that their heuristics is a special case of variational EM and opens up opportunities for other optimization strategies.</p><p>Deep generative models also combine deep learn- ing with probabilistic models, but focus on un- covering latent factors to support generative mod- eling and semi-supervised learning <ref type="bibr">(Kingma and Welling, 2013;</ref><ref type="bibr">Kingma et al., 2014</ref>). Knowledge infusion is limited to introducing structures among the latent variables (e.g., Markov chain) <ref type="bibr" target="#b16">(Johnson et al., 2016)</ref>. In DPL, we focus on learning a dis- criminative model for predicting the latent labels, using a probabilistic model defined by probabilistic logic to inject indirect supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deep Probabilistic Logic</head><p>In this section, we introduce deep probabilistic logic (DPL) as a unifying framework for indirect supervision. Label decisions are modeled as la- tent variables. Indirect supervision is represented as generalized virtual evidence, and learning maxi- mizes the conditional likelihood of virtual evidence given input. We first review the idea of virtual evi- dence and show how it can be generalized to rep- resent any form of indirect supervision. We then formulate the learning objective and show how it can be optimized using variational EM.</p><p>Given a prediction task, let X denote the set of possible inputs and Y the set of possible outputs. The goal is to train a prediction module Ψ(x, y) that scores output y given input x. Without loss of generality, we assume that Ψ(x, y) defines the conditional probability P (y|x) using a deep neural network with a softmax layer at the top. Let X = (X 1 , · · · , X N ) denote a sequence of inputs and Y = (Y 1 , · · · , Y N ) the corresponding outputs. We consider the setting where Y are unobserved, and Ψ(x, y) is learned using indirect supervision.</p><p>Virtual evidence Pearl (Pearl, 2014) first intro- duced the notion of virtual evidence, which has been used to incorporate label preference in semi- supervised learning <ref type="bibr" target="#b40">(Reynolds and Bilmes, 2005;</ref><ref type="bibr" target="#b42">Subramanya and Bilmes, 2007;</ref><ref type="bibr">Li, 2009)</ref> and grounded learning ( <ref type="bibr" target="#b29">Parikh et al., 2015)</ref>. Suppose we have a prior belief on the value of y, it can be represented by introducing a binary variable v as a dependent of y such that P (v = 1|y = l) is proportional to the prior belief of y = l. v = 1 is thus an observed evidence that imposes soft constraints over y. Direct supervision (i.e., ob- served label) for y is a special case when the belief is concentrated on a specific value y = l * (i.e., P (v = 1|y = l) = 0 for any l = l * ). The virtual evidence v can be viewed as a reified variable for a potential function Φ(y) ∝ P (v = 1|y). This en- ables us to generalize virtual evidence to arbitrary potential functions Φ(X, Y ) over the inputs and outputs. In the rest of the paper, we will simply refer to the potential functions as virtual evidences, without introducing the reified variables explicitly.</p><formula xml:id="formula_0">DPL Let K = (Φ 1 , · · · , Φ V )</formula><p>be a set of virtual evidence derived from prior knowledge. DPL com- prises of a supervision module over K and a predic- tion module over all input-output pairs <ref type="figure" target="#fig_0">(Figure 1</ref>), and defines a probability distribution:</p><formula xml:id="formula_1">P (K, Y |X) ∝ v Φ v (X, Y ) · i Ψ(X i , Y i )</formula><p>Without loss of generality, we assume that vir- tual evidences are log-linear factors, which can be compactly represented by weighted first-order log- ical formulas ( <ref type="bibr" target="#b41">Richardson and Domingos, 2006</ref>).</p><formula xml:id="formula_2">Namely, Φ v (X, Y ) = exp(w v · f v (X, Y )), where f v (X, Y )</formula><p>is a binary feature represented by a first- order logical formula. A hard constraint is the special case when w v = ∞ (in practice, it suffices to set it to a large number, e.g., 10). In prior use of virtual evidence, w v 's are generally pre-determined from prior knowledge. However, this may be sub- optimal. Therefore, we consider a general Bayesian learning setting where each w v is drawn from a pre- specified prior distribution w v ∼ P (w v |α v ). Fixed w v amounts to the special case when the prior is concentrated on the preset value. For uncertain w v 's, we can compute their maximum a posteriori (MAP) estimates and/or quantify the uncertainty.</p><p>Distant supervision Virtual evidence for distant supervision is similar to that for direct supervision. For example, for relation extraction, distant super- vision from a knowledge base of known relations will set</p><formula xml:id="formula_3">f KB (X i , Y i ) = I[In-KB(X i , r) ∧ Y i = r],</formula><p>where In-KB(X i , r) is true iff the entity tuple in X i is known to have relation r in the KB.</p><p>Data programming Virtual evidence for data programming is similar to that for distant super- vision:</p><formula xml:id="formula_4">f L (X i , Y i ) = I[L(X i ) = Y i ], where L(X i )</formula><p>is a labeling function provided by domain experts. Labeling functions are usually high-precision rules, but errors are still common, and different functions may assign conflicting labels to an instance. Exist- ing denoising strategy assumes that each function makes random errors independently, and resolves the conflicts by weighted votes <ref type="bibr" target="#b39">(Ratner et al., 2016)</ref>. In DPL, this can be done by simply treating error probabilities as uncertain parameters and inferring them during learning.</p><p>Joint inference Constraints on instances or model expectations can be imposed by introduc- ing the corresponding virtual evidence ( <ref type="bibr" target="#b9">Ganchev et al., 2010</ref>) (Proposition 2.1). The weights can be set heuristically ( <ref type="bibr" target="#b4">Chang et al., 2007;</ref><ref type="bibr" target="#b33">Poon and Domingos, 2008</ref>) or iteratively via primal-dual methods ( <ref type="bibr" target="#b9">Ganchev et al., 2010)</ref>. In addition to instance- level constraints, DPL can incorporate arbitrary high-order soft and hard constraints that cap- ture the interdependencies among multiple in- stances. For example, identical mentions in prox- imity probably refer to the same entity, which is useful for resolving ambiguous mentions by leveraging their unambiguous coreferences (e.g., an acronym in apposition of the full name). This can be represented by the virtual evidence</p><formula xml:id="formula_5">f Joint (X i , Y i , X j , Y j ) = I[Coref(X i , X j ) ∧ Y i = Y j ],</formula><p>where Coref(X i , X j ) is true iff X i and X j are coreferences. Similarly, the common de- noising strategy for distant supervision replaces the mention-level constraints with type-level con- straints ( <ref type="bibr" target="#b12">Hoffmann et al., 2011</ref>). Suppose that X E ⊂ X contains all X i 's with co-occurring entity tuple E. The new constraints simply impose that, for each E with known relation r ∈ KB, Y i = r for at least one X i ∈ X E . This can be represented by a high-order factor on (X i , Y i : X i ∈ X E ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 DPL Learning</head><p>Input:</p><formula xml:id="formula_6">Virtual evidences K = Φ 1:V , deep neu- ral network Ψ, inputs X = (X 1 , · · · , X N ), un- observed outputs Y = (Y 1 , · · · , Y N ). Output: Learned prediction module Ψ * Initialize: Φ 0 ∼ priors, Ψ 0 ∼ uniform. for t = 1 : T do q t (Y ) ← arg min q DKL( i qi(Yi) || v Φ t−1 v (X, Y ) · i Ψ t−1 (Xi, Yi)) Φ t ← arg min Φ DKL(q t (Y ) || v Φv(X, Y )) Ψ t ← arg min Ψ DKL(q t (Y ) || i Ψ(Xi, Yi)) end for return Ψ * = Ψ T .</formula><p>Parameter learning Learning in DPL maxi- mizes the conditional likelihood of virtual evi- dences P (K|X). We can directly optimize this objective by summing out latent Y to compute the gradient and run backpropagation. In this paper, however, we opted for a modular approach using variational EM. See Algorithm 1.</p><p>In the E-step, we compute a variational approx- imation q(Y ) = i q i (Y i ) by minimizing its KL divergence with P (Y |K, X), which amounts to computing marginal probabilities</p><formula xml:id="formula_7">q i (Y i ) = P (Y i |K, X) = Y −i P (Y i , Y −i |K, X)</formula><p>, with cur- rent parameters Φ, Ψ. This is a standard probabilis- tic inference problem. Exact inference is generally intractable, but there are a plethora of approximate inference methods that can efficiently produce an estimate. We use loopy belief propagation ( <ref type="bibr" target="#b28">Murphy et al., 1999</ref>) in this paper, by conducting message passing in P (K, Y |X) iteratively. Note that this inference problem is considerably simpler than end- to-end inference with probabilistic logic, since the bulk of the computation is encapsulated by Ψ.</p><p>Inference with high-order factors of large size can be challenging, but there is a rich body of lit- erature for handling such structured factors in a principled way. In particular, in distant supervision denoising, we alter the message passing schedule so that each at-least-one factor will compute mes- sages to its variables jointly by renormalizing their current marginal probabilities with noisy-or ( <ref type="bibr" target="#b17">Keith et al., 2017)</ref>, which is essentially a soft version of dual decomposition <ref type="bibr" target="#b3">(CarøE and Schultz, 1999</ref>).</p><p>In the M-step, we treat the variational approxi- mation q i (Y i ) as probabilistic labels, and use them to optimize Φ and Ψ via standard supervised learn- ing, which is equivalent to minimizing the KL 0.5 Relation in Toy KB (distant supervision) 3.2 No more than one "et al. " (data programming) 10 Relation holds for at least one instance (joint inference)</p><p>Patients with EGFR mutations show partial response to gefitinib. <ref type="bibr">Horn et al., 2001</ref>. Activities of gefitinib in NSCLC patients. J Clin Onco. <ref type="bibr">Zhang et al., 2006</ref>. Resistant mechanisms of EGFR mutations. J Thorac Onco.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;gefitinib, EGFR&gt;</head><p>Toy KB  divergence between the probabilistic labels and the conditional likelihood of Y given X under the supervision module (Φ) and prediction mod- ule (Ψ), respectively. For the prediction module, this optimization reduces to standard deep learn- ing. Likewise, for the supervision module, this optimization reduces to standard parameter learn- ing for log-linear models (i.e., learning all w v 's that are not fixed). Given the probabilistic labels, it is a convex optimization problem with a unique global optimum. Here, we simply use gradient descent, with the partial derivative for w v being</p><formula xml:id="formula_8">Y 1 Y 2 X 1 X 2 Y 1 Y 2 P(K,Y|X)  P(K, Y|X</formula><formula xml:id="formula_9">E Φ(Y,X) [f v (X, Y )] − E q(Y ) [f v (X, Y )].</formula><p>For a tied weight, the partial derivative will sum over all fea- tures that originate from the same template. The second expectation can be done by simple counting. The first expectation, on the other hand, requires probabilistic inference in the graphical model. But it can be computed using belief propagation, simi- lar to the E-step, except that the messages are lim- ited to factors within the supervision module (i.e., messages from Ψ are not longer included). Con- vergence is usually fast, upon which the marginal for each Y i is available, and</p><formula xml:id="formula_10">E Φ(Y,X) [f v (X, Y )]</formula><p>is simply the fraction of Y that renders f v (X, Y ) to be true. Again, this parameter learning prob- lem is much simpler than end-to-end learning with probabilistic logic, as it focuses on refining uncer- tain weights for indirect supervision, rather than learning complex input patterns for label prediction (handled in deep learning).</p><p>Example <ref type="figure" target="#fig_2">Figure 3</ref> shows a toy example on how DPL combines various indirect supervision for pre- dicting drug-gene interaction (e.g., gefitinib can be used to treat tumors with EGFR mutations). Indi- rect supervision is modeled by probabilistic logic, which defines a joint probability distribution over latent labeling decisions for drug-gene mention pairs in unlabeled text. Here, distant supervision prefers classifying mention pairs of known rela- tions, whereas the data programming formula op- poses classifying instances resembling citations, and the joint inference formula ensures that at least one mention pair of a known relation is classified as positive. Formula weight signifies the confidence in the indirect supervision, and can be refined itera- tively along with the prediction module.</p><p>Handling label imbalance One challenge for distant supervision is that negative examples are of- ten much more numerous. A common strategy is to subsample negative examples to attain a balanced dataset. In preliminary experiments, we found that this was often suboptimal, as many informative negative examples were excluded from training. Instead, we restored the balance by up-weighting positive examples. In DPL, an additional challenge is that the labels are probabilistic and change over iterations. In this paper, we simply used hard EM, with binary labels set using 0.5 as the probability threshold, and the up-weighting coefficient recal- culated after each E-step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Biomedical Machine Reading</head><p>There is a long-standing interest in biomedical ma- chine reading (e.g., <ref type="bibr" target="#b27">Morgan et al. (2008)</ref>; <ref type="bibr" target="#b18">Kim et al. (2009)</ref>), but prior studies focused on super- vised approaches. The advent of big biomedical data creates additional urgency for developing scal- able approaches that can generalize to new reading tasks. For example, genome sequencing cost has been dropping faster than Moore's Law, yet on- cologists can only evaluate tumor sequences for a tiny fraction of patients, due to the bottleneck in assimilating relevant knowledge from publications. Recently, <ref type="bibr" target="#b31">Peng et al. (2017)</ref> formulated precision oncology machine reading as cross-sentence rela- tion extraction <ref type="figure">(Figure 2</ref>) and developed the state- of-the-art system using distant supervision. While promising, their results still leave much room to improve. Moreover, they used heuristics to heavily filter entity candidates, with significant recall loss.</p><p>In this section, we use cross-sentence relation extraction as a case study for combining indirect supervision using deep probabilistic logic (DPL). First, we show that DPL can substantially improve machine reading accuracy in a head-to-head com- parison with <ref type="bibr" target="#b31">Peng et al. (2017)</ref>, using the same entity linking method. Next, we apply DPL to en- tity linking itself and attain similar improvement. Finally, we consider further improving the recall by removing the entity filter. By applying DPL to joint entity linking and relation extraction, we more than doubled the recall in relation extraction while attaining comparable precision as <ref type="bibr" target="#b31">Peng et al. (2017)</ref> with heavy entity filtering.</p><p>Evaluation Comparing indirect supervision methods is challenging as there is often no annotated test set for evaluating precision and recall. In such cases, we resort to the standard strategy used in prior work by reporting sample precision (estimated proportion of correct system extractions) and absolute recall (estimated number of correct system extractions). Absolute recall is proportional to recall and can be used to compare different systems (modulo estimation errors).</p><p>Datasets We used the same unlabeled text as <ref type="bibr" target="#b31">Peng et al. (2017)</ref>, which consists of about one mil- lion full text articles in PubMed Central (PMC) <ref type="bibr">3</ref> . Tokenization, part-of-speech tagging, and syntac- tic parsing were conducted using SPLAT <ref type="bibr" target="#b37">(Quirk et al., 2012)</ref>, and Stanford dependencies ( <ref type="bibr" target="#b25">de Marneffe et al., 2006</ref>) were obtained using Stanford CoreNLP ( <ref type="bibr" target="#b24">Manning et al., 2014</ref>). For entity ontolo- gies, we used DrugBank 4 and Human Gene Ontol- ogy (HUGO) <ref type="bibr">5</ref> . DrugBank contains 8257 drugs; we used the subset of 599 cancer drugs. HUGO con- tains 37661 genes. For knowledge bases, we used the Gene Drug Knowledge Database (GDKD) <ref type="bibr" target="#b7">(Dienstmann et al., 2015</ref>) and the Clinical Interpre- tations of Variants In Cancer (CIVIC) <ref type="bibr">6</ref> . Together, they contain 231 drug-gene-mutation triples, with 76 drugs, 35 genes and 123 mutations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Cross-sentence relation extraction</head><p>Let e 1 , · · · , e m be entity mentions in text T . Re- lation extraction can be formulated as classifying whether a relation R holds for e 1 , · · · , e m in T . To enable a head-to-head comparison, we used the same cross-sentence setting as <ref type="bibr" target="#b31">Peng et al. (2017)</ref>, where T spans up to three consecutive sentences and R represents the ternary interaction over drugs, genes, and mutations (whether the drug is relevant for treating tumors with the given gene mutation).</p><p>Entity linking In this subsection, we used the entity linker from Literome ( <ref type="bibr" target="#b34">Poon et al., 2014</ref>) to identify drug, gene, and mutation mentions, as in <ref type="bibr" target="#b31">Peng et al. (2017)</ref>. This entity linker first identi- fies candidate mentions by matching entity names Distant Supervision: GDKD, CIVIC Data Programming (Entity) Mention matches entity name exactly. Mention not a stop word. Mention not following figure designation. Mention's POS tags indicate it is a noun. Data Programming (Relation) Less than 30% of words are numbers in each sentence. No more than three consecutive numbers. No more than two "et al".</p><p>No more than three tokens start with uppercase. No more than three special characters. No more than three keywords indicative of table or figure. Entity mentions do not overlap. Joint Inference: Relation holds in at least one instance. <ref type="table">Table 1</ref>: DPL combines three indirect supervision strategies for cross-sentence relation extraction or synonyms in domain ontologies, then applies heuristics to filter candidates. The heuristics are designed to enhance precision, at the expense of recall. For example, one heuristics would filter can- didates of length less than four, which eliminates key cancer genes such as ER or AKT.</p><p>Prediction module We used the same graph LSTM as in <ref type="bibr" target="#b31">Peng et al. (2017)</ref> to enable head-to- head comparison on indirect supervision strategies. Briefly, a graph LSTM generalizes a linear-chain LSTM by incorporating arbitrary long-ranged de- pendencies, such as syntactic dependencies, dis- course relations, coreference, and connections be- tween roots of adjacent sentences. A word might have precedents other than the prior word, and its LSTM unit is expanded to include a forget gate for each precedent. See <ref type="bibr" target="#b31">Peng et al. (2017)</ref> for details.</p><p>Supervision module We used DPL to combine three indirect supervision strategies for cross- sentence relation extraction <ref type="table">(Table 1)</ref>. For distant supervision, we used GDKD and CIVIC as in <ref type="bibr" target="#b31">Peng et al. (2017)</ref>. For data programming, we intro- duced labeling functions that aim to correct entity and relation errors. Finally, we incorporated joint inference among all co-occurring instances of an entity tuple with the known relation by imposing the at-least-one constraint (i.e., the relation holds for at least one of the instances). For development, we sampled 250 positive extractions from DPL us- ing only distant supervision ( <ref type="bibr" target="#b31">Peng et al., 2017</ref>) and excluded them from future training and evaluation.</p><p>Experiment results We compared DPL with the state-of-the-art system of <ref type="bibr" target="#b31">Peng et al. (2017)</ref>. We also conducted ablation study to evaluate the im- pact of indirect-supervision strategies. For a fair comparison, we used the same probability thresh-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Prec Comparison of sample precision and abso- lute recall (all instances and unique entity tuples) in test extraction on PMC. DPL + EMB is our full system using PubMed-trained word embedding, whereas DPL uses the original Wikipedia-trained word embedding in <ref type="bibr" target="#b31">Peng et al. (2017)</ref>. Ablation: DS (distant supervision), DP (data programming), JI (joint inference  old in all cases (an instance is classified as positive if the normalized probability score is at least 0.5).</p><note type="other">. Abs. Rec. Unique Peng 2017 0.64 6768 2738 DPL + EMB 0.74 8478 4821 DPL 0.73 7666 4144 − DS 0.29 7555 4912 − DP 0.67 4826 2629 − DP (ENTITY) 0.70 7638 4074 − JI 0.72 7418 4011</note><p>For each system, sample precision was estimated by sampling 100 positive extractions and manually determining the proportion of correct extractions by an author knowledgeable about this domain. Ab- solute recall is estimated by multiplying sample precision with the number of positive extractions. <ref type="table" target="#tab_0">Table 2</ref> shows the results. DPL substantially out- performed <ref type="bibr" target="#b31">Peng et al. (2017)</ref>, improving sample precision by ten absolute points and raising abso- lute recall by 25%. Combining disparate indirect supervision strategies is key to this performance gain, as evident from the ablation results. While dis- tant supervision remained the most potent source of indirect supervision, data programming and joint inference each contributed significantly. Replacing out-of-domain (Wikipedia) word embedding with in-domain (PubMed) word embedding ( <ref type="bibr" target="#b36">Pyysalo et al., 2013</ref>) also led to a small gain. <ref type="bibr" target="#b31">Peng et al. (2017)</ref> only compared graph LSTM and linear-chain LSTM in automatic evaluation, where distant-supervision labels were treated as ground truth. They found significant but relatively small gains by graph LSTM. We conducted ad- ditional manual evaluation comparing the two in</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distant Supervision: HGNC Data Programming</head><p>No verbs in POS tags. Mention not a common word. Mention contains more than two characters or one word. More than 30% of characters are upper case. Mention contains both upper and lower case characters. Mention contains both character and digit. Mention contains more than six characters. Dependency label from mention to parent indicative of direct object. Joint Inference Identical mentions nearby probably refer to the same entity. Appositive mentions probably refer to the same entity. Nearby mentions that match synonyms of same entity prob- ably refer to the given entity. <ref type="table">Table 4</ref>: DPL combines three indirect supervision strategies for entity linking.</p><p>DPL. Surprisingly, we found rather large perfor- mance difference, with graph LSTM outperform- ing linear-chain LSTM by 13 absolute points in precision and raising absolute recall by over 20% <ref type="table" target="#tab_2">(Table 3</ref>). This suggests that <ref type="bibr" target="#b31">Peng et al. (2017)</ref> might have underestimated the performance gain by graph LSTM using automatic evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Entity linking</head><p>Let m be a mention in text and e be an entity in an ontology. The goal of entity linking is to predict Link(m, e), which is true iff m refers to e, for every candidate mention-entity pair m, e. We focus on genes in this paper, as they are particularly noisy.</p><p>Prediction module We used BiLSTM with at- tention over the ten-word windows before and after a mention. The embedding layer is initialized by word2vec embedding trained on PubMed abstracts and full text ( <ref type="bibr" target="#b36">Pyysalo et al., 2013</ref>). The word em- bedding dimension was 200. We used 5 epochs for training, with Adam as the optimizer. We set learning rate to 0.001, and batch size to 64.</p><p>Supervision module As in relation extraction, we combined three indirect supervision strategies using DPL <ref type="table">(Table 4)</ref>. For distant supervision, we obtained all mention-gene candidates by matching PMC text against the HUGO lexicon. We then sampled a subset of 200,000 candidate instances as positive examples. We sampled a similar number of noun phrases as negative examples. For data programming, we introduced labeling functions that used mention characteristics (longer names are less ambiguous) or syntactic context (genes are more likely to be direct objects and nouns). For joint inference, we leverage linguistic phenomena related to coreference (identical, appositive, or syn- onymous mentions nearby are likely coreferent).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Acc. F1 Prec. Rec. String Match 0.18 0.31 0.18 1.00 DS 0.64 0.71 0.62 0.83 DS + DP 0.66 0.71 0.62 0.83 DS + DP + JI 0.70 0.76 0.68 0.86  Experiment results For evaluation, we anno- tated a larger set of sample gene-mention candi- dates and then subsampled a balanced test set of 550 instances (half are true gene mentions, half not). These instances were excluded from train- ing and development. <ref type="table" target="#tab_3">Table 5</ref> compares system performance on this test set. The string-matching baseline has a very low precision, as gene mentions are highly ambiguous, which explains why <ref type="bibr" target="#b31">Peng et al. (2017)</ref> resorted to heavy filtering. By combin- ing indirect supervision strategies, DPL improved precision by over 50 absolute points, while retain- ing a reasonably high recall (86%). All indirect supervision strategies contributed significantly, as the ablation tests show. We also evaluated DPL on BioCreative II, a shared task on gene entity linking <ref type="bibr" target="#b27">(Morgan et al., 2008)</ref>. We compared DPL with GNormPlus ( <ref type="bibr" target="#b45">Wei et al., 2015)</ref>, the state-of-the-art supervised system trained on thousands of labeled examples in BioCreative II training set. Despite us- ing zero manually labeled examples, DPL attained comparable F1 and recall <ref type="table" target="#tab_4">(Table 6</ref>). The difference is mainly in precision, which indicates opportuni- ties for more indirect supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Joint entity and relation extraction</head><p>An important use case for machine reading is to improve knowledge curation efficiency by offering extraction results as candidates for curators to vet.</p><p>The key to practical adoption is attaining high recall with reasonable precision ( <ref type="bibr" target="#b31">Peng et al., 2017</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion</head><p>Scalability DPL is efficient to train, taking around 3.5 hours for relation extraction and 2.5 hours for entity linking in our PubMed-scale ex- periments, with 25 CPU cores (for probabilistic logic) and one GPU (for LSTM). For relation ex- traction, the graphical model of probabilistic logic contains around 7,000 variables and 70,000 factors. At test time, it is just an LSTM, which predicted each instance in less than a second. In general, DPL learning scales linearly in the number of training instances. For distant supervision and data pro- gramming, DPL scales linearly in the number of known facts and labeling functions. As discussed in Section 3, joint inference with high-order factors is more challenging, but can be efficiently approx- imated. For inference in probabilistic logic, we found that loopy belief propagation worked reason- ably well, converging after 2-4 iterations. Overall, we ran variational EM for three iterations, using ten epochs of deep learning in each M-step. We found Janjigian YY, Groen HJ, Horn L, Smit EF, Fu Y, <ref type="bibr">Wang F et al. (2011)</ref> Activity and tolerability of afatinib (BIBW 2992) and cetuximab in NSCLC patients with acquired resistance to erlotinib or gefitinib. J Clin Oncol 29 ( suppl ): abstr 7525 14. Fujita Y Suda K Kimura H Matsumoto K Arao T Nagai T Highly sensitive detection of EGFR T790M mutation using colony hybridization predicts favorable prognosis of patients with lung cancer harboring activating EGFR mutation  these worked well in preliminary experiments and used the same setting in all final experiments.</p><p>Accuracy To understand more about DPL's per- formance gain over distant supervision, we manu- ally inspected some relation-extraction errors fixed by DPL after training with additional indirect super- vision. <ref type="figure" target="#fig_4">Figure 4</ref> shows two such examples. While some data programming functions were introduced to prevent errors stemming from citations or flat- tened tables, none were directly applicable to these examples. This shows that DPL can generalize beyond the original indirect supervision. While the results are promising, there is still much to improve. <ref type="table" target="#tab_7">Table 8</ref> shows estimated preci- sion errors for relation extraction by DPL. (Some instances have multiple errors.) Entity linking can incorporate more indirect supervision. Joint entity linking and relation extraction can be improved by feeding back extraction results to linking. Improve- ment is also sorely needed in classifying mutations and gene-mutation associations. The prediction module can also be improved, e.g., by adding atten- tion to graph LSTM. DPL offers a flexible frame- work for exploring all these directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduce DPL as a unifying framework for indi- rect supervision, by composing probabilistic logic with deep learning. Experiments on biomedical machine reading show that this enables novel com- bination of disparate indirect supervision method- ologies, resulting in substantial gain in accuracy. Future directions include: combining DPL with deep generative models; exploring alternative opti- mization strategies; applications to other domains. <ref type="bibr">Diederik</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Deep Probabilistic Logic: A general framework for combining indirect supervision strategies by composing probabilistic logic with deep learning. Learning amounts to maximizing conditional likelihood of virtual evidence given input by summing up latent label decisions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>F</head><label></label><figDesc>F exp(0.50+3.22+100) = exp(6.4) 0 K By combining distant supervision, data programming, and joint inference, DPL derives more accurate indirect supervision by inferring that the drug-gene relation likely holds in X 1 but not in X 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of DPL combining various indirect supervision using probabilistic logic. The prediction module is omitted to avoid clutter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>J Thorac Oncol 2012 E19 deletion ALK Solid Signet ring cells Intracytoplasmic No -Crizotinib - AWDa e 12 F/66 Never Adrenal/B M1 ( IV ) E20 R803W ALK Solid No No No +d Erlotinib PD AWDa 0.7 EGFR, epidermal growth factor receptor; PFS, progression-free survival; M , male; PY, pack-year; R, resection; E , exon; KRAS, v-Ki-ras2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example of relation-extraction errors corrected by DPL with additional indirect supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison of sample precision and abso-
lute recall (all instances and unique entity tuples) in 
test extraction on PMC. Both use same indirect su-
pervision and Wikipedia-trained word embedding. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison of gene entity linking results 
on a balanced test set. The string-matching base-
line has low precision. By combining indirect su-
pervision strategies, DPL substantially improved 
precision while retaining reasonably high recall. 

F1 Precision Recall 
GNormPlus 0.78 
0.74 
0.81 
DPL 
0.74 
0.68 
0.80 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Comparison of gene entity linking results 
on BioCreative II test set. GNormPlus is the state-
of-the-art system trained on thousands of labeled 
examples. DPL used only indirect supervision. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Comparison of sample precision and abso-
lute recall (all instances and unique entity tuples) 
when all gene mention candidates are considered. 
Peng et al. (2017) used distant supervision only. 
RE: DPL relation extraction. EL: using DPL entity 
linking in RE training (TRN) and/or test (TST). 

Gene Drug Mut. Gene-Mut. Relation 
27% 
4% 20% 
45% 
24% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Error analysis for DPL relation extraction. 

subsection, we consider replacing the entity filter 
by the DPL entity linker Table 7. Specifically, we 
added one labeling function to check if the entity 
linker returns a normalized probability score above 
p TRN for gene mentions, and filtered test instances if 
the gene mention score is lower than p TST . We set 
p TRN = 0.6 and p TST = 0.3 from preliminary exper-
iments. The labeling function discouraged learning 
from noisy mentions, and the test-time filter skips 
an instance if the gene is likely wrong. Not sur-
prisingly, without entity filtering, Peng et al. (2017) 
suffered large precision loss. All DPL versions 
substantially improved accuracy, with significantly 
more gains using the DPL entity linker. 

</table></figure>

			<note place="foot" n="1"> http://ncbi.nlm.nih.gov/pubmed 2 The DPL code and datasets will be made available at http://hanover.azurewebsites.net.</note>

			<note place="foot" n="3"> www.ncbi.nlm.nih.gov/pmc 4 www.drugbank.ca 5 www.genenames.org 6 civic.genome.wustl.edu</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>We thank David McAllester, Chris Quirk, and Scott Yih for useful discussions, and the three anony-mous reviewers for helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning the structure of generative models without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="273" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dual decomposition in stochastic integer programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Claus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rüdiger</forename><surname>Carøe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="37" to="45" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Guiding semi-supervision with constraint-driven learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the association of computational linguistics</title>
		<meeting>the 45th annual meeting of the association of computational linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving coreference resolution by learning entitylevel distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="643" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Constructing biological knowledge bases by extracting information from text sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Kumlien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology</title>
		<meeting>the Seventh International Conference on Intelligent Systems for Molecular Biology</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Database of genomic biomarkers for cancer drugs and clinical targetability in solid tumors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Dienstmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">Sock</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Bot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Friend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Guinney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Discovery</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning from labeled features using generalized expectation criteria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Druck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 31st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="595" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Posterior regularization for structured latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2001" to="2049" />
			<date type="published" when="2010-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Deep learning</title>
		<imprint>
			<publisher>MIT press Cambridge</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Forty-Ninth Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Forty-Ninth Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Harnessing deep neural networks with logic rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Association for Computational Linguistics</title>
		<meeting>the 2016 Conference on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep neural networks with massive learned knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1670" to="1679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.07678</idno>
		<title level="m">Leveraging deep neural networks and knowledge graphs for entity disambiguation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Composing graphical models with neural networks for structured representations and fast inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiltschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sandeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Datta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2946" to="2954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Identifying civilians killed by police with distantly supervised entity-event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abram</forename><surname>Handler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pinkham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cara</forename><surname>Magliozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Mcduffie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan O&amp;apos;</forename><surname>Connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1547" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Overview of bionlp&apos;09 shared task on event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshinobu</forename><surname>Kano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing: Shared Task</title>
		<meeting>the Workshop on Current Trends in Biomedical Natural Language Processing: Shared Task</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A short introduction to probabilistic soft logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelika</forename><surname>Kimmig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Broecheler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS Workshop on Probabilistic Programming: Foundations and Applications</title>
		<meeting>the NIPS Workshop on Probabilistic Programming: Foundations and Applications</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1289" to="1297" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Heterogeneous supervision for relation extraction: A representation learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="46" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generalized expectation criteria for semi-supervised learning of conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gideon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="870" to="878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifty-Second Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the Fifty-Second Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Conference on Language Resources and Evaluation</title>
		<meeting>the Fifth International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the Forty-Seventh Annual Meeting of the Association for Computational Linguistics and the Fourth International Joint Conference on Natural Language Processing</title>
		<meeting>the Joint Conference of the Forty-Seventh Annual Meeting of the Association for Computational Linguistics and the Fourth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Overview of biocreative ii gene normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Alexander A Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinglong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliane</forename><surname>Aaron M Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Fluck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Ruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Divoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Fundel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hakenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome biology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Loopy belief propagation for approximate inference: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Fifteenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="467" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Grounded semantic parsing for complex knowledge extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="756" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Probabilistic reasoning in intelligent systems: networks of plausible inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cross-sentence nary relation extraction with graph lstms. Transactions of the Association for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Wen Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joint inference in information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="913" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Joint unsupervised coreference resolution with markov logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="650" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Literome: PubMed-scale genomic knowledge base in the cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlie</forename><surname>Deziel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Heckerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="page">30</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint inference for knowledge extraction from biomedical literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="813" to="821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Distributional semantics resources for biomedical text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Moen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salakoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LBM 2013</title>
		<meeting>LBM 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="39" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">MSR SPLAT, a language analysis toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisami</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics:Demonstration Session</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics:Demonstration Session</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction beyond the sentence boundary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on European chapter</title>
		<meeting>the Fifteenth Conference on European chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Data programming: Creating large training sets, quickly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher M De</forename><surname>Alexander J Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Selsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3567" to="3575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Part-ofspeech tagging using virtual evidence and negative training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sheila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="459" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Markov logic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="107" to="136" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Virtual evidence for training speech recognizers using partially labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="165" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Knowledge-based artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jude</forename><forename type="middle">W</forename><surname>Towell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="119" to="165" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Gnormplus: an integrative approach for tagging genes, gene families, and protein domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Hsuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>BioMed research international</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
