<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Character-Level Question Answering with Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Golub</surname></persName>
							<email>golubd@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Character-Level Question Answering with Attention</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1598" to="1607"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We show that a character-level encoder-decoder framework can be successfully applied to question answering with a structured knowledge base. We use our model for single-relation question answering and demonstrate the effectiveness of our approach on the Sim-pleQuestions dataset (Bordes et al., 2015), where we improve state-of-the-art accuracy from 63.9% to 70.9%, without use of ensembles. Importantly, our character-level model has 16x fewer parameters than an equivalent word-level model, can be learned with significantly less data compared to previous work, which relies on data augmentation, and is robust to new entities in testing. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Single-relation factoid questions are the most com- mon form of questions found in search query logs and community question answering websites <ref type="bibr">(Yih et al., 2014;</ref><ref type="bibr" target="#b2">Fader et al., 2013)</ref>. A knowledge-base (KB) such as Freebase, DBpedia, or Wikidata can help answer such questions after users reformulate them as queries. For instance, the question "Where was Barack Obama born?" can be answered by is- suing the following KB query:</p><p>λ(x).place of birth <ref type="bibr">(Barack Obama, x)</ref> However, automatically mapping a natural language question such as "Where was Barack Obama born?" to its corresponding KB query remains a challeng- ing task.</p><p>There are three key issues that make learning this mapping non-trivial. First, there are many para- phrases of the same question. Second, many of the KB entries are unseen during training time; however, we still need to correctly predict them at test time. Third, a KB such as Freebase typically contains mil- lions of entities and thousands of predicates, mak- ing it difficult for a system to predict these entities at scale ( <ref type="bibr">Yih et al., 2014;</ref><ref type="bibr" target="#b2">Fader et al., 2014;</ref><ref type="bibr" target="#b2">Bordes et al., 2015)</ref>. In this paper, we address all three of these issues with a character-level encoder-decoder frame- work that significantly improves performance over state-of-the-art word-level neural models, while also providing a much more compact model that can be learned from less data.</p><p>First, we use a long short-term memory (LSTM) <ref type="bibr">(Hochreiter and Schmidhuber, 1997</ref>) encoder to em- bed the question. Second, to make our model ro- bust to unseen KB entries, we extract embeddings for questions, predicates and entities purely from their character-level representations. Character- level modeling has been previously shown to gen- eralize well to new words not seen during training <ref type="bibr">(Ljubeši´Ljubeši´c et al., 2014;</ref><ref type="bibr" target="#b2">Chung et al., 2016)</ref>, which makes it ideal for this task. Third, to scale our model to handle the millions of entities and thousands of predicates in the KB, instead of using a large out- put layer in the decoder to directly predict the entity and predicate, we use a general interaction function between the question embeddings and KB embed- dings that measures their semantic relevance to de- termine the output. The combined use of character-  <ref type="figure">Figure 1</ref>: Our encoder-decoder architecture that generates a query against a structured knowledge base. We encode our question via a long short-term memory (LSTM) network and an attention mechanism to produce our context vector. During decoding, at each time step, we feed the current context vector and an embedding of the English alias of the previously generated knowledge base entry into an attention-based decoding LSTM to generate the new candidate entity or predicate.</p><p>level modeling and a semantic relevance function allows us to successfully produce likelihood scores for the KB entries that are not present in our vo- cabulary, a challenging task for standard encoder- decoder frameworks. Our novel, character-level encoder-decoder model is compact, requires significantly less data to train than previous work, and is able to generalize well to unseen entities in test time. In particular, without use of ensembles, we achieve 70.9% accu- racy in the Freebase2M setting and 70.3% accuracy in the Freebase5M setting on the SimpleQuestions dataset, outperforming the previous state-of-arts of 62.7% and 63.9% ( <ref type="bibr" target="#b2">Bordes et al., 2015</ref>) by 8.2% and 6.4% respectively. Moreover, we only use the training questions provided in SimpleQuestions to train our model, which cover about 24% of words in entity aliases on the test set. This demonstrates the robustness of the character-level model to unseen entities. In contrast, data augmentation is usually necessary to provide more coverage for unseen entities and predicates, as done in previous work <ref type="bibr" target="#b2">(Bordes et al., 2015;</ref><ref type="bibr">Yih et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work is motivated by three major threads of research in machine learning and natural lan- guage processing: semantic-parsing for open- domain question answering, character-level lan- guage modeling, and encoder-decoder methods.</p><p>Semantic parsing for open-domain question an- swering, which translates a question into a struc- tured KB query, is a key component in question an- swering with a KB. While early approaches relied on building high-quality lexicons for domain-specific databases such as GeoQuery ( <ref type="bibr" target="#b3">Tang and Mooney, 2001)</ref>, recent work has focused on building seman- tic parsing frameworks for general knowledge bases such as Freebase ( <ref type="bibr">Yih et al., 2014;</ref><ref type="bibr" target="#b2">Bordes et al., 2014a;</ref><ref type="bibr" target="#b2">Bordes et al., 2015;</ref><ref type="bibr">Berant and Liang, 2014;</ref><ref type="bibr" target="#b2">Fader et al., 2013;</ref><ref type="bibr" target="#b2">Dai et al., 2016)</ref>.</p><p>Semantic parsing frameworks for large-scale knowledge bases have to be able to successfully gen- erate queries for the millions of entities and thou- sands of predicates in the KB, many of which are unseen during training. To address this issue, recent work relies on producing embeddings for predicates and entities in a KB based on their textual descrip- tions ( <ref type="bibr" target="#b2">Bordes et al., 2014a;</ref><ref type="bibr" target="#b2">Bordes et al., 2015;</ref><ref type="bibr">Yih et al., 2014;</ref><ref type="bibr">Yih et al., 2015;</ref><ref type="bibr">Bishan Yang, 2015)</ref>. A general interaction function can then be used to measure the semantic relevance of these embedded KB entries to the question and determine the most likely KB query.</p><p>Most of these approaches use word-level embed- dings to encode entities and predicates, and there- fore might suffer from the out-of-vocabulary (OOV) problem when they encounter unseen words during test time. Consequently, they often rely on signif- icant data augmentation from sources such as <ref type="bibr">Paralex (Fader et al., 2013)</ref>, which contains 18 million question-paraphrase pairs scraped from WikiAn- swers, to have sufficient examples for each word they encounter ( <ref type="bibr" target="#b2">Bordes et al., 2014b;</ref><ref type="bibr">Yih et al., 2014;</ref><ref type="bibr" target="#b2">Bordes et al., 2015)</ref>.</p><p>As opposed to word-level modeling, character- level modeling can be used to handle the OOV is- sue. While character-level modeling has not been applied to factoid question answering before, it has been successfully applied to information retrieval, machine translation, sentiment analysis, classifica- tion, and named entity recognition ( <ref type="bibr">Huang et al., 2013;</ref><ref type="bibr" target="#b2">Shen et al., 2014;</ref><ref type="bibr" target="#b2">Chung et al., 2016;</ref><ref type="bibr" target="#b3">Zhang et al., 2015;</ref><ref type="bibr" target="#b2">Santos and Zadrozny, 2014;</ref><ref type="bibr" target="#b2">dos Santos and Gatti, 2014;</ref><ref type="bibr">Klein et al., 2003;</ref><ref type="bibr" target="#b2">dos Santos, 2014;</ref><ref type="bibr" target="#b2">dos Santos et al., 2015</ref>). Moreover, <ref type="bibr" target="#b2">Chung et al. (2015)</ref> demonstrate that gated-feedback LSTMs on top of character-level embeddings can capture long- term dependencies in language modeling.</p><p>Lastly, encoder-decoder networks have been ap- plied to many structured machine learning tasks. First introduced in <ref type="bibr" target="#b3">Sutskever et al. (2014)</ref>, in an encoder-decoder network, a source sequence is first encoded with a recurrent neural network (RNN) into a fixed-length vector which intuitively captures its "meaning", and then decoded into a desired tar- get sequence. This approach and related memory- based or attention-based approaches have been suc- cessfully applied in diverse domains such as speech recognition, machine translation, image captioning, parsing, executing programs, and conversational di- alogues ( <ref type="bibr" target="#b0">Amodei et al., 2015;</ref><ref type="bibr" target="#b3">Venugopalan et al., 2015;</ref><ref type="bibr">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b3">Vinyals et al., 2015;</ref><ref type="bibr">Zaremba and Sutskever, 2014;</ref><ref type="bibr" target="#b3">Xu et al., 2015;</ref><ref type="bibr">Sukhbaatar et al., 2015)</ref>.</p><p>Unlike previous work, we formulate question an- swering as a problem of decoding the KB query given the question and KB entries which are en- coded in embedding spaces. We therefore inte- grate the learning of question and KB embeddings in a unified encoder-decoder framework, where the whole system is optimized end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>Since we focus on single-relation question answer- ing in this work, our model decodes every ques- tion into a KB query that consists of exactly two elements-the topic entity, and the predicate. More formally, our model is a function f (q, {e}, {p}) that takes as input a question q, a set of candidate enti- ties {e} = e 1 , ..., e n , a set of candidate predicates {p} = p 1 , ..., p m , and produces a likelihood score p(e i , p j |q) of generating entity e i and predicate p j given question q for all i ∈ 1...n, j ∈ 1...m.</p><p>As illustrated in <ref type="figure">Figure 1</ref>, our model consists of three components:</p><p>1. A character-level LSTM-based encoder for the question which produces a sequence of embed- ding vectors, one for each character ( <ref type="figure">Figure  1a</ref>).</p><p>2. A character-level convolutional neural net- work (CNN)-based encoder for the predi- cates/entities in a knowledge base which pro- duces a single embedding vector for each pred- icate or entity <ref type="figure">(Figure 1c</ref>).</p><p>3. An LSTM-based decoder with an attention mechanism and a relevance function for gener- ating the topic entity and predicate to form the KB query ( <ref type="figure">Figure 1b</ref>).</p><p>The details of each component are described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoding the Question</head><p>To encode the question, we take two steps:</p><p>1. We first extract one-hot encoding vectors for characters in the question, x 1 , ..., x n , where x i represents the one-hot encoding vector for the i th character in the question. We keep the space, punctuation and original cases without tokenization.</p><p>2. We feed x 1 , ..., x n from left to right into a two- layer gated-feedback LSTM, and keep the out- puts at all time steps as the embeddings for the question, i.e., these are the vectors s 1 , ..., s n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoding Entities and Predicates in the KB</head><p>To encode an entity or predicate in the KB, we take two steps:</p><p>1. We first extract one-hot encoding vectors for characters in its English alias, x 1 , ..., x n , where x i represents the one-hot encoding vector for the i th character in the alias.</p><p>2. We then feed x 1 , ..., x n into a temporal CNN with two alternating convolutional and fully- connected layers, followed by one fully- connected layer:</p><formula xml:id="formula_0">f (x 1 , ..., x n ) = tanh(W 3 × max(tanh(W 2 × conv(tanh(W 1 × conv(x 1 , ..., x n ))))))</formula><p>where f (x 1...n ) is an embedding vector of size N , W 3 has size R N ×h , conv represents a tem- poral convolutional neural network, and max represents a max pooling layer in the temporal direction.</p><p>We use a CNN as opposed to an LSTM to embed KB entries primarily for computational efficiency. Also, we use two different CNNs to encode enti- ties and predicates because they typically have sig- nificantly different styles (e.g., "Barack Obama" vs. "/people/person/place of birth").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoding the KB Query</head><p>To generate the single topic entity and predicate to form the KB query, we use a decoder with two key components:</p><p>1. An LSTM-based decoder with attention. Its hidden states at each time step i, h i , have the same dimensionality N as the embeddings of entities/predicates. The initial hidden state h 0 is set to the zero vector: 0.</p><p>2. A pairwise semantic relevance function that measures the similarity between the hidden units of the LSTM and the embedding of an en- tity or predicate candidate. It then returns the mostly likely entity or predicate based on the similarity score.</p><p>In the following two sections, we will first de- scribe the LSTM decoder with attention, followed by the semantic relevance function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">LSTM-based Decoder with Attention</head><p>The attention-based LSTM decoder uses a similar architecture as the one described in <ref type="bibr">Bahdanau et al. (2015)</ref>. At each time step i, we feed in a context vector c i and an input vector v i into the LSTM. At time i = 1 we feed a special input vector v &lt;S&gt; = 0 into the LSTM. At time i = 2, during training, the input vector is the embedding of the true entity, while during testing, it is the embedding of the most likely entity as determined at the previous time step.</p><p>We</p><note type="other">now describe how we produce the context vector c i . Let h i−1 be the hidden state of the LSTM at time i−1, s j be the j th question character embed- ding, n be the number of characters in the question, r be the size of s j , and m be a hyperparameter. Then the context vector c i , which represents the attention- weighted content of the question, is recomputed at each time step i as follows:</note><formula xml:id="formula_1">c i = n j=1 α ij s j , α ij = exp (e ij ) Tx k=1 exp (e ik ) e ij =v a tanh (W a h i−1 + U a s j ) ,</formula><p>where {α} is the attention distribution that is ap- plied over each hidden unit s j , W a ∈ R m×N , U a ∈ R m×r , and v a ∈ R 1×m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Semantic Relevance Function</head><p>Unlike machine translation and language model- ing where the vocabulary is relatively small, there are millions of entries in the KB. If we try to di- rectly predict the KB entries, the decoder will need an output layer with millions of nodes, which is computationally prohibitive. Therefore, we resort to a relevance function that measures the semantic similarity between the decoder's hidden state and the embeddings of KB entries. Our semantic rel- evance function takes two vectors x 1 , x 2 and re- turns a distance measure of how similar they are to each other. In current experiments we use a simple cosine-similarity metric: cos(x 1 , x 2 ).</p><p>Using this similarity metric, the likelihoods of generating entity e j and predicate p k are: </p><formula xml:id="formula_2">P (e j ) = exp(λcos(h 1 , e j )) n i=1 exp(λcos(h 1 , e i )) P (p k ) = exp(λcos(h 2 , p k )) m i=1 exp(λcos(h 2 , p i ))</formula><p>where λ is a constant, h 1 , h 2 are the hidden states of the LSTM at times t = 1 and t = 2, e 1 , ..., e n are the entity embeddings, and p 1 , ..., p m are the predicate embeddings. A similar likelihood function was used to train the semantic similarity modules proposed in During inference, e 1 , ..., e n and p 1 , ..., p m are the embeddings of candidate entities and predicates. During training e 1 , ..., e n , p 1 , ..., p m are the embed- dings of the true entity and 50 randomly-sampled entities, and the true predicate and 50 randomly- sampled predicates, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inference</head><p>For each question q, we generate a candidate set of entities and predicates, {e} and {p}, and feed it through the model f (q, {e}, {p}). We then decode the most likely (entity, predicate) pair:</p><formula xml:id="formula_3">(e * , p * ) = argmax e i ,p j (P (e i ) * P (p j ))</formula><p>which becomes our semantic parse.</p><p>We use a similar procedure as the one described in <ref type="bibr" target="#b2">Bordes et al. (2015)</ref> to generate candidate entities {e} and predicates {p}. Namely, we take all entities whose English alias is a substring of the question, and remove all entities whose alias is a substring of another entity. For each English alias, we sort each entity with this alias by the number of facts that it has in the KB, and append the top 10 entities from this list to our set of candidate entities. All predi- cates p j for each entity in our candidate entity set become the set of candidate predicates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Learning</head><p>Our goal in learning is to maximize the joint likeli- hood P (e c )·P (p c ) of predicting the correct entity e c and predicate p c pair from a set of randomly sampled entities and predicates. We use back-propagation to learn all of the weights in our model.</p><p>All the parameters of our model are learned jointly without pre-training. These parameters in- clude the weights of the character-level embeddings, CNNs, and LSTMs. Weights are randomly initial- ized before training. For the i th layer in our network, each weight is sampled from a uniform distribution between − 1</p><formula xml:id="formula_4">|l i | and 1 |l i |</formula><p>, where |l i | is the number of weights in layer i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset and Experimental Settings</head><p>We evaluate the proposed model on the SimpleQues- tions dataset ( <ref type="bibr" target="#b2">Bordes et al., 2015</ref>). The dataset con- sists of 108,442 single-relation questions and their corresponding (topic entity, predicate, answer en- tity) triples from Freebase. It is split into 75,910 train, 10,845 validation, and 21,687 test questions. Only 10,843 of the 45,335 unique words in entity aliases and 886 out of 1,034 unique predicates in the test set were present in the train set. For the proposed dataset, there are two evaluation settings, called FB2M and FB5M, respectively. The former uses a KB for candidate generation which is a sub-set of Freebase and contains 2M entities, while the latter uses subset of Freebase with 5M entities.</p><p>In our experiments, the Memory Neural Networks (MemNNs) proposed in <ref type="bibr" target="#b2">Bordes et al. (2015)</ref> serve as the baselines. For training, in addition to the 76K questions in the training set, the MemNNs use 3K training questions from WebQuestions ( <ref type="bibr" target="#b1">Berant et al., 2013</ref>), 15M paraphrases from <ref type="bibr">WikiAnswers (Fader et al., 2013)</ref>, and 11M and 12M automatically gener- ated questions from the KB for the FB2M and FB5M settings, respectively. In contrast, our models are trained only on the 76K questions in the training set.</p><p>For our model, both layers of the LSTM-based question encoder have size 200. The hidden layers of the LSTM-based decoder have size 100, and the CNNs for entity and predicate embeddings have a hidden layer of size 200 and an output layer of size 100. The CNNs for entity and predicate embeddings use a receptive field of size 4, λ = 5, and m = 100. We train the models using RMSProp with a learning rate of 1e −4 .</p><p>In order to make the input character sequence long enough to fill up the receptive fields of mul- tiple CNN layers, we pad each predicate or entity using three padding symbols P , a special start sym- bol, and a special end symbol. For instance, Obama would become S start P P P ObamaP P P S end . For consistency, we apply the same padding to the ques- tions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">End-to-end Results on SimpleQuestions</head><p>Following <ref type="bibr" target="#b2">Bordes et al. (2015)</ref>, we report results on the SimpleQuestions dataset in terms of SQ ac- curacy, for both FB2M and FB5M settings in Ta- ble 1. SQ accuracy is defined as the percentage of questions for which the model generates a cor- rect KB query (i.e., both the topic entity and predi- cate are correct). Our single character-level model achieves SQ accuracies of 70.9% and 70.3% on the FB2M and FB5M settings, outperforming the previous state-of-art results by 8.2% and 6.4%, re- spectively. Compared to the character-level model, which only has 1.2M parameters, our word-level model has 19.9M parameters, and only achieves a best SQ accuracy of 53.9%. In addition, in contrast to previous work, the OOV issue is much more se- vere for our word-level model, since we use no data augmentation to cover entities unseen in the train set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation and Embedding Experiments</head><p>We carry out ablation studies in Sections 5.2.1 and 5.2.2 through a set of random-sampling experi- ments. In these experiments, for each question, we randomly sample 200 entities and predicates from the test set as noise samples. We then mix the gold entity and predicate into these negative samples, and evaluate the accuracy of our model in predicting the gold predicate or entity from this mixed set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Character-Level vs. Word-Level Models</head><p>We first explore using word-level models as an al- ternative to character-level models to construct em- beddings for questions, entities and predicates.</p><p>Both word-level and character-level models per- form comparably well when predicting the predi- cate, reaching an accuracy of around 80% <ref type="table">(Table  3)</ref>. However, the word-level model has considerable difficulty generalizing to unseen entities, and is only able to predict 45% of the entities accurately from the mixed set. These results clearly demonstrate that the OOV issue is much more severe for entities than predicates, and the difficulty word-level models have when generalizing to new entities.</p><p>In contrast, character-level models have no such issues, and achieve a 96.6% accuracy in predicting the correct entity on the mixed set. This demon- strates that character-level models encode the se- mantic representation of entities and can match en- tity aliases in a KB with their mentions in natural language questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Depth Ablation Study</head><p>We also study the impact of the depth of neural networks in our model. The results are presented in <ref type="table" target="#tab_3">Table 2</ref>. In the ablation experiments we compare the performance of a single-layer LSTM to a two- layer LSTM to encode the question, and a single- layer vs. two-layer CNN to encode the KB entries. We find that a two-layer LSTM boosts joint accu- racy by over 6%. The majority of accuracy gains are a result of improved predicate predictions, possibly because entity accuracy is already saturated in this experimental setup.   <ref type="table">Table 3</ref>: Results for a random sampling experiment where we varied the embedding type (word vs. character-level).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># of LSTM Layers # of CNN Layers Joint Accuracy Predicate Accuracy Entity Accuracy</head><p>We used 2 layered-LSTMs and CNNs for all our experiments. Our models were trained for 14 epochs and 3 days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Attention Mechanisms</head><p>In order to further understand how the model per- forms question answering, we visualize the attention distribution over question characters in the decoding process. In each sub-figure of <ref type="figure" target="#fig_1">Figure 2</ref>, the x-axis is the character sequence of the question, and the y- axis is the attention weight distribution {α i }. The blue curve is the attention distribution when gener- ating the entity, and green curve is the attention dis- tribution when generating the predicate. Interestingly, as the examples show, the attention distribution typically peaks at empty spaces. This indicates that the character-level model learns that a space defines an ending point of a complete linguis- tic unit. That is, the hidden state of the LSTM en- coder at a space likely summarizes content about the character sequence before that space, and therefore contains important semantic information that the de- coder needs to attend to.</p><p>Also, we observe that entity attention distribu- tions are usually less sharp and span longer portions of words, such as "john" or "rutters", than predicate attention distributions (e.g., <ref type="figure" target="#fig_1">Figure 2a</ref>). For enti- ties, semantic information may accumulate gradu- ally when seeing more and more characters, while for predicates, semantic information will become clear only after seeing the complete word. For ex- ample, it may only be clear that characters such as "song by" refer to a predicate after a space, as op- posed to the name of a song such as "song bye bye love" <ref type="figure" target="#fig_1">(Figures 2a, 2b)</ref>. In contrast, a sequence of characters starts to become a likely entity after see- ing an incomplete name such as "joh" or "rutt".</p><p>In addition, a character-level model can identify entities whose English aliases were never seen in training, such as "phrenology" <ref type="figure" target="#fig_1">(Figure 2d</ref>). The model apparently learns that words ending with the suffix "nology" are likely entity mentions, which is interesting because it reads in the input one character at a time.</p><p>Furthermore, as observed in <ref type="figure" target="#fig_1">Figure 2d</ref>, the atten- tion model is capable of attending disjoint regions of the question and capture the mention of a predicate that is interrupted by entity mentions. We also note that predicate attention often peaks at the padding symbols after the last character of the question, pos- sibly because sentence endings carry extra informa- tion that further help disambiguate predicate men- tions. In certain scenarios, the network may only have sufficient information to build a semantic rep- resentation of the predicate after being ensured that it reached the end of a sentence.</p><p>Finally, certain words in the question help identify both the entity and the predicate. For example, con- sider the word "university" in the question "What type of educational institution is eastern new mexico university" <ref type="figure" target="#fig_1">(Figure 2c)</ref>. Although it is a part of the entity mention, it also helps disambiguate the predi- cate. However, previous semantic parsing-based QA approaches ( <ref type="bibr">Yih et al., 2015;</ref><ref type="bibr">Yih et al., 2014</ref>) as- sume that there is a clear separation between pred- icate and entity mentions in the question. In con- trast, the proposed model does not need to make this hard categorization, and attends the word "univer- sity" when predicting both the entity and predicate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Error Analysis</head><p>We randomly sampled 50 questions where the best-performing model generated the wrong KB query and categorized the errors. For 46 out of the 50 examples, the model predicted a predicate with a very similar alias to the true predicate, i.e. "/mu- sic/release/track" vs. "/music/release/track list". For 21 out of the 50 examples, the model predicted the wrong entity, e.g., "Album" vs. "Still Here" for the question "What type of album is still here?". Finally, for 18 of the 50 examples, the model pre- dicted the wrong entity and predicate, i.e. ("Play", "/freebase/equivalent topic/equivalent type") for the question "which instrument does amapola cabase play?" Training on more data, augment- ing the negative sample set with words from the question that are not an entity mention, and having more examples that disambiguate between similar predicates may ameliorate many of these errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we proposed a new character-level, attention-based encoder-decoder model for question answering. In our approach, embeddings of ques- tions, entities, and predicates are all jointly learned to directly optimize the likelihood of generating the correct KB query. Our approach improved the state- of-the-art accuracy on the SimpleQuestions bench- mark significantly, using much less data than pre- vious work. Furthermore, thanks to character-level modeling, we have a compact model that is robust to unseen entities. Visualizations of the attention distribution reveal that our model, although built on character-level inputs, can learn higher-level seman- tic concepts required to answer a natural language question with a structured KB. In the future we want extend our system to multi-relation questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgements</head><p>We thank the anonymous reviewers, Luke Zettle- moyer, Yejin Choi, Joel Pfeiffer, and members of the UW NLP group for helpful feedback on the paper. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Yih et al.</head><label></label><figDesc>(2014) and Yih et al. (2015), Palangi et al. (2016), Huang et al. (2013).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Attention distribution over outputs of a left-to-right LSTM on question characters.</figDesc><graphic url="image-6.png" coords="8,329.97,530.40,204.80,153.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Results for a random sampling experiment where we varied the number of layers used for convolutions and</head><label>2</label><figDesc></figDesc><table>the question-encoding LSTM. We terminated training models after 14 epochs and 3 days on a GPU. 

Embedding Type Joint Accuracy Predicate Accuracy Entity Accuracy 
Character 
78.3 
80.0 
96.6 
Word 
37.6 
78.8 
45.5 

</table></figure>

			<note place="foot" n="1"> Our code is publicly available at https://github. com/davidgolub/simpleqa</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>References [amodei</surname></persName>
		</author>
		<idno>abs/1512.02595. [Bahdanau et al.2015</idno>
	</analytic>
	<monogr>
		<title level="m">Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio</title>
		<editor>ICLR. [Berant and Liang2014] J. Berant and P. Liang</editor>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics (ACL)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06147</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Computational Linguistics and Intelligent Text Processing</title>
		<editor>Klein et al.2003] Dan Klein, Joseph Smarr, Huy Nguyen, and Christopher D Manning</editor>
		<meeting>the 15th International Conference on Computational Linguistics and Intelligent Text Processing<address><addrLine>Doha, Qatar; New York, NY, USA; Lille, France; Dublin, Ireland; Dublin, Ireland, August; Sofia, Bulgaria, August; New York, NY, USA; CICLing; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<date type="published" when="1997-11" />
			<biblScope unit="volume">8724</biblScope>
			<biblScope unit="page" from="2431" to="2439" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lappoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J Mooney ;</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venugopalan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.4615</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing of the AFNLP. ACL Association for Computational Linguistics</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<meeting>the Joint Conference of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing of the AFNLP. ACL Association for Computational Linguistics<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
