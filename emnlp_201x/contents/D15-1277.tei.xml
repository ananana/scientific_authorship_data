<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Can Symbol Grounding Improve Low-Level NLP? Word Segmentation as a Case Study</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirotaka</forename><surname>Kameko</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">†Graduate School of Engineering</orgName>
								<orgName type="department" key="dep2">‡Academic Center for Computing and Media Studies</orgName>
								<orgName type="institution">The University of Tokyo Hongo</orgName>
								<address>
									<addrLine>Bunkyo-ku</addrLine>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Kyoto University Yoshida Honmachi</orgName>
								<address>
									<addrLine>Sakyo-ku</addrLine>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Can Symbol Grounding Improve Low-Level NLP? Word Segmentation as a Case Study</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a novel framework for improving a word segmenter using information acquired from symbol grounding. We generate a term dictionary in three steps: generating a pseudo-stochastically segmented corpus, building a symbol grounding model to enumerate word candidates, and filtering them according to the grounding scores. We applied our method to game records of Japanese chess with commentaries. The experimental results show that the accuracy of a word segmenter can be improved by incorporating the generated dictionary.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Today we can easily obtain a large amount of text associated with multi-modal information, and there is a growing interest in the use of non- textual information in the natural language pro- cessing (NLP) community. Many of these studies aim to output natural language sentences from a nonlinguistic modality, such as image <ref type="bibr" target="#b1">(Farhadi et al., 2010;</ref><ref type="bibr" target="#b20">Yang et al., 2011;</ref>. <ref type="bibr" target="#b3">Kiros et al. (2014)</ref> showed that multi-modal infor- mation improves the performance of a language model.</p><p>Inspired by these studies, we explore a method for improving the performance of a low-level NLP task using multi-modal information. In this work, we focus on the task of word segmentation (WS) in Japanese. WS is often performed as the first processing step for languages without clear word boundaries, and it is as important as part-of-speech (POS) tagging in English. We assume that a large set of pairs of non-textual data and sentences describing them is available as the information source. In our experiments, the pairs consist of game states in Shogi (Japanese chess) and textual comments on them, which were made by Shogi experts. We enumerate substrings (character se- quences) in the sentences and match them with Shogi states by a neural network model. The ra- tionale here is that substrings which match with non-language data well tend to be real words.</p><p>Our method consists of three steps (see <ref type="figure" target="#fig_0">Figure  1</ref>). First, we segment commentary sentences for a game state in various ways to produce word can- didates. Then, we match them with game states of a Shogi playing program. Finally, we compile the symbol grounding results at all states and incorpo- rate them to an automatic WS. To the best of our knowledge, this is the first result reporting a per- formance improvement in an NLP task by symbol grounding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Stochastically Segmented Corpus</head><p>Before symbol grounding, we need to segment the text into words that include probable candi- date words. For this purpose, we use a stochasti- cally segmented corpus (SSC) ( <ref type="bibr" target="#b9">Mori and Takuma, 2004</ref>). Then we propose to simulate it by a normal (deterministically) segmented corpus to avoid the problem of computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Stochastically Segmented Corpora</head><p>An SSC is defined as a combination of a raw corpus C r (hereafter referred to as the character sequence x nr 1 ) and word boundary probabilities of the form P i , which is the probability that a word boundary exists between two characters x i and x i+1 . These probabilities are estimated by a model based on logistic regression (LR) <ref type="bibr" target="#b0">(Fan et al., 2008</ref>) trained on a manually segmented cor- pus by referring to the surrounding characters <ref type="bibr">1</ref> . Since there are word boundaries before the first character and after the last character of the corpus, P 0 = P nr = 1. The expected frequency of a word w in an SSC is calculated as follows:</p><formula xml:id="formula_0">f r (w) = ∑ i∈O P i { ∏ k−1 j=1 (1 − P i+j ) } P i+k , where O = {i | x i+k i+1</formula><p>= w} is the set of all the occurrences of the string matching with w 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pseudo-Stochastically Segmented</head><p>Corpora The computational cost (in terms of both time and space) for calculating the expected frequen- cies in an SSC is very high 3 , so it is not a prac- tical approach for symbol grounding. In this work, we approximate an SSC using a determinis- tically segmented corpus, which we call a pseudo- stochastically segmented corpus (pSSC). The fol- lowing is the process we use to produce a pSSC from an SSC.</p><p>• For i = 1 to n r − 1 1. output a character x i , 2. generate a random number 0 ≤ p &lt; 1, 3. output a word boundary if p &lt; P i or output nothing otherwise.</p><p>Now we have a corpus in the same format as a standard segmented corpus with variable (non- constant) segmentation, where x i and x i+1 are segmented with the probability of P i . We exe- cute the above procedure m times and divide the counts by m. The law of large numbers guarantees that the approximation errors decrease to 0 when m → ∞.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Symbol Grounding</head><p>As the target of symbol grounding, we use states (piece positions) of a Shogi game and commen-taries associated with them. We should note, how- ever, that our framework is general and applica- ble to different types of combinations such as im- age/description pairs ( <ref type="bibr" target="#b13">Regneri et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Game Commentary</head><p>The Japanese language is one of the languages without clear word boundaries and we need an au- tomatic WS as the first step of NLP. In Shogi, there are many professional players and many commen- taries about game states are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Grounding Words</head><p>We build a symbol grounding model using a Shogi commentary dataset. We use a set of pairs of a Shogi state S i and a commentary sentence C i as the training set. A Shogi state S i is converted into a feature vector f(S i ). We generate m (in our ex- periment, m = 4) pSSC C ′ i from C i . C ′ i contains m corpora of the same text body but with differ- ent word segmentation, C ′ ij (j = 1, . . . , m). We treat these as m pairs of a feature vector of Shogi state f(S i ) and a sequence of words C ′ ij . We train a model which predicts words in C ′ ij using f(S i ) as input.</p><p>We use a multi-layer perceptron as the predic- tion model. The input is a vector of the features of a state. The hidden layer is a 100-dimensional vector and is activated by a bipolar sigmoid func- tion. Its output is a d-dimensional real-valued vec- tor, each of whose elements indicates whether a word in the vocabulary of d words appears in the commentary or not. The output layer is activated by a binary sigmoid function.</p><p>We use features of Shogi states which a com- puter Shogi program called Gekisashi ( <ref type="bibr" target="#b17">Tsuruoka et al., 2002</ref>) uses to evaluate the states in game tree search as input. The features of Shogi states used in this experiment are below: a) Positions of pieces (e.g. my rook is at 2h). b) Pieces captured (e.g. the opponent has a bishop). c) Combinations of a) and b) (e.g. my king is at 7h and the opponent's rook is at 7b). d) Other heuristic features.</p><p>Among them, a), b) and c) occupy the majority.</p><p>Unlike normal symbol grounding, the vocabu- lary contains many word candidates appearing in the pSSC generated from the commentaries. Some are real words and some are wrong fragments. These wrong fragments will appear more or less randomly in the commentaries than real words. The perceptron therefore cannot acquire strong re- lation between states and fragments and the output values of the perceptron will be smaller than those of real words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Word Segmentation Using Symbol Grounding Result</head><p>This section describes a baseline automatic word segmenter and a method for incorporating the symbol grounding result to it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baseline Word Segmenter</head><p>Among many Japanese WS and morphological an- alyzers (word segmentation and POS tagging), we adopt pointwise WS (Neubig et al., 2011), because it is the only word segmenter which is capable of adding new words without POS information. The input of the pointwise WS is an unseg- mented character sequence x = x 1 x 2 · · · x k . The word segmenter decides if there is a word bound- ary t i = 1 or not t i = 0 by using support vector machines (SVMs) <ref type="bibr" target="#b0">(Fan et al., 2008</ref>). The features are character n-grams and character type n-grams (n = 1, 2, 3) around the decision points in a win- dow with a width of 6 characters. Additional fea- tures are triggered if character n-grams in the win- dow match with character sequences in the dictio- nary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training a Word Segmenter with</head><p>Grounded Words As a first trial for incorporating symbol ground- ing results to an NLP task, we propose to gener- ate a dictionary based on the symbol grounding result. We can expect that the word candidates that are given high scores by the perceptron in the symbol grounding result have strong relationship to the positions. In other words, we can make a good dictionary by selecting word candidates in descending order of the scores. As a method for sum: the summation of the scores of all the out- put vectors, ave: the average of them, max: the maximum in them.</p><p>First, we acquire a V -dimensional real-valued vec- tor for each Shogi state S i as the result of symbol grounding. Then, for each candidate in C ′ ij , we get the element of the vector which corresponds to the candidate as the score of the candidate. After that, we get the summation of, the average of, or the maximum in the scores of the same candidate over the whole dataset.</p><p>Finally we select the top R percent of word can- didates in descending order of the value of sum, ave, or max and add them to the WS dictionary and retrain the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>We conducted word segmentation experiments in the following settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Corpora</head><p>The annotated corpus we used to build the base- line word segmenter is the manually annotated part (core data) of the Balanced Corpus of Con- temporary Written Japanese (BCCWJ) <ref type="bibr" target="#b7">(Maekawa, 2008)</ref>, plus newspaper articles and daily conver- sation sentences. We also used a 234,652-word dictionary (UniDic) provided with the BCCWJ. A small portion of the BCCWJ core data is re- served for testing. In addition, we manually seg- mented sentences randomly obtained from Shogi commentaries. We divided these sentences into two parts: a development set and a test set. Ta- ble 1 shows the details of these corpora.</p><p>To make a pSSC, we prepared 33,151 pairs of a Shogi position and a commentary sentence. The   <ref type="table" target="#tab_0">Table 1</ref> and sent to the symbol grounding module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Word Segmentation Systems</head><p>We built the following two word segmentation models <ref type="bibr" target="#b12">(Neubig et al., 2011</ref>) to evaluate our framework.</p><p>Baseline: The model is trained from training data shown in <ref type="table" target="#tab_0">Table 1</ref> and UniDic. +Sym.Gro.: The model is trained from the lan- guage resources for the Baseline and the symbol grounding result.</p><p>To decide the function and the value of R for +Sym.Gro. (see Section 4.2), we measured the accuracies on the development set of all the com- binations. The best combination was sum and R = 0.011 4 . In this case, 127 words were added to the dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results and Discussion</head><p>Following the standard in word segmentation ex- periments, the evaluation criteria are recall, preci- sion, and F-measure (their harmonic mean). <ref type="table" target="#tab_1">Table 2 and 3</ref> show WS accuracies on BCCWJ- test and Shogi-test, respectively. The difference in accuracy of the baseline method on BCCWJ-test and Shogi-test shows that WS of Shogi commen- taries is very difficult. Like many other domains, Shogi commentaries contain many special words and expressions, which decrease the accuracy.</p><p>When we compare the F-measures on Shogi- test <ref type="table" target="#tab_2">(Table 3)</ref>, +Sym.Gro. outperforms Baseline. The improvement is statistically significant (at 5% level). The error reduction ratio is comparable to a natural annotation case ( <ref type="bibr" target="#b5">Liu et al., 2014</ref>), despite the fact that our method is unsupervised except for <ref type="bibr">4</ref> In addition we measured the accuracies on the test set of all the combinations and found that the same function and the value of the parameter are the best. This indicates the stability of the function and the parameter. a hyperparameter. Thus we can say that WS im- provement by symbol grounding is as valuable as the annotation additions.</p><p>From a close look at the comparison of the re- call and the precision, we see that the improve- ment in the recall is higher than that of the preci- sion. This result shows that the symbol grounding successfully acquired new words with a few erro- neous words. As the final remark, the result on the general domain <ref type="table" target="#tab_1">(Table 2)</ref> shows that our frame- work does not cause a severe performance degra- dation in the general domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>The NLP task we focus on in this paper is word segmentation. One of the first empirical methods was based on a hidden Markov model <ref type="bibr" target="#b11">(Nagata, 1994)</ref>. In parallel, there were attempts at solv- ing Chinese word segmentation in a similar way <ref type="bibr">(Sproat and Chang, 1996)</ref>. These methods take words as the modeling unit.</p><p>Recently, Neubig et al. <ref type="formula">(2011)</ref> have presented a method for directly deciding whether there is a word boundary or not at each point between char- acters. For Chinese word segmentation, there are some attempts at tagging characters with BIES tags <ref type="bibr" target="#b18">(Xue, 2003)</ref> by a sequence labeller such as CRFs ( <ref type="bibr" target="#b4">Lafferty et al., 2001)</ref>, where B, I, E, and S means the beginning of a word, intermediate of a word, the end of a word, and a single charac- ter word, respectively. The pointwise WS can be seen as character tagging with the BI tag system, in which there is no constraint between neighbor- ing tags. For Japanese WS, our preliminary exper- iments showed that the combination of the BI tag system with SVMs is slightly better than the BIES tag system with CRFs. This is another reason why we used the former in this paper. Our extension of word segmentation is, however, applicable to the BIES/CRFs combination as well.</p><p>The method we describe in this paper is un- supervised and requires a small amount of anno- tated data to tune the hyperparameter. From this viewpoint, the approach based on natural annota- tion ( <ref type="bibr" target="#b19">Yang and Vozila, 2014;</ref><ref type="bibr" target="#b2">Jiang et al., 2013;</ref><ref type="bibr" target="#b5">Liu et al., 2014</ref>) may come to readers' mind. In these studies, tags in hyper-texts were regarded as partial annotations and used to improve WS performance using CRFs trainable from such data ( <ref type="bibr" target="#b16">Tsuboi et al., 2008)</ref>. <ref type="bibr" target="#b8">Mori and Nagao (1996)</ref> pro- posed a method for extracting new words from a large amount of raw text. <ref type="bibr">Murawaki and Kuro-hashi (2008)</ref> proposed an online method in a sim- ilar setting. In contrast to these studies, this paper proposes to use other modalities, game states as the first trial, than languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have described an unsupervised method for improving word segmentation based on symbol grounding results. To extract word candidates from raw sentences, we first segment sentences stochastically, and then match the word candidate sequences with game states that are described by the sentences. Finally, we selected word candi- dates referring to the grounding scores. The exper- imental results showed that we can improve word segmentation by using symbol grounding results. Our framework is general and it is worth testing on other NLP tasks. As future work, we will apply other deep neural network models to our approach. It is interesting to apply the symbol grounding re- sults to an embedding model-based word segmen- tation approach <ref type="bibr" target="#b6">(Ma and Hinrichs, 2015)</ref>. It is also interesting to extend our method to deal with other types of non-textual information such as images and economic indices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of our method.</figDesc><graphic url="image-1.png" coords="2,71.90,62.52,457.38,137.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Corpus specifications. 
#sent. 
#words 
#char. 

Training 

BCCWJ 
56,753 1,324,951 1,911,660 
Newspaper 
8,164 
240,097 
361,843 
Conversation 11,700 
147,809 
197,941 

Develepment 

Shogi-dev. 
170 
2,501 
3,340 

Test 

BCCWJ-test 
6,025 
148,929 
212,261 
Shogi-test 
3,299 
24,966 
32,481 

taking all the occurrences into account, we test the 
following three functions: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>WS accuracy on BCCWJ. 
Recall Prec. F-meas. 
Baseline 
98.99 99.06 
99.03 
+ Sym.Gro. 99.03 99.01 
99.02 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>WS accuracy on Shogi commentaries. 
Recall Prec. F-meas. 
Baseline 
90.12 91.43 
90.77 
+ Sym.Gro. 90.60 91.66 
91.13 

sentences are converted into pSSC m = 4 times 
by an LR word segmentation model trained from 
the training data in </table></figure>

			<note place="foot" n="1"> In the experiment we used the same features as those used in Neubig et al., (2011). .</note>

			<note place="foot" n="2"> For a detailed explanation and a mathematical proof of this method, please refer to Mori and Takuma (2004). 3 This is because an SSC has many words and word fragments. Additionally, word 1-gram frequencies must be calculated using floating point numbers instead of integers.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We thank the anonymous reviewers for their help-ful comments and suggestions. This work was supported by JSPS Grants-in-Aid for Scientific Research Grant Number 26540190.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Every picture tells a story: Generating sentences from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Amin</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th European Conference on Computer Vision</title>
		<meeting>the 11th European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="15" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discriminative learning with natural annotations: Word segmentation as a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yating</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="595" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<meeting>the Eighteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain adaptation for CRF-based Chinese word segmentation using free annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="864" to="874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accurate linear-time Chinese word segmentation via embedding matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhard</forename><surname>Hinrichs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1733" to="1743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Balanced corpus of contemporary written Japanese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kikuo</forename><surname>Maekawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Asian Language Resources</title>
		<meeting>the 6th Workshop on Asian Language Resources</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="101" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Word extraction from corpora and its part-of-speech estimation using distributional analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Nagao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Computational Linguistics</title>
		<meeting>the 16th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1119" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Word n-gram probability estimation from a Japanese raw corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Takuma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Speech and Language Processing</title>
		<meeting>the Eighth International Conference on Speech and Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1037" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Online acquisition of Japanese unknown morphemes using morphological constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yugo</forename><surname>Murawaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="429" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A stochastic Japanese morphological analyzer using a forward-DP backwardA * N-best search algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Computational Linguistics</title>
		<meeting>the 15th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="201" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pointwise prediction for robust, adaptable Japanese morphological analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosuke</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="25" to="36" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Grounding action descriptions in videos</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Translating video content to natural language descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Computer Vision</title>
		<meeting>the 14th International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
	<note>Manfred Pinkal, and Bernt Schiele</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A stochastic finite-state wordsegmentation algorithm for Chinese</title>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<editor>Richard Sproat and Chilin Shih William Gale Nancy Chang</editor>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="377" to="404" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Training conditional random fields using incomplete annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Tsuboi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="897" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Game-tree search algorithm based on realization probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisaku</forename><surname>Yokoyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Chikayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICGA Journal</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="152" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Chinese word segmentation as character tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Linguistics and Chinese</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="48" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised Chinese word segmentation using partial-label learning with conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vozila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="90" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Corpus-guided sentence generation of natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching</forename><forename type="middle">Lik</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiannis</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="444" to="454" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
