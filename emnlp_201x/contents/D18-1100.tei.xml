<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technology Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technology Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Google Brain</orgName>
								<address>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technology Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technology Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="856" to="861"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>856</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this work, we examine methods for data augmentation for text-based tasks such as neu-ral machine translation (NMT). We formulate the design of a data augmentation policy with desirable properties as an optimization problem , and derive a generic analytic solution. This solution not only subsumes some existing augmentation schemes, but also leads to an extremely simple data augmentation strategy for NMT: randomly replacing words in both the source sentence and the target sentence with other random words from their corresponding vocabularies. We name this method SwitchOut. Experiments on three translation datasets of different scales show that SwitchOut yields consistent improvements of about 0.5 BLEU, achieving better or comparable performances to strong alternatives such as word dropout (Sennrich et al., 2016a). Code to implement this method is included in the appendix .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Related Work</head><p>Data augmentation algorithms generate extra data points from the empirically observed training set to train subsequent machine learning algorithms. While these extra data points may be of lower qual- ity than those in the training set, their quantity and diversity have proven to benefit various learning al- gorithms <ref type="bibr" target="#b5">(DeVries and Taylor, 2017;</ref><ref type="bibr" target="#b0">Amodei et al., 2016)</ref>. In image processing, simple augmentation techniques such as flipping, cropping, or increasing and decreasing the contrast of the image are both widely utilized and highly effective ( <ref type="bibr" target="#b9">Huang et al., 2016;</ref><ref type="bibr" target="#b22">Zagoruyko and Komodakis, 2016)</ref>.</p><p>However, it is nontrivial to find simple equiva- lences for NLP tasks like machine translation, be- cause even slight modifications of sentences can result in significant changes in their semantics, or *: Equal contributions. require corresponding changes in the translations in order to keep the data consistent. In fact, indiscrim- inate modifications of data in NMT can introduce noise that makes NMT systems brittle <ref type="bibr" target="#b1">(Belinkov and Bisk, 2018)</ref>.</p><p>Due to such difficulties, the literature in data augmentation for NMT is relatively scarce. To our knowledge, data augmentation techniques for NMT fall into two categories. The first category is based on back-translation ( <ref type="bibr" target="#b19">Sennrich et al., 2016b;</ref><ref type="bibr">Poncelas et al., 2018)</ref>, which utilizes monolin- gual data to augment a parallel training corpus. While effective, back-translation is often vulner- able to errors in initial models, a common prob- lem of self-training algorithms <ref type="bibr" target="#b3">(Chapelle et al., 2009</ref>). The second category is based on word re- placements. For instance, <ref type="bibr" target="#b6">Fadaee et al. (2017)</ref> pro- pose to replace words in the target sentences with rare words in the target vocabulary according to a language model, and then modify the aligned source words accordingly. While this method gen- erates augmented data with relatively high quality, it requires several complicated preprocessing steps, and is only shown to be effective for low-resource datasets. Other generic word replacement methods include word dropout ( <ref type="bibr" target="#b18">Sennrich et al., 2016a;</ref><ref type="bibr" target="#b7">Gal and Ghahramani, 2016)</ref>, which uniformly set some word embeddings to 0 at random, and Reward Aug- mented Maximum Likelihood (RAML; <ref type="bibr" target="#b15">Norouzi et al. (2016)</ref>), whose implementation essentially replaces some words in the target sentences with other words from the target vocabulary.</p><p>In this paper, we derive an extremely simple and efficient data augmentation technique for NMT. First, we formulate the design of a data augmenta- tion algorithm as an optimization problem, where we seek the data augmentation policy that max- imizes an objective that encourages two desired properties: smoothness and diversity. This opti- mization problem has a tractable analytic solution, which describes a generic framework of which both word dropout and RAML are instances. Sec- ond, we interpret the aforementioned solution and propose a novel method: independently replacing words in both the source sentence and the tar- get sentence by other words uniformly sampled from the source and the target vocabularies, respec- tively. Experiments show that this method, which we name SwitchOut, consistently improves over strong baselines on datasets of different scales, in- cluding the large-scale WMT 15 English-German dataset, and two medium-scale datasets: IWSLT 2016 German-English and IWSLT 2015 English- Vietnamese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notations</head><p>We use uppercase letters, such as X, Y , etc., to denote random variables and lowercase letters such as x, y, etc., to denote the corresponding actual values. Additionally, since we will discuss a data augmentation algorithm, we will use a hat to denote augmented variables and their values, e.g.</p><formula xml:id="formula_0">b X, b Y , b x, b y, etc.</formula><p>We will also use boldfaced characters, such as p, q, etc., to denote probability distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Augmentation</head><p>We facilitate our discussion with a probabilistic framework that motivates data augmentation algo- rithms. With X, Y being the sequences of words in the source and target languages (e.g. in machine translation), the canonical MLE framework maxi- mizes the objective</p><formula xml:id="formula_1">J MLE (✓) = E x,y⇠b p(X,Y ) [log p ✓ (y|x)] .</formula><p>Here b p(X, Y ) is the empirical distribution over all training data pairs (x, y) and p ✓ (y|x) is a param- eterized distribution that we aim to learn, e.g. a neural network. A potential weakness of MLE is the mismatch between b p(X, Y ) and the true data distribution p(X, Y ). Specifically, b p(X, Y ) is usu- ally a bootstrap distribution defined only on the observed training pairs, while p(X, Y ) has a much larger support, i.e. the entire space of valid pairs. This issue can be dramatic when the empirical ob- servations are insufficient to cover the data space.</p><p>In practice, data augmentation is often used to remedy this support discrepancy by supplying ad- ditional training pairs. Formally, let q( b X, b Y ) be the augmented distribution defined on a larger sup- port than the empirical distribution b p(X, Y ). Then, MLE training with data augmentation maximizes</p><formula xml:id="formula_2">J AUG (✓) = E b x,b y⇠q( b X, b Y ) [log p ✓ (b y|b x)] .</formula><p>In this work, we focus on a specific family of q, which depends on the empirical observations by</p><formula xml:id="formula_3">q( b X, b Y ) = E x,y⇠b p(x,y) h q( b X, b Y |x, y) i .</formula><p>This particular choice follows the intuition that an augmented pair (b x, b y) that diverges too far from any observed data is more likely to be invalid and thus harmful for training. The reason will be more evident later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Diverse and Smooth Augmentation</head><p>Certainly, not all q are equally good, and the more similar q is to p, the more desirable q will be. Unfortunately, we only have access to limited ob- servations captured by b p. Hence, in order to use q to bridge the gap between b p and p, it is neces- sary to utilize some assumptions about p. Here, we exploit two highly generic assumptions, namely:</p><p>• Diversity: p(X, Y ) has a wider support set, which includes samples that are more diverse than those in the empirical observation set.</p><p>• </p><formula xml:id="formula_4">J(q; x, y) = E b x,b y⇠q( b X, b Y |x,y) ⇥ s(b x, b y; x, y) ⇤ + ⌧ H(q( b X, b Y |x, y)),<label>(1)</label></formula><p>where ⌧ controls the strength of the diversity objec- tive. The first term in (1) instantiates the smooth- ness assumption, which encourages q to draw sam- ples that are similar to (x, y). Meanwhile, the sec- ond term in (1) encourages more diverse samples from q. Together, the objective J(q; x, y) extends the information in the "pivotal" empirical sample (x, y) to a diverse set of similar cases. This echoes our particular parameterization of q in Section 2.2. The objective J(q; x, y) in <ref type="formula" target="#formula_4">(1)</ref> is the canonical maximum entropy problem that one often encoun- ters in deriving a max-ent model <ref type="bibr" target="#b2">(Berger et al., 1996)</ref>, which has the analytic solution: <ref type="formula">(2)</ref> is a fairly generic solution which is agnostic to the choice of the similarity measure s. Obviously, not all similarity measures are equally good. Next, we will show that some existing algo- rithms can be seen as specific instantiations under our framework. Moreover, this leads us to propose a novel and effective data augmentation algorithm.</p><formula xml:id="formula_5">q ⇤ (b x, b y|x, y) = exp {s(b x, b y; x, y)/⌧ } P b x 0 ,b y 0 exp {s(b x 0 , b y 0 ; x, y)/⌧ } (2) Note that</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Existing and New Algorithms</head><p>Word Dropout. In the context of machine trans- lation, <ref type="bibr" target="#b18">Sennrich et al. (2016a)</ref> propose to randomly choose some words in the source and/or target sen- tence, and set their embeddings to 0 vectors. In- tuitively, it regards every new data pair generated by this procedure as similar enough and then in- cludes them in the augmented training set. For- mally, word dropout can be seen as an instantiation of our framework with a particular similarity func- tion s(ˆ x, ˆ y; x, y) (see Appendix A.1).</p><p>RAML. From the perspective of reinforcement learning, <ref type="bibr" target="#b15">Norouzi et al. (2016)</ref> propose to train the model distribution to match a target distribu- tion proportional to an exponentiated reward. De- spite the difference in motivation, it can be shown (c.f. Appendix A.2) that RAML can be viewed as an instantiation of our generic framework, where the similarity measure is s(b x, b y; x, y) = r(b y; y) if b x = x and 1 otherwise. Here, r is a task-specific reward function which measures the similarity be- tween b y and y. Intuitively, this means that RAML only exploits the smoothness property on the target side while keeping the source side intact.</p><p>SwitchOut. After reviewing the two existing augmentation schemes, there are two immediate insights. Firstly, augmentation should not be re- stricted to only the source side or the target side. Secondly, being able to incorporate prior knowl- edge, such as the task-specific reward function r in RAML, can lead to a better similarity measure.</p><p>Motivated by these observations, we propose to perform augmentation in both source and target domains. For simplicity, we separately measure the similarity between the pair (b x, x) and the pair (b y, y) and then sum them together, i.e.</p><formula xml:id="formula_6">s(b x, b y; x, y)/⌧ ⇡ r x (b x, x)/⌧ x + r y (b y, y)/⌧ y ,<label>(3)</label></formula><p>where r x and r y are domain specific similarity func- tions and ⌧ x , ⌧ y are hyper-parameters that absorb the temperature parameter ⌧ . This allows us to factor q ⇤ (b x, b y|x, y) into:</p><formula xml:id="formula_7">q ⇤ (b x, b y|x, y) = exp {r x (b x, x)/⌧ x } P b x 0 exp {r x (b x 0 , x)/⌧ x } ⇥ exp {r y (b y, y)/⌧ y } P b y 0 exp {r y (b y 0 , y)/⌧ y } (4)</formula><p>In addition, notice that this factored formulation allows b</p><p>x and b y to be sampled independently.</p><p>Sampling Procedure. To complete our method, we still need to define r x and r y , and then design a practical sampling scheme from each factor in (4). Though non-trivial, both problems have been (partially) encountered in RAML ( <ref type="bibr" target="#b15">Norouzi et al., 2016;</ref><ref type="bibr" target="#b14">Ma et al., 2017</ref>). For simplicity, we follow previous work to use the negative Hamming dis- tance for both r x and r y . For a more parallelized implementation, we sample an augmented sentence b s from a true sentence s as follows:</p><p>1. Sample b n 2 {0, 1, ..., |s|} by p(b n) / e b n/⌧ .</p><p>2. For each i 2 {1, 2, ..., |s|}, with probability b n/ |s|, we can replace s i by a uniform b</p><formula xml:id="formula_8">s i 6 = s i .</formula><p>This procedure guarantees that any two sentences b s 1 and b s 2 with the same Hamming distance to s have the same probability, but slightly changes the relative odds of sentences with different Hamming distances to s from the true distribution by negative Hamming distance, and thus is an approximation of the actual distribution. However, this efficient sampling procedure is much easier to implement while achieving good performance.</p><p>Algorithm 1 illustrates this sampling procedure, which can be applied independently and in paral- lel for each batch of source sentences and target sentences. Additionally, we open source our imple- mentation in <ref type="bibr">TensorFlow and in PyTorch (respectively in Appendix A.5 and A.6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Sampling with SwitchOut.</head><p>Input : s: a sentence represented by vocab integral ids, ⌧ : the temperature, V : the vocabulary Output : b s: a sentence with words replaced 1 Function HammingDistanceSample(s, ⌧ , |V |):</p><formula xml:id="formula_9">2 Let Z(⌧ )</formula><p>P |s| n=0 e n/⌧ be the partition function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>Let p(n) e n/⌧ /Z(⌧ ) for n = 0, 1, ..., |s|.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4</head><p>Sample b n ⇠ p(n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head><p>In parallel, do:</p><p>6</p><p>Sample ai ⇠ Bernoulli(b n/ |s|). Baselines. While the Transformer network with- out SwitchOut is already a strong baseline, we also compare SwitchOut against two other baselines that further use existing varieties of data augmenta- tion: 1) word dropout on the source side with the dropping probability of word = 0.1; and 2) RAML on the target side, as in Section 2.4. Additionally, on the en-de task, we compare SwitchOut against back-translation ( <ref type="bibr" target="#b19">Sennrich et al., 2016b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>en-de de-en en-vi SwitchOut vs. Word Dropout and RAML.</p><p>We report the BLEU scores of SwitchOut, word dropout, and RAML on the test sets of the tasks in <ref type="table">Table 1</ref>. To account for variance, we run each experiment multiple times and report the me- dian BLEU. Specifically, each experiment with- out SwitchOut is run for 4 times, while each ex- periment with SwitchOut is run for 9 times due to its inherently higher variance. We also con- duct pairwise statistical significance tests using paired bootstrap <ref type="bibr" target="#b4">(Clark et al., 2011)</ref>, and record the results in <ref type="table">Table 1</ref>. For 4 of the 6 settings, SwitchOut delivers significant improvements over the best baseline without SwitchOut. For the re- maining two settings, the differences are not sta- tistically significant. The gains in BLEU with SwitchOut over the best baseline on WMT 15 en-de are all significant (p &lt; 0.0002). Notably, SwitchOut on the source demonstrates as large gains as these obtained by RAML on the target side, and SwitchOut delivers further improvements when combined with RAML.</p><p>SwitchOut vs. Back Translation. Traditionally, data-augmentation is viewed as a method to en- large the training datasets ( <ref type="bibr" target="#b11">Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b20">Szegedy et al., 2014</ref>). In the context of neural MT, <ref type="bibr" target="#b19">Sennrich et al. (2016b)</ref> propose to use artifi- cial data generated from a weak back-translation model, effectively utilizing monolingual data to en- large the bilingual training datasets. In connection, we compare SwitchOut against back translation. We only compare SwitchOut against back transla- tion on the en-de task, where the amount of bilin- gual training data is already sufficiently large 2 . The   BLEU scores with back-translation are reported in <ref type="table" target="#tab_2">Table 2</ref>. These results provide two insights. First, the gain delivered by back translation is less signifi- cant than the gain delivered by SwitchOut. Second, SwitchOut and back translation are not mutually ex- clusive, as one can additionally apply SwitchOut on the additional data obtained from back translation to further improve BLEU scores.</p><p>Effects of ⌧ x and ⌧ y . We empirically study the effect of these temperature parameters. During the tuning process, we translate the dev set of the tasks and report the BLEU scores in <ref type="figure" target="#fig_3">Figure 1</ref>. We observe that when fixing ⌧ y , the best performance is always achieved with a non-zero ⌧ x .  Where does SwitchOut Help the Most? Intu- itively, because SwitchOut is expanding the sup- port of the training distribution, we would expect that it would help the most on test sentences that are far from those in the training set and would thus benefit most from this expanded support. To test this hypothesis, for each test sentence we find its most similar training sample (i.e. nearest neighbor), then bucket the instances by the distance to their nearest neighbor and measure the gain in BLEU af- forded by SwitchOut for each bucket. Specifically, we use (negative) word error rate (WER) as the similarity measure, and plot the bucket-by-bucket performance gain for each group in <ref type="figure" target="#fig_4">Figure 2</ref>. As we can see, SwitchOut improves increasingly more as the WER increases, indicating that SwitchOut is indeed helping on examples that are far from the sentences that the model sees during training. This is the desirable effect of data augmentation tech- niques. x-axis is ordered by the WER between a test sentence and its nearest neighbor in the training set. Left: IWSLT 16 de-en. Right: IWSLT 15 en-vi.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a method to design data augmentation algorithms by solving an optimiza- tion problem. These solutions subsume a few ex- isting augmentation schemes and inspire a novel augmentation method, SwitchOut. SwitchOut de- livers improvements over translation tasks at differ- ent scales. Additionally, SwitchOut is efficient and easy to implement, and thus has the potential for wide application.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Smoothness: p(X, Y ) is smooth, and similar (x, y) pairs will have similar probabilities. To formalize both assumptions, let s(b x, b y; x, y) be a similarity function that measures how similar an augmented pair (b x, b y) is to an observed data pair (x, y). Then, an ideal augmentation policy q( b X, b Y |x, y) should have two properties. First, based on the smoothness assumption, if an aug- mented pair (b x, b y) is more similar to an empirical pair (x, y), it is more likely that (b x, b y) is sampled under the true data distribution p(X, Y ), and thus q( b X, b Y |x, y) should assign a significant amount of probability mass to (b x, b y). Second, to quantify the diversity assumption, we propose that the en- tropy H[q( b X, b Y |x, y)] should be large, so that the support of q( b X, b Y ) is larger than the support of b p and thus is closer to the support p(X, Y ). Com- bining these assumptions implies that q( b X, b Y |x, y) should maximize the objective</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>We benchmark SwitchOut on three translation tasks of different scales: 1) IWSLT 2015 English-Vietnamese (en-vi); 2) IWSLT 2016 German-English (de-en); and 3) WMT 2015 English-German (en-de). All translations are word- based. These tasks and pre-processing steps are standard, used in several previous works. Detailed statistics and pre-processing schemes are in Ap- pendix A.3. Models and Experimental Procedures. Our translation model, i.e. p ✓ (y|x), is a Transformer network (Vaswani et al., 2017). For each dataset, we first train a standard Transformer model with- out SwitchOut and tune the hyper-parameters on the dev set to achieve competitive results. (w.r.t. Lu- ong and Manning (2015); Gu et al. (2018); Vaswani et al. (2017)). Then, fixing all hyper-parameters, and fixing ⌧ y = 0, we tune the ⌧ x rate, which con- trols how far we are willing to let b x deviate from x. Our hyper-parameters are listed in Appendix A.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Dev BLEU scores with different ⌧x and ⌧y. Top left: WMT 15 en-de. Top right: IWSLT 16 de-en. Bottom: IWSLT 15 en-vi.</figDesc><graphic url="image-7.png" coords="5,154.50,514.45,60.37,60.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Gains in BLEU of RAML+SwitchOut over RAML.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Test BLEU scores of back translation (BT) compared 

to and combined with SwitchOut (median of 4 runs). 

</table></figure>

			<note place="foot" n="2"> We add the extra monolingual data from http://data.statmt.org/rsennrich/wmt16_ backtranslations/en-de/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Quoc Le, Minh-Thang Luong, Qizhe Xie, and the anonymous EMNLP reviewers, for their suggestions to improve the paper.</p><p>This material is based upon work supported in part by the Defense Advanced Research Projects Agency Information Innovation Office (I2O) Low Resource Languages for Emergent In-cidents (LORELEI) program under Contract No. HR0011-15-C0114. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copy-right notation here on.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sundaram Ananthanarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Rishita Anubhai, and more authors</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Synthetic and natural noise both break neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A maximum entropy approach to natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen A Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pietra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="71" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<editor>chapelle, o. et al.</editor>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="542" to="542" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>book reviews</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Better hypothesis testing for statistical machine translation: Controlling for optimizer instability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>1708.04552</idno>
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Data augmentation for low-resource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marzieh</forename><surname>Fadaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In NIPS</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nonautoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Hinton</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stanford neural machine translation systems for spoken language domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWLST</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Softmax q-distribution estimation for structured prediction: A theoretical interpretation for raml</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingzhou</forename><surname>Liu</surname></persName>
		</author>
		<idno>1705.07136</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Graham Neubig, and Eduard Hovy</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reward augmented maximum likelihood for neural structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Poncelas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitar</forename><surname>Shterionov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
		</author>
		<idno>1804.06189</idno>
		<title level="m">Gideon Maillette de Buy Wenniger, and Peyman Passban. 2018. Investigating backtranslation in neural machine translation. Arxiv</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Edinburgh neural machine translation systems for wmt 16</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<editor>Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich</editor>
		<meeting><address><addrLine>Scott Reed</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
