<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Machine Translation with Word Predictions</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongxiang</forename><surname>Weng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology Nanjing University Nanjing 210023</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology Nanjing University Nanjing 210023</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaixiang</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology Nanjing University Nanjing 210023</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology Nanjing University Nanjing 210023</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology Nanjing University Nanjing 210023</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Machine Translation with Word Predictions</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="136" to="145"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In the encoder-decoder architecture for neural machine translation (NMT), the hidden states of the recurrent structures in the encoder and decoder carry the crucial information about the sentence.These vectors are generated by parameters which are updated by back-propagation of translation errors through time. We argue that propagating errors through the end-to-end recurrent structures are not a direct way of control the hidden vectors. In this paper, we propose to use word predictions as a mechanism for direct supervision. More specifically, we require these vectors to be able to predict the vocabulary in target sentence. Our simple mechanism ensures better representations in the encoder and decoder without using any extra data or annotation. It is also helpful in reducing the target side vocabulary and improving the decoding efficiency. Experiments on Chinese-English and German-English machine translation tasks show BLEU improvements by 4.53 and 1.3, respectively.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The encoder-decoder based neural machine trans- lation (NMT) models <ref type="bibr" target="#b17">(Sutskever et al., 2014;</ref>) have been developing rapidly. <ref type="bibr" target="#b17">Sutskever et al. (2014)</ref> propose to encode the source sentence as a fixed-length vector repre- sentation, based on which the decoder gener- ates the target sequence, where both the en- coder and decoder are recurrent neural net- works (RNN) <ref type="bibr" target="#b17">(Sutskever et al., 2014</ref>) or their vari- ants ( <ref type="bibr" target="#b4">Chung et al., 2014;</ref>. In this framework, the fixed- length vector plays the crucial role of transition- ing the information of the sentence from the source side to the target side.</p><p>Later, attention mechanisms are proposed to en- hance the source side representations ( <ref type="bibr" target="#b11">Luong et al., 2015b</ref>). The source side context is computed at each time-step of decod- ing, based on the attention weights between the source side representations and the current hidden state of the decoder. However, the hidden states in the recurrent decoder still originate from the single fixed-length representation ( <ref type="bibr" target="#b11">Luong et al., 2015b)</ref>, or the average of the bi-directional repre- sentations ( ). Here we refer to the representation as initial state.</p><p>Interestingly, <ref type="bibr" target="#b2">Britz et al. (2017)</ref> find that the value of initial state does not affect the translation performance, and prefer to set the initial state to be a zero vector. On the contrary, we argue that initial state still plays an important role of transla- tion, which is currently neglected. We notice that beside the end-to-end error back propagation for the initial and transition parameters, there is no di- rect control of the initial state in the current NMT architectures. Due to the large number of param- eters, it may be difficult for the NMT system to learn the proper sentence representation as the ini- tial state. Thus, the model is very likely to get stuck in local minimums, making the translation process arbitrary and unstable.</p><p>In this paper, we propose to augment the current NMT architecture with a word prediction mecha- nism. More specifically, we require the initial state of the decoder to be able to predict all the words in the target sentence. In this way, there is a spe- cific objective for learning the initial state. Thus the learnt source side representation will be bet- ter constrained. We further extend this idea by ap- plying the word predictions mechanism to all the hidden states of the decoder. So the transition be- tween different decoder states could be controlled as well.</p><p>Our mechanism is simple and requires no ad- ditional data or annotation. The proposed word predictions mechanism could be used as a training method and brings no extra computing cost during decoding.</p><p>Experiments on the Chinese-English and German-English translation tasks show that both the constraining of the initial state and the decoder hidden states bring significant improvement over the baseline systems. Furthermore, using the word prediction mechanism on the initial state as a word predictor to reduce the target side vocabulary could greatly improve the decoding efficiency, without a significant loss on the translation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Many previous works have noticed the problem of training an NMT system with lots of parame- ters. Some of them prefer to use the dropout tech- nique ( <ref type="bibr">Srivastava et al., 2014;</ref><ref type="bibr" target="#b11">Luong et al., 2015b;</ref><ref type="bibr" target="#b13">Meng et al., 2016)</ref>. Another possible choice is to ensemble several models with random starting points ( <ref type="bibr" target="#b17">Sutskever et al., 2014;</ref><ref type="bibr" target="#b6">Jean et al., 2015;</ref><ref type="bibr" target="#b10">Luong and Manning, 2016)</ref>. Both techniques could bring more stable and better results. But they are general training techniques of neural networks, which are not specifically targeting the model- ing of the translation process like ours. We will make empirical comparison with them in the ex- periments.</p><p>The way we add the word prediction is similar to the research of multi-task learning. <ref type="bibr" target="#b5">Dong et al. (2015)</ref> propose to share an encoder between dif- ferent translation tasks. <ref type="bibr" target="#b9">Luong et al. (2015a)</ref> pro- pose to jointly learn the translation task for dif- ferent languages, the parsing task and the image captioning task, with a shared encoder or decoder. <ref type="bibr" target="#b21">Zhang and Zong (2016)</ref> propose to use multitask learning for incorporating source side monolingual data. Different from these attempts, our method focuses solely on the current translation task, and does not require any extra data or annotation.</p><p>In the other sequence to sequence tasks, <ref type="bibr" target="#b18">Suzuki and Nagata (2017)</ref> propose the idea for predicting words by using encoder information. However, the purpose and the way of our mechanism are dif- ferent from them.</p><p>The word prediction technique has been applied in the research of both statistical machine transla- tion (SMT) ( <ref type="bibr" target="#b1">Bangalore et al., 2007;</ref><ref type="bibr" target="#b12">Mauser et al., 2009;</ref><ref type="bibr" target="#b7">Jeong et al., 2010;</ref><ref type="bibr" target="#b19">Tran et al., 2014</ref>) and NMT ( <ref type="bibr" target="#b14">Mi et al., 2016;</ref><ref type="bibr" target="#b8">L'Hostis et al., 2016)</ref>. In these research, word prediction mechanisms are employed to decide the selection of words or con- strain the target vocabulary, while in this paper, we use word prediction as a control mechanism for neural model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Notations and Backgrounds</head><p>We present a popular NMT framework with the encoder-decoder architecture ( ) and the attention net- works ( <ref type="bibr" target="#b11">Luong et al., 2015b</ref>), based on which we propose our word prediction mechanism.</p><p>Denote a source-target sentence pair as {x, y} from the training set, where x is the source word sequence (x 1 , x 2 , · · · , x |x| ) and y is the target word sequence (y 1 , y 2 , · · · , y |y| ), |x| and |y| are the length of x and y, respectively.</p><p>In the encoding stage, a bi-directional recur- rent neural network is used (  to encode x into a sequence of vectors</p><formula xml:id="formula_0">(h 1 , h 2 , · · · , h |x| ). For each x i , the representation h i is: h i = [ − → h i ; ← − h i ]<label>(1)</label></formula><p>where [·; ·] denotes the concatenation of column vectors; − → h i and ← − h i denote the hidden vectors for the word x i in the forward and backward RNNs, respectively.</p><p>The gated recurrent unit (GRU) is used as the re- current unit in each RNN, which is shown to have promising results in speech recognition and ma- chine translation ( ). Formally, the hidden state h i at time step i of the forward RNN encoder is defined by the GRU function g− → e (·, ·), as follows:</p><formula xml:id="formula_1">− → h i = g− → e ( − → h i−1 , emb x i )<label>(2)</label></formula><formula xml:id="formula_2">= (1 − − → z i ) ⊙ − → h i−1 + − → z i ⊙ − → h ′ i − → z i = σ( − → W z [emb x i ; − → h i−1 ]) (3) − → h ′ i = tanh( − → W[emb x i ; ( − → r i ⊙ − → h i−1 )]) (4) − → r i = σ( − → W r [emb x i ; − → h i−1 ])<label>(5)</label></formula><p>where ⊙ denotes element-wise product between vectors and emb x i is the word embedding of the x i . tanh(·) and σ(·) are the tanh and sigmoid trans- formation functions that can be applied element- wise on vectors, respectively. For simplicity, we  omit the bias term in each network layer. The backward RNN encoder is defined likewise.</p><formula xml:id="formula_3">S 1 y 1 S 2 y 2 S 3 y 3 S j y j x 1 x 2 x 3 x i ... ...</formula><p>In the decoding stage, the decoder starts with the initial state s 0 , which is the average of source rep- resentations ( ).</p><formula xml:id="formula_4">s 0 = σ(W s 1 |x| |x| ∑ i=1 h i )<label>(6)</label></formula><p>At each time step j, the decoder maximizes the conditional probability of generating the jth target word, which is defined as:</p><formula xml:id="formula_5">P (y j |y &lt;j , x) = f d (t d ([emb y j−1 ; s j ; c j ])) (7) f d (u) = softmax(W f u) (8) t d (v) = tanh(W t v) (9)</formula><p>where s j is the decoder's hidden state, which is computed by another GRU (as in Equation 2):</p><formula xml:id="formula_6">s j = g d (s j−1 , [emb y j−1 ; c j ])<label>(10)</label></formula><p>and the context vector c j is from the attention mechanism ( <ref type="bibr" target="#b11">Luong et al., 2015b</ref>):</p><formula xml:id="formula_7">c j = |x| ∑ i=1 a ji h i (11) a ji = exp(e ji ) ∑ |x| k=1 exp(e jk )<label>(12)</label></formula><formula xml:id="formula_8">e ji = tanh(W att d [s j−1 ; h i ]).<label>(13)</label></formula><p>4 NMT with Word Predictions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Word Prediction for the Initial State</head><p>The decoder starts the generation of target sentence from the initial state s 0 (Equation 6) generated by the encoder. Currently, the update for the encoder only happens when a translation error occurs in the decoder. The error is propagated through multiple time steps in the recurrent structure until it reaches the encoder. As there are hundreds of millions of parameters in the NMT system, it is hard for the model to learn the exact representation of source sentences. As a result, the values of initial state may not be exact during the translation process, leading to poor translation performances. We propose word prediction as a mechanism to control the values of initial state. The intuition is that since the initial state is responsible for the translation of whole target sentence, it should at least contain information of each word in the tar- get sentence. Thus, we optimize the initial state by making prediction for all target words. For sim- plicity, we assume each target word is independent of each other.</p><p>Here the word prediction mechanism is a sim- pler sub-task of translation, where the order of words is not considered. The prediction task could be trained jointly with the translation task in a multi-task learning way ( <ref type="bibr" target="#b9">Luong et al., 2015a;</ref><ref type="bibr" target="#b5">Dong et al., 2015;</ref><ref type="bibr" target="#b21">Zhang and Zong, 2016)</ref>, where both tasks share the same encoder. In other words, word prediction for the initial state could be interpreted as an improvement for the encoder. We denote this mechanism as WP E .</p><p>As shown in <ref type="figure">Figure 1</ref>, a prediction network is added to the initial state. We define the conditional probability of WP E as follows:</p><formula xml:id="formula_9">P WP E (y|x) = |y| ∏ j=1 P WP E (y j |x)<label>(14)</label></formula><formula xml:id="formula_10">P WP E (y j |x) = f p (t p ([s 0 ; c p ]))<label>(15)</label></formula><p>where f p (·) and t p (·) are the softmax layer and non-linear layer as defined in Equation 8-9, with different parameters; c p is defined similar as the  attention network, so the source side information could be enhanced.</p><formula xml:id="formula_11">c p = |x| ∑ i=1 a i h i<label>(16)</label></formula><formula xml:id="formula_12">a i = exp(e i ) ∑ |x| k=1 exp(e k )<label>(17)</label></formula><formula xml:id="formula_13">e i = tanh(W attp [s 0 , h i ]).<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Word Predictions for Decoder's Hidden States</head><p>Similar intuition is also applied for the decoder.</p><p>Because the hidden states of the decoder are re- sponsible for the translation of target words, they should be able to predict the target words as well.</p><p>The only difference is that we remove the already generated words from the prediction task. So each hidden state in the decoder is required to predict the target words which remain untranslated. For the first state s 1 of the decoder, the predic- tion task is similar with the task for the initial state. Since then, the prediction is no longer a separate training task, but integrated into each time step of the training process. We denote this mechanism as WP D .</p><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, for each time step j in the decoder, the hidden state s j is used for the predic- tion of (y j , y j+1 , · · · , y |y| ). The conditional prob- ability of WP D is defined as:</p><formula xml:id="formula_14">P WP D (y j , y j+1 , · · · , y |y| |y &lt;j , x)<label>(19)</label></formula><formula xml:id="formula_15">= |y| ∏ k=j P WP D (y k |y &lt;j , x) P WP D (y k |y &lt;j , x) =f d (p(t d ([emb y j−1 ; s j ; c j ])))<label>(20)</label></formula><p>where f d (·) and t d (·) are the softmax layer and non-linear layer as defined in Equation 8-9; p(·)</p><p>is another non-linear transformation layer, which prepares the current state for the prediction:</p><formula xml:id="formula_16">p(u) = tanh(W p u).<label>(21)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training</head><p>NMT models optimize the networks by maximiz- ing the likelihood of the target translation y given source sentence x, denoted by L T .</p><formula xml:id="formula_17">L T = 1 |y| |y| ∑ j=1 log P (y j |y &lt;j , x)<label>(22)</label></formula><p>where P (y j |y &lt;j , x) is defined in Equation 7.</p><p>To optimize the word prediction mechanism, we propose to add extra likelihood functions L WP E and L WP D into the training procedure.</p><p>For the WP E , we directly optimize the likeli- hood of translation and word prediction:</p><formula xml:id="formula_18">L 1 = L T + L WP E (23) L WP E = log P WP E<label>(24)</label></formula><p>where P WP E is defined in Equation 14.</p><p>For the WP D , we optimize the likelihood as:</p><formula xml:id="formula_19">L 2 = L T + L WP D<label>(25)</label></formula><formula xml:id="formula_20">L WP D = |y| ∑ j=1 1 |y| − j + 1 log P WP D<label>(26)</label></formula><p>where P WP D is defined in Equation 19; the coeffi- cient of the logarithm is used to calculate the aver- age probability of each prediction.</p><p>The two mechanisms could also work together, so that both the encoder and the decoder could be improved:</p><formula xml:id="formula_21">L 3 = L T + L WP E + L WP D .<label>(27)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Making Use of the Word Predictor</head><p>The previously proposed word prediction mecha- nism could be used only as a extra training objec- tive, which will not be computed during the trans- lation. Thus the computational complexity of our models for translation stays exactly the same. On the other hand, using a smaller and specific vocabulary for each sentence or batch will improve translation efficiency. If the vocabulary is accurate enough, there is also a chance to improve the trans- lation quality <ref type="bibr" target="#b6">(Jean et al., 2015;</ref><ref type="bibr" target="#b14">Mi et al., 2016;</ref><ref type="bibr" target="#b8">L'Hostis et al., 2016)</ref>. Our word prediction mech- anism WP E provides a natural solution for generat- ing a possible set of target words at sentence level. The prediction could be made from the initial state s 0 , without using extra resources such as word dic- tionaries, extracted phrases or frequent word lists, as in <ref type="bibr" target="#b14">Mi et al. (2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data</head><p>We perform experiments on the Chinese-English (CH-EN) and German-English (DE-EN) machine translation tasks. For the CH-EN, the training data consists of about 8 million sentence pairs <ref type="bibr">1</ref> . We use NIST MT02 as our validation set, and the NIST MT03, MT04 and MT05 as our test sets. These sets have 878, 919, 1597 and 1082 source sen- tences, respectively, with 4 references for each sentence. For the DE-EN, the experiments trained on the standard benchmark WMT14, and it has about 4.5 million sentence pairs. We use new- stest 2013 (NST13) as validation set, and newstest 2014(NST14) as test set. These sets have 3000 and 2737 source sentences, respectively, with 1 refer- ence for each sentence. Sentences were encoded using byte-pair encoding (BPE) ( <ref type="bibr" target="#b2">Britz et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Systems and Techniques</head><p>We implement a baseline system with the bi- directional encoder ( ) and the attention mechanism (Luong et al., 2015b) as de- scribed in Section 3, denoted as baseNMT. Then our proposed word prediction mechanism on ini- tial state and hidden states of decoder are imple- mented on the baseNMT system, denoted as WP E and WP D , respectively. We denote the system use both techniques as WP ED . We implement sys- tems with variable-sized vocabulary following <ref type="bibr" target="#b14">(Mi et al., 2016</ref>). For comparison, we also implement systems with dropout (with dropout rate 0.5 on the output layer) and ensemble (ensemble of 4 systems at the output layer) techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implementation Details</head><p>Both our CH-EN and DE-EN experiments are im- plemented on the open source toolkit dl4mt 2 , with most default parameter settings kept the same. We train the NMT systems with the sentences of length up to 50 words. The source and target vocabular- ies are limited to the most frequent 30K words for both Chinese and English, respectively, with the out-of-vocabulary words mapped to a special to- ken UNK.</p><p>The dimension of word embedding is set to 512 and the size of the hidden layer is 1024. The recur- rent weight matrices are initialized as random or- thogonal matrices, and all the bias vectors as zero. Other parameters are initialized by sampling from the Gaussian distribution N (0, 0.01).</p><p>We use the mini-batch stochastic gradient de- scent (SGD) approach to update the parameters, with a batch size of 32. The learning rate is con- trolled by AdaDelta <ref type="bibr" target="#b20">(Zeiler, 2012)</ref>.</p><p>For efficient training of our system, we adopt a simple pre-train strategy. Firstly, the baseNMT system is trained. The training results are used as the initial parameters for pre-training our proposed models with word predictions.</p><p>For decoding during test time, we simply decode until the end-of-sentence symbol eos occurs, using a beam search with a beam width of 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Translation Experiments</head><p>To see the effect of word predictions in transla- tion, we evaluate these systems in case-insensitive IBM-BLEU ( <ref type="bibr" target="#b15">Papineni et al., 2002</ref>) on both CH-EN and DE-EN tasks.</p><p>The detailed results are show in the <ref type="table" target="#tab_3">Table 1  and Table 2</ref>    BLEU, but the improvement is smaller than on the baseNMT. On the DE-EN experiments, the phe- nomenon of experiments is similar to CH-EN ex- periments. The baseNMT system improves 0.94 through dropout method and 0.9 BLEU through ensemble method. The dropout technique also does not work on WP ED and the ensemble tech- nique improves 1.79 BLEU. These comparisons suggests that our system already learns better and stable values for the parameters, enjoying some of the benefits of general training techniques like dropout and ensemble. Compared to dropout and ensemble, our method WP ED achieves the highest improvement against the baseline system on both CH-EN and DE-EN experiments. Along with en- semble method, the improvement could be up to 5.79 BLEU and 1.79 BLEU respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Word Prediction Experiments</head><p>Since we include an explicit word prediction mechanism during the training of NMT systems, we also evaluate the prediction performance on the CH-EN experiments to see how the training is im- proved.</p><p>For each sentence in the test set, we use the ini- tial state of the given model to make prediction about the possible words. We denote the set of top n words as T n , the set of words in all the references top-n baseNMT WP E Prec. Recall Prec. Recall <ref type="table" target="#tab_3">top-10  45%  17%  73%  30%  top-20  33%  21%  63%  43%  top-50  21%  30%  41%  55%  top-100 14%  39%  28%  68%  top-1k</ref> 2% 67% 4% 89% top-5k 0.7% 84% 0.9% 95% top-10k 0.4% 90% 0.5% 97% <ref type="table">Table 5</ref>: Comparison between baseNMT and WP E in precision and recall for the different prediction size on the CH-EN experiments.</p><p>as R. We define the precision, recall of the word prediction as follows:</p><formula xml:id="formula_22">precision = |T n ∩ R| |T n | * 100% (28) recall = |T n ∩ R| |R| * 100%<label>(29)</label></formula><p>We compare the prediction performance of baseNMT and WP E . WP ED has similar prediction results with WP E , so we omit its results. As shown in <ref type="table">Table 5</ref>, baseNMT system has a relatively lower prediction precision, for example, 45% in top 10 prediction. With an explicit training, the WP E could achieve a much higher precision in all con- ditions. Specifically, the precision reaches 73% in top 10. This indicates that the initial state in WP E contains more specific information about the pre- diction of the target words, which may be a step towards better semantic representation, and leads to better translation quality.</p><p>Because the total words in the references are limited (around 50), the precision goes down, as expected, when a larger prediction set is consid- ered. On the other hand, the recall of WP E is also much higher than baseNMT. When given 1k pre- dictions, WP E could successfully predict 89% of the words in the reference. The recall goes up to 95% with 5k predictions, which is only 1/6 of the current vocabulary.</p><p>To analyze the process of word prediction, we draw the attention heatmap (Equation 16) between the initial state s 0 and the bi-directional represen- tation of each source side word h i for an example sentence. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, both examples show that the initial states have a very strong atten- tion with all the content words in the source sen- tence. The blank cells are mostly functions words or high frequent tokens such as "的 ('s)", "是 (is)", "而 (and)", "它 (it)", comma and period. This in- dicates that the initial state successfully encodes information about most of the content words in the source sentence, which contributes for a high pre- diction performance and leads to better translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Improving Decoding Efficiency</head><p>To make use of the word prediction, we conduct experiments using the predicted vocabulary, with different vocabulary size (1k to 10k) on the CH- EN experiments, denoted as WP E -V and WP ED -V. The comparison is made in both translation quality and decoding time. As all our models with fixed vocabulary size have exactly the same number of parameters for decoding (extra mechanism is used only for training), we only plot the decoding time of the WP ED for comparison. <ref type="figure">Figure 4</ref> and 5 show the results.</p><p>When we start the experiments with top 1k vo- cabulary (1/30 of the baseline settings), the trans- lation quality of both WP E -V and WP ED -V are al- ready higher than the baseNMT; while their decod- ing time is less than 1/3 of an NMT system with 30k vocabulary. When the size of vocabulary in- creases, the translation quality improves as well. With a 6k predicted vocabulary (1/5 of the baseline settings), the decoding time is about 60% of a full- vocabulary system; the performances of both sys- tems with variable size vocabulary are compara- ble their corresponding fixed-vocabulary systems, which is higher than the baseNMT by 2.53 and 4.53 BLEU, respectively.</p><p>Although the comparison may not be fair enough due to the language pair and training conditions, the above relative improvements (e.g. WP ED -V v.s. baseNMT) is much higher than previous research of manipulating the vocabular- ies ( <ref type="bibr" target="#b6">Jean et al., 2015;</ref><ref type="bibr" target="#b14">Mi et al., 2016;</ref><ref type="bibr" target="#b8">L'Hostis et al., 2016)</ref>. This is because our mechanism is not only about reducing the vocabulary itself for each sentence or batch, it also brings improvement to the overall translation model. Please note that un- like these research, we keep the target vocabulary to be 30k in all our experiments, because we are not focusing on increasing the vocabulary size in this paper. It will be interesting to combine our mecha- nism with larger vocabulary to further enhance the translation performance. Again, our mechanism requires no extra annotation, dictionary, alignment or separate discriminative predictor, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Translation Analysis</head><p>We also analyze real-case translations to see the difference between different systems <ref type="table">(Table 6)</ref>.</p><p>It is easy to see that the baseNMT system misses the translations of several important words, such as "advertising", "1.5", which are marked with un- derline in the reference. It also wrongly translates the company name "time warner inc." as the re- dundant information "internet company"; "amer- ica online" as "us line".</p><p>The results of dropout or ensemble show im- provement compared to the baseNMT. But they still make mistakes about the translation of "on- line" and the company name "time warner inc.".</p><p>With WP ED , most of these errors no longer exist, because we force the encoder and decoder to carry the exact information during translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>The encoder-decoder architecture provides a gen- eral paradigm for learning machine translation from the source language to the target language. However, due to the large amount of parameters and relatively small training data set, the end-to- end learning of an NMT model may not be able to learn the best solution. We argue that at least part of the problem is caused by the long error back- propagation pipeline of the recurrent structures in multiple time steps, which provides no direct con- trol of the information carried by the hidden states in both the encoder and decoder.</p><p>Instead of looking for other annotated data, we notice that the words in the target language sen- tence could be viewed as a natural annotation. We propose to use the word prediction mechanism to enhance the initial state generated by the en- coder and extend the mechanism to control the hidden states of decoder as well. Experiments show promising results on the Chinese-English and German-English translation tasks. As a by- product, the word predictor could be used to im- prove the efficiency of decoding, which may be source 时代华纳公司的网络公司美国线上说, 它预期二 ○ ○ 二年的广告与商业销售将由 二 ○ ○ 一年的二十七亿美元减少到十五亿美元。 reference america online , the internet arm of time warner conglomerate , said it expects advertising and commerce revenue to decline from us $ 2.7 billion in 2001 to us $ 1.5 in 2002 . baseNMT in the us line , the internet company 's internet company said on the internet that it expected that the business sales in 2002 would fall from $ UNK billion to $ UNK billion in 2001 .</p><p>baseNMT +dropout on the united states line , UNK 's internet company said on the internet that it expects to reduce the annual advertising and commercial sales from $ UNK billion in 2001 to $ 1.5 billion .</p><p>baseNMT +ensemble in the us line , the internet company 's internet company said that it expected that the advertising and commercial sales volume for 2002 would be reduced from us $ UNK billion to us $ 1.5 billion in 2001 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WP ED</head><p>the internet company of time warner inc. , the us online , said that it expects that the advertising and commercial sales in 2002 will decrease from $ UNK billion in 2001 to us $ 1.5 billion . <ref type="table">Table 6</ref>: Comparisons of different systems in translating the same example sentence, which from CH- EN test sets. ("source" indicates the source sentence; "reference" indicates the human translation; the translation results are indicated by their system names, including our best "WP ED " systems. The underline words in the reference are missed in the baseNMT output; the bold font indicates improvements over the baseNMT system; and the italic font indicates remaining translation errors.) crucial for large scale applications.</p><p>Our attempts demonstrate that the learning of the large scale neural network systems is still not good enough. In the future, it might be helpful to analyze the benefits of jointly learning other re- lated tasks together with machine translation, to provide further control of the learning process. It is interesting to demonstrate the effectiveness of the proposed mechanism on other sequence to se- quence learning tasks as well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>y</head><label></label><figDesc>Figure 1: The NMT model with word prediction for the initial state.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The NMT model with word predictions for the decoder's hidden states.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Two examples of the attention heatmap between the initial state s 0 and the bi-directional representation of each source side word h i from the CH-EN test sets. (The English translation of each source word is annotated in the parentheses after it. )</figDesc><graphic url="image-1.png" coords="7,309.60,62.80,213.63,210.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: BLEU scores with different vocabulary sizes for each sentence on the CH-EN experiments. (The performance of baseNMT, WP E , WP D , WP ED are plotted as horizontal lines for comparison.)</figDesc><graphic url="image-2.png" coords="8,72.00,62.81,227.18,159.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>1 h 1 h 2 h 2 h 3 h 3 h i h i</head><label></label><figDesc></figDesc><table>... 

... 

h ... 

... 

... 

Encoder 

Decoder 

... 

... 

... 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>1 h 1 h 2 h 2 h 3 h 3 h i h i1 y 1 ... ... y 1 y 2 y j S 2 y 2 ... ... y 2 y 3 y j S 3 y 3 ... ... y 3 y 4 y j S j y j ... y j x 1 x 2 x 3 x i</head><label></label><figDesc></figDesc><table>... 

... 

h ... 

... 

... 

Encoder 

Decoder 

... 

... 

... 

S Initial State 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>. Compared to the baseNMT sys- tem, all of our models achieve significant improve- ments. On the CH-EN experiments, simply adding word predictions to the initial state (WP E ) already brings considerable improvements. The average improvement on test set is 2.53 BLEU, showing that constraining the initial state does lead to a higher translation quality. Adding word predic-</figDesc><table>Models 

MT02(dev) MT03 MT04 MT05 Test Ave. IMP 
baseNMT 
34.04 
34.92 36.08 33.88 
34.96 
− 

WP E 
39.36 
37.17 39.11 36.20 
37.49 
+2.53 
WP D 
40.28 
38.45 40.99 37.90 
39.11 
+4.15 
WP ED 
40.25 
39.50 40.91 38.05 
39.49 
+4.53 

Table 1: Case-insensitive 4-gram BLEU scores of baseNMT, WP E , WP D , WP ED systems on the CH-EN 
experiments. (The "IMP" column presents the improvement of test average compared to the baseNMT. ) 

Models 
NST13(dev) NST14 IMP 
baseNMT 
23.56 
20.68 
− 

WP E 
24.44 
21.09 +0.41 
WP D 
25.31 
21.54 +0.86 
WP ED 
25.97 
21.98 
+1.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Case-insensitive 4-gram BLEU scores of 
baseNMT, WP E , WP D , WP ED systems on the DE-
EN experiments. 

Models 
Test 
IMP 
baseNMT 
34.86 
− 

WP ED 
39.49 +4.53 
baseNMT-dropout 
37.02 +2.06 
WP ED -dropout 
39.25 +4.29 
baseNMT-ensemble(4) 37.71 +2.75 
WP ED -ensemble(4) 
40.75 +5.79 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>our models with systems using dropout and ensemble techniques. The results show in Table 3 and 4. On the CH-EN experi- ments, the dropout method successfully improves the baseNMT system by 2.06 BLEU. However, it does not work on our WP ED system. The en- semble technique improves the baseNMT system by 2.75 BLEU. It still improves WP ED by 1.26</figDesc><table>Average case-insensitive 4-gram BLEU 
scores on the CH-EN experiments for baseNMT 
and WP ED systems, with the dropout and ensemble 
techniques. 

tions to the hidden states in the decoder (WP D ) 
leads to further improvements against baseNMT 
(4.15 BLEU), because WP D adds constraints to 
the state transitions through different time steps 
in the decoder. Using both techniques improves 
the baseline by 4.53 BLEU. On the DE-EN ex-
periments, the improvement of WP E model is 0.41 
BLEU and WP D model is 0.86 BLEU on test set. 
When use both techniques, the WP ED improves on 
the test set is 1.3 BLEU. 
We compare </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Case-insensitive 4-gram BLEU scores on 
the DE-EN experiments for baseNMT and WP ED 
systems, with the dropout and ensemble tech-
niques. 

</table></figure>

			<note place="foot" n="1"> includes LDC2002E18, LDC2003E07, LDC2003E14, LDC2004E12, LDC2004T08, LDC2005T06, LDC2005T10, LDC2006E26 and LDC2007T09</note>

			<note place="foot" n="2"> https://github.com/nyu-dl/dl4mt-tutorial</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their insightful comments. Shujian Huang is the corresponding author. This work is supported by the National Science Foundation of China (No. 61672277, 61472183), the Jiangsu Provin-cial Research Foundation for Basic Research (No. BK20170074).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR abs/1409.0473</idno>
		<ptr target="http://arxiv.org/abs/1409.0473" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Statistical machine translation through global lexical selection and sentence reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Bangalore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Kanthak</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P07-1020" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Massive Exploration of Neural Machine Translation Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/D14-1179</idno>
		<ptr target="https://doi.org/10.3115/v1/D14-1179" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Çaglar</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR abs/1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-task learning for multiple language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/P15-1166</idno>
		<ptr target="https://doi.org/10.3115/v1/P15-1166" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1723" to="1732" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/P15-1001</idno>
		<ptr target="https://doi.org/10.3115/v1/P15-1001" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A discriminative lexicon model for complex morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisami</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Conference of the Association for Machine Translation in the Americas</title>
		<meeting>the Ninth Conference of the Association for Machine Translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Vocabulary selection strategies for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gurvan L&amp;apos;hostis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Auli</surname></persName>
		</author>
		<idno>CoRR abs/1610.00072</idno>
		<ptr target="http://arxiv.org/abs/1610.00072" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multi-task sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno>CoRR abs/1511.06114</idno>
		<ptr target="http://arxiv.org/abs/1511.06114" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Achieving open vocabulary neural machine translation with hybrid wordcharacter models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>CoRR abs/1604.00788</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/D15-1166</idno>
		<ptr target="https://doi.org/10.18653/v1/D15-1166" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Extending statistical machine translation with discriminative and trigger-based lexicon models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Mauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saša</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D09-1022" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="210" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Interactive attention for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR abs/1610.05011</idno>
		<ptr target="http://arxiv.org/abs/1610.05011" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Vocabulary manipulation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
		<idno>CoRR abs/1605.03209</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bleu: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="doi">10.3115/1073083.1073135</idno>
		<ptr target="https://doi.org/10.3115/1073083.1073135" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<editor>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov</editor>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA, ACL &apos;02</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
	<note>Dropout: A simple way to prevent neural networks from overfitting</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Rnnbased encoder-decoder approach with word frequency estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<idno>CoRR abs/1701.00138</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Word translation prediction for morphologically rich languages with bilingual neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/D14-1175</idno>
		<ptr target="https://doi.org/10.3115/v1/D14-1175" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1676" to="1688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>CoRR abs/1212.5701</idno>
		<ptr target="http://arxiv.org/abs/1212.5701" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploiting source-side monolingual data in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D16-1160" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
