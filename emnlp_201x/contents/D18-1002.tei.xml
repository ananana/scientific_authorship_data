<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:24+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Removal of Demographic Attributes from Text Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Removal of Demographic Attributes from Text Data</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="11" to="21"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>11</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recent advances in Representation Learning and Adversarial Training seem to succeed in removing unwanted features from the learned representation. We show that demographic information of authors is encoded in-and can be recovered from-the intermediate representations learned by text-based neural classi-fiers. The implication is that decisions of clas-sifiers trained on textual data are not agnostic to-and likely condition on-demographic attributes. When attempting to remove such demographic information using adversarial training , we find that while the adversarial component achieves chance-level development-set accuracy during training, a post-hoc classi-fier, trained on the encoded sentences from the first part, still manages to reach substantially higher classification accuracies on the same data. This behavior is consistent across several tasks, demographic properties and datasets. We explore several techniques to improve the effectiveness of the adversarial component. Our main conclusion is a cautionary one: do not rely on the adversarial training to achieve invariant representation to sensitive features.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Consider automated systems that are used for de- termining credit ratings, setting insurance policy rates, or helping in hiring decisions about individ- uals. We would like such decisions to not take into account factors such as the gender or the race of the individual, or any other factor which we deem to be irrelevant to the decision. We refer to such irrelevant factors as protected attributes. The naive solution of not including protected attributes in the features to a Machine Learning system is insufficient: other features may be highly corre- lated with-and thus predictive of-the protected attributes ( <ref type="bibr" target="#b23">Pedreshi et al., 2008)</ref>. For example, in Credit Score modeling, text might help in credit score decisions ( <ref type="bibr" target="#b11">Ghailan et al., 2016)</ref>. By using the raw text as is, a discrimination issue might arise, as textual information can be predictive of some demographic factors ( <ref type="bibr" target="#b14">Hovy et al., 2015</ref>) and author's attributes might correlate with target vari- ables ( .</p><p>In this paper we are interested in language- based features. It is well established that textual information can be predictive of age, race, gender, and many other social factors of the author <ref type="bibr" target="#b16">(Koppel et al., 2002;</ref><ref type="bibr" target="#b3">Burger et al., 2011;</ref><ref type="bibr" target="#b22">Nguyen et al., 2013;</ref><ref type="bibr" target="#b30">Weren et al., 2014;</ref><ref type="bibr" target="#b27">Verhoeven and Daelemans, 2014;</ref><ref type="bibr" target="#b25">Rangel et al., 2016;</ref><ref type="bibr" target="#b1">Blodgett et al., 2016)</ref>, or even the audience of the text <ref type="bibr" target="#b29">(Voigt et al., 2018)</ref>. Thus, any system that incorporates raw text into its decision process is at risk of indirectly condi- tioning on such signals. Recent advances in repre- sentation learning suggest adversarial training as a mean to hide the protected attributes from the de- cision function (Section 2). We perform a series of experiments and show that: (1) Information about race, gender and age is indeed encoded into inter- mediate representations of neural networks, even when training for seemingly unrelated tasks and the training data is balanced in terms of the pro- tected attributes (Section 4); (2) The adversarial training method is indeed effective for reducing the amount of protected encoded information... (3) ...but in some cases even though the adversarial component seems to be doing a perfect job, a fair amount of protected information still remains, and can be extracted from the encoded representations (Section 5.1).</p><p>This suggests that when working with text data it is very easy to condition on sensitive properties by mistake. Even when explicitly using the adver- sarial training method to remove such properties, one should not blindly trust the adversary, and be careful to ensure the protected attributes are in-deed fully removed. We explore means for im- proving the effectiveness of the adversarial train- ing procedure (section 5.2). <ref type="bibr">1</ref> However, while successful to some extent, none of the methods fully succeed in removing all de- mographic information. Our main message, then, remains cautionary: if the goal is to ensure fair- ness or invariant representation, do not trust adversarial removal of features from text in- puts for achieving it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Learning Setup</head><p>We follow a setup in which we have some la- beled data D composed of documents x 1 , ..., x n and task labels y 1 , ..., y n . We wish to train a clas- sifier f that accurately predicts the main task la- bels y i . Each data point x i is also associated with a protected attribute z i , and we want the decision y i = f (x i ) to be oblivious to z i . Following ( <ref type="bibr" target="#b10">Ganin and Lempitsky, 2015;</ref><ref type="bibr" target="#b31">Xie et al., 2017)</ref>, we struc- ture f as an encoder h(x) that maps x into a rep- resentation vector h x , and a classifier c(h(x)) that is used for predicting y based on h x . If h x i is not predictive of z i , then the main task prediction</p><formula xml:id="formula_0">f (x i ) = c(h(x i )) does not depend on z i .</formula><p>We say that a protected attribute z has leaked if we can train a classifier c (h x i ) to predict z i with an accuracy beyond chance level, and that the pro- tected attribute is guarded if we cannot train such a classifier. We say that a classifier f (x) = c(h(x)) is guarded if z is guarded, and that it is leaky with respect to z if z leaked. Adversarial Training In order to make f obliv- ious to z, we follow the adversarial training setup ( <ref type="bibr" target="#b12">Goodfellow et al., 2014;</ref><ref type="bibr" target="#b10">Ganin and Lempitsky, 2015;</ref><ref type="bibr" target="#b0">Beutel et al., 2017;</ref><ref type="bibr" target="#b31">Xie et al., 2017)</ref>. Dur- ing training, an adversarial classifier adv(h x ) is trained to predict z, while the encoder h is trained to make adv fail. Concretely, the training proce- dure tries to jointly optimize both quantities:</p><formula xml:id="formula_1">arg min adv L(adv(h(x i )), z i ) arg min h,c L(c(h(x i )), y i ) − L(adv(h(x i )), z i )</formula><p>where L(y , y) is the loss function (in our case, cross entropy). This objective results in creating the representation h x s.t. it's maximally infor- mative for the main task, while at the same time minimally informative of the protected attribute. The optimization is performed in practice using the gradient-reversal layer (GRL) method ( <ref type="bibr" target="#b10">Ganin and Lempitsky, 2015)</ref>. The GRL is a layer g λ that is inserted between the encoded vector h x and the adversarial classifier adv. During the forward pass the layer acts as the identity, while during back- propagation it scales the gradients passed through it by −λ, causing the encoder to receive the op- posite gradients from the adversary. The meta- parameter λ controls the intensity of the reversal layer. This results in the objective:</p><formula xml:id="formula_2">arg min h,c,adv L(c(h(x i )), y i )+L(adv(g λ (h(x i ))), z i )</formula><p>Attacker Network To test the effectiveness of the adversarial training, we use an attacker net- work att(h x ). After the classifier c(h(x)) is fully trained, we use the encoder to obtain representa- tions h, and train the attacker network to predict z based on h, without access to the encoder or to the original inputs x that resulted in h. If, after training, the attacker can predict z on unseen ex- amples with an accuracy of beyond chance level, then the attribute z leaked to the representation, and the classifier is not guarded. Network Architecture In our setup, an example x i is a sequence of tokens w 1 , ..., w m i and the en- coder is a one layer LSTM network that reads in the associated embedding vectors and returns the final state: h = LST M (w 1:m ). The classifier c and the adversarial adv are both multi-layer per- ceptrons with one hidden layer, sharing the same hidden layer size and activation function (tanh). <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data, Tasks, and Protected Attributes</head><p>To perform our experiments, we need a reasonably large dataset in which the data-points x contain textual information, and for which we have both main-task labels y and protected attribute labels z. While our motivating example used prediction tasks for credit rating, insurance rates or hiring decisions, to the best of our knowledge there are no publicly available datasets for these sensitive tasks that meet our criteria. We thus opted to use much less sensitive main-tasks, for which we can obtain the needed data. We focus on Twitter mes- sages, and our protected attributes are binary-race (non-hispanic Whites vs. non-hispanic Blacks), binary-gender (Male vs. Female) <ref type="bibr">3</ref> and binary- age <ref type="bibr">(18-34 vs. 35+)</ref>. As main tasks we chose binary emoji-based sentiment prediction and bi- nary tweet-mention prediction. Both the sentiment and the mention prediction tasks are not inher- ently correlated with race, gender or age. Pro- tected attributes leakage in these seemingly benign main-tasks is a strong indicator that such leakage is likely to occur also in more sensitive tasks.</p><p>Main Tasks: Sentiment and Mention-detection Both tasks can be derived automatically from twit- ter data. We construct a binary "sentiment" task by identifying a subset of emojis which are associated with positive and negative sentiment, 4 identify- ing tweets containing these emojis, assigning them with the corresponding sentiment and removing the emojis. Tweets containing emojis from both sentiment lists are discarded. The binary men- tion task is to determine if a tweet mentions an- other user, i.e, classifying conversational vs. non- conversational tweets. We derive this dataset by identifying tweets that include @mentions tokens, and removing all such tokens from the tweets.</p><p>Protected: Race The race annotation is based on the dialectal tweets (DIAL) corpus from <ref type="bibr" target="#b1">(Blodgett et al., 2016</ref>), consisting of 59.2 million tweets by 2.8 million users. Each tweet is associated with predicted "race" information which was predicted using a technique that takes into account the geo- location of the author and the words in the tweet. We focus on the AAE (African-American English) and SAE (Standard American English) categories, which we use as proxies for non-Hispanic blacks and non-Hispanic whites.</p><p>We chose only annotations with confidence (the probability of the authors' race) of above 80%. Due to its construction, the race annotations in this dataset are highly correlated with the language be- ing used. As such, the data reflects an extreme case in which the underlying language is very pre- dictive of the protected attribute.</p><p>Protected: Age and Gender We use data from the PAN16 dataset ( <ref type="bibr" target="#b25">Rangel et al., 2016)</ref>, contain- ing manually annotated Age and Gender informa- tion of 436 Twitter users, along with up to 1k tweets for each user. User annotation was per- formed by consulting the user's LinkedIn profile. Gender was determined by considering the user's name and photograph, discarding unclear cases. Age range was determined by birth-date which was published on the user's profile, or by mapping their degree starting date.</p><p>Data-splits From the DIAL corpus we extracted 166K and 10K tweets for training and develop- ment purpose respectively (after cleaning and ex- tracting relevant tweets), whereas for the PAN16 dataset we collected 160K tweets for training and 10K for development. The train/development split in both phases of the training (task-training and attacker-training) is the same. This is the worst possible scenario for the attacker, as it is train- ing on the exact representations the adversary at- tempted to remove the protected attribute from. Each split is balanced with respect to both the main and the protected labels: a random prediction of each variable is likely to result in 50% accuracy.</p><p>Metrics Throughout this paper, we measure leakage using accuracy. We say that the protected attribute has leaked if an attacker manages to pre- dict the protected attribute with better than 50% accuracy, which is always the probability of that attribute (P (Z) = 0.5). In Appendix A we relate our metric to more standard fairness metrics, and prove that in our setup a guarded predictor guar- antees demographic parity, equality of odds, and equality of opportunity. Note however that we also show empirically that such guarded predictors are very hard to attain in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Baselines and Data Leakage</head><p>In-dataset Accuracy Upper-bounds We begin by examining how well can we perform on each task (both main-tasks and protected attributes) when training the encoder and classifier directly on that task, without any adversarial component. This provides an upper bound on the protected at- tribute leakage for the main tasks results. The re- sults in <ref type="table">Table 1</ref> indicate that the classifiers achieve reasonable accuracies for the main tasks. <ref type="bibr">5</ref> For</p><formula xml:id="formula_3">M+P+ M+P− M−P− M−P+ (a) balanced M+P+ M+P− M−P− M−P+ (b) unbalanced Figure 1: Balanced (a) vs. Unbalanced (b) dataset.</formula><p>Red(M+)/Blue(M-): Main Task. Light(P+)/Dark(P-): Protected attribute. Each class is globally balanced, but in (b) the propor- tion of the protected attribute within each main task split is unbalanced. the protected attributes, race is highly predictable (83.9%) while age and gender can also be recov- ered at above 64% accuracy.  <ref type="table">Table 1</ref>: Accuracies when training directly towards a single task.</p><p>Leakage When training directly for the protected attributes, we can recover them with relatively high accuracies. But is information about them being encoded when we train on the main tasks? In this set of experiments, we encode the train- ing and validation sets using the encoder trained on the main task, and train the attacker network to predict the protected attributes based on these vectors. This experiment suggests an upper bound on the amount of leakage of protected attributes when we do not actively attempt to prevent it. The Balanced section in <ref type="table" target="#tab_2">Table 2</ref> summarizes the validation-set accuracies. While the numbers are lower than when training directly <ref type="table">(Table 1)</ref>, they are still high enough to extract meaningful and possibly highly sensitive information (e.g. DIAL Race direct prediction is 83.9% while DIAL Race leakage on the balanced Sentiment task is 64.5%).</p><p>Leakage: Unbalanced Data The datasets we considered were perfectly balanced with respect to both main task and protected attribute labels <ref type="figure">(Fig- ure 1a)</ref>. Such extreme case is not representative of real-world datasets, in which a dataset may be well balanced w.r.t. the main task labels but not the protected attribute. For example, when train- ing a classifier to predict a fit for managerial po- sition based on Curriculum Vitae (CV) of candi- dates, the CV dataset may be perfectly balanced according to the managerial / non-managerial vari- able, but, because of existing social biases, CVs of females might be under-represented in the man- agerial category and over-represented in the non- managerial one. In such a situation, the classi- fier may perpetuate the bias by learning to favor males over females for managerial positions. We simulate this more realistic scenario by construct- ing unbalanced datasets in which the main tasks (sentiment/mention) remain balanced but the pro- tected class proportions within each main class are not, as demonstrated in <ref type="figure">Figure 1b</ref>. For example, in the sentiment/gender case, we set the positive- sentiment class to contain 80% male and 20% fe- male tweets, while the negative-sentiment class contains 20% male and 80% female tweets. We then follow the leakage experiment on the unbal- anced datasets. The attacker is trained and tested on a balanced dataset. Otherwise, the attacker can perform quite well on the male/female task simply by learning to predict sentiment, which does not reflect leakage of gender data to the representa- tion. When training the attacker on balanced data, its decisions cannot rely on the sentiment informa- tion encoded in the vectors, and must look for en- coded information about the protected attributes. The results in <ref type="table" target="#tab_2">Table 2</ref> indicate that both task accu- racy and attribute leakage are stronger in the un- balanced case.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Mitigating Data Leakage</head><p>Leakage of protected attributes information into the internal representation of the network when training on seemingly unrelated tasks is very com- mon. We explore the means of mitigating such leakage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Adversarial Training</head><p>We repeat the experiments in <ref type="table" target="#tab_2">Table 2</ref> with an ad- versarial component ( <ref type="bibr" target="#b10">Ganin and Lempitsky, 2015)</ref> as described in Section 2, in order to actively re- move the protected attribute information from the encoded representation during training. Note that the adversarial objective is in odds with the main- task one: by removing the protected attribute in- formation from the encoder, we may also hurt its ability to encode information about the main task. <ref type="figure" target="#fig_0">Figure 2</ref> shows the main task and adversary pre- diction accuracies on the development set as train- ing progresses, for the Sentiment/Race pair. After an initial peak in task prediction accuracy, the adversary prediction drops and starts to fluctu- ate around chance level (50%), as desired, along with a drop in main task accuracy as well. The adversary's accuracy remain around chance level throughout the entire training process, suggesting that the adversarial training is indeed effective for removing the protected attribute information from the encoded representation. These trends are per- sistent for all main-task/protected-attribute pairs we tried.</p><p>However, training the attacker network on the resulting encoder vectors reveals a different story. For example, when considering the encoder after 50 training epochs (adversary accuracy of 49.0%), the attacker reaches 56.0% accuracy: substantially higher than the adversarial's success rate, despite sharing the exact same architecture, and being trained and tested on the exact same dataset. <ref type="table" target="#tab_4">Table 3</ref> summarizes the attacker's recovery rate on the adversarialy-trained encoders for the differ- ent settings. In all cases, the adversarial's success rate is around 50%, while the attacker's rate is sub- stantially higher. The attacker's rates are lower than in <ref type="table">Table 1</ref>, indicating the adversarial train- ing is effective in removing some of the protected attribute. However, a substantial amount of in- formation managed to leak past the adversary, despite its seemingly perfect performance.  Attacker's Accuracy on Unseen Data We ver- ify that the attacker's recovery accuracy persists also on the more realistic scenario in which the attacker is applied to encoded sentences that did not participate in the adversarial training. We con- structed an additional dataset of 166K completely unseen samples from the Sentiment/Race case. As expected, the attacker works even better in this case, reaching an accuracy of 59.7% Vs. 56.0% on the original development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Strengthening the Adversarial Component</head><p>We explore means of strengthening the adversarial component, by tuning its capacity and its weight, as well as by using a novel adversarial-ensemble configuration.</p><p>Capacity We increase the capacity of the adver- sarial component by increasing its hidden dimen- sion, while keeping the attacker's hidden dimen- sion constant at 300 dimensions. We try hidden di- mensions of size 500, 1000, 2000, 5000 and 8000.</p><p>Weight We experiment with different weighting of the adversarial component during training by tuning the λ parameter, trying the values 0.5, 1.0 (default), 1.5, 2, 3, 5 (with values above 5 the main task training became extremely unstable, not rais- ing above 50%).</p><p>Ensemble An alternative to using larger λ val- ues is to introduce several adversaries. The po- tential benefit of this approach is that rather than focusing harder on removing a single feature, here the different adversaries could each focus on a dif- ferent aspect of the representation. This approach is potentially better suited to deal with language variability. Concretely, we suggest the following adaptation to the adversarial loss to incorporate k adversaries with different random initializations:</p><formula xml:id="formula_4">L y (c(h(x)), y) + k j=1 L z (adv j (g λ (h(x))), z)</formula><p>Other Attempts We also experienced with sev- eral other techniques: reinitializing the adversar- ial weights every t epochs; training the adver- sary without propagating the error to the encoder components for t epochs and only then starting to propagate; using adversaries with more hidden layers; adding dropout on the encoded vectors and within the encoder. None of these yielded im- provements over the above methods.</p><p>Results All methods are effective to some ex- tent, <ref type="table">Table 4</ref> summarizes the results. Increasing the capacity of the adversarial net- work helped reduce the protected attribute's leak- age, though different capacities work best on each setup. On the Sentiment/Race task, none of the higher dimensional adversaries worked better than the 300-dim one, on the PAN16 dataset it did. On PAN16/Gender the 8000-dim adversary per- formed best, and on PAN16/Age, the 500-dim one.</p><p>Increasing the weight of the adversary through the λ parameter also has a positive effect on the result (except on the Sentiment/Race pair). How- ever, too large λ values make training unstable, and require many more epochs for the main-task to stabilize around a satisfying accuracy.</p><p>The adversarial ensemble method with 2 adver- saries achieves 57.4% on Sentiment/Race, as op- posed to 56.0% with a single one, but when using 5 different adversaries, we achieve 54.8%. On the PAN16 dataset larger ensembles are more effec- tive. However, a potential issue with the ensemble method is that larger ensembles reduces training stability, similar to increasing the λ value. For ex- ample, with 5 adversaries, the main-task accuracy remained at random for 5 epochs, and only begun rising at the 6th epoch. Using 10 adversaries, the main task could not be trained.</p><p>To summarize, while all methods are effective to some extent, it appears that (a) no method and parameter setting performs equally well across the different setups; and (b) no method succeeds in completely preventing the leakage of the protected attributes. Combining the different methods (en- sembles of larger networks, larger networks with larger λ, etc.) did not improve the results.</p><p>Unbalanced Data Results We repeated the same set of experiments on the unbalanced Sen- timent/Race corpus <ref type="table" target="#tab_7">(Table 5)</ref>. In this setup, the results are somewhat similar: increasing the ad- versarial capacity and λ is ineffective, and even increases the attacker's recovery rate. However, using an ensemble of 5 adversaries does manage to reduce the leakage, but it is still far from a sat- isfying result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head><p>The gap between the adversary's dev-set accu- racy and the after-the-fact attacker accuracy on the same data is surprising. To better understand the phenomenon, we perform further analysis on the Sentiment/Race pair with the default single adver- sary.</p><p>Embedding Vs. RNN Recall that the attacker network tries to extract as much information from  <ref type="table">Table 4</ref>: Results of different adversarial configurations. Sentiment/Mention: main task accuracy.</p><note type="other">65.0 7.1 4.8 75.7 5.4 4.2 71.9 9.8 7.3 λ 0.5 63.9 6.8 6.2 75.6 7.8 6.8 73.1 4.8 3.4 1.5 64.9 7.4 5.4 75.6 4.9 2.4 72.5 6.8 5.8 2.0 64.2 7.3 5.9 76.0 -7.2 6.7 72.1 8.5 7.7 3.0 65.8 10.2 10.1 73.7 6.4 6.1 72.5 -6.3 5.2 5.0 50.0 - - 73.6 6.5 5.7 69.0 3.2 2.9 Ensemble 2 62.4 7.4 5.4 74.8 6.4 5.0 72.8 8.8 8.3 3 66.5 6.5 5.0 75.3 4.9 3.1 72.1 6.7 6.0 5 63.8 4.8 2.6 74.3 4.1 3.0 70.1 5.7 5.4</note><p>Race/Gender/Age: protected attribute recovery difference from 50% rate by the attacker (values be- low 50% are as informative as those above it). ∆: the difference between the attacker score and the corresponding adversary's accuracy. The bold numbers are the best oblivious classifiers within each configuration.  the encoder's output as possible. The encoder con- sists of two components: (1) Embedding Matrix and (2) an RNN. Therefore, the leakage can be caused due to one of them (or due to their com- bination).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>We conduct the following experiment to deter- mine which part affects the leakage more: we cre- ate a new encoder by composing 2 existing en- coders: an encoder with high leakage (Leaky, us- ing the baseline encoder) and an encoder with low leakage (Guarded, using the 5-Ensemble adver- sary). We fuse the two encoders by combining the embedding matrix of the Leaky encoder with the RNN module of the Guarded encoder, and vice versa. This yields two new encoders: an en- coder with a "leaky" Embedding Matrix module and a "strong" RNN module (Leaky-EMB), and an encoder with a "strong" Embedding Matrix mod- ule and a "leaky" RNN module (Leaky-RNN). We compare encoders Leaky-EMB and Leaky-RNN to gauge which module has a greater contribution to the data leakage. We train attacker-networks over the encoders' output to predict the protected at- tributes.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consistent Leakage: Examples Inspection</head><p>We are interested in tweets whose protected at- tribute (race) is correctly predicted by the adver- sary. However, at accuracy rates below 60%, many of the correct predictions could be attributed to chance. To identify the relevant examples, we re- peated the Sentiment/Race default adversary ex- periment 10 times with different random seeds. We then trained 10 attacker networks, and used each of them to label all examples in the devel- opment set. We then looked for tweets which are consistently and correctly classified by at least 9 attackers. 7 <ref type="table" target="#tab_11">Table 7</ref> shows some of these cases. Many of them include tokens (Naw, Bestfrand, tan) and syntactic structures (Going over Bae house) which are indeed predictive, though not the most salient features.</p><p>Leakage via Embeddings Even though we found out the RNN is much more responsible to the leakage then the Embedding, those still contribute to the leakage and are easier to in- spect. Therefore, we turn to inspect the en- coders' Embedding. We hypothesize that a pos- sible reason for the adversarial network's inabil- ity to completely remove the protected race infor- mation is word frequency. Namely, rare words, which might be strongly identified with one group, didn't get enough updates during training and therefore remained predictive towards one of the groups. To quantify this, we compared two vo- cabularies: words appearing in tweets where the predictions were consistently predicted (9 or 10 out of 10 times) by the different attackers, and words appearing in tweets that were randomly dis- tributed (50%) between the attackers. If our hy- pothesis is correct, we expect words from the sec- ond group to be more frequent than words in the first group. We discard words appearing in both groups, and associate each word with its training set frequency. One-tailed Mann-Whitney U test <ref type="bibr" target="#b19">(Mann and Whitney, 1947)</ref> showed the effect is highly significant with p &lt; e −12 .</p><p>Data Overfitting? Standard ML setups often suffer from overfitting on the training data, es- pecially when using neural-networks which tend to memorize the data they encounter. In the ad- versarial setup, the overfitting could result in the encoder-adversary pair working together to per- fectly clean the attributes from the training data, <ref type="bibr">7</ref> 776 correct and 946 consistent examples in total without generalization. Such overfitting could ex- plain the attacker success. Is this what happened? We test this hypothesis by using the same at- tacker networks experiments solely on the train- ing data. We train the attackers on 90% of the training data while using the rest 10% as held- out. If overfitting has occurred, the accuracy is likely to result in 50% accuracy. Alas, this is not the case. <ref type="table" target="#tab_12">Table 8</ref> summarize the training accura- cies of the attacker network. The Mention/Race task achieves the highest score of 64.3% whereas the Mention/Gender task achieves the lowest - 58.1%. Even though when trained directly to pre- dict these attributes without the adversarial setup, the training accuracies are much higher, a substan- tial amount of signal is still left, even in the train- ing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>The fact that intermediary vector representa- tions that are trained for one task are predic- tive of another is not surprising: it is at the core of the success of NLP methods for deriv- ing "generic" word and sentence representations (e.g. <ref type="bibr">Word2vec (Mikolov et al., 2013)</ref>, Skip- thought vectors ( <ref type="bibr" target="#b15">Kiros et al., 2015)</ref>, Contextual- ized Word Representations ( <ref type="bibr" target="#b20">Melamud et al., 2016;</ref><ref type="bibr" target="#b24">Peters et al., 2018</ref>) etc.). While usually consid- ered a positive feature, it can often have unde- sired consequences one should be aware of and po- tentially control for. Several works document bi- ases and stereotypes that are captured by unsuper- vised word embeddings ( <ref type="bibr" target="#b2">Bolukbasi et al., 2016;</ref><ref type="bibr" target="#b4">Caliskan et al., 2017</ref>) and ways of mitigating them <ref type="bibr" target="#b2">(Bolukbasi et al., 2016;</ref><ref type="bibr" target="#b33">Zhang et al., 2018</ref>). Bias and stereotyping were also documented on a com- mon NLP dataset <ref type="bibr" target="#b26">(Rudinger et al., 2017)</ref>. While these work are concerned with the learned rep- resentations encoding unwanted biases about the world, our concern is with capturing potentially sensitive demographic information about individ- ual authors of the text.</p><p>Removing sensitive attributes (demographic or otherwise) from intermediate representations in order to achieve fair classification has been ex- plored by solving an optimization problem ( <ref type="bibr" target="#b32">Zemel et al., 2013</ref>), as well as by employing adversar- ial training ( <ref type="bibr" target="#b7">Edwards and Storkey, 2015;</ref><ref type="bibr" target="#b18">Louizos et al., 2015;</ref><ref type="bibr" target="#b31">Xie et al., 2017;</ref><ref type="bibr" target="#b33">Zhang et al., 2018)</ref>, focusing on structured features. Adversarial train- ing was also applied for Image anonymization AAE ("non-hispanic blacks") SAE ("non-hispanic whites") My Brew Eattin I want to be tan again Naw im cool Why is it so hot in the house ?! Tonoght was cool Been doing Spanish homework for 2 hours . My momma Bestfrand died I wish I was still in Spain Enoy yall day Ahhhhh so much homework . Going over Bae house TWITTER-ENTITY I miss you too ! She not texting or calling ? Ok I want to move to california Real relationships go thru real shit Lol , I don't even go here . About to spend my entire check IDGAF Ahhhhh so much homework . Getting ready for school I'm so tired .  ( <ref type="bibr" target="#b7">Edwards and Storkey, 2015;</ref><ref type="bibr" target="#b9">Feutry et al., 2018)</ref>. In contrast, we consider features that are based on short user-authored text. Several works apply adversarial training to tex- tual data, in order to learn encoders that are in- variant to some properties of the text <ref type="bibr" target="#b5">(Chen et al., 2016;</ref><ref type="bibr" target="#b6">Conneau et al., 2017;</ref><ref type="bibr" target="#b34">Zhang et al., 2017;</ref><ref type="bibr" target="#b31">Xie et al., 2017)</ref>. As their main motivation is to remove information about domain or language in order to improve transfer learning, domain adaptation, or end task accuracy, they were less concerned with the ability to recover information from the result- ing representation, and did not evaluate it directly as we do here.</p><p>Recent work on creating private representation in the text domain ( <ref type="bibr" target="#b17">Li et al., 2018)</ref> share our mo- tivation of removing unintended demographic at- tributes from the learned representation using ad- versarial training. However, they report only the discrimination accuracies of the adversarial com- ponent, and do not train another classifier to verify that the representations are indeed clear of the pro- tected attribute. As our work shows, trusting the adversary is insufficient, and external verification is crucial.</p><p>Finally, our work is motivated by the desire for fairness. We use a definition in which a fair classi- fication is one that does not condition on a certain attribute (fairness by blindness), and evaluate the ability to achieve text-derived representations that are blind to a property we wish to protect. Many other definitions of fairness exist, including demo- graphic parity, equality of odds and equality of opportunity (see e.g. discussion in <ref type="bibr" target="#b13">(Hardt et al., 2016;</ref><ref type="bibr" target="#b0">Beutel et al., 2017)</ref>). Under our setup, blind- ness guarantees these metrics (Appendix A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>We show that demographic information leaks into intermediate representations of neural networks trained on text data. Systems that train on text data and do not want to condition on demographic in- formation must take active steps against accidental conditioning. Our experiments suggest that: (1) Adversarial training is effective for mitigating protected attribute leakage, but, when dealing with text data, may fail to remove it completely. (2) When using the adversarial training method, the adversary score during training cannot be trusted, and must be verified with an externally- trained attacker, preferably on unseen data. (3) Tuning the capacity and weight of the adver- sary, as well as using an ensemble of several ad- versaries, can improve the results. However, no single method is the most effective in all cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Main task and Adversary accuracy curves for Sentiment/Race.</figDesc><graphic url="image-3.png" coords="5,73.13,499.93,216.00,144.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Protected attribute leakage: balanced &amp; unbalanced data splits. 

emoji usage is highly correlated with these prop-
erties. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performances on different datasets with 
an adversarial training. ∆ is the difference be-
tween the attacker score and the corresponding ad-
versary's accuracy. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Unbalanced Sentiment/Race with the dif-
ferent methods. Sentiment: task accuracy. Race: 
Attacker's recovery accuracy beyond 50%. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Accuracies of the protected attribute with 
different encoders. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 6 summarize</head><label>6</label><figDesc>the results, implying that the leakage is caused mainly by the RNN, and less by the Embedding Matrix. 6</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Examples for correct dialectal/race predictions, which were predicted consistently by at least 9 
different attacker-classifiers. 

Data 
Task 
Protected 
Attribute 
∆ 

DIAL 
Sentiment Race 
12.2 
Mention 
Race 
14.3 
PAN16 Mention 
Gender 
8.1 
Mention 
Age 
9.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Attacker's performance on different 
datasets. Results are on a training set 10% held-
out. ∆ is the difference between the attacker score 
and the corresponding adversary's accuracy. 

</table></figure>

			<note place="foot" n="1"> The code and data acquisition are available in: https: //github.com/yanaiela/demog-text-removal</note>

			<note place="foot" n="2"> Further details regarding the architecture and training parameters can be found in the supplementary materials.</note>

			<note place="foot" n="3"> While gender is a non-binary construct, many decisions in the real-world are unfortunately still influenced by hard binary gender categories. We thus consider binary-gender to be a useful approximation in our context. 4 Complete list is available in Appendix C</note>

			<note place="foot" n="5"> While the sentiment score may seem low, we manually verified the erroneous predictions and found out that many of them are indeed ambiguous with respect to sentiment, e.g. sentences like &quot;I can&apos;t take Amanda seriously &quot; and &quot;You make me so angry, yet you make me so happy. &quot; which were predicted negative and positive respectively, but their gold label was the opposite.</note>

			<note place="foot" n="6"> A discrepancy exists to some extent in the new encoders, as their parts originate from different models that were trained separately. To test if the fusion is valid, we train a different classifier on top of the new encoders to predict the main task. The combination of the leaked RNN with the guarded embeddings results in 65.4% on the sentiment task and the other combination results in 60.9% as opposed to 67.5% and 63.8% on the leaked and guarded models, respectively. As the new models are on par with the original ones, we conclude that the new encoders are valid.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Moni Shahar, Felix Kreuk, Yova Kementchedjhieva and the BIU NLP lab for fruitful conversation and helpful comments. We also thank Su Lin Blodgett for her help in sup-plying the DIAL dataset and clarifications. This work was supported in part by the The Israeli Sci-ence Foundation (grant number 1555/15) and Ger-man Research Foundation via the German-Israeli Project Cooperation (DIP, grant DA 1600/1-1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00075</idno>
		<title level="m">Data decisions and theoretical implications when adversarially learning fair representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Demographic dialectal variation in social media: A case study of african-american english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Su Lin Blodgett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan O&amp;apos;</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1119" to="1130" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Man is to computer programmer as woman is to homemaker? debiasing word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam T</forename><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4349" to="4357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discriminating gender on twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zarrella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1301" to="1309" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantics derived automatically from language corpora contain human-like biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aylin</forename><surname>Caliskan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><forename type="middle">J</forename><surname>Bryson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">356</biblScope>
			<biblScope unit="issue">6334</biblScope>
			<biblScope unit="page" from="183" to="186" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adversarial deep averaging networks for cross-lingual sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01614</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04087</idno>
		<title level="m">Word translation without parallel data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05897</idno>
		<title level="m">Censoring representations with an adversary</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjarke</forename><surname>Felbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Mislove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sune</forename><surname>Iyad Rahwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning anonymized representations with adversarial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Feutry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Piantanida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Duhamel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09386</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving credit scorecard modeling through applying text analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Ghailan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Hoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osman</forename><surname>Mokhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hegazy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">institutions</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Equality of opportunity in supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nati</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3315" to="3323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">User review sites as a resource for largescale sociolinguistic studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Johannsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Automatically categorizing written texts by author gender. Literary and linguistic computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat Rachel</forename><surname>Shimoni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="401" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards robust and privacy-preserving text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00830</idno>
		<title level="m">The variational fair autoencoder</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">On a test of whether one of two random variables is stochastically larger than the other. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald R</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whitney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1947" />
			<biblScope unit="page" from="50" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning generic context embedding with bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="51" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">how old do you think i am?&quot; a study of language and age in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rilana</forename><surname>Gravel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dolf</forename><surname>Trieschnigg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Meder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICWSM</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discrimination-aware data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dino</forename><surname>Pedreshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Turini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="560" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Overview of the 4th author profiling task at pan 2016: cross-genre evaluations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Verhoeven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes Papers of the CLEF 2016 Evaluation Labs. CEUR Workshop Proceedings/Balog, Krisztian</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="750" to="784" />
		</imprint>
	</monogr>
	<note>et al.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Social bias in elicited natural language inferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandler</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</title>
		<meeting>the First ACL Workshop on Ethics in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="74" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Clips stylometry investigation (csi) corpus: A dutch corpus for the detection of age, gender, personality, sentiment and deception in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Verhoeven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Daelemans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC 2014-NINTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3081" to="3085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Twisty: A multilingual twitter stylometry corpus for gender and personality profiling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Verhoeven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rtgender: A corpus for studying differential responses to gender</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Voigt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinodkumar</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Examining multiple features for author profiling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename><forename type="middle">U</forename><surname>Edson Rd Weren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Kauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mizusaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viviane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leandro</forename><forename type="middle">K</forename><surname>Palazzo M De Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of information and data management</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">266</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Controllable invariance through adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="585" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning fair representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toni</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="325" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Mitigating unwanted biases with adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Hu Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Lemoine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07593</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Aspect-augmented adversarial networks for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="515" to="528" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Men also like shopping: Reducing gender bias amplification using corpus-level constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2979" to="2989" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
