<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Unified Model for Word Sense Representation and Disambiguation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Unified Model for Word Sense Representation and Disambiguation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1025" to="1035"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Most word representation methods assume that each word owns a single semantic vector. This is usually problematic because lexical ambiguity is ubiquitous, which is also the problem to be resolved by word sense disambiguation. In this paper, we present a unified model for joint word sense representation and disambiguation, which will assign distinct representations for each word sense. 1 The basic idea is that both word sense representation (WS-R) and word sense disambiguation (WS-D) will benefit from each other: (1) high-quality WSR will capture rich information about words and senses, which should be helpful for WSD, and (2) high-quality WSD will provide reliable disambiguat-ed corpora for learning better sense representations. Experimental results show that, our model improves the performance of contextual word similarity compared to existing WSR methods, outperforms state-of-the-art supervised methods on domain-specific WSD, and achieves competitive performance on coarse-grained all-words WSD.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word representation aims to build vectors for each word based on its context in a large corpus, usually capturing both semantic and syntactic information of words. These representations can be used as features or inputs, which are widely employed in information retrieval ( <ref type="bibr" target="#b19">Manning et al., 2008)</ref>, doc- ument classification <ref type="bibr" target="#b35">(Sebastiani, 2002</ref>) and other NLP tasks. <ref type="bibr" target="#b0">1</ref> Our sense representations can be downloaded at http: //pan.baidu.com/s/1eQcPK8i.</p><p>Most word representation methods assume each word owns a single vector. However, this is usual- ly problematic due to the homonymy and polyse- my of many words. To remedy the issue, <ref type="bibr" target="#b33">Reisinger and Mooney (2010)</ref> proposed a multi-prototype vector space model, where the contexts of each word are first clustered into groups, and then each cluster generates a distinct prototype vector for a word by averaging over all context vectors with- in the cluster. <ref type="bibr" target="#b11">Huang et al. (2012)</ref> followed this idea, but introduced continuous distributed vectors based on probabilistic neural language models for word representations.</p><p>These cluster-based models conduct unsuper- vised word sense induction by clustering word contexts and, thus, suffer from the following is- sues:</p><p>• It is usually difficult for these cluster-based models to determine the number of cluster- s. <ref type="bibr" target="#b11">Huang et al. (2012)</ref> simply cluster word contexts into static K clusters for each word, which is arbitrary and may introduce mis- takes.</p><p>• These cluster-based models are typically off- line , so they cannot be efficiently adapted to new senses, new words or new data.</p><p>• It is also troublesome to find the sense that a word prototype corresponds to; thus, these cluster-based models cannot be directly used to perform word sense disambiguation.</p><p>In reality, many large knowledge bases have been constructed with word senses available online, such as WordNet <ref type="bibr" target="#b23">(Miller, 1995)</ref> and Wikipedia. Utilizing these knowledge bases to learn word representation and sense representation is a natural choice. In this paper, we present a uni- fied model for both word sense representation and disambiguation based on these knowledge bases and large-scale text corpora. The unified model can (1) perform word sense disambiguation based on vector representations, and (2) learn continu- ous distributed vector representation for word and sense jointly.</p><p>The basic idea is that, the tasks of word sense representation (WSR) and word sense disam- biguation (WSD) can benefit from each other: (1) high-quality WSR will capture rich semantic and syntactic information of words and senses, which should be helpful for WSD; (2) high-quality WS- D will provide reliable disambiguated corpora for learning better sense representations.</p><p>By utilizing these knowledge bases, the prob- lem mentioned above can be overcome:</p><p>• The number of senses of a word can be de- cided by the expert annotators or web users.</p><p>• When a new sense appears, our model can be easily applied to obtain a new sense represen- tation.</p><p>• Every sense vector has a corresponding sense in these knowledge bases.</p><p>We conduct experiments to investigate the per- formance of our model for both WSR and WS- D. We evaluate the performance of WSR using a contextual word similarity task, and results show that out model can significantly improve the cor- relation with human judgments compared to base- lines. We further evaluate the performance on both domain-specific WSD and coarse-grained all- words WSD, and results show that our model yields performance competitive with state-of-the- art supervised approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>We describe our method as a 3-stage process:</p><p>1. Initializing word vectors and sense vectors.</p><p>Given large amounts of text data, we first use the Skip-gram model ( <ref type="bibr" target="#b20">Mikolov et al., 2013</ref>), a neural network based language model, to learn word vectors. Then, we assign vector representations for senses based on their def- initions (e.g, glosses in WordNet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Performing word sense disambiguation.</head><p>Given word vectors and sense vectors, we propose two simple and efficient WSD algo- rithms to obtain more relevant occurrences for each sense.</p><p>3. Learning sense vectors from relevant oc- currences. Based on the relevant occur- rences of ambiguous words, we modify the training objective of Skip-gram to learn word vectors and sense vectors jointly. Then, we obtain the sense vectors directly from the model.</p><p>Before illustrating the three stages of our method in Sections 2.2, 2.3 and 2.4, we briefly introduce our sense inventory, WordNet, in Sec- tion 2.1. Note that, although our experiments will use the WordNet sense inventory, our model is not limited to this particular lexicon. Other knowledge bases containing word sense distinctions and defi- nitions can also serve as input to our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">WordNet</head><p>WordNet <ref type="bibr" target="#b23">(Miller, 1995)</ref> is the most widely used computational lexicon of English where a concep- t is represented as a synonym set, or synset. The words in the same synset share a common mean- ing. Each synset has a textual definition, or gloss. <ref type="table">Table 1</ref> shows the synsets and the corresponding glosses of the two common senses of bank.</p><p>Before introducing the method in detail, we in- troduce the notations. The unlabeled texts are de- noted as R, and the vocabulary of the texts is de- noted as W . For a word w in W , w s i is the ith sense in WordNet W N. Each sense w s i has a gloss gloss(w s i ) in W N. The word embedding of w is denoted as vec(w), and the sense embedding of its ith sense w s i is denoted as vec(w s i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Initializing Word Vectors and Sense Vectors</head><p>Initializing word vectors. First, we use Skip- gram to train the word vectors from large amounts of text data. We choose Skip-gram for its sim- plicity and effectiveness. The training objective of Skip-gram is to train word vector representations that are good at predicting its context in the same sentence ( <ref type="bibr" target="#b20">Mikolov et al., 2013</ref>). More formally, given a sequence of training words w 1 , w 2 , w 3 ,...,w T , the objective of Skip- gram is to maximize the average log probability</p><formula xml:id="formula_0">1 T T ∑ t=1 ∑ −k≤ j≤k, j =0 log p(w t+ j |w t ) (1)</formula><p>where k is the size of the training window. The inner summation spans from −k to k to compute Sense Synset Gloss bank s <ref type="bibr" target="#b0">1</ref> (sloping land (especially the slope beside a body of water)) bank "they pulled the canoe up on the bank"; "he sat on the bank of the river and watched the currents" bank s 2 depository institution, (a financial institution that accepts deposits and channels the bank, money into lending activities) banking concern, "he cashed a check at the bank"; banking company "that bank holds the mortgage on my home" <ref type="table">Table 1</ref>: Example of a synset in WordNet.</p><p>the log probability of correctly predicting the word w t+ j given the word in the middle w t . The outer summation covers all words in the training data. The prediction task is performed via softmax, a multiclass classifier. There, we have</p><formula xml:id="formula_1">p(w t+ j |w t ) = exp(vec (w t+ j ) vec(w t )) ∑ W w=1 exp(vec (w) vec(w t ))<label>(2)</label></formula><p>where vec(w) and vec (w) are the "input" and "output" vector representations of w. This formu- lation is impractical because the cost of comput- ing p(w t+ j |w t ) is proportional to W , which is often large( 10 5 − 10 7 terms).</p><p>Initializing sense vectors. After learning the word vectors using the Skip-gram model, we ini- tialize the sense vectors based on the glosses of senses. The basic idea of the sense vector initial- ization is to represent the sense by using the sim- ilar words in the gloss. From the content words in the gloss, we select those words whose cosine similarities with the original word are larger than a similarity threshold δ . Formally, for each sense w s i in W N, we first define a candidate set from gloss(w s i )</p><formula xml:id="formula_2">cand(w s i ) = {u|u ∈ gloss(w s i ), u = w, POS(u) ∈ CW, cos(vec(w), vec(u)) &gt; δ }<label>(3)</label></formula><p>where POS(u) is the part-of-speech tagging of the word u and CW is the set of all possible part-of- speech tags that content words could have. In this paper, CW contains the following tags: noun, verb, adjective and adverb. Then the average of the word vectors in cand(w s i ) is used as the initialization value of the sense vector vec(w s i ).</p><formula xml:id="formula_3">vec(w s i ) = 1 |cand(w s i )| ∑ u∈cand(w s i ) vec(u) (4)</formula><p>For example, in WordNet, the gloss of the sense bank s 1 is "sloping land (especially the slope beside a body of water)) they pulled the canoe up on the bank; he sat on the bank of the river and watched the currents". The gloss contains a definition of the sense and two examples of the sense. The content words and the cosine similarities with the word "bank" are listed as follows: (sloping, 0.12), (land, 0.21), (slope, 0.17), (body, 0.01), (water, 0.10), (pulled, 0.01), (canoe, 0.09), (sat, 0.06), (river, 0.43), (watch, -0.11), (currents, 0.01). If the threshold, δ , is set to 0.05, then cand(bank s 1 ) is {sloping, land, slope, water, canoe, sat, riv- er}. Then the average of the word vectors in cand(bank s i ) is used as the initialization value of vec(bank s i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Performing Word Sense Disambiguation.</head><p>One of the state-of-the-art WSD results can be obtained using exemplar models, i.e., the word meaning is modeled by using relevant occurrences only, rather than merging all of the occurrences in- to a single word vector <ref type="bibr" target="#b8">(Erk and Pado, 2010</ref>). In- spired by this idea, we perform word sense disam- biguation to obtain more relevant occurrences.</p><p>Here, we perform knowledge-based word sense disambiguation for training data on an all-words setting, i.e., we will disambiguate all of the con- tent words in a sentence. Formally, the sentence S is a sequence of words (w 1 ,w 2 ,...,w n ), and we will identify a mapping M from words to senses such that M(i) ∈ Senses W N (w i ), where Senses W N (w i ) is the set of senses encoded in the W N for word w i . For sentence S, there are ∏ n i=1 |Sense W N (w i )| pos- sible mapping answers, which are impractical to compute. Thus, we design two simple algorithms, L2R (left to right) algorithm and S2C (simple to complex) algorithm, for word sense disambigua- tion based on the sense vectors.</p><p>The main difference between L2R and S2C is the order of words when performing word sense disambiguation. When given a sentence, the L2R algorithm disambiguates the words from left to right (the natural order of a sentence), whereas the S2C algorithm disambiguates the words with few- er senses first. The main idea of S2C algorithm is that the words with fewer senses are easier to disambiguate, and the disambiguation result can be helpful to disambiguate the words with more senses. Both of the algorithms have three steps:</p><p>Context vector initialization. Similar to the ini- tialization of sense vectors, we use the average of all of the content words' vectors in a sentence as the initialization vector of context.</p><formula xml:id="formula_4">vec(context) = 1 |cand(S)| ∑ u∈cand(S) vec(u) (5)</formula><p>where cand(S) is the set of content words</p><formula xml:id="formula_5">cand(S) = {u|u ∈ S, POS(u) ∈ CW }.</formula><p>Ranking words. For L2R, we do nothing in this step. For S2C, we rank the words based on the ascending order of |Senses W N (w i )|.</p><p>Word sense disambiguation. For both L2R and S2C, we denote the order of words as L and per- form word sense disambiguation according to L. First, we skip a word if the word is not a content word or the word is monosemous (|Senses W N (w i )| = 1). Then, for each word in L, we can compute the cosine similarities be- tween the context vector and its sense vectors. We choose the sense that yields the maximum cosine similarity as its disambiguation result. If the s- core margin between the maximum and the sec- ond maximum is larger than the threshold ε, we are confident with the disambiguation result of w i and then use the sense vector to replace the word vector in the context vector. Thus, we obtain a more accurate context vector for other words that are still yet to be disambiguated.</p><p>For example, given a sentence "He sat on the bank of the lake", we first explain how S2C work- s. In the sentence, there are three content word- s, "sat", "bank" and "lake", to be disambiguated. First, the sum of the three word vectors is used as the initialization of the context vector. Then we rank the words by |Senses W N(w i )|, in ascending order, that is, lake (3 senses), bank (10 senses), sat (10 senses). We first disambiguate the word "lake" based on the similarities between its sense vectors and context vector. If the score margin is larger bank input projection output sat on the of the lake sit lake 1 1</p><p>Figure 1: The architecture of our model. The training objective of Skip-gram is to train word vector representations that are not only good at predicting its context words but are also good at predicting its context words' senses. The center word "bank" is used to predict not only its context words but also the sense "sit 1 " and "lake 1 ". than the threshold ε, then we are confident with this disambiguation result and replace the word vector with the sense vector to update the contex- t vector. It would be helpful to disambiguate the next word, "bank". We repeat this process until all three words are disambiguated. For L2R, the order of words to be disambiguat- ed will be "sat", "bank" and "lake". In this time, when disambiguating "bank" (10 senses), we still don't know the sense of "lake" (3 senses).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Learning Sense Vectors from Relevant</head><p>Occurrences.</p><p>Based on the disambiguation result, we modify the training objective of Skip-gram and train the sense vectors directly from the large-scale corpus. Our training objective is to train the vector representa- tions that are not only good at predicting its con- text words but are also good at predicting its con- text words' senses. The architecture of our model is shown in <ref type="figure">Figure 1</ref>. More formally, given the disambiguation result</p><formula xml:id="formula_6">M(w 1 ), M(w 2 ), M(w 3 ),...,M(w T ), the training ob- jective is modified to 1 T T ∑ t=1 k ∑ j=−k log{p(w t+ j |w t )p(M(w t+ j )|w t )} (6)</formula><p>where k is the size of the training window. The inner summation spans from −k to k to compute the log probability of correctly predicting the word w t+ j and the log probability of correctly predicting the sense M(w t+ j ) given the word in the middle w t . The outer summation covers all words in the training data.</p><p>Because not all of the disambiguation results are correct, we only disambiguate the words that we are confident in. Similar to step 3 of our WSD algorithm, we only disambiguate words under the condition that the score margin between the max- imum and the second maximum is larger than the score margin threshold, ε.</p><p>We also use the softmax function to define p(w t+ j |w t ) and p(M(w t+ j )|w t ). Then, we use hi- erarchical softmax <ref type="bibr" target="#b25">(Morin and Bengio, 2005</ref>) to greatly reduce the computational complexity and learn the sense vectors directly from the relevant occurrences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we first present the nearest neigh- bors of some words and their senses, showing that our sense vectors can capture the semantics of words. Then, we use three tasks to evaluate our u- nified model: a contextual word similarity task to evaluate our sense representations, and two stan- dard WSD tasks to evaluate our knowledge-based WSD algorithm based on the sense vectors. Ex- perimental results show that our model not only improves the correlation with human judgments on the contextual word similarity task but also out- performs state-of-the-art supervised WSD system- s on domain-specific datasets and competes with them in a coarse-grained all-words setting.</p><p>We choose Wikipedia as the corpus to train the word vectors because of its wide coverage of topics and words usages. We use an English Wikipedia database dump from October 2013 2 , which includes roughly 3 million articles and 1 billion tokens. We use Wikipedia Extractor 3 to preprocess the Wikipedia pages and only save the content of the articles.</p><p>We use word2vec 4 to train Skip-gram. We use the default parameters of word2vec and the dimen- sion of the vector representations is 200.</p><p>We use WordNet 5 as our sense inventory. The datasets for different tasks are tagged with differ- ent versions of WordNet.  is 1.7 for the domain-specific WSD task and 2.1 for the coarse-grained WSD task. We use the S2C algorithm described in Section 2.3 to perform word sense disambiguation to ob- tain more relevant occurrences for each sense. We compare S2C and L2R on the coarse-grained WS- D task in a all-words setting.</p><p>The experimental results of our model are ob- tained by setting the similarity threshold as δ = 0 and the score margin threshold as ε = 0.1. The in- fluence of parameters on our model can be found in Section 3.5. <ref type="table" target="#tab_1">Table 2</ref> shows the nearest neighbors of word vec- tors and sense vectors based on cosine similari- ty. We see that our sense representations can i- dentify different meanings of a word, allowing our model to capture more semantic and syntactic re- lationships between words and senses. Note that each sense vector in our model corresponds to a sense in WordNet; thus, our sense vectors can be used to perform knowledge-based word sense dis- ambiguation, whereas the vectors of cluster-based models cannot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Examples for Sense Vectors</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Contextual Word Similarity</head><p>Experimental setting. A standard dataset for e- valuating a vector-space model is the WordSim- 353 dataset ( <ref type="bibr" target="#b9">Finkelstein et al., 2001</ref>), which con-Model ρ × 100 C&amp;W-S 57.0 Huang-S</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="58.6">Huang-M AvgSim 62.8 Huang-M AvgSimC 65.7 Our Model-S 64.2 Our Model-M AvgSim 66.2 Our Model-M AvgSimC</head><p>68.9 In our experiments, the similarity between a pair of words (w, w ) is computed as follows:</p><formula xml:id="formula_7">AvgSimC(w, w ) = 1 MN M ∑ i=1 N ∑ j=1 p(i|w, c)p( j|w , c )d(vec(w s i ), vec(w s j )) (7)</formula><p>where p(i|w, c) is the likelihood that word w chooses its ith sense given context c. d <ref type="bibr">(vec, vec )</ref> is a function computing the similarity between two vectors, and here we use cosine similarity.</p><p>Results and discussion. For evaluation, we compute the Spearman correlation between a model's computed similarity scores and human judgements. <ref type="table" target="#tab_2">Table 3</ref> shows our results com- pared to previous methods, including <ref type="bibr" target="#b5">(Collobert and Weston, 2008</ref>)'s language model (C&amp;W), and Huang's model which utilize the global context and multi-prototype to improve the word represen- tations.</p><p>From <ref type="table" target="#tab_2">Table 3</ref>, we observe that:</p><p>• Our single-vector version outperforms Huang's single-vector version. This indi- cates that, by training the word vectors and sense vectors jointly, our model can better capture the semantic relationships between words and senses.</p><p>• With one representation per sense, our mod- el can outperform the single-vector version without using context (66.2 vs. 64.2).</p><p>• Our model obtains the best performance (68.9) by using AvgSimC, which takes con- text into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Domain-Specific WSD</head><p>Experimental setting. We use Wikipedia as training data because of its wide coverage for spe- cific domains. To test our performance on do- main word sense disambiguation, we evaluated our system on the dataset published in ( <ref type="bibr" target="#b13">Koeling et al., 2005</ref>). This dataset consists of examples retrieved from the Sports and Finance sections of the Reuters corpus. 41 words related to the Sports and Finance domains were selected, with an aver- age polysemy of 6.7 senses, ranging from 2 to 13 senses. Approximately 100 examples for each word were annotated with senses from WordNet v.1.7 by three reviewers, yielding an inter-tagger agree- ment of 65%. ( <ref type="bibr" target="#b13">Koeling et al., 2005</ref>) did not clarify how to select the "correct" sense for each word, so we followed the work of ( <ref type="bibr">Agirre et al., 2009)</ref> and, used the sense chosen by the majority of taggers as the correct answer.</p><p>Baseline methods. As a baseline, we use the most frequent WordNet sense (MFS), as well as a random sense assignment. We also compare our results with four systems <ref type="bibr">7</ref>   <ref type="table">Table 4</ref>: Performance on the Sports and Finance sections of the dataset from ( <ref type="bibr" target="#b13">Koeling et al., 2005</ref>).</p><p>similar to the test example. The k-NN system is trained on SemCor ( <ref type="bibr" target="#b22">Miller et al., 1993)</ref>, the largest publicly available annotated corpus. Degree and Personalized PageRank are state- of-the-art systems that exploit WordNet to build a semantic graph and exploit the structural proper- ties of the graph in order to choose the appropriate senses of words in context.</p><p>Results and discussion. Similar to other work on this dataset, we use recall (the ratio of correct sense labels to the total labels in the gold standard) as our evaluation measure. <ref type="table">Table 4</ref> shows the re- sults of different WSD systems on the dataset, and the best results are shown in bold. The differences between other results and the best result in each column of the table are statistically significant at p &lt; 0.05.</p><p>The results show that:</p><p>• Our model outperforms k-NN on the t- wo domains by a large margin, support- ing the findings from ( <ref type="bibr">Agirre et al., 2009</ref>) that knowledge-based systems perform bet- ter than supervised systems when evaluated across different domains.</p><p>• Our model also achieves better results than the state-of-the-art system (+15.3% recall on Sports and +12.8% recall on Finance against Degree). The reason for this is that when dealing with short sentences or context words that are not in WordNet, our model can still compute similarity based on the context vec- tor and sense vectors, whereas Degree will have difficulty building the semantic graph.</p><p>• Moreover, our model achieves the best per- formance by only using the unlabeled text da- ta and the definitions of senses, whereas other <ref type="table">Table 5</ref>: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column, U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at p &lt; 0.05.</p><note type="other">Algorithm Type Nouns only All words F 1 F 1 Random BL U 63.5 62.7 MFS BL Semi 77.4 78.9 SUSSX-FR Semi 81.1 77.0 NUS-PT S 82.3 82.5 SSI Semi 84.1 83.2 Degree Semi 85.5 81.7 Our Model L2R U 79.2 73.9 Our Model S2C U 81.6 75.8 Our Model L2R Semi 82.5 79.6 Our Model S2C Semi 85.3 82.6</note><p>methods rely greatly on high-quality seman- tic relations or annotated data, which are hard to acquire.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Coarse-grained WSD</head><p>Experimental setting. We also evaluate our WSD model on the Semeval-2007 coarse-grained all-words WSD task ( <ref type="bibr" target="#b28">Navigli et al., 2007)</ref>. There are multiple reasons that we perform experiments in a coarse-grained setting: first, it has been ar- gued that the fine granularity of WordNet is one of the main obstacles to accurate WSD (cf. the discussion in <ref type="bibr" target="#b29">(Navigli, 2009)</ref>); second, the train- ing corpus of word representations is Wikipedia, which is quite different from WordNet.</p><p>Baseline methods. We compare our model with the best unsupervised system SUSSX-FR ( <ref type="bibr" target="#b12">Koeling and McCarthy, 2007)</ref>, and the best supervised system, NUS-PT ( <ref type="bibr" target="#b4">Chan et al., 2007)</ref>, participat- ing in the Semeval-2007 coarse-grained all-words task. We also compare with SSI ( <ref type="bibr" target="#b27">Navigli and Velardi, 2005</ref>) and the state-of-the-art system De- gree ( <ref type="bibr" target="#b26">Navigli and Lapata, 2010)</ref>. We use different baseline methods for the two WSD tasks because we want to compare our model with the state- of-the-art systems that are applicable to different datasets and show that our WSD method can per- form robustly in these different WSD tasks.</p><p>Results and discussion. We report our results in terms of F 1 -measure on the Semeval-2007 coarse- grained all-words dataset ( <ref type="bibr" target="#b28">Navigli et al., 2007)</ref>. <ref type="table">Table 5</ref> reports the results for nouns (1,108 words) and all words (2,269 words). The difference be- tween unsupervised and semi-supervised methods is whether the method uses MFS as a back-off s- trategy.</p><p>We can see that the S2C algorithm outperforms the L2R algorithm no matter on the nouns subset or on the entire set. This indicates that words with fewer senses are easier to disambiguate, and it can be helpful to disambiguate the words with more senses.</p><p>On the nouns subset, our model yields compa- rable performance to SSI and Degree, and it out- performs NUS-PT and SUSSX-FR. Moreover, our unsupervised WSD method (S2C) beats the MF- S baseline, which is notably a difficult competitor for knowledge-based systems.</p><p>On the entire set, our semi-supervised model is significantly better than SUSSX-FR, and it is com- parable with SSI and Degree. In contrast to SSI, our model is simple and does not rely on a cost- ly annotation effort to engineer the set of semantic relations.</p><p>Overall, our model achieves state-of-the-art per- formance on the Semeval-2007 coarse-grained all- words dataset compared to other systems, with a simple WSD algorithm that only relies on a large- scale unlabeled text corpora and a sense inventory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Parameter Influence</head><p>We investigate the influence of parameters on our model with coarse-grained all-words WSD task. The parameters include the similarity threshold, δ , and the score margin threshold, ε.</p><p>Similarity threshold. In <ref type="table" target="#tab_4">Table 6</ref>, we show the performance of domain WSD when the similari- ty threshold δ ranges from −0.1 to 0.3. The co- sine similarity interval is <ref type="bibr">[-1, 1]</ref>, and we focus on the performance in the interval [-0.1, 0.3] for two reasons: first, no words are removed from glosses when δ &lt; −0.1; second, nearly half of the word- s are removed when δ &gt; 0.3 and the performance drops significantly for the WSD task. From table 6, we can see that our model achieves the best per- formance when δ = 0.0.</p><p>Score margin threshold. In <ref type="table">Table 7</ref>, we show the performance on the coarse-grained all-words Parameter Nouns only All words δ = −0.10 79.8 74.3 δ = −0.05 81.0 74.6 δ = 0.00 81.6 75.8 δ = 0.05 81.3 75.4 δ = 0.10 80.8 75.2 δ = 0.15 80.0 75.0 δ = 0.20 77.1 73.3 δ = 0.30 75.0 72.1  <ref type="table">Table 7</ref>: Evaluation results on the coarse-grained all-words WSD when the score margin threshold ε ranges from 0.0 to 0.3.</p><p>WSD when the score margin threshold ε ranges from 0.0 to 0.3. When ε = 0.0, we use every disambiguation result to update the context vec- tor. When ε = 0, we only use the confident disam- biguation results to update the context vector if the score margin is larger than ε. Our model achieves the best performance when ε = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Word Representations</head><p>Distributed representations for words were pro- posed in <ref type="bibr" target="#b34">(Rumelhart et al., 1986)</ref>   <ref type="bibr">(Li-u et al., 2010;</ref><ref type="bibr" target="#b17">Liu et al., 2011b;</ref><ref type="bibr" target="#b18">Liu et al., 2012)</ref>, social tag suggestion ( <ref type="bibr" target="#b16">Liu et al., 2011a</ref>) and text classification <ref type="bibr" target="#b1">(Baker and McCallum, 1998</ref>), may also potentially benefit from distributed word rep- resentation. The main advantage is that the rep- resentations of similar words are close in vector space, which makes generalization to novel pat- terns easier and model estimation more robust. Word representations are hard to train due to the computational complexity. Recently, ( <ref type="bibr" target="#b20">Mikolov et al., 2013</ref>) proposed two particular models, Skip- gram and CBOW, to learn word representations in large amounts of text data. The training objective of the CBOW model is to combine the representa- tions of the surrounding words to predict the word in the middle, while the Skip-gram model's is to learn word representations that are good at predict- ing its context in the same sentence <ref type="bibr" target="#b20">(Mikolov et al., 2013)</ref>. Our paper uses the model architecture of Skip-gram.</p><p>Most of the previous vector-space models use one representation per word. This is problematic because many words have multiple senses. The multi-prototype approach has been widely stud- ied. <ref type="bibr" target="#b33">(Reisinger and Mooney, 2010)</ref> proposed the multi-prototype vector-space model. ( <ref type="bibr" target="#b11">Huang et al., 2012</ref>) used the multi-prototype models to learn the vector for different senses of a word. All of these models use the clustering of contexts as a word sense and can not be directly used in word sense disambiguation.</p><p>After our paper was submitted, we perceive the following recent advances: <ref type="bibr" target="#b39">(Tian et al., 2014</ref>) pro- posed a probabilistic model for multi-prototype word representation. ( <ref type="bibr" target="#b10">Guo et al., 2014</ref>) explored bilingual resources to learn sense-specific word representation. ( <ref type="bibr" target="#b30">Neelakantan et al., 2014</ref>) pro- posed an efficient non-parametric model for multi- prototype word representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Knowledge-based WSD</head><p>The objective of word sense disambiguation (WS- D) is to computationally identify the meaning of words in context <ref type="bibr" target="#b29">(Navigli, 2009)</ref>. There are t- wo approaches of WSD that assign meaning of words from a fixed sense inventory, supervised and knowledge-based methods. Supervised approach- es require large labeled training sets, which are time consuming to create. In this paper, we on- ly focus on knowledge-based word sense disam- biguation.</p><p>Knowledge-based approaches exploit knowl- edge resources (such as dictionaries, thesauri, on- tologies, collocations, etc.) to determine the senses of words in context. However, it has been shown in <ref type="bibr" target="#b7">(Cuadros and Rigau, 2006</ref>) that the amount of lexical and semantic information contained in such resources is typically insuf- ficient for high-performance WSD. Much work has been presented to automatically extend ex- isting resources, including automatically linking Wikipedia to WordNet to include full use of the first WordNet sense heuristic <ref type="bibr" target="#b38">(Suchanek et al., 2008)</ref>, a graph-based mapping of Wikipedia cat- egories to WordNet synsets <ref type="bibr" target="#b31">(Ponzetto and Navigli, 2009)</ref>, and automatically mapping Wikipedia pages to WordNet synsets ( <ref type="bibr" target="#b32">Ponzetto and Navigli, 2010)</ref>.</p><p>It was recently shown that word representation- s can capture semantic and syntactic information between words ( <ref type="bibr" target="#b20">Mikolov et al., 2013)</ref>. Some re- searchers tried to incorporate WordNet senses in a neural model to learn better word representation- s ( <ref type="bibr" target="#b3">Bordes et al., 2011)</ref>. In this paper, we have pro- posed a unified method for word sense representa- tion and disambiguation to extend the information contained in the vector representations to the ex- isting resources. Our method only requires a large amount of unlabeled text to train sense representa- tions and a dictionary to provide the definitions of word meanings, which makes it easily applicable to other resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present a unified model for word sense representation and disambiguation that us- es one representation per sense. Experimental re- sults show that our model improves the perfor- mance of contextual word similarity compared to existing WSR methods, outperforms state-of-the- art supervised methods on domain-specific WSD, and achieves competitive performance on coarse- grained all-words WSD. Our model only requires large-scale unlabeled text corpora and a sense in- ventory for WSD, thus it can be easily applied to other corpora and tasks.</p><p>There are still several open problems that should be investigated further:</p><p>2. We can explore other WSD methods based on sense vectors to improve our performance. For example, ( <ref type="bibr" target="#b14">Li et al., 2010</ref>) used LDA to perform data-driven WSD in a manner simi- lar to our model. We may integrate the advan- tages of these models and our model together to build a more powerful WSD system.</p><p>3. To learn better sense vectors, we can exploit the semantic relations (such as the hypernym and hyponym relations defined in WordNet) between senses in our model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>The version of WordNet</head><label></label><figDesc></figDesc><table>Word or sense Nearest neighbors 
bank 
banks, IDBI, CitiBank 
bank s 1 
river, slope, Sooes 
bank s 2 
mortgage, lending, loans 
star 
stars, stellar, trek 

star s 1 
photosphere, radiation, 
gamma-rays 
star s 2 
someone, skilled, genuinely 
plant 
plants, glavaticevo, herbaceous 

plant s 1 
factories, machinery, 
manufacturing 

plant s 2 
locomotion, organism, 
organisms 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Nearest neighbors of word vectors and 
sense vectors learned by our model based on co-
sine similarity. The subscript of each sense label 
corresponds to the index of the sense in Word-
Net. For example, bank s 2 is the second sense of 
the word bank in WordNet. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Spearman's ρ on the SCWS dataset. Our 
Model-S uses one representation per word to com-
pute similarities, while Our Model-M uses one 
representation per sense to compute similarities. 
AvgSim calculates the similarity with each sense 
contributing equally, while AvgSimC weighs the 
sense according to the probability of the word 
choosing that sense in context c. 

sists of 353 pairs of nouns. However, each pair of 
nouns in WordSim-353 is presented without con-
text. This is problematic because the meanings 
of homonymous and polysemous words depend 
highly on the words' contexts. Thus we choose the 
Stanford's Contextual Word Similarities (SCWS) 
dataset from (Huang et al., 2012) 6 . The SCWS 
dataset contains 2003 pairs of words and each pair 
is associated with 10 human judgments on similar-
ity on a scale from 0 to 10. In the SCWS dataset, 
each word in a pair has a sentential context. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Evaluation results on the coarse-grained 
all-words WSD when the similarity threshold δ 
ranges from −0.1 to 0.3. 

Parameter Nouns only All words 
ε = 0.00 
78.2 
72.9 
ε = 0.05 
79.5 
74.5 
ε = 0.10 
81.6 
75.8 
ε = 0.15 
81.2 
74.7 
ε = 0.20 
80.9 
75.1 
ε = 0.25 
80.2 
74.8 
ε = 0.30 
80.4 
74.9 

</table></figure>

			<note place="foot" n="2"> http://download.wikipedia.org. 3 The tool is available from http://medialab.di. unipi.it/wiki/Wikipedia_Extractor. 4 The code is available from https://code. google.com/p/word2vec/. 5 http://wordnet.princeton.edu/.</note>

			<note place="foot" n="6"> The dataset can be downloaded at http://ai. stanford.edu/ ˜ ehhuang/.</note>

			<note place="foot" n="7"> We compare only with those systems performing tokenbased WSD, i.e., disambiguating each instance of a target word separately.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by National Key Ba-sic Research Program of China (973 Program 2014CB340500) and National Natural Science Foundation of China (NSFC 61133012).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">we will incorporate cluster-based methods in our model to find senses that are not in the sense inventory. References Eneko Agirre, Oier Lopez De Lacalle, Aitor Soroa, and Informatika Fakultatea</title>
	</analytic>
	<monogr>
		<title level="m">Because the senses of words change over time (new senses appear)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1501" to="1506" />
		</imprint>
	</monogr>
	<note>Proceedings of IJCAI</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributional clustering of words for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="96" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sébastien</forename><surname>Senécal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fréderic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="137" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="301" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nus-pt: exploiting parallel texts for word sense disambiguation in the english all-words tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Seng Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="253" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quality assessment of large scale knowledge resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Montse</forename><surname>Cuadros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="534" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exemplar-based models for word meaning in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Pado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="92" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning sense-specific word embeddings by exploiting bilingual resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="497" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sussx: Wsd using automatically acquired predominant senses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Koeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="314" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Domain-specific sense distributions and predominant sense acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Koeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLTEMNLP</title>
		<meeting>HLTEMNLP</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="419" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Topic models for word sense disambiguation and token-based idiom detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Sporleder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1138" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic keyphrase extraction via topic decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="366" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A simple word trigger method for social tag suggestion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1577" to="1588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic keyphrase extraction by bridging vocabulary gap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
	<note>Yabin Zheng, and Maosong Sun</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mining the interests of chinese microbloggers via keyword extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computer Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="87" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Introduction to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Christopher D Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press Cambridge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Statistical Language Models Based on Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Brno University of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
	<note>Ph. D. thesis</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A semantic concordance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>George A Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randee</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross T</forename><surname>Tengi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bunker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on HLT</title>
		<meeting>the workshop on HLT</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="303" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international workshop on artificial intelligence and statistics</title>
		<meeting>the international workshop on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An experimental study of graph connectivity for unsupervised word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="678" to="692" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Structural semantic interconnections: a knowledge-based approach to word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Velardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1075" to="1086" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semeval-2007 task 07: Coarsegrained english all-words task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orin</forename><surname>Litkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hargraves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="30" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Word sense disambiguation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CSUR</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient nonparametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Large-scale taxonomy mapping for restructuring and integrating wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2083" to="2088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Knowledge-rich word sense disambiguation rivaling supervised systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1522" to="1531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-prototype vector-space models of word meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hintont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Machine learning in automated text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CSUR</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Yago: A large ontology from wikipedia and wordnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Fabian M Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Semantics: Science, Services and Agents on the World Wide Web</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="217" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A probabilistic model for learning multi-prototype word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
