<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:09+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Maximum Margin Reward Networks for Learning from Explicit and Implicit Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoruo</forename><surname>Peng</surname></persName>
							<email>hpeng7@illinois.edu 2 {minchang,scottyih}@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Maximum Margin Reward Networks for Learning from Explicit and Implicit Supervision</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2368" to="2378"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
					<note>2 Microsoft Research, Redmond 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural networks have achieved state-of-the-art performance on several structured-output prediction tasks, trained in a fully supervised fashion. However, annotated examples in structured domains are often costly to obtain, which thus limits the applications of neural networks. In this work, we propose Maximum Margin Reward Networks, a neural network-based framework that aims to learn from both explicit (full structures) and implicit supervision signals (delayed feedback on the correctness of the predicted structure). On named entity recognition and semantic parsing, our model outperforms previous systems on the benchmark datasets, CoNLL-2003 and WebQuestionsSP.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Structured-output prediction problems, where the goal is to determine values of a set of inter- dependent variables, are ubiquitous in NLP. Struc- tures of such problems can range from simple se- quences like part-of-speech tagging (  and named entity recognition ( <ref type="bibr" target="#b23">Lample et al., 2016)</ref>, to complex syntactic or semantic analysis such as dependency parsing  and semantic parsing <ref type="bibr" target="#b13">(Dong and Lapata, 2016)</ref>. State- of-the-art methods of these tasks are often neu- ral network models trained using fully annotated structures, which can be costly or time-consuming to obtain. Weakly supervised learning settings, where the algorithm assumes only the existence of implicit signals on whether a prediction is correct, are thus more appealing in many scenarios.</p><p>For example, <ref type="figure">Figure 1</ref> shows a weakly super- vised setting of learning semantic parsers using only question-answer pairs. When the system generates a candidate semantic parse during train- ing, the quality needs to be indirectly measured by <ref type="figure">Figure 1</ref>: Learning a semantic parser using im- plicit supervision signals (labeled answers). Since there are no gold parses, a model needs to explore different parses, where their quality can only be indirectly verified by comparing retrieved answers and the labeled answers.</p><p>comparing the derived answers from the knowl- edge base and the provided labeled answers.</p><p>This setting of implicit supervision increases the difficulty of learning a neural model, not only because the signals are vague and noisy, but also delayed. For instance, among different semantic parses that result in the same answers, typically only few of them correctly represent the meaning of the question. Moreover, the correctness of an- swers corresponding to a parse can only be eval- uated through an external oracle (e.g., executing the query on the knowledge base) after the parse is fully constructed. Early model update before the search of a full semantic parse is complete is gen- erally infeasible. 1 It is also not clear how to lever- age implicit and explicit signals integrally during learning when both kinds of labels are present.</p><p>In this work, we propose Maximum Margin Re- ward Networks (MMRN), which is a general neu- ral network-based framework that is able to learn from both implicit and explicit supervision sig- nals. By casting structured-output learning as a search problem, the key insight in MMRN is the special mechanism of rewards. Rewards can be viewed as the training signals that drive the model to explore the search space and to find the cor- rect structure. The explicit supervision signals can be viewed as a source of immediate rewards, as we can often instantly know the correctness of the current action. On the other hand, the implicit su- pervision can be viewed as a source of delayed re- wards, where the reward of the actions can only be revealed later. We unify these two types of reward signals by using a maximum margin update, in- spired by structured SVM ( .</p><p>The effectiveness of MMRN is demonstrated on three NLP tasks: named entity recognition, entity linking and semantic parsing. MMRN outperforms the current best results on CoNLL-2003 named entity recognition dataset <ref type="bibr" target="#b37">(Tjong Kim Sang and De Meulder, 2003)</ref>, reaching 91.4% F 1 , in the close setting where no gazetteer is allowed. It also performs comparably to the existing state-of-the- art systems on entity linking. Models for these two tasks are trained using explicit supervision. For semantic parsing, where only implicit super- vision signals are provided, MMRN is able to learn from delayed rewards, improving the entity link- ing component and the overall semantic parsing framework jointly, and outperforms the best pub- lished system by 1.4% absolute on the WebQSP dataset ( .</p><p>In the rest of the paper, we survey the most related work in Sec. 2 and give an in-depth dis- cussion on comparing MMRN and other learning frameworks in Sec. 7. We start the description of our method from the search formulation and the state-action spaces in our targeted tasks in Sec. 3, followed by the reward and learning algorithm in Sec. 4 and the detailed neural model design in Sec. 5. Sec. 6 reports the experimental results and Sec. 8 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Structured output prediction tasks have been stud- ied extensively in the field of natural language pro- cessing (NLP). Many supervised structured learn- ing algorithms has been proposed for capturing the relationships between output variables. These models include structured perceptron <ref type="bibr" target="#b8">(Collins, 2002;</ref><ref type="bibr" target="#b9">Collins and Roark, 2004</ref>), conditional ran- dom fields ( <ref type="bibr" target="#b22">Lafferty et al., 2001</ref>), and structured SVM ( <ref type="bibr" target="#b36">Taskar et al., 2004;</ref>. Later, the learning to search framework is pro- posed <ref type="bibr" target="#b12">(Daumé and Marcu, 2005;</ref><ref type="bibr" target="#b11">Daumé et al., 2009)</ref>, which casts the structured prediction task as a general search problem. Most recently, recurrent neural networks such as LSTM mod- els <ref type="bibr" target="#b17">(Hochreiter and Schmidhuber, 1997)</ref> have been used as a general tool for structured output mod- els ( <ref type="bibr" target="#b39">Vinyals et al., 2015)</ref>.</p><p>Latent structured learning algorithms address the problem of learning from incomplete labeled data ( <ref type="bibr" target="#b32">Quattoni et al., 2007</ref>). The main difference compared to our framework is the existence of the external envi- ronment when learning from implicit signals. <ref type="bibr" target="#b38">Upadhyay et al. (2016)</ref> first proposed the idea of learning from implicit supervision, and is the most related paper to our work. Compared to their lin- ear algorithm, our framework is more principled and general as we integrate the concept of margin in our method. Furthermore, we also extend the framework using neural models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Search-based Inference</head><p>In our framework, predicting the best structured output, inference, is formulated as a state/action search problem. Our search space can be de- scribed as follows. The initial state, s 0 , is the starting point of the search process. We define γ(s) as the set of all feasible actions that can be taken at s, and denote s = τ (s, a) as the transition function, where s is the new state af- ter taking action a from s. A path h is a se- quence of state-action pairs, starting with the ini- tial state: h = {(s 0 , a 0 ), . . . , (s k , a k )}, where</p><formula xml:id="formula_0">s i = τ (s i−1 , a i−1 ), ∀i = 1, . . . , k. We denote h ; ˆ s, ifˆsifˆ ifˆs = τ (s k , a k )</formula><p>, the final state which the path h leads to. A path essentially is a partial or complete structured prediction. For each input x, we define H(x) to be the set of all possible paths for the input. We also define E(x) = {h | h ∈ H(x), h ; ˆ s, γ(ˆ s) = ∅}, which is all possible paths that lead to terminal states. Given a state s and an action a, the scoring func- tion f θ (s, a) measures the quality of an immediate action with respect to the current state, where θ is the model parameters. The score of a path h is defined as the sum of the scores for state-action pairs in h:</p><formula xml:id="formula_1">f θ (h) = k i=0 f θ (s i , a i ).</formula><p>During test time, inference is to find the best path in E(x): arg max h∈E(x) f θ (h; x). In practice, inference is often approximated by beam search when no effi- cient algorithm exists.</p><p>In the remaining of this section, we describe the states and actions in the targeted tasks in this work: named entity recognition, entity linking and semantic parsing. The the model and learning al- gorithm will be discussed in Sec. 4 and Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Named entity recognition</head><p>The task of named entity recognition (NER) is to identify entity mentions in a sentence, as well as to assign their types, such as Person or Location. Following the conventional setting, we treat it as a sequence labeling problem using the standard BIOES encoding. For instance, a "B-LOC" tag on a word means that the word is the beginning of a multi-word location entity.</p><p>Given a sentence as input, the states represent the tags assigned to the words. Starting from the initial state, s 0 , where no tag has been assigned, the search process explores the sequence tagging from the left-to-right order. For each word, the actions are the legitimate tags that can be assigned to it, which depend on previous actions. For exam- ple, if the "S-PER" tag ("S" means a single word entity) has been assigned to the previous word, then an action of labeling the current word with either "I-PER" or "E-PER" cannot can be taken. The search reaches a terminal state when all words in the sentence have been tagged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Entity linking</head><p>The problem of entity linking (EL) is similar to NER, but instead of tagging the mention using a small set of generic entity types, the goal here is to ground the mention to a specific entity, stored in a knowledge base or described by a Wikipedia page. For example, consider the sentence "nfl news: draft results for giants" and assume that the mention candidates "nfl" and "giants" are given. A state reflects how we have assigned the entity la- bels to these candidates. Following the same left- to-right order and starting from the empty assign- ment s 0 , the first action to take is to assign the entity label to the first candidate "nfl". A legit- imate action set can be all the entities that have been associated with this mention in the training set (e.g., "National Football League" or "National Fertilizers Limited"). Once the action is com- pleted, the transition function will bring the focus to the next mention candidate (i.e., "giants"). The search reaches a terminal state when all the candi- date mentions in the sentence have been linked. Meg Griffin x y í µí¼í µí±¥. ∃í µí±¦. í µí±í µí±í µí± í µí±¡ FamilyGuySeason1, í µí±¦ ∧ í µí±í µí±í µí±¡í µí±í µí± í µí±¦, í µí±¥ ∧ í µí±ℎí µí±í µí±í µí±í µí±í µí±¡í µí±í µí±(í µí±¦,MegGriffin)</p><p>Figure 2: Semantic parses in λ-calculus (top) and query graph (bottom) of the question "who played meg in season 1 of family guy?"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Semantic parsing</head><p>Our third targeted task is semantic parsing (SP), which is a task of mapping a text utterance to a for- mal meaning representation. In this paper, we fo- cus on a specific type of semantic parsing problem that maps a natural language question to a struc- tured query, which is executed on a knowledge base to retrieve the answer to the original question. <ref type="figure">Figure 2</ref> shows the semantic parses of an ex- ample question "who played meg in season 1 of family guy", assuming the knowledge base is Free- base ( <ref type="bibr" target="#b2">Bollacker et al., 2008</ref>). An entity linking component plays an important role by mapping "meg" to MegGriffin and "season 1 of family guy" to FamilyGuySeason1. Predicates like cast, actor and character are also from the knowledge base that define the relationships be- tween these entities and the answer. Together the semantic parse in λ-calculus is shown in the top of <ref type="figure">Figure 2</ref>. Equivalently, the semantic parse can be represented as a query graph <ref type="figure">(Figure 2</ref> bottom), which is used in the STAGG system <ref type="bibr" target="#b43">(Yih et al., 2015</ref>). The nodes are either grounded entities or variables, where x is the answer entity. The edges denote the relationship between two entities.</p><p>Regardless of the choice of the formal language, the process of constructing the semantic parse is typically formulated as a search problem. A state is essentially a partial or complete semantic parse, and an action is to extend the current semantic parse by adding a new relation or constraint.</p><p>Different from previous systems which treat en- tity linking as a static component, our search space consists of the search space of both entity linking and semantic parsing. That is, the search space is the union of the search space of entity linking de- scribed in Section 3.2 and the search space of the semantic parses, which we describe below. Inte- grating search spaces allows the model to use im- plicit signals to update both the semantic parsing and the entity linking systems. To the best of our knowledge, this is the first work that jointly learns the entity linking and semantic parsing systems.</p><p>Our search space is defined as follows. Start- ing from the initial state s 0 , the model first ex- plores the entity linking search space. Once the entity linking assignment are assigned (e.g. FamilyGuySeason1 in <ref type="figure">Figure 2</ref>.) The sec- ond phase is then to determine the main rela- tionship between the topic entity and the an- swer (e.g., the cast-actor chain between FamilyGuySeason1 and x). Constraints (e.g., the character is MegGriffin) that describe the additional properties that the answer needs to have are added last. In this case, any state that is a legitimate semantic parse (consisting of one topic entity and one main relationship, as well as zero or more constraints) can lead to a terminal state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Maximum Margin Reward Networks</head><p>In this section, we introduce the learning frame- work of MMRN, which includes two main compo- nents: reward and max-margin loss. The former is a mechanism for using implicit and explicit super- vision signals in a unified way; the latter formally defines the learning objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Reward</head><p>The key insight of MMRN is that different types of supervision signals can be represented using the appropriate design of the reward function. A re- ward function is defined over a state-action pair R(s, a), representing the true quality of taking ac- tion a in the state s. The reward for a path can be formally defined as: R(h) = k i=0 R(s i , a i ). Intuitively, when the annotated action sequences (explicit supervision signals) exist, the model only needs to learn to imitate the annotated sequence. For instance, when learning NER in the fully su- pervised setting, the equivalent way of using Ham- ming distance is to define the reward R(s, a) to be 1 if a matches the annotated sequence at the cur- rent state, and 0 otherwise.</p><p>In the setting where only implicit supervision is available, the reward function can still be de- signed to capture the signals. For instance, when only the question-answer pairs exist for learning the semantic parser, the reward can be defined by comparing the answers derived from a candidate parse and the labeled answers. More formally, as- sume that s = τ (s , a) is the state after applying x y í µí± í µí± = {Lacey Chabert, Seth MacFarlane, Alex Borstein, Seth Green, John Viener, Alec Sulkin} í µí°´=µí°´= Lacey Chabert í µí± = <ref type="figure">Figure 3</ref>: For the question "who played meg in season 1 of family guy?", the candidate semantic parse s lists all the actors in "Family Guy Season 1" (Y (s)). By comparing Y (s) to the answer set A, the precision is 1 6 and the recall is 1. Therefore, the F 1 score used for the reward is <ref type="bibr">2 7</ref> .</p><p>action a to state s . Let Y (s) be the set of predicted answers generated from state s, and Y (s) = {} when s is not a legitimate semantic parse. The reward function R(s , a) can be defined by com- paring Y (s) and the labeled answers, A, to the in- put question. While a set similarity function like the Jaccard coefficient can be used as the reward function, we chose the F 1 score in this work as it was used as the evaluation metric in previous work <ref type="bibr" target="#b1">(Berant et al., 2013</ref>). <ref type="figure">Figure 3</ref> shows an ex- ample of this reward function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Max-Margin Loss &amp; Learning Algorithm</head><p>The MMRN learning algorithm can be viewed as an extension of M 3 N ( <ref type="bibr" target="#b36">Taskar et al., 2004</ref>) and Structured SVM ( ). The learning algorithm takes three steps, where the first two involve two differ- ent search procedures. The final step is to update the models with respect to the inference results.</p><p>Finding the best path The first search step is to find the best path h * by solving the following optimization problem:</p><formula xml:id="formula_2">h * = arg max h∈E(x) R(h; y) + f θ (h).<label>(1)</label></formula><p>The first term defines the path that has the highest reward. Because it is possible that several paths share the same reward, the second term leverages the current model and serves as the tie-breaker, where is a hyper-parameter that is set to a small positive number in our experiments. When explicit supervision is available, solving Eq. (1) is trivial -the search simply returns the annotated sequence. In the case of implicit super- vision, where true rewards are only revealed for complete action sequences, the search problem be- comes difficult as the rewards of early state-action pairs are zeros. In this situation, the search algo- rithm uses the model score f θ to guide the search. One possible design is to use beam search for the optimization problem, where the search procedure follows the current model in the early stage (given that R(h) = 0). After generating several complete action sequences, the true reward function is then used to find h * . The tie-breaker also picks the best sequence when there are multiple sequences that lead to the same reward. Note that h * can change between iterations because of the tie-breaker.</p><p>Finding the most violated path Once h * is found, it is used as our reference path. We would like to update the model so that the scoring func- tion f θ will behave similarly to the reward R. More formally, we aim to update the model pa- rameters θ to satisfy the following constraint.</p><formula xml:id="formula_3">f θ (h * ) − f θ (h) ≥ R(h * ) − R(h), ∀h.</formula><p>The constraint implies that the "best" action se- quence should rank higher than any other se- quence by a margin computed from rewards as R(h * ) − R(h). The degree of violation of this constraint, with respect to h, is thus</p><formula xml:id="formula_4">(R(h * ) − R(h)) − (f θ (h * ) − f θ (h)) = f θ (h) − R(h) − f θ (h * ) + R(h * ).</formula><p>The max-margin loss is defined accordingly:</p><formula xml:id="formula_5">L(h, h * ) = max(f θ (h)−R(h)−f θ (h * )+R(h * ), 0) L(h, h * )</formula><p>is our optimization goal, where we want to update the model by fixing the biggest violation. Note that the associated constraint is only violated when L(h, h * ) is positive. To find the path h in this step that maximizes the violation is equivalent to maximizing f θ (h) − R(h), given that the rest of the terms are constant with respect to h.</p><p>When there exist only explicit supervision sig- nals, our objective function reduces to the one for optimizing structured SVM without regular- ization. For implicit signals, we find h * approxi- mately before we optimize the margin loss. In this case, the search is not exact as the reward signals are delayed. Nevertheless, we found the margin loss worked well empirically, as it kept decreasing in general until being stable.</p><p>Algorithm 1 summarizes the learning procedure of MMRN. Search is used in both Line 2 and 3. In Line 4, the algorithm performs a gradient update to modify all the model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Maximum Margin Reward Networks</head><p>1: for a random labeled data (x, y) do </p><formula xml:id="formula_6">h * ← arg max h∈E(x) R(h; y) + f θ (h) 3: ˆ h ← arg max h∈E(x) f θ (h) − R(h; y) 4:</formula><p>update θ by minimizing L( ˆ h, h * ) 5: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Practical Considerations</head><p>Although the learning algorithm of MMRN is sim- ple and general, the quality of the learned model is dictated by the effectiveness of the search proce- dure. Increasing the beam size generally helps im- prove the model, but also slows down the training, and has a limited effect when dealing with a large search space. Domain-specific heuristics for prun- ing search space should thus be used when avail- able. For instance, in the task of semantic parsing, when the reward of a legitimate semantic parse is 0, it implies that none of the derived answers is in- cluded in the labeled set of answers. When all the possible follow-up actions can only make the se- mantic parse stricter (e.g., adding constraints), and result in a subset of the current derived answers, it is clear that the rewards of all these new states are 0 as well. Paths from this state can thus be pruned.</p><p>Another strategy for improving search quality is to use approximated reward in the early stage of search. Very often the true rewards at this stage are 0, and are not useful to guide the search to find the best path. The approximated reward function can be thought of as estimating whether there ex- ists a high-reward state that is reachable from the current state. The effectiveness of this strategy has been demonstrated successfully by several recent efforts <ref type="bibr">(Mnih et al., 2013;</ref><ref type="bibr" target="#b35">Silver et al., 2016;</ref><ref type="bibr" target="#b30">Narasimhan et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Neural Architectures</head><p>While the learning algorithm of MMRN described in Sec. 4 is general, the exact model design is task- dependent. In this section, we describe in detail the neural network architectures of the three tar- geted tasks, named entity recognition, entity link- ing and semantic parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Named Entity Recognition</head><p>Recall that NER is formulated as a sequence la- beling problem, and each action is to label a word with a tag using the BIOES encoding (cf. Sec. 3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input í µí±¥</head><p>Previous action embedding f í µí¼ (í µí± , í µí±)</p><p>State í µí± determines the word index í µí± Action í µí± determines the tag type word <ref type="figure">Figure 4</ref>: The action scoring model for NER.</p><p>The model of the action scoring function f θ (s, a) is depicted in <ref type="figure">Figure 4</ref>, which is basically the dot product of the action embedding and state em- bedding. The action embedding is initialized ran- domly for each action, but can be fine-tuned dur- ing training (i.e. back-propagate the error through the network and update the word/entity type em- beddings). The state embedding is the concate- nation of bi-LSTM word embeddings of the cur- rent word, the character-based word embeddings, and the embedding of the previous action. We also include the orthographic embeddings pro- posed by <ref type="bibr" target="#b25">Limsopatham and Collier (2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Entity Linking</head><p>An action in entity linking is to determine whether a mention should be linked to a particular entity (cf. Sec. 3.2). As shown in <ref type="figure">Figure 5</ref>, we design the scoring function as a feed-forward neural network that takes as input three different input vectors: (1) surface features from hand-crafted mention-entity statistics that are similar to the ones used in <ref type="bibr" target="#b41">(Yang and Chang, 2015)</ref>; (2) mention context embed- dings from a bidirectional LSTM module; (3) en- tity embeddings constructed from entity type em- beddings. All these embeddings, except the fea- ture vectors, are fine-tuned during training. Some unique properties of our entity linking model are worth noticing. First, we add mention context embeddings from a bidirectional LSTM module as additional input. While using LSTMs is a common practice for sequence labeling, it is not usually used for short-text entity linking. For each mention, we only extract the output from the bi-LSTM module at the start and end tokens of the mention, and concatenate them as the men- tion context embeddings. Second, we construct entity embeddings using the average of its Free- base ( <ref type="bibr" target="#b2">Bollacker et al., 2008</ref>) type embeddings 2 , <ref type="bibr">2</ref> We use only the 358 most frequent Freebase entity types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Avg.{ …</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistic features Input í µí±¥</head><p>Two hidden layers = Average of entity type embeddings f í µí¼ (í µí± , í µí±)</p><p>State í µí± determines the mention index í µí± Action í µí± determines the entity index Mention í µí± <ref type="figure">Figure 5</ref>: The action scoring model for EL.</p><p>initialized using pre-trained embeddings. Adding these two types of embeddings has shown to im- prove the performance in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Semantic Parsing</head><p>Our semantic parsing model follows the STAGG system ( <ref type="bibr" target="#b43">Yih et al., 2015)</ref>, which uses a stage- wise search procedure to expand the candidate semantic parses gradually (cf. Sec. 3.3). Com- pared to the original system, we make two notable changes. First, we use a two-layer feed-forward neural network to replace the original linear ranker that scores the candidate semantic parses. Second, instead of using a separately trained entity link- ing system, we incorporate our entity linking net- works described in Sec. 5.2 as part of the semantic parsing model. The training process will thus fine tune the entity linking component to improve the semantic parsing system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>It is important to have a general machine learn- ing model working for both implicit and explicit supervision signals. We valid our learning frame- work when the explicit supervision signals are pre- sented, as well as demonstrate the support of the scenario where supervision signals are mixed. Specifically, in this section, we report the exper- imental results of MMRN on named entity recogni- tion and entity linking, both using explicit super- vision, and on semantic parsing, using implicit su- pervision. In all our experiments, we tuned hyper- parameters on the development set (each task re- spectively), and then re-trained the models on the combination of the training and development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Named entity recognition</head><p>We use the CoNLL-2003 shared task data for the NER experiments, where the standard evaluation System F 1</p><p>Collobert et al. <ref type="formula" target="#formula_2">(2011)</ref> 89.59 <ref type="bibr" target="#b18">Huang et al. (2015)</ref> 90.10 Chiu and Nichols (2015) 90.77 <ref type="bibr" target="#b33">Ratinov and</ref><ref type="bibr">Roth (2009) 90.88 Lample et al. (2016)</ref> 90.94 <ref type="bibr" target="#b27">Ma and Hovy (2016)</ref> 91.21   <ref type="table">Table 2</ref>: Explicit Supervision: Entity Linking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MMRN-NER</head><p>Our system trained with MMRN is comparable to the state-of-art NTEL system. metric is the F 1 score. The pre-trained word em- beddings are 100-dimension GloVe vectors trained on 6 billion tokens (Pennington et al., 2014) 3 . The search procedure is conducted using beam search, and the reward function is simply the number of correct tag assignments to the words. The results are shown in <ref type="table" target="#tab_0">Table 1</ref>, compared with recently proposed systems based on neural models. When the beam size is set to 20, MMRN achieves 91.4, which is the best published result so far (without using any gazetteers). Notice that when beam size is 5, the performance drops to 90.03. This demonstrates the importance of search quality when applying MMRN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Entity linking</head><p>For entity linking, we adopt two publicly avail- able datasets for tweet entity linking: NEEL (Cano et al., 2014) 4 and TACL ( <ref type="bibr" target="#b16">Guo et al., 2013;</ref><ref type="bibr" target="#b15">Fang and Chang, 2014;</ref><ref type="bibr" target="#b41">Yang and Chang, 2015;</ref><ref type="bibr" target="#b42">Yang et al., 2016)</ref>. We follow prior works ( <ref type="bibr" target="#b16">Guo et al., 2013;</ref><ref type="bibr" target="#b41">Yang and Chang, 2015)</ref> and perform the standard evaluation for an end-to-end entity link- ing system by computing precision, recall, and F 1 scores, according to the entity references and the system output. An output entity is considered cor- rect if it matches the gold entity and the mention boundary overlaps with the gold mention bound- ary. Interested readers can refer to <ref type="bibr" target="#b4">(Carmel et al., 2014</ref>) for more detail.</p><p>We initialize the word embeddings from pre- trained GloVe vectors trained on the twitter cor- pus, and type embeddings from the pre-trained skip-gram model ( <ref type="bibr" target="#b28">Mikolov et al., 2013)</ref>  <ref type="bibr">5</ref> . Sizes of both word embeddings are set to 200. Inference is done using a dynamic programming algorithm.</p><p>Results of entity linking experiments are pre- sented in <ref type="table">Table 2</ref>, which are compared with those of S-MART ( <ref type="bibr" target="#b41">Yang and Chang, 2015)</ref>  <ref type="bibr">6</ref> and NTEL ( <ref type="bibr" target="#b42">Yang et al., 2016)</ref>  <ref type="bibr">7</ref> , two state-of-the-art en- tity linking systems for short texts. Our MMRN-EL is comparable to the best system. We also con- ducted two ablation studies by removing the entity type vectors (MMRN-EL -Entity), and by removing the LSTM vectors (MMRN-EL -LSTM). Both show significant performance drops, which validates the importance of these two additional input vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Semantic parsing</head><p>For semantic parsing, we use the dataset We- bQSP 8 ( ) in our experiments. This dataset is a clean and enhanced version of the widely used WebQuestions dataset <ref type="bibr" target="#b1">(Berant et al., 2013)</ref>, which consists of pairs of questions and an- swers found in Freebase. Compared to WebQues- tions, WebQSP excludes questions with ambigu- ous intent, and provides verified answers and full semantic parses to the remaining 4,737 questions.</p><p>We follow the implicit supervision setting in ( , using 3, 098 question-answer pairs for training, and 1, 639 for testing. A subset of 620 pairs from the training set is used for hyper- parameter tuning. Because there can be multiple answers to a question, the quality of a semantic parser is measured using the averaged F 1 score of the predicted answers.</p><p>We experiment with two configurations of in- corporating the entity linking component. MMRN- PIPELINE trains an MMRN-EL model using the en- tity linking labels in WebQSP separately. Given a question, the entities in it are first predicted, and used as input to the semantic parsing system. In contrast, MMRN-JOINT incorporates the MMRN-EL model in the whole framework. During this joint training process, 15 entity link results are sam- pled according to the current MMRN-EL model, and passed to the downstream networks. In both cases, we use the previous entity linking model trained on the NEEL dataset to initialize the pa- rameters. As discussed in Sec. 4.1, in this implicit supervision setting, we directly set the (delayed) reward function to be the F 1 score, which can be obtained by comparing the annotated answers with predicted answers. <ref type="table" target="#tab_2">Table 3</ref> summarizes the results of the MMRN- based semantic parsing systems and other strong baselines. The SP column reports the aver- aged F 1 scores. Compared to the pipeline ap- proach (MMRN-PIPELINE), the joint learning frame- work (MMRN-JOINT) improves significantly, reach- ing 68.1% F 1 . To compare different learning methods, we also apply REINFORCE <ref type="bibr" target="#b40">(Williams, 1992)</ref>, a popular policy gradient algorithm, to train our joint model using the same setting and re- ward function. 9 MMRN-JOINT outperforms REIN- FORCE and its variant, REINFORECE+, which re-normalizes the probabilities of the sampled can- didate sequences. Its result is also better than the state-of-the-art STAGG system. Note that we use the same architectures and initialization procedures for MMRN-PIPELINE/JOINT and REIN- FORCE/REINFORCE+. Therefore, the superior performance of MMRN-JOINT shows that the joint learning plays a crucial role in addition to the choices of architecture. Comparing to STAGG, note that  did not jointly train the entity linker and semantic parser together, but they did improve the results by taking the top 10 predic- tions of their entity linking system for re-ranking parses. Our algorithm further allows to update the entity linker with the labels for semantic parsing and shows superior performance.</p><p>Our joint model also improves the entity link- ing prediction on the questions in WebQSP us- ing the implicit signals (the EL columns in Ta- <ref type="bibr">9</ref> The REINFORCE algorithm uses warm initialization- the entity linking parameters are initialized using the model trained on the NEEL dataset.   <ref type="formula" target="#formula_2">(2016)</ref> proposed Neural Symbolic Machine (NSM) and reported the best result of 69.0 F1 score on the WebQSP dataset us- ing the weak supervision settings. <ref type="bibr">10</ref> The NSM architecture for semantic parsing is significantly different from the architecture used in (  and the one used in this paper. In contrast, MMRN is a general learning framework that allows joint training on existing models (i.e. entity link- ing and semantic parsing modules). This allows MMRN to use the labels of semantic parsing task as implicit supervision signals for the entity linking module. It would be interesting to apply MMRN on the newly proposed architectures as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>We discuss several issues that are highly related to MMRN in this section.</p><p>Learning to Search There are two main dif- ferences between MMRN and search-based algo- rithms, such as SEARN ( <ref type="bibr" target="#b11">Daumé et al., 2009)</ref> and DAGGER <ref type="bibr" target="#b34">(Ross et al., 2011</ref>). First, both SEARN and DAGGER focus on imitation learn- ing, assuming explicit supervision signals exist. They use a two-step model learning approach:</p><p>(1) create cost-sensitive examples by listing state- action pairs and their corresponding (estimated) losses; (2) apply cost-aware training algorithms. In contrast, MMRN directly updates the parameters using back-propagation based on search results of each example. Second, SEARN mixes the op- timal and current policies during learning, while MMRN performs search twice and simply pushes the current policy towards the optimal one. Re- cently,  extend this line of work and discuss different roll-in and roll-out strate- gies during training for structured contextual ban- dit settings. As MMRN uses two search procedures, there is no need to mix different search policies.</p><p>Reinforcement Learning In many reinforce- ment learning scenarios, the search space is not fully controllable by the agent. For example, a chess playing agent cannot control the move made by its opponent, and has to commit a single move and wait for the opponent. Note that the agent can still think ahead and build a search tree, but only one move can be made in the end. In contrast, in scenarios like semantic parsing, the whole search space is controlled by the agent itself. Therefore, from the initial state, we can explore several search paths and get their real rewards. This may ex- plain why MMRN can be more efficient than RE- INFORCE, as MMRN can use the reward signals of multiple paths more effectively. In addition, MMRN is not a probabilistic model, so it does not need to handle normalization issues, which often causes large variance in estimating the gradient direction when optimizing the expected reward.</p><p>Semantic Parsing MMRN can be applied for many semantic parsing tasks. One key step is to design the right approximated reward for a given task to guide the beam search to nd the reference parses in MMRN, given that the actual reward is of- ten very sparse. In our companion paper, <ref type="bibr" target="#b19">(Iyyer et al., 2017)</ref>, we used a simple form of approx- imated reward to get feedback as early as possi- ble during search. In other words, the semantic parse will be executed as soon as the parse is ex- ecutable (even if the parse is still not completed) during search. The execution results will be used to calculate the Jaccard coefficient with respect to the labeled answers as the approximated rewards. The use of approximated reward has been proven to be effective in <ref type="bibr" target="#b19">(Iyyer et al., 2017</ref>).</p><p>An important research direction for semantic parsing is to reduce the supervision cost. In , the authors demonstrated that label- ing semantic parses is possible and often more effective with a sophisticated labeling interface. However, collecting answers may still be easier or faster for certain problems or annotators. This sug- gests that we could allow the annotators to choose to label semantic parses or answers in order to minimize the supervision cost. MMRN would be an ideal learning algorithm for this scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>This paper proposes Maximum Margin Reward Networks, a structured learning framework that can learn from both explicit and implicit supervi- sion signals. In the future, we plan to apply Max- imum Margin Reward Networks on other struc- tured learning tasks. Improving MMRN for dealing with large search space is an important future di- rection as well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Explicit Supervision: Named Entity 
Recognition. Our MMRN with beam size 20 out-
performs current best systems, which are based on 
neural networks. 

NEEL-Test TACL 
F 1 
F 1 
S-MART 
77.7 
63.6 
NTEL 
77.9 
68.1 
MMRN-EL 
78.5 
67.5 
MMRN-EL -Entity 
77.4 
66.5 
MMRN-EL -LSTM 
76.6 
66.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Implicit Supervision: Semantic Pars-
ing. By updating the entity linking and semantic 
parsing models jointly, MMRN-JOINT improves over 
MMRN-PIPELINE by 5 points in F 1 and outperforms 
REINFORCE+ (SP). It also improves the entity 
linking result on the WebQSP questions (EL). 

ble 3). The F 1 score of MMRN-JOINT on entity link-
ing is 2.4 points higher than the baseline MMRN-
PIPELINE. Note that the entity linking results of 
MMRN-PIPELINE (line 1) are exactly the results of 
the entity linking component MMRN-EL. The result 
is also better than REINFORCE, and comparable 
to REINFORCE+. 
Recently Liang et al. </table></figure>

			<note place="foot" n="1"> Existing weakly supervised methods (Clarke et al., 2010; Artzi and Zettlemoyer, 2013) often leverage domain-specific heuristics, which are not always available.</note>

			<note place="foot" n="3"> Available at http://nlp.stanford.edu/projects/glove/ 4 NEEL dataset was originally created for an entity linking competition: http://microposts2016.seas. upenn.edu/challenge.html</note>

			<note place="foot" n="5"> Available at https://code.google.com/archive/p/word2vec/ 6 The winning system of the NEEL challenge. 7 To have a fair comparison, we compare to the results of NTEL which do not use pretrained user embedding. 8 Available at http://aka.ms/WebQSP</note>

			<note place="foot" n="10"> The paper is published after the submission of this paper.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their in-sightful comments. The first author is partly sponsored by DARPA under agreement number FA8750-13-2-0008. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and con-clusions contained herein are those of the authors and should not be interpreted as necessarily rep-resenting the official policies or endorsements, ei-ther expressed or implied, of DARPA or the U.S. Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Weakly supervised learning of semantic parsers for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Making sense of microposts:(# microposts2014) named entity extraction &amp; linking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amparo</forename><forename type="middle">E</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Rowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Stankovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aba-Sah</forename><surname>Dadzie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CEUR Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ERD&apos;14: entity recognition and disambiguation challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-June Paul</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR Forum</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to search better than your teacher</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nichols</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08308</idno>
		<title level="m">Named entity recognition with bidirectional LSTM-CNNs</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Driving semantic parsing from the world&apos;s response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">Roth</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGDAT</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incremental parsing with the perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Search-based structured prediction. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning as search optimization: approximate large margin methods for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Language to logical form with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In ACL</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Entity linking on microblogs with spatial and temporal signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microblog entity linking by leveraging extra posts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<title level="m">Bidirectional LSTM-CRF models for sequence tagging</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Search-based neural structured learning for sequential question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cutting-plane training of structural svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunnam John</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cmu</forename><surname>Edu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umd</forename><forename type="middle">Edu</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02206</idno>
		<title level="m">Learning to search better than your teacher</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<title level="m">Neural architectures for named entity recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Neural symbolic machines: Learning semantic parsers on freebase with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Kenneth D Forbus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00020</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bidirectional LSTM for named entity recognition in twitter messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nut</forename><surname>Limsopatham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WNUT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramón</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">End-to-end sequence labeling via bi-directional LSTM-CNNSCRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01354</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<title level="m">Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing atari with deep reinforcement learning</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving information extraction by acquiring external evidence with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Yala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Louis-Philippe Morency, Morency Collins, and Trevor Darrell</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariadna</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sybor</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hidden conditional random fields. PAMI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Bagnell</surname></persName>
		</author>
		<editor>AISTATS</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veda</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Max-margin markov networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning from explicit and implicit supervision jointly for algebra word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">S-mart: Novel tree-based structured learning algorithms applied to tweet entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Toward socially-infused information extraction: Embedding authors, mentions, and entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Semantic parsing via staged query graph generation: Question answering with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The value of semantic parse labeling for knowledge base question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jina</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning structural SVMs with latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Nam John</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
