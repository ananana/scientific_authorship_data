<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">EECS Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1580" to="1590"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A convex relaxation for weakly supervised relation extractionÉdouard extraction´extractionÉdouard Grave Abstract A promising approach to relation extraction , called weak or distant supervision, exploits an existing database of facts as training data, by aligning it to an unla-beled collection of text documents. Using this approach, the task of relation extraction can easily be scaled to hundreds of different relationships. However, distant supervision leads to a challenging multiple instance, multiple label learning problem. Most of the proposed solutions to this problem are based on non-convex formulations , and are thus prone to local minima. In this article, we propose a new approach to the problem of weakly supervised relation extraction, based on dis-criminative clustering and leading to a convex formulation. We demonstrate that our approach outperforms state-of-the-art methods on the challenging dataset introduced by Riedel et al. (2010).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Information extraction refers to the broad task of automatically extracting structured information from unstructured documents. An example is the extraction of named entities and the relations be- tween those entities from natural language texts. In the age of the world wide web and big data, information extraction is quickly becoming perva- sive. For example, in 2013, more than 130, 000 scientific articles were published about cancer. Keeping track with that quantity of information is almost impossible, and it is thus of utmost im- portance to transform the knowledge contained in this massive amount of documents into structured databases.</p><p>Traditional approaches to information extrac- tion relies on supervised learning, yielding high  precision and recall results ( <ref type="bibr">Zelenko et al., 2003)</ref>. Unfortunately, these approaches need large amount of labeled data, and thus do not scale well to the great number of different types of fact found on the Web or in scientific articles. A promising approach, called distant or weak supervision, is to exploit an existing database of facts as training data, by aligning it to an unlabeled collection of text documents <ref type="bibr">(Craven and Kumlien, 1999)</ref>. In this article, we are interested in weakly super- vised extraction of binary relations. A challenge pertaining to weak supervision is that the obtained training data is noisy and ambiguous ( <ref type="bibr">Riedel et al., 2010)</ref>. Let us start with an example: if the fact Attended(Turing, King s College) exists in the knowledge database and we observe the sen- tence</p><p>Turing studied as an undergraduate from 1931 to 1934 at King's College, Cambridge. which contains mentions of both entities Turing and King s College, then this sentence might ex- press the fact that Alan Turing attended King's College, and thus, might be a useful example for learning to extract the relation Attended. How- ever, the sentence Celebrations for the centenary of Alan Tur- ing are being planned at King's College. also contains mentions of Turing and King s College, but do not express the re- lation Attended. Thus, weak supervision lead to noisy examples. As noted by <ref type="bibr">Riedel et al. (2010)</ref>, such negative extracted sentences for existing facts can represent more than 30% of the data. Moreover, a given pair of entities, such as (Roy Lichtenstein, New York City), car verify multiple relations, such as BornIn and DiedIn. Weak supervision thus lead to ambiguous examples.</p><p>This challenge is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. A solution to address it is to formulate the task of weakly su- pervised relation extraction as a multiple instance, multiple label learning problem ( <ref type="bibr">Hoffmann et al., 2011;</ref><ref type="bibr">Surdeanu et al., 2012</ref>). However, these for- mulations are often non-convex and thus suffer from local minimum.</p><p>In this article, we make the following contribu- tions:</p><p>• We propose a new convex relaxation for the problem of weakly supervised relation ex- traction, based on discriminative clustering,</p><p>• We propose an efficient algorithm to solve the associated convex program,</p><p>• We demonstrate that our approach obtains state-of-the-art results on the dataset intro- duced by <ref type="bibr">Riedel et al. (2010)</ref>.</p><p>To our knowledge, this paper is the first to propose a convex formulation for solving the problem of weakly supervised relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Supervised learning. Many approaches based on supervised learning have been proposed to solve the problem of relation extraction, and the corresponding literature is to large to be summa- rized here. One of the first supervised method for relation extraction was inspired by syntactic pars- ing: the system described by <ref type="bibr">Miller et al. (1998)</ref> combines syntactic and semantic knowledge, and thus, part-of-speech tagging, parsing, named en- tity recognition and relation extraction all happen at the same time. The problem of relation ex- traction was later formulated as a classification problem: <ref type="bibr">Kambhatla (2004)</ref> proposed to solve this problem using maximum entropy models using lexical, syntactic and semantic features. Kernel methods for relation extraction, based on shallow parse trees or dependency trees were introduced by <ref type="bibr">Zelenko et al. (2003)</ref>, <ref type="bibr">Culotta and Sorensen (2004)</ref> and <ref type="bibr">Bunescu and Mooney (2005)</ref>.  <ref type="bibr">, 2006</ref>) or self-supervision ( <ref type="bibr" target="#b1">Banko et al., 2007)</ref>.</p><p>One of the limitations of these systems is the fact that they extract uncanonicalized relations.</p><p>Weakly supervised learning. Weakly super- vised learning refers to a broad class of meth- ods, in which the learning system only have ac- cess to partial, ambiguous and noisy labeling. <ref type="bibr">Craven and Kumlien (1999)</ref> were the first to pro- pose a weakly supervised relation extractor. They aligned a knowledge database (the Yeast Protein Database) with scientific articles mentioning a par- ticular relation, and then used the extracted sen- tences to learn a classifier for extracting that rela- tion.</p><p>Later, many different sources of weak label- ings have been considered. <ref type="bibr">Bellare and McCallum (2007)</ref> proposed a method to extract bibliographic relations based on conditional random fields and used a database of BibTex entries as weak super- vision. <ref type="bibr">Wu and Weld (2007)</ref>  Multiple instance learning. The methods we previously mentionned transform the weakly su- pervised problem into a fully supervised one, lead- ing to noisy training datasets (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Mul-tiple instance learning ( <ref type="bibr">Dietterich et al., 1997</ref>) is a paradigm in which the learner receives bags of examples instead of individual examples. A pos- itively labeled bag contains at least one positive example, but might also contains negative exam- ples. In the context of relation extraction, <ref type="bibr">Bunescu and Mooney (2007)</ref> introduced a kernel method for multiple instance learning, while <ref type="bibr">Riedel et al. (2010)</ref> proposed a solution based on a graphical model. Both these methods allow only one label per bag, which is an asumption that is not true for relation extraction (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Thus, <ref type="bibr">Hoffmann et al. (2011)</ref> proposed a multiple instance, multi- ple label method, based on an undirected graphical model, to solve the problem of weakly supervised relation extraction. Finally, <ref type="bibr">Surdeanu et al. (2012)</ref> also proposed a graphical model to solve this prob- lem. One of their main contributions is to cap- ture dependencies between relation labels, such as the fact that two labels cannot be generated jointly (e.g. the relations SpouseOf and BornIn).</p><p>Discriminative clustering. Our approach is based on the discriminative clustering framework, introduced by <ref type="bibr">Xu et al. (2004)</ref>. The goal of dis- criminative clustering is to find a labeling of the data points leading to a classifier with low classifi- cation error. Different formulations of discrimina- tive clustering have been proposed, based on sup- port vector machines ( <ref type="bibr">Xu et al., 2004</ref>), the squared loss ( <ref type="bibr" target="#b0">Bach and Harchaoui, 2007)</ref> or the logistic loss ( <ref type="bibr">Joulin et al., 2010)</ref>. A big advantage of dis- criminative clustering is that weak supervision or prior information can easily be incorporated. Our work is closely related to the method proposed by <ref type="bibr">Bojanowski et al. (2013)</ref> for learning the names of characters in movies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Weakly supervised relation extraction</head><p>In this article, our goal is to extract binary relations between entities from natural lan- guage text. Given a set of entities, a binary relation r is a collection of ordered pairs of entities. The statement that a pair of entities (e 1 , e 2 ) belongs to the relation r is denoted by r(e 1 , e 2 ) and this triple is called a fact or relation instance. For example, the fact that Ernest Hemingway was born in Oak Park is denoted by BornIn(Ernest Hemingway, Oak Park). A given pair of entities, such as (Edouard Manet, Paris), can belong to different relations, such as BornIn and DiedIn. An entity mention is a contiguous sequence of tokens refering to an entity, while a pair mention or relation mention candidate is a sequence of text in which a pair of entities is mentioned. In the following, relation mention candidates will be re- stricted to pair of entities that are mentioned in the same sentence. For example, the sentence:</p><p>Ernest Hemingway was born in Oak Park.</p><p>contains two entity mentions, corresponding to two relation mention candidates.</p><p>In- deed, the pairs (Hemingway, Oak Park) and (Oak Park, Hemingway) are two distinct pairs of entities, where only the first one verifies the rela- tion BornIn.</p><p>Given a text corpus, aggregate extraction corre- sponds to the task of extracting a set of facts, such that each extracted fact is expressed at least once in the corpus. On the other hand, the task of senten- tial extraction corresponds to labeling each rela- tion mention candidate by the relation it expresses, or by a None label if it does not express any rela- tion. Given a solution to the sentential extraction problem, it is possible to construct a solution for the aggregate extraction problem by returning all the facts that were detected. We will follow this approach, by building an instance level classifier, and aggregating the results by extracting the facts that were detected at least once in the corpus.</p><p>In the following, we will describe a method to learn such a classifier using a database of facts in- stead of a set of labeled sentences. This setting is known as distant supervision or weak supervi- sion, since we do not have access to labeled data on which we could directly train a sentence level relation extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">General approach</head><p>In this section, we propose a two step procedure to solve the problem of weakly supervised relation extraction:</p><p>1. First, we describe a method to infer the re- lation labels corresponding to each relation mention candidate of our training set, 2. Second, we train a supervised instance level relation extractor, using the labels infered during step 1.</p><p>In the second step of our approach, we will simply use a multinomial logistic regression model. We</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(Lichtenstein, New York City)</head><p>Roy Lichtenstein was born in New York City.</p><p>Lichtenstein left New York to study in Ohio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BornIn</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DiedIn</head><p>N relation mention candidates represented by vectors x n I pairs of entities p i K relations</p><formula xml:id="formula_0">E in R ik</formula><p>Figure 2: Instance of the weakly supervised relation extraction problem, with notations used in the text.</p><p>now describe the approach we propose for the first step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Notations</head><p>Let (p i ) 1≤i≤I be a collection of I pairs of entities. We suppose that we have N relation mention can- didates, represented by the vectors (x n ) <ref type="bibr">1≤n≤N</ref> . Let E ∈ R I×N be a matrix such that E in = 1 if the relation mention candidate n corresponds to the pair of entities i, and E in = 0 otherwise. The ma- trix E thus indicates which relation mention can- didate corresponds to which pair of entities. We suppose that we have K relations, indexed by the integers {1, ..., K}. Let R ∈ R I×K be a matrix such that R ik = 1 if the pair of entities i verifies the relation k, and R ik = 0 otherwise. The matrix R thus represents the knowledge database. See <ref type="figure">Fig. 2</ref> for an illustration of these notations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Problem formulation</head><p>Our goal is to infer a binary matrix Y ∈ {0, 1} N ×(K+1) , such that Y nk = 1 if the relation mention candidate n express the relation k and Y nk = 0 otherwise (and thus, the integer K + 1 represents the relation None).</p><p>We take an approach inspired by the discrimi- native clustering framework of <ref type="bibr">Xu et al. (2004)</ref>. We are thus looking for a (K + 1)-class indicator matrix Y, such that the classification error of an optimal multiclass classifier f is minimum. Given a multiclass loss function and a regularizer Ω, this problem can be formulated as:</p><formula xml:id="formula_1">min Y min f N n=1 (y n , f (x n )) + Ω(f ), s.t. Y ∈ Y</formula><p>where y n is the nth line of Y. The constraints Y ∈ Y are added in order to take into account the information from the weak supervision. We will describe in the next section what kind of con- straints are considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Weak supervision by constraining Y</head><p>In this section, we show how the information from the knowledge base can be expressed as con- straints on the matrix Y.</p><p>First, we suppose that each relation mention candidate express exactly one relation (including the None relation). This means that the matrix Y contains exactly one 1 per line, which is equivalent to the constraint:</p><formula xml:id="formula_2">∀n ∈ {1, ..., N }, K k=1 Y nk = 1.</formula><p>Second, if the pair i of entities verifies the rela- tion k we suppose that at least one relation men- tion candidate indeed express that relation. Thus we want to impose that for at least one relation mention candidate n such that E in = 1, we have Y nk = 1. This is equivalent to the constraint:</p><formula xml:id="formula_3">∀(i, k) such that R ik = 1, N n=1 E in Y nk ≥ 1.</formula><p>Third, if the pair i of entities does not verify the re- lation k, we suppose that no relation mention can- didate express that relation. Thus, we impose that for all mention candidate n such that E in = 1, we have Y nk = 0. This is equivalent to the constraint:</p><formula xml:id="formula_4">∀(i, k) such that R ik = 0, N n=1 E in Y nk = 0.</formula><p>Finally, we do not want too many relation men- tion candidates to be classified as None. We thus impose ∀i ∈ {1, ..., I},</p><formula xml:id="formula_5">N n=1 E in Y n(K+1) ≤ c N n=1 E in ,</formula><p>where c is the proportion of relation mention can- didates that do not express a relation, for entity pairs that appears in the knowledge database.</p><p>We can rewrite these constraints using only ma- trix operations in the following way:</p><formula xml:id="formula_6">Y1 = 1 (EY) • S ≥ ˜ R,<label>(1)</label></formula><p>where • is the Hadamard product (a.k.a. the ele- mentwise product), the matrix S ∈ R I×(K+1) is defined by</p><formula xml:id="formula_7">S ik = 1 if R ik = 1 −1 if R ik = 0 or k = K + 1,</formula><p>and the matrix˜Rmatrix˜ matrix˜R ∈ R I×(K+1) is defined by˜R</p><formula xml:id="formula_8">by˜ by˜R = [R, −cE1].</formula><p>The set Y is thus defined as the set of matrices Y ∈ {0, 1} N ×(K+1) that verifies those two linear constraints. It is important to note that besides the boolean constraints, the two other constraints are convex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Squared loss and convex relaxation</head><p>In this section, we describe the problem we ob- tain when using the squared loss, and its associated convex relaxation. We then introduce an efficient algorithm to solve this problem, by computing its dual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Primal problem</head><p>Following <ref type="bibr" target="#b0">Bach and Harchaoui (2007)</ref>, we use lin- ear classifiers W ∈ R D×(K+1) , the squared loss and the squared 2 -norm as the regularizer. In that case, our formulation becomes:</p><formula xml:id="formula_9">min Y,W 1 2 Y − XW 2 F + λ 2 W 2 F , s.t. Y ∈ {0, 1} N ×(K+1) Y1 = 1, (EY) • S ≥ R.</formula><p>where · F is the Frobenius norm and the ma- trix X = [x 1 , ..., x N ] ∈ R N ×D represents the relation mention candidates. Thanks to using the squared loss, we have a closed form solution for the matrix W:</p><formula xml:id="formula_10">W = (X X + λI D ) −1 X Y.</formula><p>Replacing the matrix W by its optimal solution, we obtain the following cost function:</p><formula xml:id="formula_11">min Y 1 2 Y (I N − X(X X + λI D ) −1 X )Y.</formula><p>Then, by applying the Woodbury matrix identity and relaxing the constraint Y ∈ {0,</p><formula xml:id="formula_12">1} N ×(K+1) into Y ∈ [0, 1] N ×(K+1)</formula><p>, we obtain the following convex quadratic problem in Y:</p><formula xml:id="formula_13">min Y 1 2 tr Y (XX + λI N ) −1 Y , s.t. Y ≥ 0, Y1 = 1, (EY) • S ≥ R.</formula><p>Since the inequality constraints might be in- feasible, we add the penalized slack variables ξ ∈ R I×(K+1) , finally obtaining:</p><formula xml:id="formula_14">min Y,ξ 1 2 tr Y (XX + λI N ) −1 Y + µξ 1 s.t. Y ≥ 0, ξ ≥ 0, Y1 = 1, (EY) • S ≥ R − ξ.</formula><p>This convex problem is a quadratic program. In the following section, we will describe how to solve this problem efficiently, by exploiting the structure of its dual problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dual problem</head><p>The matrix Q = (XX + λI N ) appearing in the quadratic program is an N by N matrix, where N is the number of mention relation candidates. Computing its inverse is thus expensive, since N can be large. Instead, we propose to solve the dual of this problem. Introducing dual variables Λ ∈ R I×(K+1) , Σ ∈ R N ×(K+1) and ν ∈ R N , the dual problem is equal to</p><formula xml:id="formula_15">min Λ,Σ,ν 1 2 tr Z QZ − tr Λ R − ν 1 s.t. 0 ≤ Λ ik ≤ µ, 0 ≤ Σ nk ,</formula><p>where</p><formula xml:id="formula_16">Z = E (S • Λ) + Σ + ν1 .</formula><p>The derivation of this dual problem is given in Ap- pendix A. Solving the dual problem instead of the primal has two main advantages. First, the dual does not depend on the inverse of the matrix Q, while the primal does. Since traditional features used for re- lation extraction are indicators of lexical, syntactic and named entities properties of the relation men- tion candidates, the matrix X is extremely sparse.</p><p>Using the dual problem, we can thus exploit the sparsity of the matrix X in the optimization pro- cedure. Second, the constraints imposed on dual variables are simpler than constraints imposed on primal variables. Again, we will exploit this struc- ture in the proposed optimization procedure.</p><p>Given a solution of the dual problem, the asso- ciated primal variable Y is equal to:</p><formula xml:id="formula_17">Y = (XX + λI N )Z.</formula><p>Thus, we do not need to compute the inverse of the matrix (XX + λI N ) to obtain a solution to the primal problem once we have solved the dual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Optimization of the dual problem</head><p>We propose to solve the dual problem using the accelerated projected gradient descent algo- rithm <ref type="bibr">(Nesterov, 2007;</ref><ref type="bibr" target="#b2">Beck and Teboulle, 2009)</ref>. Indeed, computing the gradient of the dual cost function is efficient, since the matrix X is sparse. Moreover, the constraints on the dual variables are simple and it is thus efficient to project onto this set of constraints. See Appendix B for more de- tails.</p><p>Complexity. The overall complexity of one step of the accelerated projected gradient descent al- gorithm is O <ref type="figure">(N F K)</ref>, where F is the average number of features per relation mention candi- date. This means that the complexity of solving the quadratic problem corresponding to our ap- proach is linear with respect to the number N of relation mention candidates, and thus our algo- rithm can scale to large datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Discussion</head><p>Before moving to the experimental sections of this article, we would like to discuss some properties of our approach.</p><p>Kernels. First of all, one should note that our proposed formulation only depends on the (lin- ear) kernel matrix XX T . It is thus possible to re- place this matrix by any other kernel. However, in the case of a general kernel, the optimization algorithm presented in the previous section has a quadratic complexity O(KN 2 ) with respect to the number N of relation mention candidates, and it is thus not applicable as is. We plan to explore the use of kernels in future work. <ref type="bibr">K+1)</ref> of the relaxed problem, a very sim- ple way to obtain a relation label for each relation mention candidate of the training set is to com- pute the orthogonal projection of the matrix Y on the set of indicator matrices</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rounding. Given a continuous solution</head><formula xml:id="formula_18">Y ∈ [0, 1] N ×(</formula><formula xml:id="formula_19">M ∈ {0, 1} N ×(K+1) | M1 = 1 .</formula><p>This projection consists in taking the maximum value along the rows of the matrix Y. It should be noted that the obtained matrix does not neces- sarily verify the inequality constraints defined in Eq. 1. In the following, we will use this rounding, refered to as argmax rounding, to obtain relation labels for each relation mention candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Dataset and features</head><p>In this section, we describe the dataset used in the experimental section and the features used to rep- resent the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Dataset</head><p>We consider the dataset introduced by <ref type="bibr">Riedel et al. (2010)</ref>. This dataset consists of articles from the New York Times corpus <ref type="bibr">(Sandhaus, 2008)</ref>, from which named entities where extracted and tagged using the Stanford named entity recog- nizer ( <ref type="bibr">Finkel et al., 2005</ref>). Consecutive tokens with the same category were treated as a single mention. These named entity mentions were then aligned with the Freebase knowledge database, by using a string match between the mentions and the canonical names of entities in Freebase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Features</head><p>We use the features extracted by <ref type="bibr">Riedel et al. (2010)</ref>, which were first introduced by <ref type="bibr">Mintz et al. (2009)</ref>. These features capture how two en- tity mentions are related in a given sentence, based on syntactic and lexical properties. Lexical fea- tures include: the sequence of words between the two entities, a window of k words before the first entity and after the second entity, the correspond- ing part-of-speech tags, etc.. Syntactic features are based on the dependency tree of the sentence, and include: the path between the two entities, neigh- bors of the two entities that do not belong to the path. The OpenNLP 3 part-of-speech tagger and the Malt parser ( <ref type="bibr">Nivre et al., 2007)</ref> were used to extract those features.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mintz et al. (2009) Hoffmann et al. (2011) Surdeanu et al. (2012) This work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Implementation details</head><p>In this section, we discuss some important imple- mentation details.</p><p>Kernel normalization. We normalized the ker- nel matrix XX , so that its diagonal coefficients are equal to 1. This corresponds to normalizing the vectors x n so that they have a unit 2 -norm.</p><p>Choice of parameters. We kept 20% of the ex- amples from the training set as a validation set, in order to choose the parameters of our method. We then re-train a model on the whole training set, us- ing the chosen parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experimental evaluation</head><p>In this section, we evaluate our approach to weakly supervised relation extraction by comparing it to state-of-the art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Baselines</head><p>We now briefly present the different methods we compare to.</p><p>Mintz et al. This baseline corresponds to the method described by <ref type="bibr">Mintz et al. (2009)</ref>. We use the implementation of <ref type="bibr">Surdeanu et al. (2012)</ref>, which slightly differs from the original method: each relation mention candidate is treated inde- pendently (and not collapsed across mentions for a given entity pair). This strategy allows to predict multiple labels for a given entity pair, by OR-ing the predictions for the different mentions.</p><p>Hoffmann et al. This method, introduced by <ref type="bibr">Hoffmann et al. (2011)</ref>, is based on probabilis- tic graphical model of multi-instance multi-label learning. They proposed a learning method for this model, based on the perceptron algo- rithm <ref type="bibr">(Collins, 2002</ref>) and a greedy search for the inference. We use the publicly available code of Hoffmann et al. <ref type="bibr">4</ref> .</p><p>Surdeanu et al. Finally, we compare our method to the one described by <ref type="bibr">Surdeanu et al. (2012)</ref>. This method is based on a two-layer graphical model, the first layer corresponding to Precision /location/location/contains /people/person/place_lived /person/person/nationality /people/person/place_of_birth /business/person/company a relation classifier at the mention level, while the second layer is aggregating the different predic- tion for a given entity pair. In particular, this sec- ond layer capture dependencies between relation labels, such as the fact that two labels cannot be generated jointly (e.g. the relations SpouseOf and BornIn). This model is trained by using hard discriminative Expectation-Maximization. We use the publicly available code of Surdeanu et al. <ref type="bibr">5</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Precision / recall curves</head><p>Following standard practices in relation extrac- tion, we report precision/recall curves for the dif- ferent models. In order to rank aggregate extrac- tions for our model, the score of an extracted fact r(e 1 , e 2 ) is set to the maximal score of the differ- ent extractions of that fact. This is sometimes ref- ered to as the soft-OR function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Discussion</head><p>Comparison with the state-of-the-art. We re- port results for the different methods on the dataset 5 nlp.stanford.edu/software/mimlre.shtml introduced by <ref type="bibr">Riedel et al. (2010)</ref> in <ref type="figure" target="#fig_4">Fig. 3</ref>. We observe that our approach generally outperforms the state of the art. Indeed, at equivalent recall, our method achieves better (or similar) precision than the other methods, except for very low re- call (smaller than 0.05). The improvement over the methods proposed by <ref type="bibr">Hoffmann et al. (2011)</ref> and <ref type="bibr">Surdeanu et al. (2012)</ref>, which are currently the best published results on this dataset, can be as high as 5 points in precision for the same recall point. Moreover, our method achieves a higher re- call (0.30) than these two methods (0.25).</p><p>Performance per relation. The dataset in- troduced by <ref type="bibr">Riedel et al. (2010)</ref> is highly unbalanced: for example, the most common relation, /location/location/contains, rep- resents almost half of the positive relations, while some relations are mentioned less than ten times. We thus decided to also report precision/recall curves for the five most common relations of that dataset in <ref type="figure" target="#fig_6">Fig. 4</ref>. First, we observe that the perfomances vary a lot from a relation to another. Upon examination of the data, this can partly be explained by the fact that al- most no sentences extracted for the relation /people/person/place of birth in fact express this relation. In other words, many facts present in Freebase are not expressed in the corpus, and are thus impossible to extract. On the other hand, most facts for the relation /people/person/place lived are missing in Freebase. Therefore, many extractions produced by our system are considered false, but are in fact true positives. The problem of incomplete knowledge base was studied by <ref type="bibr">Min et al. (2013)</ref>.</p><p>Sentential extraction. We finally report preci- sion/recall curves for the task of sentential extrac- tion, in <ref type="figure" target="#fig_7">Fig. 5</ref>, using the manually labeled dataset of <ref type="bibr">Hoffmann et al. (2011)</ref>. We observe that for most values of recall, our method achieves simi- lar precision that the one proposed by <ref type="bibr">Hoffmann et al. (2011)</ref>, while extending the highest recall from 0.52 to 0.68. Thanks to this higher recall, our method achieves a highest F1 score of 0.66, com- pared to 0.61 obtained by the method proposed by <ref type="bibr">Hoffmann et al. (2011)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Runtime</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mintz et al. (2009) 7 min Hoffmann et al. (2011) 2 min Surdeanu et al. (2012) 3 hours This work</head><p>3 hours <ref type="table">Table 1</ref>: Comparison of running times for the dif- ferent methods compared in the experimental sec- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this article, we introduced a new formulation for weakly supervised relation extraction. Our method is based on a constrained discriminative formulation of the multiple instance, multiple la- bel learning problem. Using the squared loss, we obtained a convex relaxation of this formula- tion, allowing us to obtain an approximate solu- tion to the initial integer quadratic program. Thus, our method is not sensitive to initialization. We demonstrated the competitiveness of our approach on the dataset introduced by <ref type="bibr">Riedel et al. (2010)</ref>, on which our method outperforms the state of the art methods for weakly supervised relation extrac- tion, on both aggregate and sentential extraction. As noted earlier, another advantage of our method is the fact that it is easily kernelizable. We would like to explore the use of kernels, such as the ones introduced by <ref type="bibr">Zelenko et al. (2003)</ref>, <ref type="bibr">Culotta and Sorensen (2004)</ref> and <ref type="bibr">Bunescu and Mooney (2005)</ref>, in future work. We believe that such kernels could improve the relatively low re- call obtained so far by weakly supervised method for relation extraction. <ref type="bibr">Kedar Bellare and Andrew McCallum. 2007. Learn</ref> ing extractors from unlabeled text using relevant databases. In Sixth international workshop on in- formation integration on the web. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of a knowledge database comprising two facts and training sentences obtained by aligning this database to unlabeled text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Unsupervised learning.</head><label></label><figDesc>The open information extraction paradigm, simultaneously proposed by Shinyama and Sekine (2006) and Banko et al. (2007), does not rely on any labeled data or even existing relations. Instead, open information ex- traction systems only use an unlabeled corpus, and output a set of extracted relations. Such systems are based on clustering (Shinyama and Sekine</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>described a method to learn relations based on Wikipedia infoboxes. Knowledge databases, such as Freebase 1 (Mintz et al., 2009; Sun et al., 2011) and YAGO 2 (Nguyen and Moschitti, 2011) were also considered as a source of weak supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Precision/recall curves for different methods on the Riedel et al. (2010) dataset, for the task of aggregate extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Precision/recall curves per relation for our method, on the Riedel et al. (2010) dataset, for the task of aggregate extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Precision/recall curves for the task of sentential extraction, on the manually labeled dataset of Hoffmann et al. (2011).</figDesc></figure>

			<note place="foot" n="1"> www.freebase.com 2 www.mpi-inf.mpg.de/yago-naga/yago</note>

			<note place="foot" n="3"> opennlp.apache.org</note>

			<note place="foot" n="4"> www.cs.washington.edu/ai/raphaelh/mr/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The author is supported by a grant from Inria (Associated-team STATWEB) and would like to thank Armand Joulin for helpful discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Derivation of the dual</head><p>In this section, we derive the dual problem of the quadratic program of section 5. We introduce dual variables Λ ∈ R I×(K+1) , Σ ∈ R N ×( <ref type="bibr">K+1)</ref> , Ω ∈ R I×(K+1) and ν ∈ R N , such that Λ ≥ 0, Σ ≥ 0 and Ω ≥ 0.</p><p>The Lagrangian of the problem is</p><p>To find the dual function g we minimize the La- grangian over Y and ξ. Minimizing over ξ, we find that the dual function is equal to −∞ unless µ − Λ ik − Ω ik = 0, in which case, we are left with</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Minimizing over Y, we then obtain</head><p>Replacing Y by its optimal value, we then obtain the dual function</p><p>where</p><p>Thus, the dual problem is</p><p>We can then eliminate the dual variable Ω, since the constraints Ω ik = µ − Λ ik and Ω ik ≥ 0 are equivalent to µ ≥ Λ ik . We finally obtain</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B Optimization details</head><p>Gradient of the dual cost function. The gradi- ent of the dual cost function f with respect to the dual variables Σ, Λ and ν is equal to</p><p>The most expensive step to compute those gra- dients is to compute the matrix product XX Z.</p><p>Since the matrix X is sparse, we efficiently com- pute this product by first computing the product X Z, and then by left multiplying the result by X. The complexity of these two operations is</p><p>where F is the average number of fea- tures per relation mention candidate.</p><p>Projecting Σ and Λ. The componentwise pro- jection operators associated to the constraints on Σ and Λ are defined by:</p><p>The complexity of projecting Σ and Λ is O(N K). Thus, the cost of those operations is ne gligible compared to the cost of computing the gradients of the dual cost function.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DIFFRAC: a discriminative and flexible framework for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Za¨ıdza¨ıd</forename><surname>Harchaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Open information extraction for the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Broadhead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A fast iterative shrinkage-thresholding algorithm for linear inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
