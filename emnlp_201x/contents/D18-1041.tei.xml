<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Domain Neural Machine Translation with Word-Level Domain Context Discrimination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiali</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huating</forename><surname>Wen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xie</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Mobile Internet Group</orgName>
								<orgName type="institution">Tencent Technology Co</orgName>
								<address>
									<settlement>Ltd, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjing</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Meiya Pico information Co</orgName>
								<address>
									<settlement>Ltd, Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Domain Neural Machine Translation with Word-Level Domain Context Discrimination</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="447" to="457"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>447</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>With great practical value, the study of Multi-domain Neural Machine Translation (NMT) mainly focuses on using mixed-domain parallel sentences to construct a unified model that allows translation to switch between different domains. Intuitively, words in a sentence are related to its domain to varying degrees, so that they will exert disparate impacts on the multi-domain NMT modeling. Based on this intuition, in this paper, we devote to distinguishing and exploiting word-level domain contexts for multi-domain NMT. To this end, we jointly model NMT with monolingual attention-based domain classification tasks and improve NMT as follows: 1) Based on the sentence representations produced by a domain classifier and an adversarial domain classifier, we generate two gating vectors and use them to construct domain-specific and domain-shared annotations, for later translation predictions via different attention models; 2) We utilize the attention weights derived from target-side domain classifier to adjust the weights of target words in the training objective, enabling domain-related words to have greater impacts during model training. Experimental results on Chinese-English and English-French multi-domain translation tasks demonstrate the effectiveness of the proposed model. Source codes of this paper are available on Github https://github.com/DeepLearnXMU/WDCNMT.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, neural machine translation (NMT) has achieved great advancement ( <ref type="bibr" target="#b22">Nal and Phil, 2013;</ref><ref type="bibr" target="#b31">Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>. However, two difficulties are encoun- tered in the practical applications of NMT. On the one hand, training a NMT model for a spe- cific domain requires a large quantity of paral- lel sentences in such domain, which is often not readily available. Hence, the much more com- mon practice is to construct NMT models using mixed-domain parallel sentences. In this way, the domain-shared translation knowledge can be fully exploited. On the other hand, the translated sentences often belong to multiple domains, thus requiring a NMT model general to different do- mains. Since the textual styles, sentence structures and terminologies in different domains are of- ten remarkably distinctive, whether such domain- specific translation knowledge is effectively pre- served could have a direct effect on the perfor- mance of the NMT model. Therefore, how to simultaneously exploit the exclusive and shared translation knowledge of mixed-domain parallel sentences for multi-domain NMT remains a chal- lenging task.</p><p>To tackle this problem, recently, researchers have carried out many constructive and in-depth studies ( <ref type="bibr" target="#b16">Kobus et al., 2016;</ref><ref type="bibr" target="#b38">Zhang et al., 2016;</ref><ref type="bibr" target="#b24">Pryzant et al., 2017;</ref><ref type="bibr" target="#b10">Farajian et al., 2017)</ref>. How- ever, most of these studies mainly focus on the uti- lization of domain contexts as a whole in NMT, while ignoring the discrimination of domain con- texts at finer-grained level. In each sentence, some words are closely associated with its do- main, while others are domain-independent. In- tuitively, these two kinds of words play differ-ent roles in multi-domain NMT, nevertheless, they are not being distinguished by the current models. Take the sentence shown in <ref type="figure">Figure 1</ref> for exam- ple. The Chinese words "'OE¬"(congress), "AE Y"(bills), "\"(inclusion), and "AE §"(agenda) are frequently used in Laws domain and imply the Laws style of the sentence, while other words in this sentence are common in all domains and they mainly indicate the semantic meaning of the sen- tence. Thus, it is reasonable to distinguish and encode these two types of words separately to capture domain-specific and domain-shared con- texts, allowing the exclusive and shared knowl- edge to be exploited without any interference from the other. Meanwhile, the English words "prior- ity","government", "bill" and "agenda" are also closely related to Laws domain. To preserve the domain-related text style and idioms in generated translations, it is also reasonable for our model to pay more attention to these domain-related words than the others during model training. On this ac- count, we believe that it is significant to distin- guish and explore word-level domain contexts for multi-domain NMT.</p><p>In this paper, we propose a multi-domain NMT model with word-level domain context discrimi- nation. Specifically, we first jointly model NMT with monolingual attention-based domain classi- fication tasks. In source-side domain classifica- tion and adversarial domain classification tasks, we perform two individual attention operations on source-side annotations to generate the domain- specific and domain-shared vector representations of source sentence, respectively. Meanwhile, an attention operation is also placed on target-side hidden states to implement target-side domain classification. Then, we improve NMT with the following two approaches:</p><p>(1) According to the sentence representations produced by source-side domain classifier and ad- verisal domain classifier, we generate two gating vectors for each source annotation. With these two gating vectors, the encoded information of source annotation is selected automatically to construct domain-specific and domain-shared annotations, both of which are used to guide translation pre- dictions via two attention mechanisms;</p><p>(2) Based on the attention weights of the target words from target-side domain classifier, we em- ploy word-level cost weighting strategy to refine our model training. In this way, domain-specific target words will be assigned greater weights than others in the objective function of our model.</p><p>Our work demonstrates the benefits of sepa- rate modeling of the domain-specific and domain- shared contexts, which echoes with the success- ful applications of the multi-task learning based on shared-private architecture in many tasks, such as discourse relation recognition ( ), word segmentation ( ), text clas- sification ( <ref type="bibr" target="#b19">Liu et al., 2017a)</ref>, and image classifica- tion ( . Overall, the main contribu- tions of our work are summarized as follows:</p><p>• We propose to construct domain-specific and domain-shared source annotations from ini- tial annotations, of which effects are respec- tively captured for translation predictions.</p><p>• We propose to adjust the weights of target words in the training objective of NMT ac- cording to their relevance to different do- mains.</p><p>• We conduct experiments on large-scale multi-domain Chinese-English and English- French datasets.</p><p>Experimental results demonstrate the effectiveness of our model. <ref type="figure">Figure 2</ref> illustrates the architecture of our model, which includes a neural encoder equipped with a domain classifier and an adversarial domain clas- sifier, and a neural decoder with two attention models and a target-side domain classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Encoder</head><p>As shown in the lower part of <ref type="figure">Figure 2</ref>, our encoder leverages the sentence representations produced by these two classifiers to construct domain-specific and domain-shared annotations from initial ones, preventing the exclusive and shared translation knowledge from interfering with each other. In our encoder, the input sen- tence x=x 1 , x 2 , ..., x N are first mapped to word vectors and then fed into a bidirectional GRU ( <ref type="bibr">Cho et al., 2014</ref>) to obtain</p><formula xml:id="formula_0">− → h = − → h 1 , − → h 2 , ..., − → h N and ← − h = ← − h 1 , ← − h 2 , ..., ← −</formula><p>h N in the left-to-right and right-to-left directions, respectively. These two sequences are then concatenated as</p><formula xml:id="formula_1">h i = { − → h ⊤ i , ← − h ⊤ i } ⊤ to</formula><p>form the word-level semantic rep- resentation of the input sentence.</p><p>Domain Classifier and Adversarial Domain Classifier. With annotations {h i } N i=1 , we employ</p><formula xml:id="formula_2">✒ ✞ ☛ ✁ Decoder ✂ ✄ ☎ ✄ ✆ ✄ ✝ Domain Classifier ✟ ✠ ✟ ☎ ✟ ✡ ☞ ☞ Encoder ✂ Domain-Specific Annotations ✒ ✌ ☛ ✁ ✂ Domain-Shared Annotations ✒ ✌ ☛ ✍ ✁ Domain Classifier ✎ ✏ ☛ ✑ ✓ E r ( ) ✒ ✌ ☛ ✁ ✟ ✠ ✟ ☎ ✟ ✡ ☞ E s ( ) E r (y) ✟ ✠ ✟ ☎ ✟ ✡ ☞ Adversarial Domain Classifier ✂ ✂ ✔ ☎ ✔ ✆ ✔ ✕ Figure 2:</formula><p>The architecture illustration of our model. Note that our two source-side domain classifiers are used to produce domain-specific and domain-shared annotations, respectively, and our target-side domain classifier is only used during model training.</p><p>two attention-like aggregators to generate the se- mantic representations of sentence x, denoted by the vectors E r (x) and E s (x), respectively. Based on these two vectors, we employ the same neu- ral network to model two classifiers with different context modeling objectives: One is a domain classifier that aims to dis- tinguish different domains in order to generate domain-specific source-side contexts. It is trained using the objective function J s dc (x; θ s dc ) = log p(d|x; θ s dc ), where d is the domain tag of x and θ s dc is its parameter set. The other is an adversarial domain classifier capturing source-side domain- shared contexts. To this end, we train it using the following adversarial loss functions:</p><formula xml:id="formula_3">J s1 adc (x; θ s1 adc ) = log p(d|x; θ s1 adc , θ s2 adc ), (1) J s2 adc (x; θ s2 adc ) = H(p(d|x; θ s1 adc , θ s2 adc )), (2)</formula><p>where</p><formula xml:id="formula_4">H(p(·))=− K k=1 p k (·) log p k (·)</formula><p>is an en- tropy of distribution p(·) with K domain labels, θ s1 adc and θ s2 adc denote the parameters of softmax layer and the generation layer of E s (x) in this classifier, respectively. By this means, E r (x) and E s (x) are expected to encode the domain-specific and domain-shared semantic representations of x, respectively. It should be noted that our utiliza- tion of domain classifiers is similar to adversarial training used in <ref type="bibr" target="#b24">(Pryzant et al., 2017</ref>) which injects domain-shared contexts into annotations. How- ever, by contrast, we introduce domain classifier and adversarial domain classifier simultaneously to distinguish different kinds of contexts for NMT more explicitly.</p><p>Here we describe only the modeling procedure of the domain classifier, while it is also applicable to the adversarial domain classifier. Specifically, E r (x) is defined as follows:</p><formula xml:id="formula_5">E r (x) = N i=1 α i h i ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_6">α i = exp(e i ) N i ′ exp(e i ′ ) , e i = (v a ) ⊤ tanh(W a h i ),</formula><p>and v a and W a are the relevant attention pa- rameters. Then, we feed E r (x) into a fully connected layer with ReLU function ( <ref type="bibr" target="#b1">Ballesteros et al., 2015)</ref>, and then pass its output through a softmax layer to implement domain classification</p><formula xml:id="formula_7">p(·|x; θ s dc ) =sof tmax(W s⊤ dc ReLU (E r (x)) + b s dc ), (4)</formula><p>where W s dc and b s dc are softmax parameters. Domain-Specific and Domain-Shared Anno- tations. Since domain-specific and domain-shared contexts have different effects on NMT, and thus should be distinguished and separately captured by NMT model. Specifically, we first leverage the sentence representations E r (x) and E s (x) to gen- erate two gating vectors, g r i and g s i , for annotation h i in the following way:</p><formula xml:id="formula_8">g r i = sigmoid(W (1) gr E r (x) + W (2) gr h i + b gr ),<label>(5)</label></formula><formula xml:id="formula_9">g s i = sigmoid(W (1) gs E s (x) + W (2) gs h i + b gs ),<label>(6)</label></formula><p>where W * gr , W * gs , b gr and b gs denote the rele- vant matrices and bias, respectively. With these two vectors, we construct domain-specific and domain-shared annotations h r i and h s i from h i :</p><formula xml:id="formula_10">h r i = g r i ⊙ h i ,<label>(7)</label></formula><formula xml:id="formula_11">h s i = g s i ⊙ h i .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Neural Decoder</head><p>The upper half of <ref type="figure">Figure 2</ref> illustrates the architec- ture of our decoder. In particular, with the atten- tion weights of target words from the domain clas- sifier, we employ word-level cost weighting strat- egy to refine model training. Formally, our decoder applies a nonlinear func- tion g( * ) to define the conditional probability of translation y=y 1 , y 2 , ..., y M :</p><formula xml:id="formula_12">p(y|x) = M j=1 p(y j |x, y &lt;j ) = M j=1 g(y j−1 , s j , c r j , c s j ),<label>(9)</label></formula><p>where the vector s j denotes the GRU hidden state. It is updated as</p><formula xml:id="formula_13">s j = GRU (s j−1 , y j−1 , c r j , c s j ).<label>(10)</label></formula><p>Here the vectors c r j and c s j represent the domain- specific and domain-shared contexts, respectively.</p><p>Domain-Specific and Domain-Shared Con- text Vectors. When generating y j , we define c r j as a weighted sum of the domain-specific annota- tions {h r i }:</p><formula xml:id="formula_14">c r j = N i=1 exp(e r j,i ) N i ′ =1 exp(e r j,i ′ ) · h r i ,<label>(11)</label></formula><p>where e r j,i = a(s j−1 , h r i ), and a(*) is a feedforward neural network. Mean- while, we produce c s j from the domain-shared an- notations {h s i } as in Eq. 11. By introducing c r j and c s j into s j , our decoder is able to distinguish and simultaneously exploit two types of contexts for translation predictions.</p><p>Domain Classifier. We equip our decoder with a domain classifier with parameters θ tdc , which maximizes the training objective i.e., J t dc (y; θ t dc ) = log p(d|y; θ t dc ). To do this, we also apply attention operation to produce the domain-aware semantic representation E r (y) of y,</p><formula xml:id="formula_15">E r (y) = M j=1 β j s j ,<label>(12)</label></formula><p>where</p><formula xml:id="formula_16">β j = exp(e j ) M j ′ exp(e j ′ ) , e j = (v b ) ⊤ tanh(W b s j ),</formula><p>and v b and W b are the related parameters. Like- wise, we stack a domain classifier on top of E r (y).</p><p>Note that this classifier is only used in model training to infer attention weights of target words. These weights measure their semantic relevance to different domains and can be utilized to adjust their cost weights in NMT training objective.</p><p>NMT Training Objective with Word-Level Cost Weighting. Formally, we define the objec- tive function of NMT as follows:</p><formula xml:id="formula_17">J nmt (x, y; θ nmt ) = M j=1 (1 + β j ) log p(y j |x, y &lt;j ; θ nmt ), (13)</formula><p>where β j is the attention weight of y j obtained by Eq. <ref type="formula" target="#formula_15">(12)</ref>, and θ nmt denotes the parameter set of NMT. By this scaling strategy, domain- specific words are emphasized with a bonus, while domain-shared words are updated as usual.</p><p>Please note that scaling costs with a multiplica- tive scalar essentially changes the magnitude of parameter update but without changing its direc- tion ( <ref type="bibr" target="#b2">Chen et al., 2017a</ref>). Besides, although our scaling strategy is similar to the cost weighting proposed by <ref type="bibr" target="#b2">Chen et al. (2017a)</ref>, our approach dif- fers from it in two aspects: First, we employ word- level cost weighting rather than sentence-level one to refine NMT training; Second, our approach is less time-consuming for multi-domain NMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Overall Training Objective</head><p>Given a mixed-domain training corpus D = {(x, y, d)}, we train the proposed model accord-ing to the following objective function:</p><formula xml:id="formula_18">J (D; θ) = (x,y,d)∈D {J nmt (x, y; θ nmt ) + J s dc (x; θ s dc ) + J t dc (y; θ t dc )<label>(14)</label></formula><p>+ J s1 adc (x; θ s1 adc ) + λ · J s2 adc (x; θ s2 adc )} where J nmt ( * ), J s dc ( * ), J t dc ( * ) and J s * adc ( * ) are the objective functions of NMT, source-side do- main classifier, target-side domain classifier, and source-side adversarial domain classifier, respec- tively, θ={θ nmt , θ s dc , θ t dc , θ s1 adc , θ s2 adc }, and λ is the hyper-parameter for adversarial learning.</p><p>Particularly, to ensure encoding accuracy of domain-shared contexts, we follow  to adopt an alternative two-phase strat- egy in training, where we alternatively optimize J (D; θ) with θ s1 adc and {θ-θ s1 adc } respectively fixed at a time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head><p>To investigate the effectiveness of our model, we conducted multi-domain translation experiments on Chinese-English and English-French datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>Datasets. For Chinese-English translation, our data comes from UM-Corpus ( <ref type="bibr" target="#b33">Tian et al., 2014)</ref> and LDC 1 . To ensure data quality, we chose only the parallel sentences with domain label Laws, Spoken, and Thesis from UM-Corpus, and the LDC bilingual sentences related to News domain as our dataset. We used randomly selected sen- tences from UM-Corpus and LDC as development set, and combined the test set of UM-Corpus and randomly selected sentences from LDC to con- struct our test set. For English-French transla- tion, we conducted experiments on the datasets of OPUS corpus 2 , containing sentence pairs from Medical, News, and Parliamentary domains. We also divided these datasets into training, develop- ment and test sets. <ref type="table">Table 1</ref> provides the statistics of the corpora used in our experiments.</p><p>We performed word segmentation on Chi- nese sentences using Stanford Segmenter 3 , and tokenized English and French sentences using MOSES script <ref type="bibr">4</ref>   <ref type="table">Table 1</ref>: Sentence numbers of data sets in our ex- periments.</p><p>Encoding ( <ref type="bibr" target="#b26">Sennrich et al., 2016)</ref> to convert all words into subwords. The translation quality was evaluated by case-sensitive BLEU ( <ref type="bibr" target="#b23">Papineni et al., 2002</ref>).</p><p>Contrast Models. Since our model is essen- tially a standard attentional NMT model enhanced with word-level domain contexts, we refer to it as +WDC. We compared it with the following mod- els, namely:</p><p>• OpenNMT <ref type="bibr">5</ref> . A famous open-source NMT system used widely in the NMT community trained on mix-domain training set.</p><p>• DL4NMT-single ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>. A reimplemented attentional NMT trained on a single domain dataset.</p><p>• DL4NMT-mix ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>. A reimplemented attentional NMT trained on mix-domain training set.</p><p>• DL4NMT-finetune ( <ref type="bibr" target="#b21">Luong and Manning, 2015)</ref>. A reimplemented attentional NMT which is first trained using out-of-domain training corpus and then fine-tuned using in- domain dataset.</p><p>• +Domain Control (+DC) ( <ref type="bibr" target="#b16">Kobus et al., 2016)</ref>. It directly introduces embeddings of source domain tag to enrich annotations of encoder.</p><p>• +Multitask Learning (+ML1) ( <ref type="bibr" target="#b9">Dong et al., 2015)</ref>. It adopts a multi-task learning frame- work that shares encoder representation and separates the decoder modeling of different domains.</p><p>• +Multitask Learning (+ML2) <ref type="bibr" target="#b24">(Pryzant et al., 2017)</ref>. This model jointly trains NMT with domain classification via multi- task learning.</p><p>• +Adversarial Discriminative Mixing (+ADM) <ref type="bibr" target="#b24">(Pryzant et al., 2017)</ref>. It employs adversarial training to achieve the domain adaptation in NMT.</p><p>• +Target Token Mixing (+TTM) <ref type="bibr" target="#b24">(Pryzant et al., 2017)</ref>. This model is similar to +DC, with the only difference that it enriches source annotations by adding target-side do- main tag rather than source-side one.</p><p>Note that our model uses two annotation se- quences, thus we also compared it with the afore- mentioned models with two times of hidden state size (2×hd). To further examine the effectiveness of the proposed components in our model, we also provided the performance of the following vari- ants of our model:</p><p>• +WDC(S). It only exploits the source-side word-level domain contexts for multi-domain NMT.</p><p>• +WDC(T). It only employ word-level cost weighting on the target side to refine the model training.</p><p>Implementation Details. Following the com- mon practice, we only used the training sentences within 50 words to efficiently train NMT models. Thus, 85.40% and 88.96% of the Chinese-English and English-French parallel sentences were cov- ered in our experiments. In addition, we set the vocabulary size for Chinese-English and English- French as 32,000 and 32,000, respectively. In do- ing so, our vocabularies covered 99.97% Chinese words and 99.99% English words of the Chinese- English corpus, and almost 100% English words and 99.99% French words of the English-French corpus, respectively.</p><p>We applied Adam ( <ref type="bibr" target="#b15">Kingma and Ba, 2015</ref>) to train models and determined the best model pa- rameters based on the model performance on de- velopment set. The used hyper-parameter were set as follows: β 1 and β 2 of Adam as 0.9 and 0.999, word embedding dimension as 500, hidden layer size as 1000, learning rate as 5×10 −4 , batch size as 80, gradient norm as 1.0, dropout rate as 0.1, and beamsize as 10. Other settings were set fol- lowing ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Laws Spoken Thesis News  Overall Evaluation of the Chinese- English translation task. 2×hd = two times of hid- den state size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results on Chinese-English Translation</head><p>We first determined the optimal hyper-parameter λ (see Eq. <ref type="formula" target="#formula_18">(14)</ref>) on the development set. To do this, we gradually varied λ from 0.1 to 1.0 with an increment of 0.1 in each step. Since our model achieved the best performance when λ=0.1, hence, we set λ=0.1 for all experiments thereafter. <ref type="table" target="#tab_2">Table 2</ref> shows the overall experimental results. Using almost the same hyper-parameters, our re- implemented DL4NMT outperforms OpenNMT in all domains, demonstrating that our baseline is competitive in performance. Moreover, on all test sets of different domains, our model signifi- cantly outperforms other contrast models no mat- ter which hyper-parameters they use. Further- more, we arrive at the following conclusions: First, our model surpasses DL4NMT-single, DL4NMT-mix and DL4NMT-finetune, all of which are commonly used in domain adaptation for NMT. Please note that DL4NMT-finetune re- quires multiple adapted NMT models to be con- structed, while ours is a unified one that works well in all domains.</p><p>Second, compared with +DC, +ML2 and +ADM which all exploit source-side domain con- texts for multi-domain NMT, our +WDC(S) still</p><formula xml:id="formula_19">(b) An Example Sentence in Thesis Domain ✒ ✁ ✂ ✄ ☎ ✆ ✝ ✣ ✞ ✟ ✠ ✡ ✄ ☎ ☛ ☞ ✌ ✱ de ✘ ✍ ✄ ☞ ✎ ✏ ✑ ✠ ✓ ✄ ✔ ✕ yìnglì ✖ ✗ ✎ ✙ ✚ shíyàn ❆ ✛ jìsuàn (a) An Example Sentence in Laws Domain ✪ ✜ àomén ✭ ✢ tèbié ❃ ✤ ✥ ✦ ✧ ★ ✩ ✫ ✬ ✮ ★ ✩ ✯ ✰ ✶ ✲ ✳ ✴ ✵ ✷ ✸ ✬ ✹ ✵ ✺ de ✻ ✼ ✽ ✬ ✸ ★ ✾ ✬ ✿ ★ ✩ ❀ ✲ ❁ ❂ ★ ✷ ✸ Figure 3:</formula><p>The correlation heat map of the gating vectors(blue/green) to domain-specific/domain- shared annotations in two example sen- tences. Note that domain-specific words "e €"(Macao), "á{¬"(Legislative Council), " )"(Formation), "•{"(Method), "µ4"(Seal), "O Ž"(Calculation), "¢ " (Experiment) are strengthened by g r i , while most of the domain- shared words ""(of) and " †"(and) are focused by g s i .</p><p>exhibits better performance. This is because that these models focus on one aspect of domain con- texts, while our model considers both domain- specific and domain-shared contexts on the source side. Third, +WDC(T) also outperforms DL4NMT, revealing that it is reasonable and effective to em- phasize domain-specific words in model training..</p><p>Last, +WDC achieves the best performance when compared with both +WDC(S) and +WDC(T). Therefore, we believe that word-level domain contexts on the both sides are com- plementary to each other, and utilizing them simultaneously is beneficial to multi-domain NMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental Analysis</head><p>Furthermore, we conducted several visualization experiments to empirically analyze the individual effectiveness of the added model components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Visualizations of Gating Vectors</head><p>We first visualized the gating vectors g r i and g s i to quantify their effects on extracting domain- specific and domain-shared contexts from initial source-side annotations. Since both g r i and g s i are high dimension vectors, which are difficult to be visualized directly, we followed  and <ref type="bibr" target="#b39">Zhou et al. (2017)</ref> to visualize their individ- ual contributions to the final output, which can be The visualization of the sentence representations and their corresponding average annotations, where the triangle-shaped(purple), circle-shaped(red), square-shaped(green) and pentagonal-shaped(blue) points denote News, Laws, Spoken and Thesis sentences, respectively. approximated by their first derivatives. <ref type="figure">Figure 3</ref> shows the first derivative heat maps for two example sentences in Laws and Thesis domain, respectively. We can observe that with- out any loss of semantic meanings from source sentences, most of the domain-specific words are strengthened by g r i , while most of the domain- shared words, especially function words, are fo- cused by g s i . This result is consistent with our ex- pectation for the function of two gating vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Visualizations of Sentence Representations and Annotations</head><p>Furthermore, we applied the hypertools ( <ref type="bibr" target="#b12">Heusser et al., 2018)</ref> to visualize the sentence representa- tions E r (x) and E s (x), and the domain-specific and domain-shared annotation sequences</p><formula xml:id="formula_20">{h r i } N i=1</formula><p>and</p><formula xml:id="formula_21">{h s i } N i=1</formula><p>. Here we represent each annotation sequence with its average vector in the figure.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 4</ref> (a) and (b), the sentence representation vectors and the average annotation vectors of different domains are clearly distributed in different regions. By contrast, their distribu- tions are much more concentrated in <ref type="figure" target="#fig_1">Figure 4</ref> (c) and (d). Thus, we conclude that our model is able to distinctively learn domain-specific and domain- shared contexts. Moreover, from <ref type="figure" target="#fig_1">Figure 4 (</ref>  those of the other domains, this may be caused by the more formal and consistent sentence styles in Laws domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Illustrations of Domain-Specific Target Words</head><p>Lastly, for each domain, we presented the top ten target words with the highest weights learned by our target-side domain classifier. To do this, we calculated the average attention weight of each word in the training corpus as its corresponding domain weight.</p><p>As is clearly shown in <ref type="table" target="#tab_4">Table 3</ref> that most listed target words are closely related to their domains. This result validates the aforementioned hypothe- sis that some words are domain-dependent while others are domain-independent, and our target- side domain classifier is capable of distinguishing them with different attention weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results on English-French Translation</head><p>Likewise, we determined the optimal λ=0.1 on the development set. <ref type="table" target="#tab_6">Table 4</ref> gives the results of English-French multi-domain translation. Similar to the previous experimental result in Section 3.2, our model continues to achieve the best perfor- mance compared to all contrast models using two different hidden state size settings, which demon- strates again that our model is effective and gen- eral to different language pairs in multi-domain NMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>In this work, we study on multi-domain machine translation in the field of domain adaptation for machine translation, which has attracted great at- tention since SMT ( <ref type="bibr" target="#b8">Clark et al., 2012</ref>  The first category is to transfer out-of-domain knowledge to in-domain translation. The con- ventional method is fine-tuning, which first trains the model on out-of-domain dataset and then fine- tunes it on in-domain dataset <ref type="bibr" target="#b21">(Luong and Manning, 2015;</ref><ref type="bibr" target="#b40">Zoph et al., 2016;</ref><ref type="bibr" target="#b28">Servan et al., 2016)</ref>. Freitag and Al-Onaizan (2016) proceeded further by ensembling the fine-tuned model with the origi- nal one. <ref type="bibr" target="#b7">Chu et al. (2017)</ref> fine-tuned the model us- ing the mix of in-domain and out-of-domain train- ing corpora. From the perspective of data selec- tion, <ref type="bibr" target="#b2">Chen et al. (2017a)</ref> scaled the top-level costs of NMT system according to each training sen- tence's similarity to the development set. <ref type="bibr" target="#b35">Wang et al. (2017a)</ref> explored the data selection strategy based on sentence embeddings for NMT domain adaptation. Moreover, <ref type="bibr" target="#b36">Wang et al. (2017b)</ref> further proposed several sentence and domain weighting methods with a dynamic weight learning strategy. However, these approaches usually only perform well on target domain while being highly time consuming in transferring translation knowledge to all the constitute domains.</p><p>The second category is to directly use a mixed-domain training corpus to construct NMT model for the translated sentences derived from different domains. In this aspect, <ref type="bibr" target="#b16">Kobus et al. (2016)</ref> in- corporated domain information into NMT by ap- pending a domain indicator token to each source sequence. Similarly, <ref type="bibr" target="#b14">Johnson et al. (2016)</ref> added an artificial token to the input sequence to indicate the required target language. Contrastingly, <ref type="bibr" target="#b10">Farajian et al. (2017)</ref> utilized the similarity between each test sentence and the training instances to dy- namically set the hyper-parameters of the learn- ing algorithm and update the generic model on the fly. <ref type="bibr" target="#b24">Pryzant et al. (2017)</ref> proposed three novel models: discriminative mixing that jointly models NMT with domain classification, adversarial dis- criminative mixing, and target token mixing which appends a domain token to the target sequence. <ref type="bibr" target="#b25">Sajjad et al. (2017)</ref> explored data concatenation, model stacking, data selection and multi-model ensemble to train multi-domain NMT. By exploit- ing domain as a tag or a feature, Tars and Fishel (2018) treated text domains as distinct languages in order to use multi-lingual approaches when im- plementing multi-domain NMT. Inspired by topic- based SMT, some researchers resorted to incor- porating topical contexts into NMT.  used the topic information of input sen- tence as an additional input to decoder. <ref type="bibr" target="#b38">Zhang et al. (2016)</ref> enhanced the word representation by adding its topic embedding. However, these meth- ods require to have explicit document boundaries between training data, which unfortunately do not exist in most datasets. Overall, our work is related to the second type of approach with <ref type="bibr" target="#b24">(Pryzant et al., 2017)</ref> and <ref type="bibr" target="#b2">(Chen et al., 2017a</ref>) most related to ours. <ref type="bibr">Unlike (Pryzant et al., 2017)</ref> applying adversarial training to only capture domain-shared translation knowledge, we further exploit domain-specific translation knowl- edge for multi-domain NMT. Also, in sharp con- trast to <ref type="bibr" target="#b2">(Chen et al., 2017a</ref>), our model not only exploits the source-side word-level domain con- texts differently, but also employs a word-level cost weighting strategy for multi-domain NMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this work, we have explored how to uti- lize word-level domain contexts for multi-domain NMT. By jointly modeling NMT and domain clas- sification tasks, we utilize the sentence represen- tations of source-side domain classifier and ad- versarial domain classifier to construct domain- specific and domain-shared source annotations, which are then exploited by decoder. Moreover, using the attentional weights of target-side domain classifier, we adjust the weights of target words in the training objective to refine model training. Ex- perimental results and in-depth analyses demon- strate the effectiveness of the proposed model.</p><p>In the future, we would like to extend the pro- posed word-level cost weighting strategy to source words. Besides, our method is also general to other NMT models. Therefore, we plan to ap- ply our method to the NMT with complex ar- chitectures, for example, lattice-to-sequence NMT ( , hierarchy-to-sequence NMT ( <ref type="bibr" target="#b30">Su et al., 2018)</ref>, NMT with context-aware encoder (  and <ref type="bibr">Transformer (Vaswani et al., 2017</ref>) and so on.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: Word-level correlation heat map to Laws domain for a Chinese(CH)-English(EN) parallel sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4:</figDesc><graphic url="image-438.png" coords="7,307.47,160.17,108.66,83.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>.</head><label></label><figDesc></figDesc><table>Then, we employed Byte Pair 

1 https://www.ldc.upenn.edu/. 
2 http://opus.nlpl.eu/ 
3 https://nlp.stanford.edu/ 
4 http://www.statmt.org/moses/ 

Task 
Domain 
Train 
Dev 
Test 

CH-EN 

Laws 
219K 
600 
456 
Spoken 
219K 
600 
455 
Thesis 
299K 
800 
625 
News 
300K 
800 
650 

EN-FR 

Medical 
1.09M 800 2000 
News 
180K 
800 2000 
Parliamentary 2.04M 800 2000 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>b), we observe that the sentence representation vectors of Laws domain does not completely coincide with</figDesc><table>Domain 
Top10 Target Words 

Laws 

Article, Chapter, Principles, regulations, Pro-
visions, Political, Servants, specify, China, 
Municipal 

Spoken 

meanly, Rusty, 1910s, scours, mountaintops, 
paralyze, Puff, perpetrators, hitter, weightlift-
ing 

Thesis 

aggregation, Activities, Computation, Alzhei-
mer, nn, Contemporarily, EVALUATION, 
ethoxycarbonyl, sCRC, Announced 

News 

months, agency, outweighed, unconstitution-
ally, Congolese, session, Asia, news, hurts, 
francs 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Examples of Domain-Specific Target 
Words. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>;</head><label></label><figDesc></figDesc><table>Huck et al., 

Model 
Medical Parliamentary News 

Contrast Models (1×hd) 
OpenNMT 
78.78 
32.96 
30.22 
DL4NMT-single 
77.34 
33.28 
29.56 
DL4NMT-mix 
78.48 
33.16 
31.62 
DL4NMT-finetune 
78.61 
33.72 
34.04 
+DC 
79.34 
33.38 
33.94 
+ML1 
77.29 
33.39 
31.92 
+ML2 
78.65 
33.55 
33.48 
+ADM 
76.74 
33.06 
33.43 
+TTM 
78.27 
33.29 
33.37 
Contrast Models (2×hd) 
DL4NMT-single 
78.50 
33.38 
30.23 
DL4NMT-mix 
78.84 
33.19 
33.28 
DL4NMT-finetune 
79.17 
33.88 
34.20 
+DC 
79.96 
33.44 
33.52 
+ML1 
78.38 
33.20 
31.90 
+ML2 
79.41 
33.55 
33.62 
+ADM 
79.31 
33.50 
33.34 
+TTM 
79.36 
33.13 
33.68 
Our Models 
+WDC(S) 
82.76 
34.13 
34.31 
+WDC(T) 
81.51 
33.76 
33.78 
+WDC 
83.35 
34.17 
34.87 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Overall Evaluation on the English-French 
translation task. 

2015; Sennrich et al., 2013). As for NMT, the 
dominant strategies for domain adaptation gener-
ally fall into two categories: 
</table></figure>

			<note place="foot" n="5"> http://opennmt.net/.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors were supported by National Natural Science Foundation of China (No. 61672440), the Fundamental Research Funds for the Central Uni-versities (Grant No. ZK1024), and Scientific Re-search Project of National Language Committee of China (Grant No. YB135-49). We also thank the reviewers for their insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improved transition-based parsing by modeling characters instead of words with lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cost weighting for neural machine translation domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Larkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the First Workshop on Neural Machine Translation</title>
		<meeting>of the First Workshop on Neural Machine Translation</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Guided alignment training for topic-aware neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Matusov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Khadivi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Thorsten</forename><surname>Peter</surname></persName>
		</author>
		<idno>CoRR abs/1607.01628</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adversarial multi-criteria learning for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<imprint>
			<pubPlace>Fethi Bougares, Holger</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An empirical comparison of domain adaptation methods for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhui</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><surname>Dabre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">One system, many domains: Open-domain statistical machine translation via feature augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AMTA</title>
		<meeting>of AMTA</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-task learning for multiple language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-domain neural machine translation through unsupervised adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amin Farajian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WMT</title>
		<meeting>of WMT</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fast domain adaptation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<idno>CoRR abs/1612.06897</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hypertools: a python toolbox for gaining geometric insights into high-dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">C</forename><surname>Heusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirsten</forename><surname>Ziman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><forename type="middle">L W</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><forename type="middle">R</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mixeddomain vs. multi-domain statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haddow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MT Summit</title>
		<meeting>of MT Summit</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macduff</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR abs/1611.04558</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Domain control for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Kobus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josep</forename><surname>Crego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<idno>CoRR abs/1612.06140</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visualizing and understanding neural models in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial multi-task learning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Implicit discourse relation classification via multi-task neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stanford neural machine translation systems for spoken language domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IWSLT</title>
		<meeting>of IWSLT</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalchbrenner</forename><surname>Nal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blunsom</forename><surname>Phil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Effective domain mixing for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><surname>Pryzant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WMT</title>
		<meeting>of WMT</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Neural machine translation training in a multi-domain scenario</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahim</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<idno>CoRR abs/1708.08712</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A multi-domain translation model framework for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Aransa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Domain specialization: a post-training domain adaptation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Servan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josep</forename><surname>Crego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<idno>CoRR abs/1612.06141</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lattice-based recurrent neural network encoders for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI 2017</title>
		<meeting>of AAAI 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3302" to="3308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A hierarchyto-sequence attentional neural machine translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiali</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech &amp; Language Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="623" to="632" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Multi-domain neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Tars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Fishel</surname></persName>
		</author>
		<idno>CoRR abs/1805.02282</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Um-corpus: A large english-chinese parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Liang Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulo</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Quaresma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sentence embedding for neural machine translation domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Instance weighting for neural machine translation domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A context-aware recurrent encoder for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech &amp; Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2424" to="2432" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Topic-informed neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangyou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Selective encoding for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Transfer learning for low-resource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
