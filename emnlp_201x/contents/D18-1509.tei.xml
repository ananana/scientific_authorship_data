<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Tree-based Decoder for Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technology Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technology Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Google Brain</orgName>
								<address>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technology Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technology Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Tree-based Decoder for Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="4772" to="4777"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recent advances in Neural Machine Translation (NMT) show that adding syntactic information to NMT systems can improve the quality of their translations. Most existing work utilizes some specific types of linguistically-inspired tree structures, like constituency and dependency parse trees. This is often done via a standard RNN decoder that operates on a linearized target tree structure. However, it is an open question of what specific linguistic formalism , if any, is the best structural representation for NMT. In this paper, we (1) propose an NMT model that can naturally generate the topology of an arbitrary tree structure on the target side, and (2) experiment with various target tree structures. Our experiments show the surprising result that our model delivers the best improvements with balanced binary trees constructed without any linguistic knowledge; this model outperforms standard seq2seq models by up to 2.1 BLEU points, and other methods for incorporating target-side syntax by up to 0.7 BLEU. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most NMT methods use sequence-to-sequence (seq2seq) models, taking in a sequence of source words and generating a sequence of tar- get words <ref type="bibr" target="#b12">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b24">Sutskever et al., 2014;</ref><ref type="bibr" target="#b2">Bahdanau et al., 2015)</ref>. While seq2seq models can implicitly discover syn- tactic properties of the source language ( <ref type="bibr" target="#b23">Shi et al., 2016</ref>), they do not explicitly model and leverage such information. Motivated by the success of adding syntactic information to Statistical Machine Translation (SMT) ( <ref type="bibr" target="#b10">Galley et al., 2004;</ref><ref type="bibr">Menezes and Quirk, 2007;</ref><ref type="bibr" target="#b9">Galley et al., 2006</ref>), recent works have established that explicitly leveraging syn- tactic information can improve NMT quality, ei-ther through syntactic encoders ( <ref type="bibr" target="#b7">Eriguchi et al., 2016)</ref>, multi-task learning objec- tives ( <ref type="bibr" target="#b5">Chen et al., 2017;</ref><ref type="bibr" target="#b8">Eriguchi et al., 2017)</ref>, or direct addition of syntactic tokens to the target se- quence ( <ref type="bibr" target="#b17">Nadejde et al., 2017;</ref><ref type="bibr" target="#b0">Aharoni and Goldberg, 2017)</ref>. However, these syntax-aware mod- els only employ the standard decoding process of seq2seq models, i.e. generating one target word at a time. One exception is <ref type="bibr" target="#b25">Wu et al. (2017)</ref>, which uti- lizes two RNNs for generating target dependency trees. Nevertheless, <ref type="bibr" target="#b25">Wu et al. (2017)</ref> is specifi- cally designed for dependency tree structures and is not trivially applicable to other varieties of trees such as phrase-structure trees, which have been used more widely in other works on syntax-based machine translation. One potential reason for the dearth of work on syntactic decoders is that such parse tree structures are not friendly to recurrent neural networks <ref type="bibr">(RNNs)</ref>.</p><p>In this paper, we propose TrDec, a method for incorporating tree structures in NMT. TrDec simul- taneously generates a target-side tree topology and a translation, using the partially-generated tree to guide the translation process ( § 2). TrDec employs two RNNs: a rule RNN, which tracks the topology of the tree based on rules defined by a Context Free Grammar (CFG), and a word RNN, which tracks words at the leaves of the tree ( § 3). This model is similar to neural models of tree-structured data from syntactic and semantic parsing <ref type="bibr" target="#b6">(Dyer et al., 2016;</ref><ref type="bibr" target="#b1">Alvarez-Melis and Jaakkola, 2017;</ref><ref type="bibr" target="#b26">Yin and Neubig, 2017)</ref>, but with the addition of the word RNN, which is especially important for MT where fluency of transitions over the words is critical.</p><p>TrDec can generate any tree structure that can be represented by a CFG. These structures include linguistically-motivated syntactic tree representa- tions, e.g. constituent parse trees, as well as syntax- free tree representations, e.g. balanced binary trees <ref type="bibr">( § 4)</ref>. This flexibility of TrDec allows us to com- pare and contrast different structural representa- tions for NMT.</p><p>In our experiments ( § 5), we evaluate TrDec us- ing both syntax-driven and syntax-free tree repre- sentations. We benchmark TrDec on three tasks: Japanese-English and German-English translation with medium-sized datasets, and Oromo-English translation with an extremely small dataset. Our findings are surprising -TrDec performs well, but it performs the best with balanced binary trees con- structed without any linguistic guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Generation Process</head><p>TrDec simultaneously generates the target se- quence and its corresponding tree structure. We first discuss the high-level generation process using an example, before describing the prediction model ( § 3) and the types of trees used by TrDec ( § 4). <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the generation process of the sentence "_The _cat _eat s _fi sh _.", where the sentence is split into subword units, de- limited by the underscore "_" <ref type="bibr" target="#b22">(Sennrich et al., 2016)</ref>. The example uses a syntactic parse tree as the intermediate tree representation, but the pro- cess of generating with other tree representations, e.g. syntax-free trees, follows the same procedure.</p><p>Trees used in TrDec have two types of nodes: terminal nodes, i.e. the leaf nodes that represent subword units; and nonterminal nodes, i.e. the non- leaf nodes that represent a span of subwords. Ad- ditionally, we define a preterminal node to be a nonterminal node whose children are all terminal nodes. In <ref type="figure" target="#fig_0">Fig. 1</ref> Left, the green squares represent preterminal nodes.</p><p>TrDec generates a tree in a top-down, left-to- right order. The generation process is guided by a CFG over target trees, which is constructed by taking all production rules extracted from the trees of all sentences in the training corpus. Specifically, a rule RNN first generates the top of the tree struc- ture, and continues until a preterminal is reached. Then, a word RNN fills out the words under the preterminal. The model switches back to the rule RNN after the word RNN finishes. This process is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref> Right. Details are as follows:</p><p>Step 1. The source sentence is encoded by a se- quential RNN encoder, producing the hidden states.</p><p>Step 2. The generation starts with a derivation tree with only a Root node. A rule RNN, initialized by the last encoder hidden state computes the prob- ability distribution over all CFG rules whose left hand side (LHS) is Root, and selects a rule to apply to the derivation. In our example, the rule RNN selects ROOT 7 ! S.</p><p>Step 3. The rule RNN applies production rules to the derivation in a top-down, left-to-right order, ex- panding the current opening nonterminal using a CFG rule whose LHS is the opening nonterminal. In the next two steps, TrDec applies the rules S 7 ! NP VP PUNC and NP 7 ! pre to the opening nonter- minals S and NP, respectively. Note that after these two steps a preterminal node pre is created.</p><p>Step 4a. Upon seeing a preterminal node as the current opening nonterminal, TrDec switches to using a word RNN, initialized by the last state of the encoder, to populate this empty preterminal with phrase tokens, similar to a seq2seq decoder. For example the subword units _The and _cat are generated by the word RNN, ending with a special end-of-phrase token, i.e. heopi.</p><p>Step 4b. While the word RNN generates subword units, the rule RNN also updates its own hidden states, as illustrated by the blue cells in <ref type="figure" target="#fig_0">Fig. 1</ref> Right.</p><p>Step 5. After the word RNN generates heopi, TrDec switches back to the rule RNN to continue generating the derivation from where the tree left off. In our example, this next stage is the opening nonterminal node VP. From here, TrDec chooses the rule VP 7 ! pre NP.</p><p>TrDec repeats the process above, intermingling the rule RNN and the word RNN as described, and halts when the rule RNN generates the end-of- sentence token heosi, completing the derivation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>We now describe the computations during the gen- eration process discussed in § 2. At first, a source sentence x, which is split into subwords, is encoded using a standard bi-directional Long Short-Term Memory (LSTM) network <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997</ref>). This bi-directional LSTM outputs a set of hidden states, which TrDec will reference using an attention function ( <ref type="bibr" target="#b2">Bahdanau et al., 2015)</ref>.</p><p>As discussed, TrDec uses two RNNs to generate a target parse tree. In our work, both of these RNNs use LSTMs, but with different parameters.</p><p>Rule RNN. At any time step t in the rule RNN, there are two possible actions. If at the previous time step t 1, TrDec generated a CFG rule, then the state s tree t is computed by:</p><formula xml:id="formula_0">s tree t = LSTM([y CFG t1 ; c t1 ; s tree p ; s word t ]</formula><p>, s tree t1 ) where y CFG t1 is the embedding of the CFG rule at time step t1; c t1 is the context vector computed by attention at s tree t1 , i.e. input feeding ( <ref type="bibr" target="#b14">Luong et al., 2015)</ref>; s tree p is the hidden state at the time step that generates the parent of the current node in the par- tial tree; s word t is the hidden state of the most recent time step before t that generated a subword (note that s word t comes from the word RNN, discussed below); and [·] denotes a concatenation.</p><p>Meanwhile, if at the previous time step t 1, TrDec did not generate a CFG rule, then the update at time step t must come from a subword being generated by the word RNN. In that case, we also update the rule RNN similarly by replacing the embedding of the CFG rule with the embedding of the subword.</p><p>Word RNN. At any time step t, if the word RNN is invoked, its hidden state s word t is:</p><formula xml:id="formula_1">s word t = LSTM([s tree p ; w t1 ; c t1 ], s word t1 )</formula><p>, where s tree p is the hidden state of rule RNN that generated the CFG rule above the current terminal; w t1 is the embedding of the word generated at time step t 1; and c t1 is the attention context computed at the previous word RNN time step t1.   , where W varies depending on whether a rule or a subword unit is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Tree Structures</head><p>Unlike prior work on syntactic decoders designed for utilizing a specific type of syntactic informa- tion ( <ref type="bibr" target="#b25">Wu et al., 2017)</ref>, TrDec is a flexible NMT model that can utilize any tree structure. Here we consider two categories of tree structures:</p><p>Syntactic Trees are generated using a third- party parser, such as Berkeley parser ( <ref type="bibr" target="#b20">Petrov et al., 2006;</ref><ref type="bibr" target="#b21">Petrov and Klein, 2007)</ref>. <ref type="figure" target="#fig_1">Fig. 2</ref> Top Left illustrates an example constituency parse tree. We also consider a variation of standard constituency parse trees where all of their nonterminal tags are replaced by a null tag, which is visualized in <ref type="figure" target="#fig_1">Fig. 2</ref> Top Right. In addition to constituency parse trees, TrDec can also utilize dependency parse trees via a simple procedure that converts a dependency tree into a constituency tree. Specifically, this procedure cre- ates a parent node with null tag for each word, and then attaches each word to the parent node of its head word while preserving the word order. An example of this procedure is provided in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>Balanced Binary Trees are syntax-free trees constructed without any linguistic guidance. We use two slightly different versions of binary trees. Version 1 <ref type="figure" target="#fig_1">(Fig. 2</ref> Bottom Left) is constructed by recursively splitting the target sentence in half and creating left and right subtrees from the left and right halves of the sentence respectively. Version 2 <ref type="figure" target="#fig_1">(Fig. 2 Bottom Right)</ref>, is constructed by apply- ing Version 1 on a list of nodes where consecutive words are combined together. All tree nodes in both versions have the null tag. We discuss these con- struction processes in more detail in Appendix A.1.</p><p>In the experiments detailed later, we evaluated TrDec with four different settings of tree structures: 1) the fully syntactic constituency parse trees; 2) constituency parse trees with null tags; 3) depen- dency parse trees; 4) a concatenation of both ver- sion 1 and version 2 of the binary trees, (which effectively doubles the amount of the training data and leads to slight increases in accuracy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Datasets. We evaluate TrDec on three datasets: 1) the KFTT (ja-en) dataset <ref type="bibr" target="#b18">(Neubig, 2011)</ref>, which consists of Japanese-English Wikipedia arti- cles; 2) the IWSLT2016 German-English (de-en) dataset ( <ref type="bibr" target="#b3">Cettolo et al., 2016)</ref>, which consists of TED Talks transcriptions; and 3) the LORELEI Oromo-English (or-en) dataset 2 , which largely con- sists of texts from the Bible. Details are in Tab. 1. English sentences are parsed using Ckylark ( <ref type="bibr" target="#b19">Oda et al., 2015</ref>) for the constituency parse trees, and Stanford Parser (de <ref type="bibr" target="#b15">Marneffe et al., 2006;</ref><ref type="bibr" target="#b4">Chen and Manning, 2014</ref>) for the dependency parse trees. We use byte-pair encoding <ref type="bibr" target="#b22">(Sennrich et al., 2016</ref>) with 8K merge operations on ja-en, 4K merge operations on or-en, and 24K merge operations on de-en.  Baselines. We compare TrDec against three base- lines: 1) seq2seq: the standard seq2seq model with attention; 2) CCG: a syntax-aware transla- tion model that interleaves Combinatory Categorial Grammar (CCG) tags with words on the target side <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LDC2017E29</head><p>of a seq2seq model (Nadejde et al., 2017); 3) CCG- null: the same model with CCG, but all syntactic tags are replaced by a null tag; and 4) LIN: a stan- dard seq2seq model that generates linearized parse trees on the target side <ref type="bibr" target="#b0">(Aharoni and Goldberg, 2017)</ref>.</p><p>Results. Tab. 2 presents the performance of our model and the three baselines. For our model, we report the performance of TrDec-con, TrDec-con- null, TrDec-dep, and TrDec-binary (settings 1,2,3,4 in § 4). On the low-resource or-en dataset, we ob- serve a large variance with different random seeds, so we run each model with 6 different seeds, and re- port the mean and standard deviation of these runs. TrDec-con-null and TrDec-con achieved compa- rable results, indicating that the syntactic labels have neither a large positive nor negative impact on TrDec. For ja-en and or-en, syntax-free TrDec out- performs all baselines. On de-en, TrDec loses to CCG-null, but the difference is not statistically sig- nificant (p &gt; 0.1).</p><p>Model ja-en de-en or-en (mean ± std) seq2seq</p><p>21  Length Analysis. We performed a variety of analyses to elucidate the differences between the translations of different models, and the most con- clusive results were through analysis based on the length of the translations. First, we categorize the ja-en test set into buckets by length of the refer- ence sentences, and compare the models for each length category. <ref type="figure">Fig. 4</ref> shows the gains in BLEU score over seq2seq for the tree-based models. Since TrDec-con outperforms TrDec-dep for all datasets, we only focus on TrDec-con for analyzing TrDec's performance with syntactic trees. The relative per- formance of CCG decreases on long sentences. However, TrDec, with both parse trees and syntax- free binary trees, delivers more improvement on longer sentences. This indicates that TrDec is bet-ter at capturing long-term dependencies during de- coding. Surprisingly, TrDec-binary, which does not utilize any linguistic information, outperforms TrDec-con for all sentence length categories. Second, <ref type="figure">Fig. 5</ref> shows a histogram of translations by the length difference between the generated out- put and the reference. This provides an explanation of the difficulty of using parse trees. Ideally, this distribution will be focused around zero, indicat- ing that the MT system is generating translations about the same length as the reference. However, the distribution of TrDec-con is more spread out than TrDec-binary, which indicates that it is more difficult for TrDec-con to generate sentences with appropriate target length. This is probably because constituency parse trees of sentences with similar number of words can have very different depth, and thus larger variance in the number of generation steps, likely making it difficult for the MT model to plan the sentence structure a-prior before actually generating the child sentences. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose TrDec, a novel tree-based decoder for NMT, that generates translations along with the tar- get side tree topology. We evaluate TrDec on both linguistically-inspired parse trees and synthetic, syntax-free binary trees. Our model, when used with synthetic balanced binary trees, outperforms CCG, the existing state-of-the-art in incorporating syntax in NMT models. The interesting result that syntax-free trees out- perform their syntax-driven counterparts elicits a natural question for future work: how do we bet- ter model syntactic structure in these models? It would also be interesting to study the effect of us- ing source-side syntax together with the target-side syntax supported by TrDec.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example generation process of TrDec. Left: A target parse tree. The green squares represent preterminal nodes. Right: How our RNNs generate the parse tree on the left. The blue cells represent the activities of the rule RNN, while the grey cells represent the activities of the word RNN. heopi and heosi are the end-of-phrase and end-of-sentence tokens. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example of four tree structures (Details of preterminals and subword units omitted for illustration purpose).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Conversion of a dependency tree for TrDec. Left: original dependency tree. Right: after conversion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Softmax.</head><label></label><figDesc>At any step t, our softmax logits are W · tanh [s tree t , s word t ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: The gains of BLEU score over seq2seq.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : # sentences in each dataset.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>BLEU scores of TrDec and other baselines. Sta-

tistical significance is indicated with ⇤ (p &lt; 0.05) and ⇤⇤ 
(p &lt; 0.001), compared with the best baseline. 

</table></figure>

			<note place="foot" n="1"> Our code is available at https://github.com/ cindyxinyiwang/TrDec_pytorch.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This material is based upon work supported in part by the Defense Advanced Research Projects Agency Information Innovation Office (I2O) Low Resource Languages for Emergent In-cidents (LORELEI) program under Contract No. HR0011-15-C0114, and the National Science Foun-dation under Grant No. 1815287. The views and conclusions contained in this document are those of the authors and should not be interpreted as rep-resenting the official policies, either expressed or implied, of the U.S. Government. The U.S. Gov-ernment is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards string-to-tree neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tree-structured decoding with doubly recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Melis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The iwslt 2016 evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stuker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWSLT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improved neural machine translation with a syntax-aware encoder and decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tree-to-sequence attentional neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to parse and translate improves neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scalable inference and training of context-rich syntactic translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Graehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Deneefe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Thayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">What&apos;s in a translation rule? In NAACL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computations</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling source syntax for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Deyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Using dependency order templates to improve generality in translation</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Arul Menezes and Chris Quirk. In WMT</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Predicting target language CCG supertags improves neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Nadejde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The Kyoto free translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<ptr target="http://www.phontron.com/kftt" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ckylark: A more robust pcfg-la parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sakriani</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoki</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL Software Demonstration</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning accurate, compact, and interpretable tree annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING-ACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved inference for unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Does string-based neural mt learn source syntax? In EMNLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkit</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In NIPS</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sequence-to-dependency neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A syntactic neural model for general-purpose code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
