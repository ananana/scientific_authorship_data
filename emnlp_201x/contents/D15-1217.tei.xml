<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Latent Words Language Models for Robust Modeling to Out-Of Domain Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Masumura</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Engineering</orgName>
								<orgName type="institution">Tohoku University</orgName>
								<address>
									<country>Japan †</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taichi</forename><surname>Asami</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanobu</forename><surname>Oba</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokazu</forename><surname>Masataki</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumitaka</forename><surname>Sakauchi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Ito</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Engineering</orgName>
								<orgName type="institution">Tohoku University</orgName>
								<address>
									<country>Japan †</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NTT Media Intelligence Laboratories</orgName>
								<orgName type="institution" key="instit2">NTT Corporation</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Latent Words Language Models for Robust Modeling to Out-Of Domain Tasks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper focuses on language modeling with adequate robustness to support different domain tasks. To this end, we propose a hierarchical latent word language model (h-LWLM). The proposed model can be regarded as a generalized form of the standard LWLMs. The key advance is introducing a multiple latent variable space with hierarchical structure. The structure can flexibly take account of linguistic phenomena not present in the training data. This paper details the definition as well as a training method based on layer-wise inference and a practical usage in natural language processing tasks with an approximation technique. Experiments on speech recognition show the effectiveness of h-LWLM in out-of domain tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language models (LMs) are essential for auto- matic speech recognition or statistical machine translation <ref type="bibr" target="#b21">(Rosenfeld, 2000</ref>). The performance of LMs strongly depends on quality and quantity of their training data. Superior performance is usu- ally obtained by using enormous domain-matched training data sets to construct <ref type="bibr">LMs (Brants et al., 2007)</ref>. Unfortunately, in many cases, large amounts of domain-matched training data sets are not available. Therefore, LM technology that can robustly work for domains that differ from that of the training data is needed <ref type="bibr" target="#b7">(Goodman, 2001</ref>).</p><p>For robust language modeling, several tech- nologies have been proposed. Fundamental tech- niques are smoothing <ref type="bibr" target="#b4">(Chen and Goodman, 1999</ref>) and clustering <ref type="bibr" target="#b2">(Brown et al., 1992</ref>). Other solu- tions are Bayesian modeling <ref type="bibr" target="#b24">(Teh, 2006</ref>) and en- semble modeling ( <ref type="bibr" target="#b25">Xu and Jelinek, 2004;</ref><ref type="bibr" target="#b6">Emami and Jelinek, 2005</ref>). Moreover, continuous rep- resentation of words in neural network LMs can also support robust modeling ( <ref type="bibr" target="#b0">Bengio et al., 2003;</ref><ref type="bibr" target="#b17">Mikolov et al., 2010</ref>). However, previous works are focused on maximizing performance in the same domain as that of the training data. In other words, it is uncertain that these technologies ro- bustly support out-of domain tasks.</p><p>In contrast, latent words LMs (LWLMs) <ref type="bibr" target="#b5">(Deschacht et al., 2012)</ref> are clearly effective for out- of domain tasks. We employed the LWLM to speech recognition and the resulting performance was significantly superior in out-of domain tasks while the performance was comparable in domain- matched task to conventional <ref type="bibr">LMs (Masumura et al., 2013a;</ref><ref type="bibr" target="#b16">Masumura et al., 2013b</ref>). LWLMs are generative models that employ a latent word space. The latent space can flexibly take into ac- count relationships between words and the model- ing helps to efficiently increase the robustness to out-of domain tasks <ref type="bibr">(Sec. 2)</ref>.</p><p>In this paper, we focus on LWLMs and aim to make them more flexible for greater robustness to out-of domain tasks. To this end, this paper takes note of a fact that standard LWLM simply repre- sents the latent space as n-gram model of latent words. However, function and meaning of words are essentially hierarchical and upper layers ought to be useful to increase the robustness to out-of domain tasks. The conventional LWLMs do not model the hierarchy, while the latent words are used to represent function and meaning of words. Thus, we tried to model the hierarchy in the latent space by estimating a latent word of a latent word recursively.</p><p>This paper proposes a novel LWLM with mul- tiple latent word spaces that are hierarchically structured; we call it the hierarchical LWLM (h- LWLM). The proposed model can be regarded as a generalized form of the standard LWLMs. The hierarchical structure can take into account the abstraction process of function and meaning of words. Therefore, it can be expected that h-LWLMs flexibly calculate generative probability for unseen words unlike non-hierarchical LWLMs. To create the hierarchical latent word structure from training data sets, we also propose a layer- wise inference. The inference is inspired by a deep Boltzmann machine <ref type="bibr" target="#b22">(Salakhutdinov and Hinton, 2009</ref>) that stacks up restricted Boltzmann ma- chines ( <ref type="bibr" target="#b8">Hinton et al., 2006</ref>). In addition, we detail an n-gram approximation technique to apply the proposed model to practical natural language pro- cessing tasks (see <ref type="bibr">Sec. 3)</ref>.</p><p>In experiments, we construct LMs from sponta- neous lecture task data and apply them to a contact center dialogue task and a voice mail task as out- of domain tasks. The effectiveness of the proposed method is shown by perplexity and speech recog- nition evaluation (Sec. 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Latent Words Language Models</head><p>LWLMs are generative models with single latent word space <ref type="bibr" target="#b5">(Deschacht et al., 2012</ref>). The latent word is represented as a specific word that is se- lected from the entire vocabulary. Thus, the num- ber of latent words equals the number of observed words.</p><p>Bayesian modeling of LWLM produces the gen- erative probability of observed word sequence w = w 1 , · · · , w K as:</p><formula xml:id="formula_0">P (w) = θ K k=1 h k P (w k |h k , θ) P (h k |l k , θ)P (θ)dθ, (1)</formula><p>where θ indicates a model parameter of the LWLM, h = h 1 , · · · , h K denotes a latent word sequence and l k denotes context latent words h k−n+1 , · · · , h k−1 . P (h k |l k , θ) repre- sents the transition probability which can be ex- pressed by an n-gram model for latent words, and P (w k |h k , θ) represents the emission probability that models the dependency between the observed word and the latent word. More details are shown in previous works <ref type="bibr" target="#b5">(Deschacht et al., 2012;</ref><ref type="bibr" target="#b15">Masumura et al., 2013a;</ref><ref type="bibr" target="#b16">Masumura et al., 2013b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hierarchical LWLMs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Definition</head><p>This paper introduces h-LWLM. The proposed model has multiple latent word spaces in a hier- archical structure. Thus, it assumes that there is a latent word behind a latent word. The proposed model can be regarded as a generalized form of the standard LWLM. Thus, standard LWLMs cor- respond to h-LWLMs with just one layer. The la- tent words in all layers are represented as a specific word that is selected from the entire vocabulary.</p><p>A graphic rendering of h-LWLM is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. In a generative process of the h-LWLM, a latent word in the highest layer is first generated depending on its context latent words. Next, a la- tent word in a lower layer is recursively generated depending on the latent word in the upper layer. Finally, an observed word is generated depending on the latent word in the lowest layer.</p><p>Bayesian modeling of h-LWLM produces the following generative probability:</p><formula xml:id="formula_1">P (w) = Θ K k=1 h (1) k · · · h (M ) k P (w k |h (1) k , Θ) · · · P (h (M −1) k |h (M ) k , Θ)P (h (M ) k |l (M ) k , Θ)P (Θ)dΘ,<label>(2)</label></formula><p>where M is the number of layers and Θ indi- cates a model parameter of h-LWLM.</p><formula xml:id="formula_2">h (m) = h (m) 1 , · · · , h (m) K</formula><p>denotes a latent word sequence in the m-th layer.</p><formula xml:id="formula_3">P (h (M ) k |l (M )</formula><p>k , Θ) represents the transition probability which is expressed by n- gram model for latent words in the highest layer.</p><formula xml:id="formula_4">P (h (m) k |h (m+1) k</formula><p>, Θ) and P (w k |h <ref type="formula">(1)</ref> k , Θ) represent the emission probabilities that respectively model the dependency between latent words in two layers and the dependency between the observed word and the latent word in the lowest layer.</p><p>As the integral with respect to Θ is analytically </p><formula xml:id="formula_5">h (0) = w 3:</formula><p>for m = 1 to M do 4:</p><formula xml:id="formula_6">θ (m) , h (m) ∼ P (h (m) |h (m−1) , θ (m) ) 5:</formula><p>end for 6:</p><formula xml:id="formula_7">Θ t = θ (1) , · · · , θ (M ) 7: end for 8: return Θ 1 , · · · , Θ T</formula><p>intractable, the equation can be approximated as:</p><formula xml:id="formula_8">P (w) = 1 T K k=1 T t=1 h (1) k · · · h (M ) k P (w k |h (1) k , Θ t ) · · · P (h (M −1) k |h (M ) k , Θ t )P (h (M ) k |l (M ) k , Θ t ).<label>(3)</label></formula><p>The probability distribution can be approximated using T instances of point estimated parameter; Θ t indicates the t-th point estimated parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parameter Inference</head><p>This paper proposes a layer-wise inference pro- cedure for constructing h-LWLMs from training data. The detailed procedure is shown in Algo- rithm 1, and <ref type="figure" target="#fig_1">Figure 2</ref> shows an image representa- tion of the procedure as increased with the number of layers. In the procedure, LWLM structure is re- cursively accumulated by estimating a latent word sequence in an upper layer from a latent word se- quence in the lower layer.</p><p>Line 4 in Algorithm 1 denotes the key proce- dure of estimating a latent word sequence in an up- per layer from a latent word sequence in the lower layer. θ (m) denotes model parameter of LWLM structure in m-th layer; it can be defined from both h (m) and h (m−1) . For the inference of h (m) from h (m−1) , Gibbs sampling is suitable <ref type="bibr" target="#b3">(Casella and George, 1992;</ref><ref type="bibr" target="#b20">Robert et al., 1993;</ref><ref type="bibr" target="#b23">Scott, 2002</ref>). Gibbs sampling picks a new value for h (m) k ac- cording to its probability distribution which is es- timated from both h k . The probability distribution is given by:</p><formula xml:id="formula_9">P (h (m) k |h (m) −k , h (m−1) , θ (m) ) ∝ P (h (m−1) k |h (m) k , θ (m) ) k+n−1 j=k P (h (m) j |l (m) j , θ (m) ). (4)</formula><p>For the inference, the prior distribution is neces- sary for each probability distribution. Usually, a hierarchical Pitman-Yor prior <ref type="bibr" target="#b24">(Teh, 2006</ref>) is used for each transition probability and a Dirichlet prior <ref type="bibr" target="#b13">(MacKay and Peto, 1994)</ref> is used for each emis- sion probability.</p><p>As shown in line 6, t-th point estimated param- eter Θ t indicates parameters of each LWLM for all layers in t-th iteration. The transition proba- bilities except for M -th layer are only used in the layer-wise inference procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Usage</head><p>It is impractical to directly apply the h-LWLM to natural language processing tasks since the pro- posed model has multiple latent word spaces and we have to consider all possible latent word as- signment for calculating generative probabilities. Therefore, this paper introduces an n-gram ap- proximation technique as well as that for standard LWLM ( <ref type="bibr" target="#b15">Masumura et al., 2013a</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 : Random sampling for trained h-LWLM.</head><p>Input: Model parameters Θ 1 , · · · , Θ T , number of sampled words K Output: Sampled data w 1: for k = 1 to K do 2:</p><formula xml:id="formula_10">Θ t ∼ P (Θ t ) = 1 T 3: h (M ) k ∼ P (h (M ) k |l (M ) k , Θ t ) 4:</formula><p>for m = M − 1 to 1 do</p><formula xml:id="formula_11">5: h (m) k ∼ P (h (m) k |h (m+1) k , Θ t ) 6:</formula><p>end for 7:</p><formula xml:id="formula_12">w k ∼ P (w k |h (1) k , Θ t ) 8: end for 9: return w = w 1 , · · · , w K</formula><p>The n-gram approximation is conducted as fol- lowing steps. First, a lot of text data that permit h- LWLMs to be approximated by n-gram structure is generated by random sampling using trained h-LWLM. Next, an n-gram model is constructed from the generated data. The random sampling is based on Algorithm 2. The sampled data w in line 9 is only used for n-gram model estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Conditions</head><p>Our basic assumption is domain-matched train- ing data is not available. Thus, for LM train- ing, we used the Corpus of Spontaneous Japanese (CSJ) whose domain is a spontaneous lecture task ( <ref type="bibr" target="#b14">Maekawa et al., 2000</ref>). We divided CSJ into a training set and a small validation set (Valid). The validation set was used for optimizing several hy- per parameters of LMs. For evaluation, a contact center dialogue task (Test 1) and a voice mail task (Test 2) were prepared. In contact center dialogue task, two speakers, an operator and a customer, talked to each other as in call center dialogues. 24 phone calls (24 operator channels and 24 customer channels) were used in the evaluation. In the voice mail task, a person spoke small voice messages us- ing a smart phone. 237 messages are used in the evaluation. The training data had about 7M words, the vocabulary size was about 80K. The validation data size and test data size (both tasks) were about 20K words.</p><p>For speech recognition evaluation, we prepared an acoustic model based on hidden Markov mod- els with deep neural networks (DNN-HMM) <ref type="bibr" target="#b9">(Hinton et al., 2012</ref>). The DNN-HMM had 8 hidden layers with 2048 nodes. The speech recognizer was a weighted finite state transducer (WFST) de- coder ( <ref type="bibr" target="#b19">Mohri et al., 2001;</ref><ref type="bibr" target="#b10">Hori et al., 2007)</ref>.</p><p>As a baseline, 3-gram LM with interpolated Kneser-Ney smoothing (MKN) <ref type="bibr" target="#b12">(Kneser and Ney, 1995</ref>) and 3-gram hierarchical Pitman-Yor LM (HPY) <ref type="bibr" target="#b11">(Huang and Yor, 2007)</ref> were constructed from the training data. We also trained a class- based recurrent neural network LM with 500 hid- den nodes and 500 classes (RNN) for comparison to state-of-the art language modeling <ref type="bibr" target="#b18">(Mikolov et al., 2011</ref>). In addition, we constructed 3-gram standard LWLM and 3-gram h-LWLMs (LW). LW with 1 layer represents standard LWLM, and LW with 2-5 layers represent proposed h-LWLMs. The number of instances was set to 10 for each LW. For their n-gram approximation, we generated one billion words and approximated each as a 3-gram HPYLM. Moreover, we constructed interpolated model with LW and HPY (LW+HPY). <ref type="figure" target="#fig_3">Figure 3</ref> shows the relation between number of layers in h-LWLM and perplexity (PPL) reduc- tion for each condition. In addition, <ref type="table">Table 1</ref> shows speech recognition results in terms of word error rate (WER) for each condition. RNN was only tested in PPL evaluation as RNN cannot be con- verted into WFST format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>For the validation set (same domain as that of training set), PPL was not improved by the hier- archical structure in LW. LW is comparable to MKN and HPY, and inferior to RNN in terms of PPL. On the other hand, in test sets (out-of domain tasks), PPL improved with the increase in the number of layers in LW. LW with 5 layers was superior to 1 layer in terms of PPL and WER. The best re- sults were obtained by LW+HPY with 5 layers. In fact, when we generated one billion words using a trained LWLM or trained h-LWLM, the num- ber of observed trigrams in h-LWLM with 5 lay- ers was 101M while the number of observed tri- grams in non-hierarchical LWLM was 94M. Thus, h-LWLM can generate unseen words unlike non- hierarchical LWLM. Moreover, trigram coverage in each test data slightly increased with number of layers. These results show that h-LWLM with multiple layers offers robust performance not pos- sible with other models while its performance in the same domain as that of training data was not improved. As a result, LW+HPY with 5 layers </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup</head><p>Layer </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper proposed h-LWLM for robust model- ing and detailed its definition, inference proce- dure, and approximation method. The proposed model has a hierarchical latent word space and it can flexibly handle linguistic phenomena not present in the training data. Our experiments showed that h-LWLM offers improved robustness to out-of domain tasks; h-LWLM is also superior to standard LWLM in terms of PPL and WER. Furthermore, our approach is significantly supe- rior to the conventional n-gram models or the re- current neural network LM in out-of domain tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphical representation of h-LWLM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Layer-wise inference procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>−k repre- sents all latent words in the m-th layer except for h (m)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Perplexity (PPL) results.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rejean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large language models in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ftanz</forename><forename type="middle">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="858" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer</forename><forename type="middle">C</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Explaining the Gibbs sampler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Casella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Edward I George</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="167" to="174" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="359" to="383" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The latent words language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koen</forename><surname>Deschacht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">De</forename><surname>Belder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="384" to="409" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Random clusterings for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Emami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="581" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A bit of progress in language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">T</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="403" to="434" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep bilief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee-Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel</forename><surname>Rahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient WFST-based onepass decoding with on-the-fly hypothesis rescoring in extremely large vocabulary continuous speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiori</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhiro</forename><surname>Minami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1352" to="1365" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical Pitman-Yor language models for ASR in meetings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Yor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<meeting>IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="124" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A hierarchical Dirichlet language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><forename type="middle">C</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural language engineering</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="289" to="308" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spontaneous speech corpus of Japanese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kikuo</forename><surname>Maekawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanae</forename><surname>Koiso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadaoki</forename><surname>Furui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>International Conference on Language Resources and Evaluation (LREC)</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="947" to="952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Use of latent words language models in ASR: a sampling-based implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Masumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokazu</forename><surname>Masataki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanobu</forename><surname>Oba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osamu</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Takahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8445" to="8449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Viterbi decoding for latent words language models using Gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Masumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanobu</forename><surname>Oba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokazu</forename><surname>Masataki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osamu</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Takahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH)</title>
		<meeting>Annual Conference of the International Speech Communication Association (INTERSPEECH)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3429" to="3433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH)</title>
		<meeting>Annual Conference of the International Speech Communication Association (INTERSPEECH)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Extensions of recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><forename type="middle">Kombrink</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weighted finite-state transducers in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="69" to="88" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bayesian estimation of hidden Markov chains: A stochastic implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><forename type="middle">P</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Celeux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Diebolt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics &amp; Probability Letters</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="77" to="83" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Two decades of statistical language modeling: Where do we go from here?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="1270" to="1278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bayesian methods for hidden Markov models: Recursive computing in the 21st century</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="337" to="351" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A hierarchical bayesian language model based on Pitman-Yor processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="985" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Random forests in language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Empirical Methods on Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods on Natural Language essing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="325" to="332" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
