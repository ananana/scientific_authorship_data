<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Re-evaluating Automatic Summarization with BLEU and 192 Shades of ROUGE</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
							<email>graham.yvette@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">ADAPT Centre School of Computer Science and Statistics Trinity College Dublin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Re-evaluating Automatic Summarization with BLEU and 192 Shades of ROUGE</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We provide an analysis of current evaluation methodologies applied to summariza-tion metrics and identify the following areas of concern: (1) movement away from evaluation by correlation with human assessment ; (2) omission of important components of human assessment from evaluations , in addition to large numbers of metric variants; (3) absence of methods of significance testing improvements over a baseline. We outline an evaluation methodology that overcomes all such challenges , providing the first method of significance testing suitable for evaluation of summarization metrics. Our evaluation reveals for the first time which metric variants significantly outperform others, optimal metric variants distinct from current recommended best variants, as well as machine translation metric BLEU to have performance on-par with ROUGE for the purpose of evaluation of summariza-tion systems. We subsequently replicate a recent large-scale evaluation that relied on, what we now know to be, suboptimal ROUGE variants revealing distinct conclusions about the relative performance of state-of-the-art summarization systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We provide an analysis of current evalua- tion methodologies applied to summariza- tion metrics and identify the following ar- eas of concern: (1) movement away from evaluation by correlation with human as- sessment; (2) omission of important com- ponents of human assessment from eval- uations, in addition to large numbers of metric variants; (3) absence of methods of significance testing improvements over a baseline. We outline an evaluation methodology that overcomes all such chal- lenges, providing the first method of sig- nificance testing suitable for evaluation of summarization metrics. Our evaluation re- veals for the first time which metric vari- ants significantly outperform others, op- timal metric variants distinct from cur- rent recommended best variants, as well as machine translation metric BLEU to have performance on-par with ROUGE for the purpose of evaluation of summariza- tion systems. We subsequently replicate a recent large-scale evaluation that relied on, what we now know to be, suboptimal ROUGE variants revealing distinct conclu- sions about the relative performance of state-of-the-art summarization systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic metrics of summarization evaluation have their origins in machine translation (MT), with ROUGE ( <ref type="bibr" target="#b9">Lin and Hovy, 2003)</ref>, the first and still most widely used automatic summarization metric, comprising an adaption of the BLEU score ( <ref type="bibr" target="#b12">Papineni et al., 2002</ref>). Automatic evaluation in MT and summarization have much in common, as both involve the automatic comparison of system- generated texts with one or more human-generated reference texts, contrasting either system-output translations or peer summaries with human ref- erence translations or model summaries, depend- ing on the task. In both MT and summarization evaluation, any newly proposed automatic metric must be assessed by the degree to which it pro- vides a good substitute of human assessment, and although there are obvious parallels between eval- uation of systems in the two areas, when it comes to evaluation of metrics, summarization has di- verged considerably from methodologies applied to evaluation of metrics in MT.</p><p>Since the inception of BLEU, evaluation of au- tomatic metrics in MT has been by correlation with human assessment. In contrast in summa- rization, over the years since the introduction of ROUGE, summarization evaluation has seen a va- riety of different methodologies applied to evalu- ation of its metrics. Evaluation of summarization metrics has included, for example, the ability of a metric/significance test combination to distinguish between sets of human and system-generated sum- maries ( <ref type="bibr" target="#b14">Rankel et al., 2011</ref>), or by accuracy of conclusions drawn from metrics when combined with a particular significance test, Wilcoxon rank- sum ( <ref type="bibr" target="#b11">Owczarzak et al., 2012)</ref>.</p><p>Besides moving away from well-established methods such as correlation with human judg- ment, previous summarization metric evaluations have been additionally limited by inclusion of only a small proportion of possible metrics and vari- ants. For example, although the most commonly used metric ROUGE has a very large number of possible variants, it is common to include only a small range of those in evaluations. This has the obvious disadvantage that superior variants may exist but remain unidentified due to their omission.</p><p>Despite such limitations, however, subsequent evaluations of state-of-the-art summarization sys- tems operate under the assumption that recom- mended ROUGE variants are optimal and rely on this assumption to draw conclusions about the rel- ative performance of systems ( <ref type="bibr" target="#b6">Hong et al., 2014)</ref>. This forces us to raise some important questions. Firstly, to what degree was the divergence away from evaluation methodologies still applied to MT metrics today well-founded? For example, were the original methodology, by correlation with hu- man assessment, to be applied, would a distinct variant of ROUGE emerge as superior and subse- quently lead to distinct system rankings? Sec- ondly, were all variants of ROUGE to be included in evaluations, would a variant originally omitted from the evaluation emerge as superior and lead to further differences in summarization system rank- ings? Furthermore, although methods of statistical significance testing are commonly applied to eval- uation of summarization systems, attempts to iden- tify significant differences in performance of met- rics are extremely rare, and when they have been applied unfortunately have not used an appropriate test.</p><p>This motivates our review of past and current methodologies applied to the evaluation of sum- marization metrics. Since MT evaluation in gen- eral has its own imperfections, we do not at- tempt to indiscriminately impose all MT evalua- tion methodologies on summarization, but specif- ically revisit evaluation methodologies applied to one particular area of summarization, evaluation of metrics. Correlations with human assessment reveal an extremely wide range in performance among variants, highlighting the importance of an optimal choice of ROUGE variant in system eval- uations. Since distinct variants of ROUGE achieve significantly stronger correlation with human as- sessment than previous recommended best vari- ants, we subsequently replicate a recent evaluation of state-of-the-art summarization systems reveal- ing distinct conclusions about the relative perfor- mance of systems. In addition, we include in the evaluation of metrics, an evaluation of BLEU for the purpose of summarization evaluation, and con- trary to common belief, precision-based BLEU is on-par with recall-based ROUGE for evaluation of summarization systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>When ROUGE ( <ref type="bibr" target="#b9">Lin and Hovy, 2003</ref>) was first pro- posed, the methodology applied to its evaluation, in one respect, was similar to that applied to met- rics in MT, as ROUGE variants were evaluated by correlation with a form of human assessment. Where the evaluation methodology diverged from MT, however, was with respect to the precise rep- resentation of human assessment that was em- ployed. In MT evaluation of metrics, although experimentation has taken place with regards to methods of elicitation of assessments from human judges <ref type="bibr" target="#b0">(Callison-Burch et al., 2008)</ref>, human as- sessment is always aimed to encapsulate the over- all quality of translations. In contrast in summa- rization, metrics are evaluated by the degree to which metric scores correlate with human cover- age scores for summaries, a recall-based formu- lation of the number of peer summary units that a human assessor believed had the same meaning as model summaries. Substitution of overall qual- ity assessments with a recall-based manual metric, unfortunately has the potential to introduce bias into the evaluation of metrics in favor of recall- based formulations.</p><p>One dimension of summary quality omitted from human coverage scores is, for example, the order in which the units of a summary are ar- ranged within the summary. Despite unit order quite likely being something of importance to a human assessor, assessment of metrics by correla- tion with human coverage scores does not in any respect take into account the order in which the units of a summary appear, and evaluation by hu- man coverage scores alone means that a summary with its units scrambled or even reversed in the- ory receives precisely the same metric score as the original. Given current evaluation methodologies for assessment of metrics, a metric that scores two such summaries differently would be unfairly pe- nalized for it. Furthermore, when the linguistic quality of summaries has been assessed in parallel with annotations used to compute human cover- age scores, it has been shown that the two dimen- sions of quality do not correlate with one another (no significant correlation) ( <ref type="bibr" target="#b13">Pitler et al., 2010</ref>), providing evidence that coverage scores alone do not fully represent human judgment of the overall quality of summaries.</p><p>Subsequent summarization metric evaluations depart from correlation with human judgment fur-ther by evaluating metrics according to the abil- ity of a metric/significance test combination to identify a significant difference between the qual- ity of human and system-generated summaries <ref type="bibr" target="#b14">(Rankel et al., 2011</ref>). Unfortunately, the evalua- tion of metrics with respect to how well they dis- tinguish between high-quality human summaries and all system-generated summaries, does not pro- vide insight into the task of metrics, to score better quality system-generated summaries higher than worse quality system-generated summaries, how- ever. This is in contrast to evaluation of MT met- rics by correlation with human judgment, where metrics only receive credit for their ability to ap- propriately score system-output documents rela- tive to other system-output documents. Since dif- ferences in quality levels between pairs of system- generated summaries are likely to be far smaller than differences in system and human-generated summaries, the methodology unfortunately sets too low a bar for summarization metrics to meet.</p><p>Furthermore, the approach to metric evaluation unfortunately does not work in the long-term, as the performance of summarization systems im- proves and approaches or achieves the quality of a human, a metric that accurately identifies this achievement would be unfairly penalized for it. Separate from the evaluation of metrics, Rankel et al. (2011) make the highly important recommen- dation of paired tests for identification of signifi- cant differences in performance of summarization systems. Since data used in the evaluation of sum- marization systems is not independent, paired tests are more appropriate and more powerful. <ref type="bibr" target="#b11">Owczarzak et al. (2012)</ref> diverge further from correlation with human judgment for evaluation of metrics by assessing the accuracy of metrics to identify significant differences between pairs of systems when combined with a significance test. Although the approach to evaluation of metrics provides insight into the accuracy of conclusions drawn from metric/test combinations, the evalua- tion is limited by inclusion of only six variants of ROUGE, fewer than 4% of possible ROUGE vari- ants. Despite such limitations, however, subse- quent evaluations relied on recommended ROUGE variants to rank state-of-the-art systems <ref type="bibr" target="#b6">(Hong et al., 2014</ref>).</p><p>Although methods of identifying significant dif- ferences in performance are commonly applied to the evaluation of systems in summarization, the application of significance tests to the evaluation of summarization metrics is extremely rare, and when attempts have been made, unfortunately ap- propriate tests have not been applied. Computa- tion of confidence intervals for individual correla- tion with human coverage scores, for example, un- fortunately does not provide insight into whether or not a difference in correlation with human cov- erage scores is significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Summarization Metric Evaluation</head><p>When large-scale human evaluation of summa- rization systems takes place, human evaluation commonly takes the form of annotation of whether or not system-generated summary units express the meaning of model summary units, annotations subsequently used to compute human coverage scores. In addition, an evaluation of the linguis- tic quality of summaries is commonly carried out. As described in Section 2, when used for the eval- uation of metrics, linguistic quality is commonly omitted, however, with metrics only assessed by the degree to which they correlate with human coverage scores. In contrast, we include all avail- able human assessment data for evaluating met- rics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Combining Quality and Coverage</head><p>In DUC-2004 ( <ref type="bibr" target="#b10">Over et al., 2007)</ref>, human annota- tions used to compute summary coverage are car- ried out by identification of matching peer units (PUs), the units in a peer summary that express content of the corresponding model summary. In addition, an overall coverage estimate (E) is pro- vided by the human annotator, the proportion of the corresponding model summary or collective model units (MUs) expressed overall by a given peer summary. Human coverage scores (CS) are computed by combining Matching PUs with cov- erage estimates as follows:</p><formula xml:id="formula_0">CS = |M atching P U s| · E |M U s|<label>(1)</label></formula><p>In addition to annotations used to compute human coverage scores, human assessors were asked to rate the linguistic quality of summaries under 7 different criteria, providing ratings from A to E, with A denoting highest and E least quality rat- ing. <ref type="figure">Figure 1</ref> is a scatter-plot of human coverage scores and corresponding linguistic quality scores Coverage Score (%) Mean Ling. Quality (%) <ref type="figure">Figure 1</ref>: Scatter-plot of mean linguistic qual- ity and coverage scores for human assessments of summaries in <ref type="bibr">DUC-2004</ref> for all human-assessed summaries from DUC- 2004, where, for the purpose of comparison, each of the 7 linguistic quality ratings are converted to a corresponding percentage quality (A= 100%; B= 75%; C= 50%; D= 25%; E= 0%). The lo- cation of all points almost exclusively within the upper left corner of the plot in <ref type="figure">Figure 1</ref> indicates that the linguistic quality of almost all summaries reaches at least as high a level as its corresponding coverage score. This follows the intuition that a summary is unlikely to obtain high coverage with- out sufficient linguistic quality, while the same cannot be said for the converse, that a high level of linguistic quality necessarily leads to high cov- erage. More importantly, however, linguistic qual- ity scores provide an additional dimension of hu- man assessment, allowing greater discriminatory power between the quality of summaries than was possible with coverage scores alone. <ref type="figure">Figure 2</ref> includes linguistic quality and cover- age score distributions from DUC-2004 human evaluation, where each distribution is skewed in opposing directions, in addition to the distribution of the average of the two scores for summaries.</p><p>For the purpose of metric evaluation, we com- bine human coverage and linguistic quality scores using the average of the two scores, and use this as a gold standard human score for evaluation of metrics:  <ref type="figure">Figure 2</ref>: Combining linguistic quality and cover- age scores provided by human assessors in DUC- 2004</p><formula xml:id="formula_1">Human Assessment Score = CS+M LQ 2 0.0 0.2 0.4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ROUGE</head><p>scores can be tested for statistical significance us- ing, for example, Wilcoxon signed-rank test, while paired t-test can be applied to difference of mean ROUGE scores for systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Metric Evaluation by Pearson's r</head><p>Moses ( <ref type="bibr" target="#b7">Koehn et al., 2007</ref>) multi-bleu 1 was used to compute BLEU ( <ref type="bibr" target="#b12">Papineni et al., 2002</ref>) scores for summaries and prepare4rouge 2 applied to sum- maries before running ROUGE ( <ref type="bibr" target="#b9">Lin and Hovy, 2003)</ref>. <ref type="table">Table 1</ref> shows the Pearson correlation of each variant of ROUGE with human assessment, in addition to BLEU's correlation with the same hu- man assessment of summaries from DUC-2004. Somewhat surprisingly, BLEU MT evaluation met- ric achieves strongest correlation with human as- sessment overall, r = 0.797, with performance of ROUGE variants ranging from r = 0.786, just below that of BLEU, to as low as r = 0.293. For many pairs of metrics, differences in correla- tion with human judgment are small, however, and prior to concluding superiority in performance of one metric over another, significance tests should be applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Metric Significance Testing</head><p>In MT, recent work has identified the suitabil- ity of Williams significance test <ref type="bibr" target="#b17">(Williams, 1959)</ref> for evaluation of automatic MT metrics ( <ref type="bibr" target="#b5">Graham, 2015)</ref>, and, for similar reasons, Williams test is suited to significance testing differences in performance of competing summarization met- rics which we detail further below. Williams test has additionally been used in evaluation of sys- tems that automatically assess spoken and writ- ten language quality <ref type="bibr" target="#b19">(Yannakoudakis et al., 2011;</ref><ref type="bibr" target="#b18">Yannakoudakis and Briscoe, 2012;</ref><ref type="bibr" target="#b1">Evanini et al., 2013</ref>). Evaluation of a given summarization metric, M new , by Pearson correlation takes the form of quantifying the correlation, r(M new , H), that ex- ists between metric scores for systems and corre- sponding human assessment scores, and contrast- ing this correlation with the correlation for some baseline metric, r(M base , H).</p><p>One approach to testing for significance that may seem reasonable is to apply a significance test <ref type="bibr">1</ref> https://github.com/moses-smt/mosesdecoder/ commits/master/scripts/generic/multi-bleu.perl 2 http://kavita-ganesan.com/content/ prepare4rouge-script-prepare-rouge-evaluation separately to the correlation of each metric with human assessment, with the hope that the new metric will achieve a significant correlation where the baseline metric does not. The reasoning here is flawed however: the fact that one correlation is significantly higher than zero (r(M new , H)) and that of another is not, does not necessarily mean that the difference between the two correlations is significant. Instead, a specific test should be ap- plied to the difference in correlations. For this same reason, confidence intervals for individual correlations with human assessment are also not useful.</p><p>If samples that data are drawn from are inde- pendent, and differences in correlations are com- puted on independent data sets, the Fisher r to z transformation is applied to test for significant dif- ferences in correlations. Data used for the eval- uation of summarization metrics are not indepen- dent, as evaluations comprise three sets of scores for precisely the same set of summaries (corre- sponding to variables X 1 , X 2 and X 3 below), and subsequently three correlations: r(M base , H), r(M new , H) and r(M new , M base ). If r(M base , H) and r(M new , H) are both &gt; 0, then the third correlation, between metric scores themselves, r(M base , M new ), must also be &gt; 0. The strength of this correlation, directly between scores of pairs of summarization metrics, should be taken into account using a significance test of the dif- ference in correlation between r(M base , H) and r(M new , H).</p><p>Williams test <ref type="bibr">3 (Williams, 1959)</ref> evaluates the significance of a difference in dependent correla- tions <ref type="bibr" target="#b16">(Steiger, 1980)</ref>. It is formulated as follows as a test of whether the population correlation be- tween X 1 and X 3 equals the population correla- tion between X 2 and X 3 :</p><formula xml:id="formula_2">t(n − 3) = (r 13 − r 23 ) (n − 1)(1 + r 12 ) 2K (n−1) (n−3) + (r 23 +r 13 ) 2 4 (1 − r 12 ) 3 ,</formula><p>where r ij is the correlation between X i and X j , n is the size of the population, and:  <ref type="table">Table 1</ref>: Pearson correlation (r) of BLEU and 192 variants of ROUGE (R-*) with human assessment in DUC-2004, with (Y) and without (N) stemming, with (Y) and without (N) removal of stop words (RSW), aggregated at the summary level using precision (P), recall (R) or f-score (F), aggregated at the system level by average (A) or median (M) summary score, correlations marked with • signify a metric/variant whose correlation with human assessment is not significantly weaker than that of any other metric/variant (an optimal variant) according to pairwise Williams significance tests, variants employed in <ref type="bibr" target="#b6">Hong et al. (2014)</ref> are in bold.</p><formula xml:id="formula_3">K = 1 −</formula><formula xml:id="formula_4">P/R/F r BLEU 0.797 • R-2 Y Y A P 0.786 • R-3 N N A F 0.785 • R-2 N Y A P 0.783 • R-3 N Y A P 0.781 • R-3 Y N A F 0.779 • R-3 N N A R 0.777 • R-4 N N A F 0.771 • R-3 N N A P 0.771 • R-3 Y N A R 0.770 • R-2 N Y A F 0.769 • R-4 N N A R 0.768 • R-2 Y Y A F 0.768 • R-3 Y N A P 0.767 • R-3 N N M F 0.766 • R-3 N Y A F 0.764 • R-3 Y Y A P 0.764 • R-4 Y N A F 0.763 • R-4 N N A P 0.762 • R-4 Y N A R 0.761 • R-3 N N M P 0.760 • R-4 Y Y A P 0.759 • R-2 Y N A P 0.759 • R-4 N Y A P 0.758 • R-2 N N A P 0.757 • R-3 N N M R 0.753 • R-4 Y N A P 0.752 • R-3 Y Y A F 0.748 • R-2 N N A F 0.747 • R-2 Y N A F 0.747 • R-3 N Y A R 0.746 • R-3 Y N M P 0.744 • R-2 N Y M P 0.743 • R-3 Y N M F 0.743 • R-2 N Y A R 0.742 • R-2 Y Y M P 0.741 • R-2 N Y M F 0.740 • R-3 Y N M R 0.739 • R-2 Y Y A R 0.737 • R-2 Y Y M F 0.735 • R-2 N N M P 0.734 • R-3 Y Y M P 0.733 • R-3 Y Y A R 0.730 R-4 Y Y A F 0.729 • R-3 Y Y M F 0.726 • R-S4 Y N A P 0.725 • R-SU4 N N A P 0.724 • R-2 Y N M P 0.724 R-S4 N Y A P 0.724 R-SU4 Y N A P 0.723 • R-S4 N N A P 0.723 • R-2 N Y M R 0.722 • R-4 N Y A F 0.721 • R-1 N N A P 0.720 • R-2 N N M F 0.719 • R-SU4 N Y A P 0.719 R-1 Y N A P 0.714 • R-2 Y Y M R 0.714 • R-3 Y Y M R 0.713 • R-4 Y Y A R 0.712 • R-S4 Y Y A P 0.711 R-SU4 Y Y A P 0.710 R-2 N N A R 0.710 • R-W N Y A P 0.709 • R-2 Y N A R 0.707 • Metric Stemming RSW Ave./Med P/R/F r R-2 Y N M F 0.706 R-3 N Y M P 0.704 • R-1 N Y A P 0.704 • R-4 N N M R 0.703 • R-L N Y A P 0.700 • R-W Y Y A P 0.700 • R-4 N Y A R 0.700 • R-1 Y N M P 0.699 • R-S4 N Y M P 0.698 R-1 Y Y A P 0.698 • R-3 N Y M F 0.697 • R-W N N A P 0.696 • R-W Y N A P 0.695 • R-4 N N M F 0.695 • R-S4 N Y M F 0.693 R-S4 N Y A F 0.691 R-SU4 N Y M P 0.690 R-1 N N M P 0.690 • R-2 N N M R 0.689 R-L Y Y A P 0.688 • R-3 N Y M R 0.687 • R-S4 N N M P 0.687 R-S4 Y N A F 0.687 R-S4 N N A F 0.687 R-4 N N M P 0.687 • R-L N N A P 0.686 • R-SU4 N N M P 0.686 R-L Y N A P 0.683 • R-W N N M P 0.682 • R-W Y N M P 0.680 • R-SU4 Y N M P 0.678 R-SU4 N Y A F 0.678 R-S4 Y Y A F 0.676 R-SU4 N Y M F 0.676 R-SU4 N N A F 0.673 R-1 N Y M P 0.673 R-2 Y N M R 0.672 R-SU4 Y N A F 0.671 R-S4 N Y M R 0.670 R-S4 Y N M P 0.670 R-SU4 Y Y A F 0.668 R-S4 N N M F 0.666 R-W N Y M P 0.664 R-S4 Y Y M P 0.664 R-SU4 Y Y M P 0.663 R-L N N M P 0.661 • R-SU4 N N M F 0.658 R-1 N Y A F 0.656 R-W Y Y M P 0.656 R-S4 N Y A R 0.656 R-L Y N M P 0.656 • R-W N Y A F 0.655 R-1 N Y M F 0.653 R-L N Y A F 0.652 R-1 Y Y M P 0.651 R-S4 Y Y M F 0.649 R-1 Y Y A F 0.649 R-SU4 Y Y M F 0.649 R-SU4 N Y M R 0.646 R-L N Y M P 0.645 R-W N Y M F 0.642 R-W Y Y A F 0.642 R-4 Y N M R 0.641 R-S4 Y Y A R 0.641 R-4 Y N M F 0.639 Metric Stemming RSW Ave./Med P/R/F r R-L Y Y A F 0.638 R-1 N N A F 0.637 R-S4 Y N M F 0.634 R-4 Y N M P 0.634 R-1 N N M F 0.634 R-SU4 N Y A R 0.633 R-L Y Y M P 0.633 R-SU4 Y Y M R 0.631 R-1 Y N A F 0.630 R-1 Y Y M F 0.629 R-S4 Y Y M R 0.626 R-S4 N N A R 0.626 R-SU4 Y N M F 0.625 R-S4 Y N A R 0.624 R-L N Y M F 0.623 R-SU4 Y Y A R 0.622 R-1 Y N M F 0.617 R-1 N Y M R 0.615 R-W N Y A R 0.613 R-S4 N N M R 0.611 R-L N Y M R 0.609 R-1 N Y A R 0.604 R-L N Y A R 0.601 R-W N N M F 0.600 R-L N N M F 0.599 R-W Y Y A R 0.598 R-W N Y M R 0.597 R-1 Y Y A R 0.595 R-1 Y Y M R 0.591 R-L N N A F 0.586 R-W Y Y M F 0.586 R-W Y N M F 0.585 R-L Y Y A R 0.583 R-L Y Y M F 0.582 R-L Y Y M R 0.579 R-L Y N A F 0.579 R-W N N A F 0.579 R-SU4 N N M R 0.576 R-W Y N A F 0.576 R-SU4 N N A R 0.574 R-SU4 Y N A R 0.571 R-L Y N M F 0.569 R-W Y Y M R 0.567 R-S4 Y N M R 0.566 R-SU4 Y N M R 0.525 R-1 N N M R 0.488 R-1 Y N M R 0.477 R-W Y N M R 0.477 R-1 N N A R 0.470 R-W N N M R 0.470 R-L N N M R 0.470 R-1 Y N A R 0.459 R-W N N A R 0.456 R-W Y N A R 0.452 R-L Y N M R 0.423 R-L N N A R 0.416 R-L Y N A R 0.406 R-4 Y Y M P 0.307 R-4 Y Y M F 0.302 R-4 N Y M P 0.301 R-4 Y Y M R 0.297 R-4 N Y M F 0.296 R-4 N Y M R 0.293</formula><p>outperform, as a metric that happens to correlate strongly with a higher number of competing met- rics in a given competition would be at an un- fair advantage. This increased power also means, somewhat counter-intuitively, it can happen for a pair of competing metrics for which the correla- tion between metric scores is strong, that a small difference in competing correlations with human assessment is significant, while, for a different pair of metrics with a larger difference in corre- lation, the difference is not significant, because r(M base , M new ) is weak. For example, in Ta- ble 1 the difference in correlation with human as- sessment of BLEU and that of median ROUGE-L precision with stemming and stop-words retained, 0.141 (0.797 − 0.656), is not significant, while the smaller difference in correlation with human assessment between correlations of BLEU and av- erage ROUGE-3 recall with stemming and stop- words removed, 0.067 (0.797 − 0.73) is signifi- cant, since scores of the latter pair of metrics cor- relate with one another with more strength.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As part of this research, we have made avail- able an open-source implementation of statistical tests for evaluation of summarization metrics, at</head><p>https://github.com/ygraham/nlp-williams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Significance Test Results</head><p>In <ref type="table">Table 1</ref>, • identifies variants of ROUGE not sig- nificantly outperformed by any other variant. <ref type="figure" target="#fig_1">Fig- ure 3</ref> shows pairwise Williams significance test outcomes for BLEU, the top ten ROUGE variants, as well as current recommended ROUGE variants ( <ref type="bibr" target="#b11">Owczarzak et al. (2012)</ref>) used to compare sys- tems in <ref type="bibr" target="#b6">Hong et al. (2014)</ref>. Current recommended best variants of ROUGE are shown to be signifi- cantly outperformed by several other ROUGE vari- ants.</p><p>Although BLEU achieves strongest correlation with human assessment overall, <ref type="figure" target="#fig_1">Figure 3</ref> reveals the difference between BLEU's correlation with human assessment and that of the best-performing ROUGE variant as not statistically significant, and since ROUGE holds the distinct advantage over BLEU of facilitating standard methods of signif- icance testing differences in scores for systems, for this reason alone we recommend the use of the best-performing ROUGE variant over BLEU, aver- age ROUGE-2 precision with stemming and stop- words removed.  variants that can be attributed to each of ROUGE's configuration options. Contrary to prior belief, the vast majority of optimal ROUGE variants are precision-based, showing that the assumption that recall-based metrics are superior for evaluation of summarization systems to be inaccurate, and a likely presence of bias in favor of recall-based metrics in evaluations by correlation with human coverage scores alone. Furthermore, since there exists a vast number of possible formulations that could potentially be applied to evaluation of sum- maries that are neither purely precision nor recall- based, evaluation methodologies should avoid re- liance on assumptions that either precision or re- call is superior and instead base conclusions on empirical evidence where possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summarization System Evaluation</head><p>Since we have established that the variants of ROUGE used to rank state-of-the-art and baseline summarization systems in <ref type="bibr" target="#b6">Hong et al. (2014)</ref> have significantly weaker correlations with human as- sessment than several other ROUGE variants, this motivates our replication of the evaluation. We evaluate systems using the variant of ROUGE that N-gram Count R-3 28.7 R-2 25.0 R-4</p><p>18.8 R-1 7.5 R-L 7.5 R-W 7.5 R-S4 2.5 R-SU4 2.   achieves strongest correlation with human assess- ment, average ROUGE-2 precision with stemming and stop-words removed. <ref type="table" target="#tab_6">Table 3</ref> shows ROUGE scores for summarization systems originally presented in <ref type="bibr" target="#b6">Hong et al. (2014)</ref>. System rankings diverge considerably from those of the original evaluation. Notably, the system now taking first place had originally ranked in fourth position.</p><p>Since the best variant of ROUGE is based on av- erage ROUGE scores as opposed to median ROUGE scores, a difference of means significance test is appropriate provided the normality assumption of score distributions for systems is not violated. In addition, since data used to evaluate systems are not independent, paired tests are also appropri- ate <ref type="bibr" target="#b14">(Rankel et al., 2011</ref>). ROUGE score distri- butions for systems were tested for normality us- ing the Shapiro-Wilk test <ref type="bibr" target="#b15">(Royston, 1982)</ref> where score distributions for none of the included sys- tems were shown to be significantly non-normal. <ref type="figure" target="#fig_2">Figure 4</ref> shows outcomes of paired t-tests for summary score distributions of each pair of sys- tems, revealing three summarization systems not significantly outperformed by any other as DPP, ICSISUMM and REGSUM. In addition, as ex- pected, all state-of-the-art systems significantly outperform all baseline systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Human Assessment Combinations</head><p>In order to evaluate metrics by correlation with hu- man assessment, it is necessary to obtain a single human evaluation score per system. For example, in the evaluation of metrics in Section 3, we com- bined linguistic quality and coverage into a sin- gle score using the mean of the two scores. Other combinations are of course possible, but without any additional human evaluation data, it is chal- lenging to identify the combination that best rep-  resents an overall human assessment for a given summary. One possibility would be to search for optimal weights for combining quality and cover- age, but there is a risk with this approach that we will not find the most representative combination but simply the combination that best describes the metrics.</p><p>An additional variation of human assessment scores is by combining coverage and quality with a variant of the arithmetic mean, such as the har- monic or geometric mean. <ref type="table" target="#tab_8">Table 4</ref> shows correla- tions of BLEU and the top ten performing variants of ROUGE when evaluated against the arithmetic (mean), harmonic and geometric mean of quality and coverage scores for summaries. In addition, <ref type="table" target="#tab_8">Table 4</ref> includes correlations of metric scores with coverage alone, as well as linguistic quality scores alone to provide additional insight, although lin- guistic quality scores alone do not provide a suffi- cient evaluation of metrics -since it is possible to generate summaries with perfect linguistic quality without inclusion of any relevant content whatso- ever.</p><p>BLEU MT metric achieves highest correlation across all human evaluation combinations and highest again when evaluated against human cov- erage scores alone, and BLEU's brevity penalty, that like recall penalizes a system for too short out- put, is a probable cause of the metric overcom- ing the recall-based bias of an evaluation based on coverage scores alone. In addition, our rec- ommended variant, ave. ROUGE-2 prec. with stemming and stop words removed is not signif- icantly outperformed by BLEU or any other vari- ant of ROUGE for any of the three combined mean human assessment scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>An analysis of evaluation of summarization met- rics was provided with an evaluation of BLEU and 192 variants of ROUGE. Detail of the first suitable summarization metric significance test, Williams test, was provided. Results reveal superior vari- ants of metrics distinct from previously best rec- ommendations. Replication of a recent evalua- tion of state-of-the-art summarization systems also revealed contrasting conclusions about the rela- tive performance of systems. In addition, BLEU achieves strongest correlation with human assess- ment overall, but does not significantly outperform the best-performing ROUGE variant.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Pairwise significance test outcomes for BLEU, best-performing ROUGE (rows 2-9), and ROUGE applied in Hong et al. (2014) (bottom 3 rows), with (ST1) and without (ST0) stemming, with (RS1) and without (RS0) removal of stop words, for average (A) or median (M) ROUGE precision (P), recall (R) or f-score (F), colored cells denote significant win for row i metric over column j metric with Williams test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Summarization system pairwise significance test outcomes (paired t-test) for state-ofthe-art (top 7 rows) and baseline systems (bottom 5 rows) of Hong et al. (2014) evaluated with best-performing ROUGE variant: average ROUGE2 precision with stemming and stop words removed, colored cells denote a significant greater mean score for row i system over column j system according to paired t-test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 shows proportions of optimal ROUGE</head><label>2</label><figDesc></figDesc><table>BLEU 
R2_ST1_RS1_A_P 
R3_ST0_RSW0_A_F 
R2_ST0_RS1_A_P 
R3_ST0_RS1_A_P 
R3_ST1_RS0_A_F 
R3_ST0_RS0_A_R 
R4_ST0_RS0_A_F 
R3_ST0_RS0_A_P 
R4_ST1_RS0_M_R 
R2_ST1_RS0_M_R 
R1_ST1_RS0_M_R 

R1_ST1_RS0_M_R 

R2_ST1_RS0_M_R 

R4_ST1_RS0_M_R 

R3_ST0_RS0_A_P 

R4_ST0_RS0_A_F 

R3_ST0_RS0_A_R 

R3_ST1_RS0_A_F 

R3_ST0_RS1_A_P 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>5</head><label>5</label><figDesc></figDesc><table>Stemming 

Not Stemmed 53.8 
Stemmed 
46.2 

Stop-words 

Not Rem. 56.2 
Removed 43.8 

Summary-level Agg. 

Prec. 
52.5 
F-score 
25.0 
Recall 
22.5 

System-level Agg. 

Average 
63.7 
Median 
36.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Proportions of optimal ROUGE variants 
attributed to each ROUGE configuration option 
(%). 

ROUGE 
ROUGE 

System 
Best 
Original 

DPP 
8.498 
9.62 
ICSISumm 
8.317 
9.78 
RegSum 
8.187 
9.75 
Submodular 
8.047 
9.35 
CLASSY11 
7.717 
9.20 
CLASSY04 
7.690 
8.96 
OCCAMS V 
7.643 
9.76 
GreedyKL 
6.918 
8.53 
FreqSum 
6.838 
8.11 
TsSum 
6.671 
8.15 
Centroid 
6.660 
7.97 
LexRank 
6.655 
7.47 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Summarization systems originally in-
cluded in Hong et al. (2014) evaluated with the 
best-performing ROUGE variant (Best): average 
ROUGE-2 precision with stemming and stop words 
removed; and evaluated with original suboptimal 
variant (median ROUGE-2 recall with stemming 
and without removal of stop-words) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Correlation of top-ten metric variants for each alternate combination of linguistic quality and 
coverage, • denotes a metric not significantly outperformed by any other under that particular human 
evaluation combination, highest correlations highlighted in bold font. 

</table></figure>

			<note place="foot">ROUGE includes a large number of distinct variants, including eight choices of n-gram counting method (ROUGE-1; 2; 3; 4; S4; SU4; W; L), binary settings such as word-stemming of summaries and an option to remove or retain stop-words. Additional configurations include the use of precision, recall or f-score to compute individual summary scores. Finally, options for computation of the overall score for a system is by computation of the mean or median of that system&apos;s summary score distribution. In total, therefore, when employing ROUGE for the evaluation of summarization systems, there are 192 (8 x 2 x 2 x 3 x 2) possible system-level variants to choose from. The fact that final overall ROUGE scores for systems are comprised of the mean or median of ROUGE scores of individual summaries, is, again, a divergence from MT evaluation, as ngram counts used to compute BLEU scores are computed at the document as opposed to sentencelevel. However, in this respect, ROUGE has a distinct advantage over BLEU, as the fact that ROUGE comprises the mean or median score of individual summary scores makes possible the application of standard methods of significance testing differences in system-level ROUGE scores, while BLEU is restricted to the application of randomized methods (Koehn, 2004; Graham et al., 2014). For this purpose, differences in median ROUGE</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We wish to thank the anonymous reviewers. This research is supported by Science Foun-dation Ireland through the CNGL Programme (Grant 12/CE/I2267) in the ADAPT Centre (www. adaptcentre.ie) at Trinity College Dublin.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Further meta-evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cameron</forename><surname>Fordyce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Schroeder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Wkshp. Statistical Machine Translation</title>
		<meeting>3rd Wkshp. Statistical Machine Translation<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="70" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Prompt-based content scoring for automated spoken language assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keelan</forename><surname>Evanini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shasha</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Zechner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2013 Conf. North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>2013 Conf. North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="157" to="162" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Testing for significance of increased correlation with human judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2014 Conf. Empirical Methods in Natural Language Processing</title>
		<meeting>2014 Conf. Empirical Methods in Natural Language essing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="172" to="176" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Randomized significance tests in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitika</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Wkshp. Statistical Machine Translation</title>
		<meeting>9th Wkshp. Statistical Machine Translation<address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="266" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Accurate evaluation of segment-level machine translation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitika</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2015 Conf. North American Chapter of the Association for Computational Linguistics-Human Language Technologies</title>
		<meeting>2015 Conf. North American Chapter of the Association for Computational Linguistics-Human Language Technologies<address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1183" to="1191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving evaluation of machine translation quality estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. FiftyThird Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>FiftyThird Annual Meeting of the Association for Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1804" to="1813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A repository of state of the art and competitive baseline summaries for generic news summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Favre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th edition of the Language Resources and Evaluation Conference</title>
		<meeting>9th edition of the Language Resources and Evaluation Conference<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1608" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 45th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>45th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2004 Conf. Empirical Methods in Natural Language Processing</title>
		<meeting>2004 Conf. Empirical Methods in Natural Language essing<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic evaluation of summaries using n-gram cooccurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2003 Conf. North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>2003 Conf. North American Chapter of the Association for Computational Linguistics on Human Language Technology<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
		<title level="m">Duc in context. Information Processing &amp; Management</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1506" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An assessment of the accuracy of automatic evaluation in summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karolina</forename><surname>Owczarzak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Wkshp. on Evaluation Metrics and System Comparison for Automatic Summarization</title>
		<meeting>Wkshp. on Evaluation Metrics and System Comparison for Automatic Summarization<address><addrLine>Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BLEU: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic evaluation of linguistic quality in multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="544" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ranking human and machine summarization systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Rankel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">V</forename><surname>Slud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianne</forename><forename type="middle">P</forename><surname>O&amp;apos;leary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2011 Conf. Empirical Methods in Natural Language Processing</title>
		<meeting>2011 Conf. Empirical Methods in Natural Language essing<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="467" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Algorithm as 181: The W test for normality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patrick Royston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="176" to="180" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tests for comparing elements of a correlation matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">H</forename><surname>Steiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="245" to="251" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<title level="m">Regression analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling coherence in ESOL learner texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Seventh Wkshp. on Building Educational Applications Using NLP</title>
		<meeting>Seventh Wkshp. on Building Educational Applications Using NLP<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="33" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A new dataset and method for automatically grading ESOL texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Medlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
