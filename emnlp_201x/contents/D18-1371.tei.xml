<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Ranking Models for Temporal Dependency Structure Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
							<email>yuchenz@brandeis.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Brandeis University</orgName>
								<orgName type="institution" key="instit2">Brandeis University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
							<email>xuen@brandeis.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Brandeis University</orgName>
								<orgName type="institution" key="instit2">Brandeis University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Ranking Models for Temporal Dependency Structure Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3339" to="3349"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3339</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We design and build the first neural temporal dependency parser. It utilizes a neural ranking model with minimal feature engineering, and parses time expressions and events in a text into a temporal dependency tree structure. We evaluate our parser on two domains: news reports and narrative stories. In a parsing-only evaluation setup where gold time expressions and events are provided, our parser reaches 0.81 and 0.70 f-score on unlabeled and labeled parsing respectively, a result that is very competitive against alternative approaches. In an end-to-end evaluation setup where time expressions and events are automatically recognized , our parser beats two strong baselines on both data domains. Our experimental results and discussions shed light on the nature of temporal dependency structures in different domains and provide insights that we believe will be valuable to future research in this area.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Temporal relation classification is important for a range of NLP applications that include but are not limited to story timeline construction, ques- tion answering, summarization, etc. Most work on temporal information extraction models the task as a pair-wise classification problem ( <ref type="bibr" target="#b3">Bethard et al., 2007;</ref><ref type="bibr" target="#b7">Chambers et al., 2007;</ref><ref type="bibr" target="#b6">Chambers and Jurafsky, 2008;</ref><ref type="bibr" target="#b14">Ning et al., 2018a)</ref>: given an individual pair of time expressions and/or events, the system predicts whether they are temporally related and which specific relation holds between them. An alternative approach is to model the temporal re- lations in a text as a temporal dependency struc- ture (TDS) for the entire text ( . Such a temporal dependency structure has the advantage that (1) it can be easily used to infer additional temporal relations between time expres- sions and/or events that are not directly connected via the transitivity properties of temporal relations, (2) it is computationally more efficient because a model does not need to consider all pairs of time expressions and events in a text, and (3) it is easier to use for downstream applications such as time- line construction.</p><p>However, most existing automatic systems are pair-wise models trained with traditional statisti- cal classifiers using a large number of manually crafted features ( ). The few exceptions include the work of , which describes a temporal dependency parser based on traditional feature-based classi- fiers, and <ref type="bibr" target="#b8">Dligach et al. (2017)</ref>, which describes a system using neural network based models to clas- sify individual temporal relations. More recently, a semi-structured approach has also been proposed ( <ref type="bibr" target="#b15">Ning et al., 2018b)</ref>.</p><p>In this work, taking advantage of a newly avail- able data set annotated with temporal dependency structures -the Temporal Dependency Tree (TDT) Corpus 1 ( <ref type="bibr" target="#b22">Zhang and Xue, 2018)</ref>, we develop a neural temporal dependency structure parser us- ing minimal hand-crafted linguistic features. One of the advantages of neural network based models is that they are easily adaptable to new domains, and we demonstrate this advantage by evaluating our temporal dependency parser on data from two domains: news reports and narrative stories. Our results show that our model beats a strong logistic regression baseline. Direct comparison with exist- ing models is impossible because the only similar dataset used in previous work that we are aware of is not available to us ), but we show that our models are competitive against similar systems reported in the literature.</p><p>The main contributions of this work are:</p><p>• We design and build the first end-to-end neu-ral temporal dependency parser. The parser is based on a novel neural ranking model that takes a raw text as input, extracts events and time expressions, and arranges them in a tem- poral dependency structure.</p><p>• We evaluate the parser by performing exper- iments on data from two domains: news re- ports and narrative stories, and show that our parser is competitive against similar parsers. We also show the two domains have very dif- ferent temporal structural patterns, an obser- vation that we believe will be very valuable to future temporal parser development.</p><p>The rest of the paper is organized as follows. Since temporal structure parsing is a relatively new task, we give a brief problem description in §2. We describe our end-to-end pipeline system in §3. Details of the neural ranking model are dis- cussed in §4. In §5 we present and discuss our ex- perimental results, and error analysis are presented in §6. In §7 we discuss related work and situate our work in the broader context, and we conclude our paper in §8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Description</head><p>In this section we give a brief description of the temporal dependency parsing task (more details in <ref type="bibr" target="#b22">Zhang and Xue (2018)</ref>). In a temporal structure parsing task, a text is parsed into a dependency tree structure that represents the inherent tempo- ral relations among time expressions and events in the text. The nodes in this tree are mostly time ex- pressions and events which are represented as con- tiguous spans of words in the text. They can also be pre-defined meta nodes, which serve as refer- ence times for other time expressions and events, and they constitute the top-most part of the tree. For example, Past Ref, Present Ref, Future Ref, and Document Creation Time (DCT) are all pre- defined meta nodes. The edges in the tree repre- sent temporal relations between each parent-child pair. The temporal relations can be one of In- cludes, Before, Overlap, and After, or Depend- on which holds between two time expressions. Unlike syntactic dependency parsing where each word in a sentence is a node in the dependency structure, in a temporal dependency structure only some of the words in a text are nodes in the struc- ture. Therefore, this process naturally falls into two stages: first time expression and event recog- nition, and then temporal relation parsing. <ref type="figure">Fig-  ure</ref> 1 is an example temporal dependency tree for a news report paragraph. Due to the fact that different types of time ex- pressions and events behave differently in terms of what can be their antecedents, and recognition of these types can be helpful for determining tempo- ral relations, finer classifications of time expres- sions and events are also defined. Time expres- sions are further classified into Vague Time, Abso- lute Concrete Time, and Relative Concrete Time, according to whether or not the time expression can be located on the timeline, and whether or not the interpretation of its temporal location de- pends on another time expression. Events are fur- ther classified into Eventive Event, State, Habitual Event, Completed Event, Ongoing Event, Modal- ized Event, Generic Habitual, and Generic State, according to the eventuality type of the event. Our experiments will show that these fine-grained clas- sifications are very helpful for the overall temporal structure parsing accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Pipeline System</head><p>We build a two-stage pipeline system to tackle this temporal structure parsing problem. The first stage performs event and time expression identification. In this stage, given a text as input, spans of words that indicate events or time expressions are identi- fied and categorized. We model this stage as a se- quence labeling process. A standard Bi-LSTM se- quence model coupled with BIO labels is applied here. Word representations are the concatenation of word and POS tag embeddings.</p><p>The second stage performs the actual tempo- ral structure parsing by identifying the antecedent for each time expression and event, and identify- ing the temporal relation between them. In this stage, given events and time expressions identi- fied in the first stage as input, the model outputs a temporal dependency tree in which each child node is an event or time expression that is tempo- rally related to another event or time expression or pre-defined meta node as its parent node. This stage is modeled as a ranking process: for each node, a finite set of neighboring nodes are first se- lected as its candidate parents. These candidates are then ranked with a neural network model and the highest ranking candidate is selected as its par- ent. We use a ranking model because it is sim- ple, more intuitive and easier to train than a tradi- tional transition-based or graph-based model, and Jorn Utzon, the Danish architect who designed the Sydney Opera House, has died e1 in Copenhagen. Born e2 in 1918 t1 , the learned model rarely makes mistakes that vio- late the structural constraint of a tree. Since the model we use for Stage 1 is a very standard model with little modifications, we don't describe it in detail in this paper due to the limita- tion of space. Our neural ranking model for Stage 2 is described in detail in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Neural Ranking Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Description</head><p>We use a neural ranking model for the parsing stage. For each time expression or event node i in a text, a group of candidate parent nodes (time ex- pressions, events, or pre-defined meta nodes) are selected. In practice, we select a window from the beginning of the text to two sentences after node i, and select all nodes in this window and all pre-defined meta nodes as the candidate parents if node i is an event. Since the parent of a time expression can only be a pre-defined meta node or another time expression as described in <ref type="bibr" target="#b22">Zhang and Xue (2018)</ref>, we select all time expressions in the same window and all pre-defined meta nodes as the candidate parents if node i is a time expres- sion. Let y i be a candidate parent of node i, a score is then computed for each pair of (i, y i ).Through ranking, the candidate with the highest score is then selected as the final parent for node i.</p><p>Model architecture is shown in <ref type="figure" target="#fig_3">Figure 2</ref>. Word embeddings are used as word representations (e.g. w k ). A Bi-LSTM sequence layer is built on each word over the entire text, computing Bi-LSTM output vectors for each word (e.g. w * k ). The node representation for each time expression or event is the summation of the Bi-LSTM output vectors of all words in the text span (e.g. x i ). The pair repre- sentation for node i and one of its candidates y i is the concatenation of the Bi-LSTM output vectors of these two nodes g i,y</p><formula xml:id="formula_0">i = [x i , x y i ]</formula><p>, which is then sent through a Multi-Layer Perceptron to compute a score for this pair s i,y i . Finally all pair scores of the current node i are concatenated into vector c i , and taking sof tmax on it generates the final dis- tribution o i , which is the probability distribution of each candidate being the parent of node i.</p><p>Formally, the Forward Computation is:</p><formula xml:id="formula_1">w * k = BiLST M (w k ) x i = sum(w * k−1 , w * k , w * k+1 ) g i,y i = [x i , x y i ] h i,y i = tanh(W 1 · g i,y i + b 1 ) s i,y i = W 2 · h i,y i + b 2 c i = [s i,1 , ..., s i,i−1 , s i,i+1 , ..., s i,i+t ] o i = sof tmax(c i )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learning</head><p>Let D be the training data set of K texts, N k the number of nodes in text D k , and y i the gold parent for node i. Our neural model is trained to max- imize P (y 1 , ..., y N k |D k ) over the whole training set. More specifically, the cost function is defined as follows:</p><formula xml:id="formula_2">C = −log K k P (y 1 , ..., y N k |D k ) = −log K k N k i P (y i |D k ) = K k N k i −logP (y i |D k )</formula><p>For each training example, cross-entropy loss is minimized:</p><formula xml:id="formula_3">L = −logP (y i |D k ) = −log exp[s i,y i ] y i exp[s i,y i ]</formula><p>where s i,y i is the score for child-candidate pair (i, y i ) as described in §4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Decoding</head><p>During decoding, the parser constructs the tem- poral dependency tree incrementally by identify- ing the parent node for each event or time expres- sion in textual order. To ensure the output parse is a valid dependency tree, two constraints are ap- plied in the decoding process: (i) there can only be one parent for each node, and (ii) descendants of a node cannot be its parent to avoid cycles. Candi- dates violating these constraints are omitted from the ranking process. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Temporal Relation Labeling</head><p>The neural model described above generates an unlabeled temporal dependency tree, with each parent being the most salient reference time for the child. However it doesn't model the specific tem- poral relation (e.g. "before", "overlap") between a parent and a child. We extend this basic archi- tecture to both identify parent-child pairs and pre- dict their temporal relations. In this new model, instead of ranking child-candidate pairs (i, y i ), we rank child-candidate-relation tuples (i, y i , l k ), where l k is the kth relation in the pre-defined set of possible temporal relation labels L. We com- pute this ranking by re-defining the pair score s i,y [k] is the scalar score for y i being the parent of i with temporal re- lation l k . Accordingly, the lengths of c i and o i are number of candidates * |L|. Finally, the tuple (i, y i , l k ) associated with the highest score in o i predicts that y i is the parent for i with temporal relation label l k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Variations of the Basic Neural Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Linguistically Enriched Models</head><p>A variation of the basic neural model is a model that takes a few linguistic features as input ex- 2 An alternative decoding approach would be to perform a global search for a Maximum Spanning Tree. However, due to the nature of temporal structures, our greedy decoding process rarely hits the constraints. plicitly. In this model, we extend the pair rep- resentation g i,y</p><p>i with local features:</p><formula xml:id="formula_4">g i,y i = [x i , x y i , φ i,y i ].</formula><p>Time and event type feature: Stage 1 of the pipeline not only extracts text spans that are time expressions or events, but also labels them with pre-defined categories of different types of time expressions and events. Readers are referred to <ref type="bibr" target="#b22">Zhang and Xue (2018)</ref> for the full category list. Through a careful examination of the data, we no- tice that time expressions or events are selective as to what types of time expression or events can be their parent. In other words, the category of the child time expression or event has a strong indication on which candidate can be its parent. For example, a time expression's parent can only be another time expression or a pre-defined meta node, and can never be an event; and an eventive event's parent is almost certainly another even- tive event, and is highly unlikely to be a stative event. Therefore, we include the time expression and event type information predicted by stage 1 in this model as a feature. More formally, we rep- resent a time/event type as a fixed-length embed- ding t, and concatenate it to the pair representation</p><formula xml:id="formula_5">g i,y i = [x i , x y i , t i , t y i ].</formula><p>Distance features: Distance information can be useful for predicting the parent of a child. In- tuitively, candidates that are closer to the child are more likely to be the actual parent. Through data examination, we also find that a high percentage of nodes have parents in close proximity. Therefore, we include two distance features in this model: the node distance between a candidate and the child nd i,y i , and whether they are in the same sentence ss i,y i . One-hot representations are used for both features to represent according conditions listed in <ref type="table" target="#tab_0">Table 1.</ref> conditions for feature nd i,y The final pair representation for our linguisti- cally enriched model is as follows:</p><note type="other">i : i.node id − y i .node id = 1 i.node id − y i .node id &gt; 1 and i.sent id = y i .sent id i.node id − y i .node id &gt; 1 and i.sent id = y i .sent id i.node id − y i .node id &lt; 1 conditions for feature ss i,y i : i.sent id = y i .sent id i.sent id = y i .sent id</note><formula xml:id="formula_6">g i,y i = [x i , x y i , t i , t y i , nd i,y i , ss i,y i ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Attention Model on Time and Event Representation</head><p>In the basic neural model, a straight-forward sum- pooling is used as the multi-word time expression and event representation. However, multi-word event expressions usually have meaning-bearing head words. For example, in the event "took a trip", "trip" is more representative than "took" and "a". Therefore, we add an attention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>) over the Bi-LSTM out- put vectors in each multi-word expression to learn a task-specific notion of headedness ( <ref type="bibr" target="#b11">Lee et al., 2017)</ref>:</p><formula xml:id="formula_7">α t = tanh(W · w * t ) w i,t = exp[α t ] EN D(i) k=ST ART (i) exp[α k ] ˆ x i = EN D(i) t=ST ART (i) w i,t · w * t wherê</formula><p>x i is a weighted sum of Bi-LSTM output vectors in span i. The weights w i,t are automati- cally learned. The final pair representation for our attention model is as follows:</p><formula xml:id="formula_8">g i,y i = [x i , x y i , t i , t y i , nd i,y i , ss i,y i , ˆ x i , ˆ x y i ]</formula><p>This model variation is also beneficial in an end-to-end system, where time expression and event spans are automatically extracted in Stage 1. When extracted spans are not guaranteed correct time expressions and events, an attention layer on a slightly larger context of an extracted span has a better chance of finding representative head words than a sum-pooling layer strictly on words within a event or time expression span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data</head><p>All of our experiments are conducted on the datasets described in <ref type="bibr" target="#b22">Zhang and Xue (2018)</ref>. This is a temporal dependency structure corpus in Chi- nese. It covers two domains: news reports and narrative fairy tales. It consists of 115 news ar- ticles sampled from Chinese TempEval2 datasets ( <ref type="bibr" target="#b19">Verhagen et al., 2010</ref>) and Chinese Wikipedia News 3 , and 120 fairy tale stories sampled from Grimm Fairy Tales 4 . 20% of this corpus, dis- tributed evenly on both domains, are double an- notated with high inter-annotator agreements. We use this part of the data as our development and test datasets (10% documents for development and 10% for testing), and the remaining 80% as our training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baseline Systems</head><p>We build two baseline systems to compare with our neural model. The first is a simple baseline which links every time expression or event to its immediate previous time expression or event. Ac- cording to our data, if only position information is considered, the most likely parent for a child is its immediate previous time expression or event. This baseline uses the most common temporal relation edge label in the training datasets, i.e. "overlap" for news data, and "before" for grimm data.</p><p>The second baseline is a more competitive base- line for stage 2 in the pipeline. It takes the output of the first stage as input, and uses a similar rank- ing architecture but with logistic regression clas- sifiers instead of neural classifiers. The purpose of this baseline is to compare our neural models against a traditional statistical model under oth- erwise similar settings. We conduct robust fea- ture engineering on this logistic regression model to make sure it is a strong benchmark to compete against. <ref type="table" target="#tab_1">Table 2</ref> lists the features and feature com- binations used in this model. time type and event type features: i.type and y i .type if i.type = absolute time and y i .type = root if i.type = time and y i .type = root are i.type and y i .type time, eventive, or stative are i.type and y i .type root, time, or event are i.type and y i .type root, time, eventive, or stative if i.type = y i .type = event andˆyandˆ andˆy.type = state, for alî y between i and y  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation</head><p>We perform two types of evaluations for our sys- tems. First, we evaluate the stages of the pipeline and the entire pipeline, i.e. end-to-end systems where both time expression and event recognition, as well as temporal dependency structures are au- tomatically predicted. Our models are compared against the two strong baselines described in §5.2. These evaluations are described in §5.3.1.</p><p>The second evaluation focuses only on the tem- poral relation structure parsing part of our pipeline (i.e. Stage 2), using gold standard time expression and event spans and labels. Since most previous work on temporal relation identification use gold standard time expression and event spans, this evaluation gives us some sense of how our models perform against models reported in previous work even though a strict comparison is impossible be- cause different data sets are used. These evalua- tions are described in §5.3.2.</p><p>All neural networks in this work are imple- mented in Python with the DyNet library ( <ref type="bibr">Neubig et al., 2017</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">End-to-End System Evaluation</head><p>Stage 1: Time and Event Recognition For Stage 1 in the pipeline, we perform BIO tagging with the full set of time expression and event types (i.e. a 11-way classification on all extracted spans). Extracted spans will be nodes in the final dependency tree, and time/event types will support features in the next stage. We evaluate Stage 1 performance using 10-fold cross-validation of the entire data set. We use the "exact match" evalua- tion metrics for BIO sequence labeling tasks, and compute precision, recall, and f-score for each la- bel type. We first ignore fine-grained time/event types and only evaluate unlabeled span detection and time/event binary classification to show how well our system identify events and time expressions, and how well our system distinguishes time ex- pressions from events.  formance on both news and narrative domains. Time expressions have a higher recognition rate than events in news data, which is consistent with the observation that time expressions usually have a more limited vocabulary and more strict lexical patterns. On the other hand, due to the scarcity of time expressions in the Grimm data, time expres- sion recognition in this domain has a very high precision but low recall, which results in a much lower f-score than news. Labeled full set evaluation results on time/event type classification are reported in <ref type="table" target="#tab_4">Table 4</ref>. Time expressions have higher recognition rates than events on both domains, and dominant event types ("event", "state", etc.) have higher and more sta- ble recognition rates than other types. Event types with very few training instances, such as "modal- ized event" (&lt;7%), achieve lower and more un- stable recognition rates. Other types with less than 2% instances achieve close to 0 recognition f-scores, and are not reported in this table.</p><p>Stage 2: Temporal Dependency Parsing For Stage 2 in the pipeline, we conduct experiments on the five systems described above: a simple base- line, a logistic regression baseline, a basic neural model, a linguistically enriched neural model, and an attention neural model. All models are trained on automatically predicted spans of time expres- sions and events, and time/event types generated by Stage 1 using 10-fold cross-validation, with gold standard edges (and edge labels) mapped onto the automatic spans. Evaluations in Stage 2 are against gold standard spans and edges, and evaluation metrics are precision, recall, and f-score on child, parent tuples for unlabeled trees, and child, relation, parent triples for la- beled trees.</p><p>Bottom rows in <ref type="table" target="#tab_6">Table 5</ref> report the end-to-end performance of our five systems on both domains. On both labeled and unlabeled parsing, our ba- sic neural model with only lexical input performs comparable to the logistic regression model. And our enriched neural model with only three sim- ple linguistic features outperforms both the logis- tic regression model and the basic neural model on news, improving the performance by more than 10%. However, our models only slightly improve the unlabeled parsing over the simple baseline on narrative Grimm data. This is probably due to (1) it is a very strong baseline to link every node to its immediate previous node, since in an narrative discourse linear temporal sequences are very com- mon; and (2) most events breaking the temporal linearity in a narrative discourse are implicit sta- tive descriptions which are harder to model with only lexical and distance features. Finally, atten- tion mechanism improves temporal relation label- ing on both domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Temporal Relation Evaluation</head><p>To facilitate comparison with previous work where gold events are used as parser input, we re- port our results on temporal dependency parsing with gold time expression and event spans in Ta- ble 5 (top rows). These results are in the same ball- park as what is reported in previous work on tem- poral relation extraction. The best performance in  are 0.84 and 0.65 f- scores for unlabeled and labeled parses, achieved by temporal structure parsers trained and evalu- ated on narrative children's stories. Our best per- forming model (Neural-attention) reports 0.81 and 0.70 f-scores on unlabeled and labeled parses re- spectively, showing similar performance. It is im- portant to note, however, that these two works use different data sets, and are not directly compara- ble. Finally, parsing accuracy with gold time/event spans as input is substantially higher than that with predicted spans, showing the effects of error prop- agation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Error Analysis</head><p>We perform error analysis on the output of our best model (Neural-attention) on the development data sets. We focus on analyzing our neural ranking model (i.e. Stage 2), with gold time expression and event spans and labels as input.</p><p>First, we look at errors by the types of an- tecedents. Most events in both news and grimm data depend on their immediate previous event or time expression as their reference time parent. 71% of the events in the news data set and 78% of the events in the Grimm data have the immediate previous node as their antecedent. The confusion matrix in <ref type="table">Table 6</ref> illustrates how strongly this bias affects our models. Our model learns the bias and incorrectly links around half of the events (47% in news and 46% in grimm) to their immediate pre- vious node when the correct temporal dependency is further back in the text.  <ref type="table">Table 6</ref>: Parent node confusion matrix. Rows are gold parents and columns are automatically parsed parents. "pre" means the parent is the immediate previous node of the child event, "far" means the parent is further back from the child event.</p><p>Second, we look at errors in temporal rela- tion labels. Considering only correctly recognized parent-child pairs, we draw a confusion matrix as in <ref type="table">Table 7</ref>. Our data has very few after relations in both domains, which explains why the model has difficulty identifying this relation. There are also very few include and depend-on relations in the Grimm data, however they are identified with a news be af ov in de total <ref type="table" target="#tab_0">before  1  0  21  2  0  24  after  1  0  1  0  0  2  overlap  1  0 295 0  0  296  include  0  0  4  52  0  56  depend-on  0  0  0  0 117</ref>   <ref type="table">Table 7</ref>: Temporal relation confusion matrix. Rows are gold relation labels and columns are automatic re- lation labels. "be, af, ov, in, de" stand for "before, after, overlap, include, and depend-on".</p><p>relatively high accuracy. This is probably because, according to the temporal dependency structure design ( <ref type="bibr" target="#b22">Zhang and Xue, 2018)</ref>, these relations hold only between restricted pairs of parent and child: include requires a time expression parent and an event child, and depend-on requires that the parent be the rootf. The main confusion among temporal relations is between before and overlap.</p><p>In news data, with a high occurrence of overlap relations (60% overlap and 5% before), most be- fore parents are wrongly recognized as overlap.</p><p>Grimm data has a more balanced distribution of these two temporal relations (46% overlap and 50% before), however, 13% before and 17% over- lap are wrongly labeled as the other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Related Work on Temporal Relation Modeling</head><p>There is a significant amount of research on tem- poral relation extraction ( <ref type="bibr" target="#b3">Bethard et al., 2007;</ref><ref type="bibr" target="#b1">Bethard, 2013;</ref><ref type="bibr" target="#b6">Chambers and Jurafsky, 2008;</ref><ref type="bibr" target="#b5">Chambers et al., 2014;</ref><ref type="bibr" target="#b14">Ning et al., 2018a</ref>). Most of the previous work models temporal relation extraction as pair-wise classification between in- dividual pairs of events and/or time expressions. Some of the models also add a global reasoning step to local pair-wise classification, typically us- ing Integer Linear Programming, to exploit the transitivity property of temporal relations <ref type="bibr" target="#b6">(Chambers and Jurafsky, 2008)</ref>. Such a pair-wise clas-sification approach is often dictated by the way the data is annotated. In most of the widely used temporal data sets, temporal relations be- tween individual pairs of events and/or time ex- pressions are annotated independently of one an- other ( <ref type="bibr" target="#b17">Pustejovsky et al., 2003;</ref><ref type="bibr" target="#b5">Chambers et al., 2014;</ref><ref type="bibr" target="#b18">Styler IV et al., 2014;</ref><ref type="bibr" target="#b16">O'Gorman et al., 2016;</ref><ref type="bibr" target="#b12">Mostafazadeh et al., 2016)</ref>. Our work is most closely related to that of , which also treats tem- poral relation modeling as temporal dependency structure parsing. However, their dependency structure, as described in , is only over events, excluding time expressions which are an important source of temporal infor- mation, and it also excludes states (stative events), which makes the temporal dependency structure incomplete. Moreover, their corpus only consists of data in the narrative stories domain. We instead choose to develop our model based on the data set described in <ref type="bibr" target="#b22">Zhang and Xue (2018)</ref>, which in- troduces a more comprehensive and linguistically grounded annotation scheme for temporal depen- dency structures. This structure includes both events and time expressions, and uses the linguis- tic notion of temporal anaphora to guide the anno- tation of the temporal dependency structure. Since in this temporal dependency structure each parent- child pair is considered to be an instance of tem- poral anaphora, the parent is also called the an- tecedent and the child is also referred to as the anaphor. Their corpus consists of data from two domains: news reports and narrative stories.</p><p>More recently, <ref type="bibr" target="#b15">Ning et al. (2018b)</ref> proposed a semi-structured approach to model temporal rela- tions in a text. Based on the observation that not all pairs of events have well-defined temporal re- lations, they propose a multi-axis representation in which well-defined temporal relations only hold between events on the same axis. The temporal relations between events in a text form multiple disconnected subgraphs. Like other work before them, their annotation scheme only covers events, to the exclusion of time expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Related Work on Neural Dependency Parsing</head><p>Most prior work on neural dependency parsing is aimed at syntactic dependency parsing, i.e. pars- ing a sentence into a dependency tree that rep- resents the syntactic relations among the words.</p><p>Recent work on dependency parsing typically uses transition-based or graph-based architectures combined with contextual vector representations learned with recurrent neural networks (e.g. Bi- LSTMs) <ref type="bibr" target="#b9">(Kiperwasser and Goldberg, 2016)</ref>. Temporal dependency parsing is, however, dif- ferent from syntactic dependency parsing. In tem- poral dependency parsing, for each event or time expression, there is more than one other event or time expression that can serve as its reference time, while the most closely related one is se- lected as the gold standard reference time parent. This naturally falls into a ranking process where all possible reference times are ranked and the best is selected. In this sense our neural ranking model for temporal dependency parsing is closely related to the neural ranking model for corefer- ence resolution described in <ref type="bibr" target="#b11">Lee et al. (2017)</ref>, both of which extract related spans of words (entity mentions for coreference resolution, and events or time expressions for temporal dependency pars- ing). However, our temporal dependency parsing model differs from Lee et al's coreference model in that the ranking model for coreference only needs to output the best candidate for each indi- vidual pairing and cluster all pairs that are coref- erent to each other. In contrast, our ranking model for temporal dependency parsing needs to rank not only the candidate antecedents but also the temporal relations between the antecedent and the anaphor. In addition, the model also adds connec- tivity and acyclic constraints in the decoding pro- cess to guarantee a tree-structured output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Future Work</head><p>In this paper, we present the first end-to-end neu- ral temporal dependency parser. We evaluate the parser with both gold standard and automatically recognized time expressions and events. In both experimental settings, the parser outperforms two strong baselines and shows competitive results against prior temporal systems.</p><p>Our experimental results show that the model performance drops significantly when automati- cally predicted event and time expressions are used as input instead of gold standard ones, indi- cating an error propagation problem. Therefore, in future work we plan to develop joint models that simultaneously extract events and time ex- pressions, and parse their temporal dependency structure.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 1: Example text and its temporal dependency tree. DCT is Document Creation Time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>i</head><label></label><figDesc>. Here, pair score s i,y i is no longer a scalar score but a vector s i,y i of size |L|, where s i,y i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Neural Ranking Model Architecture. x i is the current child node, and x a , x b , x c , x d are the candidate parent nodes for x i. Arrows from Bi-LSTM layer to x a , x b , x c , x d are not shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>type = state and i.sent id = y i .sent id if i.type = state and i.node id − y i .node id = 1 if i.type = y i .type = event and i.node id − y i .node id = 1 if i.type = state and y i .type = event and i.node id − y i .node id = 1 and i.node id in sent = 1 and i.sent id = 1 other features: if i and y i are in quotation marks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>news grimm pre far total pre far total pre 317 11 328 750 60 810 far 65 72 137 104 122 226 total 382 83 465 854 182 1036</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Conditions for node distance and same sen-
tence features. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Features in the logistic regression system.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>). The code is publicly available 5 . For Stage 1, all models are trained with Adam optimizer with early stopping and learning rate 0.001. The dimensions of word embeddings, POS tag embeddings, Bi-LSTM output vectors, and MLP hidden layers are tuned on the dev set to 256,</figDesc><table>5 https://github.com/yuchenz/tdp_ 
ranking 

evaluated 
news 
grimm 
label 
p 
r 
f 
p 
r 
f 
all span 
.81 .74 .78 .83 .74 .78 
time 
.83 .81 .82 .97 .62 .76 
event 
.81 .73 .77 .83 .74 .78 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Stage 1 cross-validation on span detection 
and binary time/event recognition. 

time/event type news grimm 
vague time 
.77 
.82 
concrete absolute 
.67 
-
concrete relative 
.75 
-
event 
.61 
.77 
state 
.65 
.61 
completed 
.62 
.26 
modalized 
.46 
.31 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Stage 1 (time/event type recognition) cross-
validation f1-scores on the full set. 

32, 256, and 256 respectively. POS tags in Stage 1 
are acquired using the joint POS tagger from Wang 
and Xue (2014). The tagger is trained on Chinese 
Treebank 7.0 (Xue et al., 2010). For Stage 2, the 
dimensions of word embeddings, time/event type 
embeddings, Bi-LSTM output vectors, and MLP 
hidden layers are tuned on the dev set to 32, 16, 
32, and 32 respectively. The optimizer is Adam 
with early stopping and learning rate 0.001. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 shows</head><label>3</label><figDesc>the cross- validation results on these two evaluations. Span detection and event recognition show similar per-model news grimm unlabeled f labeled f unlabeled f labeled f dev test dev test dev test dev test</figDesc><table>temporal relation 
parsing with 
gold spans 

Baseline-simple .64 
.68 
.47 .43 .78 
.79 
.39 .39 
Baseline-logistic .81 
.79 
.63 .54 .74 
.74 
.60 .63 
Neural-basic 
.78 
.75 
.67 .57 .72 
.74 
.60 .63 
Neural-enriched .80 
.78 
.67 .59 .76 
.77 
.63 .65 
Neural-attention .83 
.81 
.76 .70 .79 
.79 
.66 .68 

end-to-end 
systems with 
automatic spans 

Baseline-simple .39 
.40 
.26 .25 .44 
.47 
.27 .25 
Baseline-logistic .36 
.34 
.24 .22 .43 
.49 
.33 .37 
Neural-basic 
.37 
.36 
.21 .23 .42 
.45 
.33 .35 
Neural-enriched .51 
.52 
.32 .35 .44 
.49 
.33 .37 
Neural-attention .54 
.54 
.36 .39 .44 
.49 
.35 .39 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Stage 2 results (f-scores) with gold spans and timex/event labels (top), and automatic spans and 
timex/event labels generated by stage 1 (bottom). Best performances are in bold. 

</table></figure>

			<note place="foot" n="1"> https://github.com/yuchenz/ structured_temporal_relations_corpus</note>

			<note place="foot" n="3"> https://zh.wikinews.org 4 https://www.grimmstories.com/zh/ grimm_tonghua/index</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cleartk-timeml: A minimalist approach to tempeval 2013</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Joint Conference on Lexical and Computational Semantics (* SEM)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="10" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Annotating story timelines as temporal dependency structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Kolomiyets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariefrancine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth international conference on language resources and evaluation (LREC)</title>
		<meeting>the eighth international conference on language resources and evaluation (LREC)</meeting>
		<imprint>
			<publisher>ELRA</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2721" to="2726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Timelines from text: Identification of syntactic temporal relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klingenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSC 2007. International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
	<note>Semantic Computing</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semeval-2017 task 12: Clinical tempeval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guergana</forename><surname>Savova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Pustejovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</title>
		<meeting>the 11th International Workshop on Semantic Evaluation (SemEval-2017)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="565" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dense event ordering with a multi-pass architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Mcdowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="273" to="284" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Jointly combining implicit constraints improves temporal ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Classifying temporal relations between events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-2007</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural temporal relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Dligach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guergana</forename><surname>Savova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04351</idno>
		<title level="m">Simple and accurate dependency parsing using bidirectional lstm feature representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extracting narrative timelines as temporal dependency structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Kolomiyets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariefrancine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="88" to="97" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07045</idno>
		<title level="m">End-to-end neural coreference resolution</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Caters: Causal and temporal relation scheme for semantic annotation of event structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alyson</forename><surname>Grealish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the The 4th Workshop on EVENTS: Definition, Detection, Coreference, and Representation</title>
		<meeting>the The 4th Workshop on EVENTS: Definition, Detection, Coreference, and Representation<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03980</idno>
		<title level="m">Swabha Swayamdipta, and Pengcheng Yin. 2017. Dynet: The dynamic neural network toolkit</title>
		<meeting><address><addrLine>Yusuke Oda, Matthew Richardson, Naomi Saphra</addrLine></address></meeting>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Improving temporal relation extraction with a globally acquired statistical resource</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoruo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06020</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A multiaxis annotation scheme for event temporal relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07828</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Richer event description: Integrating event coreference with temporal, causal and bridging annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Tim O&amp;apos;gorman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Wright-Bettner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing News Storylines</title>
		<imprint>
			<biblScope unit="page">47</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The timebank corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Pustejovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roser</forename><surname>Sauri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Setzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beth</forename><surname>Sundheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Ferro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Corpus linguistics</title>
		<meeting><address><addrLine>Lancaster, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">40</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Temporal annotation in the clinical domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>William F Styler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Finan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piet</forename><forename type="middle">C</forename><surname>De Groen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guergana</forename><surname>Savova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="143" to="154" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 13: Tempeval-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Verhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roser</forename><surname>Sauri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Pustejovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th international workshop on semantic evaluation</title>
		<meeting>the 5th international workshop on semantic evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="57" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint pos tagging and transition-based constituent parsing in chinese with non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Chinese treebank 7.0. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuhong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiyu</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Structured interpretation of temporal relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 11th edition of the Language Resources and Evaluation Conference (LREC-2018)</title>
		<meeting>11th edition of the Language Resources and Evaluation Conference (LREC-2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
