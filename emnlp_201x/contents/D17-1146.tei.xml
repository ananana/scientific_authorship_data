<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Memory-augmented Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
							<email>fengyang@ict.ac.cn, {byryuer, andizhang912}@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Speech and Language Technologies</orgName>
								<orgName type="department" key="dep2">RIIT</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Huilan Corporation</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyue</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Speech and Language Technologies</orgName>
								<orgName type="department" key="dep2">RIIT</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andi</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
							<email>wangdong99@mails.tsinghua.edu.cn, andrew.abel@xjtlu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Speech and Language Technologies</orgName>
								<orgName type="department" key="dep2">RIIT</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Abel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Speech and Language Technologies</orgName>
								<orgName type="department" key="dep2">RIIT</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Xi&apos;An Jiaotong Liverpool-University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Memory-augmented Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1390" to="1399"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural machine translation (NMT) has achieved notable success in recent times, however it is also widely recognized that this approach has limitations with handling infrequent words and word pairs. This paper presents a novel memory-augmented NMT (M-NMT) architecture, which stores knowledge about how words (usually infrequently encountered ones) should be translated in a memory and then utilizes them to assist the neural model. We use this memory mechanism to combine the knowledge learned from a conventional statistical machine translation system and the rules learned by an NMT system, and also propose a solution for out-of-vocabulary (OOV) words based on this framework. Our experiments on two Chinese-English translation tasks demonstrated that the M-NMT architecture out-performed the NMT baseline by 9.0 and 2.7 BLEU points on the two tasks, respectively. Additionally, we found this architecture resulted in a much more effective OOV treatment compared to competitive methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural Machine Translation (NMT) has been shown to have highly promising performance, par- ticularly when a large amount of training data is available ( <ref type="bibr" target="#b9">Johnson et al., 2016;</ref><ref type="bibr" target="#b14">Mi et al., 2016)</ref>. Although there are different model architectures ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>, the common principle behind the NMT approach is the same: encoding the meaning of the input into a concept space and performing translation based on this encoding. This 'meaning src. ref.</p><p>Humans have 23 pairs of chromosomes. NMT There are 23-year history of human history. <ref type="table">Table 1</ref>: An example of Chinese-to-English 'meaning drift' with NMT.</p><p>encoding' principle leads to a deeper understand- ing and learning of the translation rules, and hence a better translation than conventional statistic ma- chine translation (SMT) that considers only sur- face forms, i.e., words and phrases ( <ref type="bibr" target="#b11">Koehn et al., 2003)</ref>.</p><p>Despite positive results obtained so far, a par- ticular problem of the NMT approach is that it has a tendency towards overfitting to frequent ob- servations (words, word co-occurrences, transla- tion pairs, etc.), but overlooking special cases that are not frequently observed. For example, NMT is good at learning translation pairs that are fre- quently observed, and can make use of them well at run-time, but for low-frequency pairs in the training data, the system may 'forget' to use them when they should be. Unfortunately, rare words are inevitable for all translation tasks due to Zipf's law, and indeed they are often the most impor- tant parts of a sentence, e.g., domain-specific en- tity names. <ref type="table">Table 1</ref> shows an example, where the word ' ( chromosomes)' is an infre- quent word. As the system does not know (or has effectively 'forgotten') this keyword, it does not translate correctly, and an irrelevant transla- tion is produced, leading to the phenomenon of 'meaning drift'. This weakness with regard to infrequent words/pairs with NMT has been no- ticed by a number of researchers, and some stud- ies have been conducted to address this problem, e.g., <ref type="bibr" target="#b13">Luong et al. (2014)</ref>; <ref type="bibr" target="#b3">Cho et al. (2014)</ref>; ; <ref type="bibr" target="#b0">Arthur et al. (2016)</ref>; <ref type="bibr" target="#b2">Bentivogli et al. (2016)</ref>; <ref type="bibr" target="#b25">Zhang et al. (2017)</ref>.</p><p>Superficially, this problem appears to be caused by the imperfect embeddings of infrequent words or the limited vocabulary size of NMT systems, but we argue that the deeper reason should be at- tributed to the nature of neural models: the trans- lation function, represented by various neural net- works, is shared amongst all of the translation pairs, so high-frequency and low-frequency pairs impact each other by adapting their shared pa- rameters. Due to the overwhelming proportion of high-frequency pairs in the training data, the resulting trained model will naturally be much more focused on these frequently observed pairs. More seriously, because the translation function is smooth, infrequent pairs tend to be wrongly seen as noise in the training process and so are largely ignored by the model. In contrast to this, the conventional SMT ap- proach is based on statistics of words and/or phrases, which, in principle, is a symbolic method that uses a discrete model and involves little pa- rameter sharing. The discrete model means that no matter how infrequently a pair occurs, its probabil- ity cannot be smoothed out, and the lack of shared parameters means that the frequent words or pairs have much less impact on infrequent words or pairs. Essentially, SMT memorizes as many of the observed patterns as possible, usually using a phrase table.</p><p>The respective advantages of SMT and NMT suggest that neither the pure neural approach nor the pure symbolic approach can provide a com- plete solution for machine translation, and a com- bined system that exploits the advantages of both approaches would be ideal. This idea has been adopted in early research into neural-based MT methods, where neural models were utilized to improve SMT performance ( <ref type="bibr" target="#b24">Zhang et al., 2015</ref>). However, this seems to be counterintuitive, as in- tuitively learning general rules should be the first step, rather than first memorizing special cases and then learning general rules. This suggests that the combined system should be primarily based on the neural architecture, with symbolic knowledge as a complementary support.</p><p>This paper presents such a neural-symbolic ar- chitecture, which involves a neural model compo- nent to deal with frequently seen patterns, and a memory component to provide knowledge for in- frequently used words and pairs. More specifi- cally, each memory element stores a source-target pair, specifying that a word defined by the source part should be translated to the word defined by the target part. This knowledge is then used to improve the neural model. This is analogy to an experienced translator, who can work well in most cases using their own knowledge (i.e. the neural model aspect), but for unfamiliar and uncommon words that they have little experience of, they will still need to refer to a dictionary (i.e., the mem- ory). This proposed memory-augmented NMT, or M-NMT, is therefore arguably much more similar to human translators than either NMT or SMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Attention-based NMT</head><p>Before introducing our M-NMT architecture, we will give a brief review of our implementation of the attention-based RNN model first presented by <ref type="bibr" target="#b1">Bahdanau et al. (2015)</ref>. This model is regarded as the state-of-the-art model and will be used as the baseline system in this study. Additionally, the neural model component of the M-NMT architec- ture uses the same attention-based RNN model, as being presented in the following.</p><p>The attention-based RNN model is based on an encoder-decoder frame, where the input word se- quence [x 1 , x 2 , ...] in the source language is em- bedded as a sequence of hidden states [h 1 , h 2 , ...] by a bi-directional RNN with GRU as the hidden units, and another RNN is used to produce the tar- get sentence [y 1 , y 2 , ...]. To force the generation to focus on a particular segment of the input at each generation step, <ref type="bibr" target="#b1">Bahdanau et al. (2015)</ref> proposed an attention mechanism. Specifically, when gen- erating the i-th target word, the attention factor of the j-th source word (and its neighbors, precisely) is measured by the relevance between the current hidden state of the decoder, denoted by s t−1 , and the hidden state of the encoder at the j-th word h j . This can be calculated by any similarity func- tion, but a multiple layer perceptron (MLP) is of- ten used, given by:</p><formula xml:id="formula_0">α ij = e ij e ik ; e ij = a(s i−1 , h j )</formula><p>where a(·, ·) is the MLP-based relevance function, and α ij is the attention factor of x j at decoding step i. The semantic content that the decoder fo- cuses on, i.e. attended content, is then derived by:</p><formula xml:id="formula_1">c i = α ij h j .</formula><p>The decoder updates the hidden state with a re- current function f d , formulated by: </p><formula xml:id="formula_2">s i = f d (y i−1 , s i−1 , c i ),<label>(1)</label></formula><p>and the next word y i is generated according to the following posterior:</p><formula xml:id="formula_3">p(y i ) = σ(y T i W z i )<label>(2)</label></formula><p>where σ(·) is the softmax function, W is a param- eter matrix for word vector projection. The inter- mediate variable z i is computed by a neural net with a single maxout hidden layer g, given by:</p><formula xml:id="formula_4">z i = g(y i−1 , s i−1 , c i ).</formula><p>We used Tensorflow to implement this model, and the training recipe largely followed the seminal pa- per of <ref type="bibr" target="#b1">Bahdanau et al. (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Memory-Augmented NMT</head><p>This section presents the M-NMT architecture.</p><p>We first introduce the model, and then describe how the memory is constructed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Architecture</head><p>The M-NMT architecture is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. It involves two components: the model and the memory components. The model component is a typical attention-based RNN model as presented in Section 2, which is regarded as being good at dealing with frequent words and pairs, and the memory component provides knowledge for infre- quent words and pairs that are not easy for the neu- ral model component to learn. The outputs of the two components are combined to produce a final consolidated translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Memory Elements</head><p>We define each item of memory as a mapping from a word in the source language to its translation in the target language. If there are multiple transla- tions for a word, then several of the best will be added to the memory according to the probability of the translation, until the maximum number of target words is reached. A memory element can be formally written by:</p><formula xml:id="formula_5">u jl = y jl x j</formula><p>where y jl is the l-th translation of word x j . This mapping will be saved as a memory element and will be used during translation. We refer to this memory as the global memory, which is static dur- ing all the running time. The global memory is shown on the bottom-right of <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>To translate an input sentence, the memory el- ements the source words of which are in the in- put sentence are selected to form a local memory. This is shown in the right-middle of <ref type="figure" target="#fig_0">Figure 1</ref>. In order to include the context information in the lo- cal memory, the source part x j is replaced by its annotation h j :</p><formula xml:id="formula_6">u jl = y jl h j</formula><p>A consequence of the source encoding is that if a source word occurs multiple times in the sen- tence, all the occurrences should be put into the local memory, with different h j to distinguish the context of each. Finally, the local memory is com- pressed as follows. For each distinct target word˜y word˜ word˜y k in the local memory, all the elements with˜ywith˜ with˜y k as the target are merged into a single element u k , for which the source part is the average of the source part of all the elements to be merged, given by:</p><formula xml:id="formula_7">u k = ˜ y k ˜ h k = ˜ y k j p(x j |˜y|˜y k )h j ; ∀˜y∀˜y k ∈ {y jl } (3)</formula><p>where p(x j |˜y|˜y k ) means the probability that x j is translated intõ y k and can be obtained from either a human-defined dictionary or the dictionary of an SMT system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Memory Attention</head><p>In order to use the information stored in the mem- ory to improve NMT, we need to pick up appropri- ate elements from the local memory at each trans- lation step. A similar attention mechanism as in the neural model is designed. Denote the attention factor of each memory element u k at each transla- tion step i by α m ik , and assume it is derived from a relevance function e m ik :</p><formula xml:id="formula_8">α m ik = e m ik K k=1 e m ik ,</formula><p>where K is the number of target words in the merged memory. The relevance function can be changed, but in this study, we use a simple design:</p><formula xml:id="formula_9">e m ik = (v m ) tanh(W m s s i−1 +W m u u k +W m y y i−1 )<label>(4)</label></formula><p>where tanh(·) is the hyperbolic function, s i−1 is the current state of the decoder of the neural model, and y i−1 is the generated word in the previ- ous step. The parameters of the memory attention mechanism include θ m = {v m , W m s , W m u , W m y }, as defined in Eq. 4.</p><p>The attention factor α m ik can be used in different ways, here they are simply treated as the posterior to predict the next word to generate. Since the nor- malization is over all the target words in the local memory rather than the full vocabulary, treating α m ik as the posterior of all words is only an approx- imation, but was found in our experiments to be a good solution. This memory-based posterior is combined with the posterior of the neural model, resulting in a consolidated posterior, given by:</p><formula xml:id="formula_10">˜ p(y i ) = βα m ik + (1 − β)p(y i )</formula><p>where p(y i ) is the posterior produced by the neu- ral model, as shown in Eq. 2, and β is a pre- defined interpolation factor. Here α m ik corresponds to the attention to the same word in the merged memory as the predicted word y i . This sim- ple posterior combination indicates the flexibility of the M-NMT architecture. Existing knowledge can be compiled into the local memory to im- prove model-based prediction, or if no knowledge is available, the system will rely on conventional NMT.</p><p>An advantage of this simple combination is that the memory component can be trained indepen- dently of the neural model. We set the objective of the training is to let the memory attention as accurate as possible. Given the n-th training se- quence, at each translation step i, the target at- tention should be 1 on the current word y n i and 0 elsewhere. The objective function therefore can be written as the cross entropy between the target attention and the output of the attention function, given as follows:</p><formula xml:id="formula_11">L(θ) = n i log(α m ik n i )</formula><p>where k n i is the position of y n i in the merged memory. The optimization is conducted with re- spect to the parameters θ m . The optimization al- gorithm is the stochastic gradient descent (SGD) with AdaDelta to adjust the learning rate <ref type="bibr" target="#b23">(Zeiler, 2012)</ref>.</p><p>It should be noted that joint training of the mem- ory and the model is possible, but it requires a large amount of GPU memory and risks over- fitting. Therefore, we only train the memory, with the model component unchanged. Efficient model-memory joint training is beyond the scope of this paper and can be investigated in future work. Particularly, the parameter β could be opti- mized to balance the contribution from the model part and the memory part, but constrains have to be carefully settled to avoid overfitting to the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Memory for SMT Integration</head><p>The M-NMT architecture is a flexible framework that provides extra knowledge to the conventional model-based NMT. If the knowledge is generated by a conventional SMT system, it is essentially an elegant combination of SMT and NMT. In this work, we use the translation dictionary produced by an SMT system as the knowledge to create the memory, which involves first aligning the train- ing sentence pairs using the GIZA++ toolkit <ref type="bibr" target="#b15">(Och and Ney, 2003</ref>) in both directions, and applying the "intersection" refinement rules ( <ref type="bibr" target="#b11">Koehn et al., 2003)</ref> to get a single one-to-one alignment for each sentence pair, and then extracting the trans- lation dictionary based on these alignments. We can see the dictionary as the phrase pairs of the length 1 and leave the phrase pairs longer than 1 as future work.</p><p>The key information provided by the dictionary is the conditional probability that a source and a target word are translated to each other. This in- formation is used twice during local memory con- struction. Firstly, the conditional p(y jl |x j ) is used to select the most possible target words y jl to par- ticipate the local memory, and secondly, the con- ditional p(x j |˜y|˜y k ) is used to merge the elements whose target words are˜yare˜ are˜y k , as shown in Eq. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Memory for OOV Treatment</head><p>The memory also provides a flexible way to ad- dress OOV words. OOV words can be defined in multiple ways, but here we focus on true OOVs that are totally new in both bilingual and monolin- gual data (i.e. rare words that are not present in any training data). One example is when a model is migrated to a specific domain. To address these OOVs, we firstly need a manually defined dictio- nary to specify how an OOV word should be trans- lated, where the target word could be either an in- vocabulary word or an OOV. This dictionary will be used as the knowledge to construct the local memory at run-time. Specif- ically, if an OOV word is encountered on either the source or target side during local memory con- struction, the vector of a similar word is borrowed to represent the OOV word. Since the words are totally new, the similar word has to be de- fined manually. To avoid any confusion with other words, the selected similar word should not ap- pear in the existing input sequence if the OOV is in the source side, and should not match any target words in the existing local memory. To achieve this, several candidates have to be pre-defined for each OOV, so that alternative choices are available at run-time. A problem of this approach is that the vocabulary of the neural model is fixed, so can- not output probabilities for OOV words. To solve this, we let the selected similar word entirely over- written by the OOV word, and any prediction for the similar word will be 're-directed' to the OOV word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>The idea of memory augmentation was inspired by recent advances in the neural Turing ma- chine ( <ref type="bibr" target="#b5">Graves et al., 2014</ref><ref type="bibr" target="#b6">Graves et al., , 2016</ref> and memory net- work ( <ref type="bibr">Weston et al., 2014</ref>). These new models equip neural networks with an external memory that can be accessed and manipulated via some trainable operations. The memory idea has been utilized in NMT. For example,  used a memory to extend the state of the decoder RNN in the attention-based NMT. In this case, the contribution of the memory is to provide tempo- rary variables to assist RNN decoding. In contrast, our work uses memory to store knowledge. The memory in Wang et al.'s work could be considered to be note paper, while the memory in our work is more like a dictionary.</p><p>The idea of combining SMT and NMT was adopted by early NMT research, but these com- binations were mostly based on the SMT frame- work, as discussed in depth in the review paper from <ref type="bibr" target="#b24">Zhang et al. (2015)</ref>. <ref type="bibr" target="#b4">Cohn et al. (2016)</ref> pro- posed to enhance the attention-based NMT by us- ing some structural knowledge from a word-based alignment model. The focus of their work was to use the extra knowledge to produce a better atten- tion. In contrast, our work promotes the target words directly using the word mapping stored in the memory. <ref type="bibr" target="#b0">Arthur et al. (2016)</ref> proposed to in- volve lexical knowledge to assist with translation, particularly for low-frequency words. This is sim- ilar to our proposed idea, with the key difference that their work uses the attention information to select the target words, while ours trains a sepa- rate attention, based on both the source and target words.</p><p>Regarding handling OOV words, <ref type="bibr" target="#b8">Jean et al. (2015)</ref> presented an efficient training method to support a larger vocabulary, which helps alleviate the OOV problem significantly. <ref type="bibr" target="#b18">Stahlberg et al. (2016)</ref> used SMT to produce candidate results in the form of lattice and NMT to re-score the results. As SMT uses a larger vocabulary than NMT, some OOV words can be retained. <ref type="bibr" target="#b17">Sennrich et al. (2016)</ref> proposed a subword approach, where OOV words are expected to be spelled out by subword units. <ref type="bibr" target="#b13">Luong et al. (2014)</ref> proposed a post-processing ap- proach that learns the position of the source word when an UNK symbol is produced during decod- ing. By this position information, the UNK sym- bol (unknown words) can be replaced by the cor- rect translation using a lexical table.  proposed a replace-and-restore approach that replaces infrequent words with similar words before the training and decoding, and restores rare words and their target words, obtained from a lex- ical table. Compared to the work of ( <ref type="bibr" target="#b13">Luong et al., 2014</ref>) and ( , which relies on post- processing, our M-NMT approach is more like pre-processing. This means that the required in- formation for OOV words is prepared before de- coding. This seems more flexible than the post- processing methods, as we can easily deal with multiple targets for OOVs, by letting the decoder select which target is the most appropriate. Never- theless, we do share the same idea of using similar words as in ( , which we think is inevitable if the OOV words are totally new.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data</head><p>The experiments were conducted for Chinese- English translation using two datasets, the rela- tively small IWSLT dataset, and the much larger NIST dataset. As we will see, the NMT and SMT approaches exhibit different behaviours on these two datasets, and the memory-augmentation ap- proach offers different contributions to them.</p><p>The IWSLT corpus The training data consists of 44K sentences from the tourism and travel do- main. The development set was composed of the ASR devset 1 and devset 2 from IWSLT 2005, and testing used the IWSLT 2005 test set.</p><p>The NIST corpus The training data consists of 1M sentence pairs with 19M source tokens and 24M target tokens from the LDC corpora of LDC2002E18, LDC2003E07, LDC2003E14, and Hansard's portion of LDC2004T07, LDC2004T08 and LDC2005T06. We use the NIST 2002 test set as the development set and the NIST 2003 test set as the test set.</p><p>Memory data To construct the memory, we used the GIZA++ toolkit ( <ref type="bibr" target="#b11">Koehn et al., 2003)</ref> to align the training data in both directions, and kept the word pairs that appeared in the phrase tables of both directions. The global memory size is 80K for the IWSLT task, and 500K for the NIST task. These word pairs were then filtered according to the conditional probability p(w t |w s ) where w s and w t are source and target language words, respec- tively. For each w s , at most two candidates of w t were retained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Systems</head><p>We used a conventional SMT system and an attention-based RNN NMT system as the base- lines, and investigated a variety of M-NMT archi- tectures.</p><p>SMT baseline: For the SMT system (denoted by Moses), Moses ( <ref type="bibr" target="#b10">Koehn et al., 2007)</ref>, a state-of- the-art open-source toolkit, was used. The default configuration was used where the phrase length was 7 and the following features were employed: relative translation frequencies and lexical trans- lation probabilities on both directions, distortion distance, language model and word penalty. For the language model, the KenLM toolkit <ref type="bibr" target="#b7">(Heafield, 2011</ref>) was used to build a 5-gram language model (with the Keneser-Ney smoothing) on the target side of the training data.</p><p>NMT baseline: For NMT, we reproduced the attention-based RNN model proposed by <ref type="bibr" target="#b1">Bahdanau et al. (2015)</ref>, which is denoted by NMT. The implementation was based on Tensorflow <ref type="bibr">1</ref> . We compared our implementations with a public im- plementation using Theano 2 , and achieved com- parable (even slightly better) performances on the same data sets with the same parameter settings. M-NMT system: The M-NMT system was imple- mented by combining the memory structure and the NMT system. The model part is the same as the NMT baseline, while the attention function of the memory part was trained. During the training, if the target word is an UNK symbol, or the target word is not in the memory (due to the limited word pairs in the memory), this word is simply skipped from back-propagation. This skipping is impor- tant as it avoids bias caused by the large amount of UNK symbols. The trained M-NMT system can be readily used to deal with OOV words, without any re-training.</p><formula xml:id="formula_12">System Attending Attended M-NMT(s, u y ) s i−1 u k (y) M-NMT(s, u xy ) s i−1 u k (x), u k (y) M-NMT(sy, u y ) s i−1 , y i−1 u k (y) M-NMT(sy, u xy ) s i−1 , y i−1 u k (x), u k (y)</formula><p>For M-NMT, the complete form of the relevance function for memory attention is e m ik (s i−1 , y i−1 , u k ). Of these, s i−1 and y i−1 are 'attending factors' that represent the information used 'to attend', while u k , which consists of a source part u k (x) and a target part u k (y), involves 'attended factors' that represent the content 'to be attended'. To investigate the contribution of dif- ferent attending and attended factors, these factors are combined in different ways, leading to differ- ent M-NMT variants, as shown in 2. Note that M- NMT(s; u y ) is the simplest configuration and the attention essentially learns a target-side language model. M-NMT(s; u xy ) involves the source part of the memory, which implicitly learns a bilingual language model. Involving the decoding history y i−1 makes this learning more explicit.</p><p>Settings For a fair comparison, the models con- figurations in the NMT system and the M-NMT system were intentionally set to be identical. The number of hidden units, the word embedding di- mensionality and the vocabulary size were empiri- cally set to 500, 310 and 30000, respectively. In the training process, the batch size of the SGD algorithm was set to 80, and the parameters for AdaDelta were set to be ρ = 0.95 and = 10 −6 . The decoding is implemented as a beam search, where the beam size was set to be 5.</p><p>Evaluation metrics The translation performance was evaluated using the BLEU score with case- insensitive n ≤ 4-grams ( <ref type="bibr" target="#b16">Papineni et al., 2002</ref> 49.8 32.3 M-NMT(sy, u y ) 50.7 32.5 M-NMT(s, u xy ) 51.4 32.8 M-NMT(sy, u xy ) 52.9 34.0 <ref type="table">Table 3</ref>: BLEU scores with different translation systems on the two Chinese-English translation datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">SMT-NMT Integration Experiments</head><p>In the first experiment, the M-NMT architecture combined SMT and NMT by using SMT to con- struct the memory to assist with NMT. For com- parison purposes, the lexical prediction approach proposed by <ref type="bibr" target="#b0">(Arthur et al., 2016</ref>) was also imple- mented. This uses the phrase table produced by SMT to improve NMT. Our implementation is a linear combination, and for a fair comparison, the neural model part was kept unchanged. At each step i, the auxiliary probability provided by the lexical part is P (y i ) = j α ij P (y i |x j ), where α ij is the attention weight from the neural model, and P (y i |x j ) is obtained from the phrase table. This can be regarded as a simple memory approach, with memory attention borrowed from the neural model, rather than being learned separately. <ref type="table">Table 3</ref> shows the BLEU results with different systems. Firstly, it can be observed that with the small IWSLT05 dataset, the SMT outperforms the baseline NMT, but with the large NIST dataset, NMT outperforms SMT. This is unsurprising as neural models often need more training data. Sec- ondly, the results show that with both datasets, the lexical approach (NMT-L) can improve NMT performance, showing that using SMT knowledge helps NMT. However, the improvement seems less significant than reported in ( <ref type="bibr" target="#b0">Arthur et al., 2016)</ref>. This is likely to be because our implementation focuses on creating a simple, extensible, and gen- eralizable system, and therefore does not allow re- training the neural model.</p><p>The M-NMT system provides significant per- formance improvement, even with the simplest setting (M-NMT(s, u y )). More information fac- tors tend to offer better performance, and the best M-NMT system, M-NMT(sy, u xy ), outperforms the baseline NMT by 9.0 and 2.7 BLEU points  on the two datasets respectively. Notably, the im- provement with the IWSLT05 dataset is impres- sive, the best M-NMT system outperforms even the very strong SMT baseline, which strongly sup- ports our conjecture that NMT must be equipped with a symbolic structure to deal with infrequent words. It also suggests that the M-NMT architec- ture is a promising way to apply neural methods to low-resource tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">OOV Treatment Experiments</head><p>Here the M-NMT architecture was used to han- dle OOV words. The experiments were conducted on the NIST dataset, for which we collected 312 test sentences containing OOV words. This test set was divided into two subsets: the T-INV set, con- taining sentences with source OOV words whose translations are NOT OOV in the target language; and the T-OOV set, containing sentences with OOV words that are OOV in both source and trans- lation. There were 491 source-side OOV words in total, among which 276 words have in-vocabulary translations and 215 words only have OOV trans- lation. We constructed a translation table with three items for each OOV word: (1) its transla- tion; (2) its similar word; (3) the similar word of its translation, if the translation is also an OOV word. All the above was designed by hand, and for each OOV word, there was only a single trans- lation. Although it is not difficult to collect most of this information automatically (e.g., by using an SMT phrase table), we are simulating the scenario where OOV words are newly coined, or where the system is migrated to a new domain, meaning that some words are totally new to the system. Han- dling OOV words of this type is certainly chal- lenging, but it is also practically valuable. For comparison, the place-holder approach pro-  posed by <ref type="bibr" target="#b13">Luong et al. (2014)</ref> was also imple- mented. Here, OOV words in the target language are substituted by position-aware UNKs, and a post-processing step is used to replace UNKs with the correct translation. We denote this system as 'NMT-PL'. For M-NMT, only the best configu- ration M-NMT(sy, u xy ) was tested in this exper- iment. Two scenarios were considered: the origi- nal M-NMT system, and the M-NMT system with OOV words involved in the memory (denoted by M-NMT+OOV).</p><p>The results with the NIST dataset are shown in <ref type="table" target="#tab_3">Table 4</ref>. In addition to the BLEU scores, we also report the OOV recall rates, defined as the propor- tion of OOV words that are correctly translated. It can be seen that both the basic NMT and M- NMT systems work badly with OOV words: they can only process OOV words whose translations are not OOV in the target language, and the recall rate is very low (0.05 approximately). The place- holder approach (NMT-PL) can address both types of OOV words, but the recall rate is still low. The M-NMT system with OOV memory, in contrast, is much more effective in OOV word translation, as shown in <ref type="table" target="#tab_3">Table 4</ref>. We also implemented the replace-and-restore approach reported by , but found performance to be poor (the BLEU scores are 13.9 on T-INV and 13.3 on T-OOV). This may be due to no re-training of the neural model (again, in order to keep the extensi- bility and generalizability of the M-NMT system).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Frequency Analysis</head><p>To form a better understanding of the memory mechanism, we distribute the test sentences into four bins according to the lowest word frequency among the sentence and compute the recall rates for words in these bins. Once a generated transla- tion word is also in one of the references, we treat it as one hit. The experiment was conducted with the NIST dataset. The results in <ref type="figure" target="#fig_1">Figure 2</ref> show that M-NMT offers more improvement with infrequent words, in accordance with our argument that the memory mechanism helps NMT in dealing with infrequent words.</p><p>Finally, we demonstrate the translation with M- NMT for the example in <ref type="table">Table 1</ref>, as shown in Ta- ble 5. It can be clearly seen that the memory has remembered the infrequent word 'Chromosome', which resulted in an improved translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper presented a memory-augmented NMT approach, which introduces a memory mechanism for conventional NMT to assist with translating words not well learned by the neural model. Our experiments demonstrated that the new architec- ture is highly effective, providing performance im- provement by 9.0 and 2.7 BLEU scores on two Chinese-English translation datasets, respectively. Additionally, it offers a very flexible and effective OOV treatment. In our experiments, The OOV re- calls are 28% and 40% for the OOV words whose target words are INV and OOV, respectively, a sig- nificant improvement on competing methods. Fu- ture work will investigate better model-memory integration, e.g., by joint training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The structure of the M-NMT architecture.</figDesc><graphic url="image-1.png" coords="3,133.71,62.81,327.40,296.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The recall rates of words in different frequency bins. src. ref. Humans have 23 pairs of chromosomes. Moses A total of 23 human chromosome. NMT There are 23-year history of human history. M-NMT There have a total of 23 species of chromosomes .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>M-NMT systems with different configu-
rations. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>System 
IWSLT05 NIST03 
Moses 
52.5 
30.6 
NMT 
43.9 
31.3 
NMT-L 
45.9 
31.7 
Arthur et al. 
M-NMT(s, u y ) 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>The OOV recall rates and BLEU scores 
on sentences with OOV words. 'T-INV' refers to 
the case where the target words of the OOV input 
are in-vocabulary, and 'T-OOV' means the case 
where the target words are also OOV. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>The translations from different systems 
for the Chinese-to-English 'meaning drift' exam-
ple. 

</table></figure>

			<note place="foot" n="1"> https://www.tensorflow.org/ 2 https://github.com/lisa-groundhog/GroundHog</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This paper was supported by the National Natural Science Foundation of China (NSFC) under the project NO.61371136, NO.61633013, NO.61472428, NO.61472428.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Incorporating discrete translation lexicons into neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1557" to="1567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural versus phrasebased machine translation quality: a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.04631</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incorporating structural alignment biases into an attentional neural translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Duy Vu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vymolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural Turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabskabarwi´nskabarwi´nska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gómez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Kenlm: Faster and smaller language model queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Sixth Workshop on Statistical Machine Translation</title>
		<meeting>of the Sixth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corrado</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04558</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><forename type="middle">Birch</forename><surname>Mayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL, Demonstration Session</title>
		<meeting>of ACL, Demonstration Session</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Ondrej Bojar, Alexandra Constantin, and Evan Herbst</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
	<note>Proc. of NAACL</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards zero unknown word in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2852" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8206</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Coverage embedding models for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baskaran</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.03148</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 16</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Syntactically guided neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Waite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="299" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Memory-enhanced decoder for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="278" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint/>
	</monogr>
<note type="report_type">Bordes. 2014. Memory networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep neural networks in machine translation: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="16" to="25" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Flexible and creative chinese poetry generation using neural memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Abel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03773</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
