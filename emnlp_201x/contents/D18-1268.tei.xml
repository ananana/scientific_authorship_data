<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Cross-lingual Transfer of Word Embedding Spaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoki</forename><surname>Otani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Cross-lingual Transfer of Word Embedding Spaces</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2465" to="2474"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2465</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Cross-lingual transfer of word embeddings aims to establish the semantic mappings among words in different languages by learning the transformation functions over the corresponding word embedding spaces. Successfully solving this problem would benefit many downstream tasks such as to translate text classification models from resource-rich languages (e.g. English) to low-resource languages. Supervised methods for this problem rely on the availability of cross-lingual supervision , either using parallel corpora or bilingual lexicons as the labeled data for training, which may not be available for many low resource languages. This paper proposes an un-supervised learning approach that does not require any cross-lingual labeled data. Given two monolingual word embedding spaces for any language pair, our algorithm optimizes the transformation functions in both directions simultaneously based on distributional matching as well as minimizing the back-translation losses. We use a neural network implementation to calculate the Sinkhorn distance , a well-defined distributional similarity measure, and optimize our objective through back-propagation. Our evaluation on benchmark datasets for bilingual lexicon induction and cross-lingual word similarity prediction shows stronger or competitive performance of the proposed method compared to other state-of-the-art supervised and unsupervised base-line methods over many language pairs.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word embeddings are well known to capture meaningful representations of words based on large text corpora ( <ref type="bibr" target="#b22">Mikolov et al., 2013;</ref><ref type="bibr" target="#b23">Pennington et al., 2014</ref>). Training word vectors us- ing monolingual corpora is a common practice in various NLP tasks. However, how to estab- lish cross-lingual semantic mapping among mono- lingual embeddings remain an open challenge as the availability of resources and benchmarks are highly imbalanced across languages.</p><p>Recently, increasing effort of research has been motivated to address this challenge. Success- ful cross-lingual word mapping will benefit many cross-lingual learning tasks, such as transform- ing text classification models trained in resource- rich languages to low-resource languages. Down- stream applications include word alignment, text classification, named entity recognition, depen- dency parsing, POS-tagging, and more <ref type="bibr">(Søgaard et al., 2015</ref>). Most methods for cross-lingual transfer of word embeddings are based on super- vised or semi-supervised learning, i.e., they re- quire cross-lingual supervision such as human- annotated bilingual lexicons and parallel cor- pora ( <ref type="bibr" target="#b21">Lu et al., 2015;</ref><ref type="bibr" target="#b27">Smith et al., 2017;</ref><ref type="bibr" target="#b3">Artetxe et al., 2016</ref>). Such a requirement may not be met for many language pairs in the real world.</p><p>This paper proposes an unsupervised approach to the cross-lingual transfer of monolingual word embeddings, which requires zero cross-lingual su- pervision. The key idea is to optimize the mapping in both directions for each language pair (say A and B), in the way that the word embedding trans- lated from language A to language B will match the distribution of word embedding in language B. And when translated back from B to A, the word embedding after two steps of transfer will be max- imally close to the original word embedding. A similar property holds for the other direction of the loop (from B to A and then from A back to B). Specifically, we use the Sinkhorn distance <ref type="bibr" target="#b9">(Cuturi, 2013</ref>) to capture the distributional similarity between two set of embeddings after transforma- tion, which we found empirically superior to the KL-divergence ( <ref type="bibr" target="#b34">Zhang et al., 2017a</ref>) and distance to nearest neighbor ( <ref type="bibr" target="#b4">Artetxe et al., 2017;</ref><ref type="bibr" target="#b8">Conneau et al., 2017</ref>) with regards to the quality of learned transformation as well as the robustness under dif- ferent training conditions. Our novel contributions in the proposed work include:</p><p>• We propose an unsupervised learning frame- work which incorporates the Sinkhorn dis- tance as a distributional similarity measure in the back-translation loss function.</p><p>• We use a neural network to optimize our model, especially to implement the Sinkhorn distance whose calculation itself is an opti- mization problem.</p><p>• Unlike previous models which only consider cross-lingual transformation in a single direc- tion, our model jointly learns the word em- bedding transfer in both directions for each language pair.</p><p>• We present an intensive comparative evalua- tion where our model achieved the state-of- the-art performance for many language pairs in cross-lingual tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We divide the related work into supervised and un- supervised categories. Representative methods in both categories are included in our comparative evaluation (Section 3.4). We also discuss some related work in unsupervised domain transfer in addition.</p><p>Supervised Methods: There is a rich body of su- pervised methods for learning cross-lingual trans- fer of word embeddings based on bilingual dic- tionaries ( <ref type="bibr" target="#b22">Mikolov et al., 2013;</ref><ref type="bibr" target="#b11">Faruqui and Dyer, 2014;</ref><ref type="bibr" target="#b3">Artetxe et al., 2016;</ref><ref type="bibr" target="#b32">Xing et al., 2015;</ref><ref type="bibr" target="#b10">Duong et al., 2016;</ref><ref type="bibr" target="#b15">Gouws and Søgaard, 2015)</ref>, sentence-aligned corpora <ref type="bibr" target="#b20">(Kočisk`Kočisk`y et al., 2014;</ref><ref type="bibr" target="#b19">Hermann and Blunsom, 2014;</ref> and document-aligned corpora <ref type="bibr" target="#b31">(Vuli´cVuli´c and Moens, 2016;</ref><ref type="bibr" target="#b28">Søgaard et al., 2015</ref>). The most relevant line of work is that by <ref type="bibr" target="#b22">Mikolov et al. (2013)</ref> where they showed monolingual word embed- dings are likely to share similar geometric prop- erties across languages although they are trained separately and hence cross-lingual mapping can be captured by a linear transformation across em- bedding spaces. Several follow-up studies tried to improve the cross-lingual transformation in vari- ous ways <ref type="bibr" target="#b11">(Faruqui and Dyer, 2014;</ref><ref type="bibr" target="#b3">Artetxe et al., 2016;</ref><ref type="bibr" target="#b32">Xing et al., 2015;</ref><ref type="bibr" target="#b10">Duong et al., 2016;</ref><ref type="bibr" target="#b1">Ammar et al., 2016;</ref><ref type="bibr" target="#b3">Artetxe et al., 2016;</ref><ref type="bibr" target="#b36">Zhang et al., 2016;</ref><ref type="bibr" target="#b26">Shigeto et al., 2015)</ref>. Nevertheless, all these methods require bilingual lexicons for supervised learning. Vuli´c <ref type="bibr" target="#b30">Vuli´c and Korhonen (2016)</ref> showed that 5000 high-quality bilingual lexicons are sufficient for learning a reasonable cross-lingual mapping.</p><p>Unsupervised Methods have been studied to es- tablish cross-lingual mapping without any human- annotated supervision. Earlier work simply re- lied on word occurrence information only <ref type="bibr" target="#b24">(Rapp, 1995;</ref><ref type="bibr" target="#b12">Fung, 1996)</ref> while later efforts have con- sidered more sophisticated statistics in addition ( <ref type="bibr" target="#b17">Haghighi et al., 2008)</ref>. The main difficulty in unsupervised learning of cross-lingual mapping is the formulation of the objective function, i.e., how to measure the goodness of an induced map- ping without any supervision is a non-trivial ques- tion. <ref type="bibr" target="#b7">Cao et al. (2016)</ref> tried to match the mean and standard deviation of the embedded word vec- tors in two different languages after mapping the words in the source language to the target lan- guage. However, such an approach has shown to be sub-optimal because the objective function only carries the first and second order statistics of the mapping. <ref type="bibr" target="#b4">Artetxe et al. (2017)</ref> tried to im- pose an orthogonal constraint to their linear trans- formation model and minimize the distance be- tween the transferred source-word embedding and its nearest neighbor in the target embedding space. Their method, however, requires a seed bilingual dictionary as the labeled training data and hence is not fully unsupervised. ( <ref type="bibr" target="#b34">Zhang et al., 2017a;</ref><ref type="bibr" target="#b5">Barone, 2016</ref>) adapted a generative adversarial network (GAN) to make the transferred embed- ding of each source-language word indistinguish- able from its true translation in the target embed- ding space ( <ref type="bibr" target="#b13">Goodfellow et al., 2014</ref>). The adver- sarial model could be optimized in a purely un- supervised manner but is often suffered from un- stable training, i.e. the adversarial learning does not always improve the performance over simpler baselines. Unsupervised Domain Transfer: Generally speaking, learning the cross-lingual transfer of word embedding can be viewed as a domain trans- fer problem, where the domains are word sets in different languages. Thus various work in the field of unsupervised domain adaptation or unsu- pervised transfer learning can shed light on our problem. For example, <ref type="bibr" target="#b18">He et al. (2016)</ref> proposed a semi-supervised method for machine transla- tion to utilize large monolingual corpora. <ref type="bibr" target="#b25">Shen et al. (2017)</ref> used unsupervised learning to trans- fer sentences of different sentiments. Recent work in computer vision addresses the problem of im- age style transfer without any annotated training data ( <ref type="bibr" target="#b37">Zhu et al., 2017;</ref><ref type="bibr" target="#b29">Taigman et al., 2016;</ref><ref type="bibr" target="#b33">Yi et al., 2017</ref>). Among those, our work is mostly inspired by the work on CycleGAN ( <ref type="bibr" target="#b37">Zhu et al., 2017)</ref>, and we adopt their cycled consistent loss over images into our back-translation loss. One key difference of our method from CycleGAN is that they used the training loss of an adversarial classifier as an indicator of the distributional dis- tance, but instead, we introduce the Sinkhorn dis- tance in our objective function and demonstrate its superiority over the representative method using adversarial loss ( <ref type="bibr" target="#b34">Zhang et al., 2017a</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>Our system takes two sets of monolingual word embeddings of dimension d as input, which are trained separately on two languages. We denote them as</p><formula xml:id="formula_0">X = {x i } n i=1 , Y = {y j } m j=1 , x i , y j ∈ R d .</formula><p>During the training of monolingual word embed- ding for X and Y , we also have the access to the word frequencies, represented by vectors r ∈ N n and c ∈ N m for X and Y , respectively. Specifi- cally, r i is the frequency for word (embedding) x i and similarly for c j of y j . As illustrated in <ref type="figure">Fig- ure 3</ref>, our model has two mappings: G : X → Y and F : Y → X. We further denote transferred embedding from X as G(X) := {G(x i )} n i=1 and correspondingly for F (Y ).</p><p>In the unsupervised setting, the goal is to learn the mapping G and F without any paired word translation. To achieve this, our loss function consists of two parts: Sinkhorn distance (Cuturi, 2013) for matching the distribution of transferred embedding to its target embedding distribution; and a back-translation loss for preventing degen- erated transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sinkhorn Distance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Definition</head><p>Sinkhorn distance is a recently proposed distance between probability distributions. We use the Sinkhorn distance to measure the closeness be- tween G(X) and Y , and also between F (Y ) and X. During the training, our model optimizes G and F for lower Sinkhorn distance to make the transferred embeddings match the distribution of the target embeddings. Here we only illus- trate the Sinkhorn distance between G(X) and Y , the derivation for F (Y ) and X is very similar. Although the vocabulary sizes of two languages could be different, we are able to sample mini- batches of equal size from G(X) and Y . therefore we assume n = m in the following derivation.</p><p>To compute Sinkhorn distance, we firstly com- pute a distance matrix M (G) ∈ R n×m between G(X) and Y where M (G) ij is the distance measure between G(x i ) and y j . The superscript on M (G) indicates the distance that depends on a parameter- ized transformation G. For instance, if we choose Euclidean distance as a measure (see Section 3.1.3 for more discussions), we will have</p><formula xml:id="formula_1">M (G) ij = G(x i ) − y j 2.</formula><p>Given the distance matrix, the Sinkhorn dis- tance between P G(X) and P Y is defined as:</p><formula xml:id="formula_2">d sh (G) := min P ∈Uα(r,c) P, M (G)<label>(1)</label></formula><p>where ·, ·· is the Forbenius dot-product and U α (r, c) is an entropy constrained transport poly- </p><formula xml:id="formula_3">Algorithm 1 Computation of Sinkhorn Distance d sh (G) 1: procedure SINKHORN(M (G) , r, c, λ, I) 2: K (G) := e −λM (G) 3: v = 1 m /m normalized</formula><formula xml:id="formula_4">u = r./K (G) v 7: v = c./K (G) T u 8: i = i + 1 9: d sh (G) = u T ((K (G) ⊗ M (G) )v) 10: return d sh (G) The Sinkhorn distance</formula><p>tope, defined as</p><formula xml:id="formula_5">U α (r, c) = {P ∈ R + n×m |P 1 m = r, P T 1 n = c, h(P ) ≤ h(r) + h(c) − α}<label>(2)</label></formula><p>Note that P is non-negative and the first two con- straints make its element-wise sum be 1. There- fore, P can be seen as a set of probability distri- butions. The same applies for r and c since they are frequencies. h is the entropy function defined on any probability distributions and α is a hyper- parameter to choose. For any probabilistic matrix P ∈ U α (r, c), it can be viewed as the joint proba- bility of (G(X), Y ). The first two constraints en- sure that P has marginal distribution on G(X) as P G(X) and on Y as P Y . We can also view P ij as the evidence for establishing a translation between word vector x i and word vector y j . An intuitive interpretation of equation <ref type="formula" target="#formula_2">(1)</ref> is that we are trying to find the optimal transport proba- bility P under the entropy constraint such that the total distance to transport from G(X) to Y is min- imized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Computing Sinkhorn Distance d sh (G)</head><p>Cuturi <ref type="formula" target="#formula_2">(2013)</ref> showed that the optimal solu- tion of formula (1) has the form P * = diag(u) <ref type="bibr">Kdiag(v)</ref> , where u and v are some non- negative vectors and K (G) := e −λM <ref type="bibr">(G)</ref> ; λ is the Lagrange multiplier for the entropic constraint in 2 and each α in Equation (1) has one correspond- ing λ. The Sinkhorn distance can be efficiently computed by a matrix scaling algorithm. We present the pseudo code in Algorithm 1. Note that the computation of d sh (G) only requires matrix- vector multiplication. Therefore, we can compute and back propagate the gradient of d sh (G) with re- gards to the parameters in G using standard deep learning libraries. We show our implementation details in Section 3.4 and supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Choice of the Distance Metric</head><p>In Section 3.1.1, we used the Euclidean distance of vector pairs to define M (G) and Sinkhorn dis- tance d sh (G). However, in our preliminary exper- iment, we found that Euclidean distance of unnor- malized vectors gave poor performance. There- fore, following the common practice, we normal- ize all word embedding vectors to have a unit L2 norm in the construction of M (G) .</p><p>As pointed out in Theorem 1 of Cuturi (2013), M (G) must be a valid metric in order to make d sh (G) a valid metric. For example, the com- monly used cosine distance, which is defined as CosDist(a, b) = 1 − cos(a, b), is not a valid met- ric because it does not satisfy triangle inequality 1 . Thus, for constructing M (G) , we propose the square root cosine distance (SqrtCosDist) be- low:</p><formula xml:id="formula_6">SqrtCosDist(a, b) := 2 − 2cos(a, b) (3) M (G) ij = SqrtCosDist(G(x i ), y j ) (4) Theorem 1. SqrtCosDist is a valid metric. Proof. ∀a, b ∈ R d , letâletˆletâ = a a , ˆ b = b b . We have cos(a, b) = ˆ a, ˆ b andâandâ, ˆ a = ˆ b, ˆ b = 1. Then SqrtCosDist(a, b) = 2 − 2cos(a, b) = ˆ a, ˆ a + ˆ b, ˆ b − 2â, ˆ b = ˆ a − ˆ b, ˆ a − ˆ b = ˆ a − ˆ b</formula><p>Obviously, the last term is the Euclidean distance between normalized input vectorsâvectorsˆvectorsâ andˆbandˆandˆb. Since Euclidean distance is a valid metric, it follows that SqrtCosDist satisfies all the axioms for a valid metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Objective Function</head><p>Given enough capacity, G is capable to trans- fer X to Y for arbitrary word-to-word mappings.</p><p>To ensure that, we learn a meaningful translation and also to regularize the search space of possible transformations, we enforce the word embedding after the forward and the backward transformation <ref type="figure">, c)</ref> , which violates the triangle inequality.</p><formula xml:id="formula_7">1 If we select a = [1, 0], b = [ √ 2 2 , √ 2 2 ], c = [0, 1] We have CosDist(a, c) ≥ CosDist(a, b) + CosDist(b</formula><p>should not diverge much from its original direc- tion. We simply choose the back-translation loss based on the cosine similarity:</p><formula xml:id="formula_8">d bt (G, F ) = i 1 − cos(x i , F (G(x i )))+ j 1 − cos(y i , G(F (y i ))) (5)</formula><p>where cos is the cosine similarity.</p><p>Putting everything together, we minimize the following objective function.</p><formula xml:id="formula_9">L X,Y,r,c (G, F ) = d sh (G) + d sh (F ) + βd bt (G, F )<label>(6)</label></formula><p>where hyper-parameter β controls the relative weight of the last term against the first two terms in the objective function. By definition, computa- tion of d sh (G) or d sh (F ) involves another mini- mization problem as shown in Equation (1). We solve it using the matrix scaling algorithm in Sec- tion 3.1.2, and treat d sh (G) as a deterministic and differentiable function of parameters in G. The same holds for d sh (F ) and F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Wasserstein GAN Training for Good Initial Point</head><p>In preliminary experiments, we found that our ob- jective 6 is sensitive to the initialization of the weight in G and F in the purely unsupervised setting. It requires a good initial setting of the parameters to avoid getting stuck in the poor lo- cal minimal. To address this sensitivity issue, we employed a similar approach as in ( <ref type="bibr" target="#b35">Zhang et al., 2017b;</ref><ref type="bibr" target="#b0">Aldarmaki et al., 2018)</ref> to firstly used an adversarial training approach to learn G and F and use them as the initial point for training our full objective 6. More specifically, we choose to mini- mize the optimal transport distance below.</p><formula xml:id="formula_10">d ot (G) := min P ∈U (r,c) P, M (G)<label>(7)</label></formula><p>U is the transport polytope without entropy con- straint, defined as follows.</p><formula xml:id="formula_11">U = {P ∈ R + n×m |P 1 m = r, P T 1 n = c} (8)</formula><p>We optimize the distance above by its dual form and through adversarial training, which is also known as Wasserstein GAN (WGAN) ( <ref type="bibr" target="#b2">Arjovsky et al., 2017</ref>). We applied the optimization trick proposed by <ref type="bibr" target="#b16">Gulrajani et al. (2017)</ref>.</p><p>Although the first phase of adversarial training could be unstable, and the performance is lower than using the Sinkhorn distance, the adversarial training narrows down the search space of model parameters and boosting the training of our pro- posed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation</head><p>We implemented transformation G and F by a linear transformation. The dimension of the in- put and output are the same with the word em- bedding dimension d. <ref type="bibr">2</ref> For all the experiments in the subsequent section, the β in (6) was set to be 0.1. For hyper-parameters from the computation of Sinkhorn distance, we choose λ = 10 and run the matrix scaling algorithm for 20 iterations. Due to the space constraint, a detailed implementation description is presented in the supplementary ma- terial. The code of our implementation is publicly available 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conducted an evaluation of our approach in comparison with state-of-the-art super- vised/unsupervised methods on several evaluation benchmarks for bilingual lexicon induction (Task 1) and word similarity prediction (Task 2). We include our main results in this section and report the ablation study in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Monolingual Word Embedding Data</head><p>All the methods being evaluated in both tasks take monolingual word embedding in each language as the input data. We use publicly available pre- trained word embeddings trained on Wikipedia ar- ticles: (1) a smaller set of word embeddings of dimension 50 trained on comparable Wikipedia dump in five languages (Zhang et al., 2017a) 4 and (2) a larger set of word embeddings of dimen- sion 300 trained on Wikipedia dump in 294 lan- guages ( <ref type="bibr" target="#b6">Bojanowski et al., 2016)</ref>  nience, we name the two sets WE-Z and WE-C, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Bilingual Lexicon Data</head><p>We need true translation pairs of words for eval- uating methods in bilingual lexicon induction (Task 1). We followed previous studies and pre- pared two datasets below.</p><p>LEX-Z: <ref type="bibr" target="#b34">Zhang et al. (2017a)</ref> constructed the bilingual lexicons from various resources. Since their ground truth word pairs are not released, we followed their procedure, crawled bilingual dictio- naries and randomly separated them into the train- ing and testing set of equal size. <ref type="bibr">6</ref> Note that our proposed method did not utilize the training set. It was only used by supervised baseline methods de- scribed in Section 4.2. There are eight language pairs (order counted); the corresponding dataset statistics are summarized in <ref type="table">Table 1</ref>. We use WE- Z embeddings in this dataset. <ref type="bibr" target="#b8">Conneau et al. (2017)</ref> and contains more translation pairs than LEX-Z. They divided them into training and testing set. We run our model and the baseline methods on 16 language pairs. For each language pair, the training set contains 5, 000 unique query words and the testing set has 1, 500 query words. We followed <ref type="bibr" target="#b8">Conneau et al. (2017)</ref> and set the search space of candidate translations to be the 200, 000 most frequent words in each target lan- guage. We use WE-C embeddings in this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LEX-C: This lexicon was constructed by</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Bilingual Word Similarity Data</head><p>For bilingual word similarity prediction (Task 2) we need the true labels for evaluation. Fol- lowing <ref type="bibr" target="#b8">Conneau et al. (2017)</ref>, we used the Se- mEval 2017 competition dataset, where human annotators measured the cross-lingual similarity of nominal word pairs according to the five-point Likert scale. This dataset contains word pairs across five languages: English (en), German (de), Spanish (es), Italian (it), and Farsi (fa). Each lan- guage pair has about 1,000 word pairs annotated with a real similarity score ranging from 0 to 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Methods</head><p>We evaluated the same set of supervised and un- supervised baselines for comparative evaluation in # tokens vocab. size bi. lex. size <ref type="table" target="#tab_7">tr-en  tr  6m  7,482  18,404  en  28m  13,220  27,327   es-en  es  61m  4,</ref>    <ref type="bibr">(2017)</ref>. <ref type="bibr">7</ref> We fed all the supervised methods with the bilingual dictionaries in the training portions of the LEX-Z and LEX-C datasets, respectively. For unsupervised baselines we include the methods of <ref type="bibr" target="#b34">Zhang et al. (2017a)</ref> and <ref type="bibr" target="#b8">Conneau et al. (2017)</ref>, whose source code is publicly available as provided by the authors. <ref type="bibr">8</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results in Bilingual Lexicons Induction (Task 1)</head><p>Bilingual lexicon induction is a task to induce a translation in the target language for each query word in the source language. After the query word and the target-language words are represented in the same embedding space (or after our system maps the query word from the source embedding space to the target embedding space), the k near- est target words are retrieved based on their cosine similarity scores with respect to the query vector. If the k retrieved target words contain any valid translation according to the gold bilingual lexicon, the translation (retrieval) is considered success- ful. The fraction of the correctly translated source words in the test set is defined as accuracy@k, Methods tr-en en-tr es-en en-es zh-en en-zh it-en en-it    Methods bg-en en-bg ca-en en-ca sv-en en-sv lv-en en-lv  The accuracy@k scores of all methods in bilingual lexicon induction on LEX-C. The best score for each language pair is bold-faced for the supervised and unsupervised categories, respectively. Languages are paired among English(en), Bulgarian(bg), Catalan(ca), Swedish(sv) and Latvian(lv). "-" means that during the training time, the model failed to converge to reasonable local minimal and hence the result is omitted in the table.</p><p>which is conventional metric in benchmark evalu- ations. <ref type="table">Table 2</ref> shows the accuracy@1 for all the meth- ods on LEX-Z in our evaluation. We can see that our method outperformed the other unsupervised baselines by a large margin on all the eight lan- guage pairs. Compared with the supervised meth- ods, our method is still competitive (the best or the second-best scores on four out of eight language pairs), even ours does not require cross-lingual su- pervision. Also, we notice the performance vari- ance over different language pairs. Our method outperforms all the methods (supervised and un- supervised combined) on the English-Spanish (en- es) pair, perhaps for the reasons that these two lan- guages are most similar to each other, and that the monolingual word embeddings for this pair in the comparable corpus are better aligned than the other language pairs. On the other hand, all the methods including ours have the worst perfor- mance on the English-Turkish (en-tr) pair. An- other observation is the performance differences in the two directions of the language pair. For exam- ple, the performance of it-en is better than en-it for all methods in table 2. A part of the reason is that there are more unique English words than non- English words in the evaluation set. This would cause direction "xx-en" to be easier than "en-xx" because there are often multiple valid ground truth English translations for each query in "xx". But the same may not hold for the opposite direction of "en-xx". Nevertheless, the relative performance of our method compared to others is quite robust over different language pairs and different directions of Methods de-en en-de es-en en-es fr-en en-fr it-en en-it   <ref type="table">Table 4</ref>: The accuracy@k scores of all methods in bilingual lexicon induction on LEX-C. The best score for each language pair is bold-faced for the supervised and unsupervised categories, respectively. Languages are paired among English (en), German (de), Spanish (es), French (fr) and Italian (it). "-" means that during the training time, the model failed to converge to reasonable local minimal and hence the result is omitted in the table.</p><p>translation. <ref type="table" target="#tab_7">Table 3</ref> and <ref type="table">Table 4</ref> summarize the results of all the methods on the LEX-C dataset. Several points may be worth noticing. Firstly, the perfor- mance scores on LEX-C are not necessarily con- sistent with those on LEX-Z <ref type="table">(Table 2)</ref> even if the methods and the language pairs are the same; this is not surprising as the two datasets differ in query words, word embedding quality, and training-set sizes. Secondly, the performance gap between the best supervised methods and the best unsu- pervised methods in both <ref type="table" target="#tab_7">Table 3</ref> and <ref type="table">Table 4</ref> are larger than that in <ref type="table">Table 2</ref>. This is attributed to the large amount of good-quality supervision in LEX-C (5,000 human-annotated word pairs) and the larger candidate size in WE-C (200, 000 can- didates). Thirdly, the average performance in Ta- ble 3 is lower than that in <ref type="table">Table 4</ref>, indicating that the language pairs in the former are more difficult than that in the latter. Nevertheless, we can see that our method has much stronger performance than other unsupervised methods in <ref type="table" target="#tab_7">Table 3</ref>, i.e., on the harder language pairs, and that it performed com- parably with the model by <ref type="bibr" target="#b8">Conneau et al. (2017)</ref> in <ref type="table">Table 4</ref> on the easier language pairs. Combin- ing all these observations, we see that our method is highly robust for various language pairs and un- der different training conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results in Cross-lingual Word Similarity</head><p>Prediction (Task 2)</p><p>We evaluate models on cross-lingual word similar- ity prediction (Task 2) to measure how much the predicted cross-language word similarities match  <ref type="table">Table 5</ref>: Performance (measured using Pearson cor- relation) of all the methods in cross-lingual seman- tic word similarity prediction on the benchmark data from <ref type="bibr" target="#b8">Conneau et al. (2017)</ref>. The best score in the su- pervised and unsupervised category is bold-faced, re- spectively. The languages include English (en), Ger- man (de), Spanish (es), Persian (fa) and Italian (it). "-" means that the model failed to converge to reasonable local minimal during the training process. the ground truth annotated by humans. Follow- ing the convention in benchmark evaluations for this task, we compute the Pearson correlation be- tween the model-induced similarity scores and the human-annotated similarity scores over testing word pairs for each language pair. A higher cor- relation score with the ground truth represents the better quality of induced embeddings. All systems use the cosine similarity between the transformed embedding of each query and the word embedding of its paired translation as the predicted similarity score. <ref type="table">Table 5</ref> summarizes the performance of all the methods in cross-lingual word similarity predic- tion. We can see that the unsupervised methods, including ours, perform equally well as the super- vised methods, which is highly encouraging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we presented a novel method for cross-lingual transformation of monolingual em- beddings in an unsupervised manner. By simul- taneously optimizing the bi-directional mappings w.r.t. Sinkhorn distances and back-translation losses on both ends, our model enjoys its predic- tion power as well as robustness, with the impres- sive performance on multiple evaluation bench- marks. For future work, we would like to extend this work in the semi-supervised setting where in- sufficient bilingual dictionaries are available.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Zhang et al. (2017b), Conneau et al. (2017) and Artetxe et al. (2017) also tried adver- sarial approaches for the induction of seed bilin- gual dictionaries, as a sub-problem in the cross- lingual transfer of word embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The model takes monolingual word embedding X and Y as input. G and F are embedding transfer functions parameterized by a neural network, which are represented by solid arrows. The dashed lines indicate the input for our objective losses, namely the Sinkhorn distance and back-translation loss .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 :</head><label>1</label><figDesc>The statistics of LEX-Z. The languages are Spanish (es), French (fr), Chinese (zh), Turkish (tr) and English (en). Number of tokens is the size of training corpus of WE-Z. The bilingual lexicon size means the number of unique words of a language in the gold bilin- gual lexicons. both Task 1 and Task 2. The supervised base- lines include the methods of Shigeto et al. (2015); Zhang et al. (2016); Artetxe et al. (2016); Xing et al. (2015); Mikolov et al. (2013); Artetxe et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Supervised</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 :</head><label>2</label><figDesc>The accuracy@k scores of all methods in bilingual lexicon induction on LEX-Z. The best score for each language pair is bold-faced for the supervised and unsupervised categories, respectively. Language pair "A-B" means query words are in language A and the search space of word translations is in language B. Languages are paired among English(en), Turkish (tr), Spanish (es), Chinese (zh) and Italian (it).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Supervised</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="6"> The bilingual dictionaries we crawled are submitted as supplementary material.</note>

			<note place="foot" n="7"> The implementations are available from https:// github.com/artetxem/vecmap. 8 We used implementation by Zhang et al. (2017a) from http://nlp.csai.tsinghua.edu.cn/ ˜ zm/ UBiLexAT and that of Conneau et al. (2017) from https: //github.com/facebookresearch/MUSE</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers for their helpful com-ments. This work is supported in part by De-fense Advanced Research Projects Agency Infor-mation Innovation Oce (I2O), the Low Resource Languages for Emergent Incidents (LORELEI) Program, Issued by DARPA/I2O under Contract No. HR0011-15-C-0114, and in part by the Na-tional Science Foundation (NSF) under grant IIS-1546329.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised word mapping using structural similarities in monolingual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aldarmaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="185" to="196" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.01925</idno>
		<title level="m">Massively multilingual word embeddings</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning principled bilingual mappings of word embeddings while preserving monolingual invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agirre</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2289" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning bilingual word embeddings with (almost) no bilingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agirre</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Towards cross-lingual distributed representations without parallel text trained with adversarial autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V M</forename><surname>Barone</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.02996</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics</title>
		<meeting>the 26th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1818" to="1827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jégou</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04087</idno>
		<title level="m">Word translation without parallel data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2292" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.09403</idno>
		<title level="m">Learning crosslingual word embeddings without bilingual corpora</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="462" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Compiling Bilingual Lexicon Entries From a Non-Parallel English-Chinese Corpus A Non-parallel Corpus of Chinese and English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Very Large Corpora</title>
		<meeting>the Third Workshop on Very Large Corpora</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="173" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bilbowa: Fast bilingual distributed representations without word alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corrado</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="748" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simple taskspecific bilingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1386" to="1390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5769" to="5779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning bilingual lexicons from monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: Hlt</title>
		<meeting>ACL-08: Hlt</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="771" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dual learning for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="820" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Multilingual models for compositional distributed semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.4641</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kočisk`kočisk`y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.0947</idno>
		<title level="m">Learning bilingual word representations by marginalizing alignments</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep multilingual correlation for improved word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="250" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Identifying Word Translations in Non-Parallel Texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 33rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="320" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Style transfer from non-parallel text by cross-alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6833" to="6844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ridge regression, hubness, and zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shigeto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="135" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Offline bilingual word vectors, orthogonal transformations and the inverted softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Turban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Y</forename><surname>Hammerla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03859</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Inverted indexing for cross-lingual nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">ˇ</forename><forename type="middle">Z</forename><surname>Agi´cagi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannsen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference of the Asian Federation of Natural Language Processing (ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unsupervised cross-domain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolf</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02200</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the role of seed lexicons in learning bilingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="247" to="257" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bilingual distributed word representations from documentaligned comparable data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-F</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="953" to="994" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Normalized word embedding and orthogonal transform for bilingual word translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1006" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Dualgan: Unsupervised dual learning for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adversarial training for unsupervised bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1959" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Earth mover&apos;s distance minimization for unsupervised bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1934" to="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Ten pairs to tag-multilingual pos tagging via coarse mapping between embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gaddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
