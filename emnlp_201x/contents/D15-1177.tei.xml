<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models of Meaning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
							<email>jianpeng.cheng@stcatz.oxon.org</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
							<email>d.kartsaklis@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models of Meaning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Deep compositional models of meaning acting on distributional representations of words in order to produce vectors of larger text constituents are evolving to a popular area of NLP research. We detail a compositional distributional framework based on a rich form of word embeddings that aims at facilitating the interactions between words in the context of a sentence. Embeddings and composition layers are jointly learned against a generic objective that enhances the vectors with syntactic information from the surrounding context. Furthermore, each word is associated with a number of senses, the most plausible of which is selected dynamically during the composition process. We evaluate the produced vectors qualitatively and quantitatively with positive results. At the sentence level, the effectiveness of the framework is demonstrated on the MSRPar task, for which we report results within the state-of-the-art range.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Representing the meaning of words by using their distributional behaviour in a large text corpus is a well-established technique in NLP research that has been proved useful in numerous tasks. In a distributional model of meaning, the semantic representation of a word is given as a vector in some high dimensional vector space, obtained ei- ther by explicitly collecting co-occurrence statis- tics of the target word with words belonging to a representative subset of the vocabulary, or by di- rectly optimizing the word vectors against an ob- jective function in some neural-network based ar- chitecture <ref type="bibr">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b10">Mikolov et al., 2013)</ref>.</p><p>Regardless their method of construction, distri- butional models of meaning do not scale up to larger text constituents such as phrases or sen- tences, since the uniqueness of multi-word expres- sions would inevitably lead to data sparsity prob- lems, thus to unreliable vectorial representations. The problem is usually addressed by the provision of a compositional function, the purpose of which is to prepare a vectorial representation for a phrase or sentence by combining the vectors of the words therein. While the nature and complexity of these compositional models may vary, approaches based on deep-learning architectures have been shown to be especially successful in modelling the meaning of sentences for a variety of tasks ( <ref type="bibr" target="#b24">Socher et al., 2012;</ref><ref type="bibr" target="#b4">Kalchbrenner et al., 2014</ref>).</p><p>The mutual interaction of distributional word vectors by a means of a compositional model pro- vides many opportunities for interesting research, the majority of which still remains to be explored. One such direction is to investigate in what way lexical ambiguity affects the compositional pro- cess. In fact, recent work has shown that shal- low multi-linear compositional models that explic- itly handle extreme cases of lexical ambiguity in a step prior to composition present consistently bet- ter performance than their "ambiguous" counter- parts ( . A first attempt to test these obser- vations in a deep compositional setting has been presented by <ref type="bibr">Cheng et al. (2014)</ref> with promising results.</p><p>Furthermore, a second important question re- lates to the very nature of the word embeddings used in the context of a compositional model. In a setting of this form, word vectors are not any more just a means for discriminating words based on their underlying semantic relationships; the main goal of a word vector is to contribute to a bigger whole-a task in which syntax, along with seman- tics, also plays a very important role. It is a central point of this paper, therefore, that in a composi- tional distributional model of meaning word vec- tors should be injected with information that re- flects their syntactical roles in the training corpus.</p><p>The purpose of this work is to improve the current practice in deep compositional models of meaning in relation to both the compositional pro- cess itself and the quality of the word embed- dings used therein. We propose an architecture for jointly training a compositional model and a set of word embeddings, in a way that imposes dynamic word sense induction for each word dur- ing the learning process. Note that this is in con- trast with recent work in multi-sense neural word embeddings ( <ref type="bibr" target="#b15">Neelakantan et al., 2014)</ref>, in which the word senses are learned without any composi- tional considerations in mind.</p><p>Furthermore, we make the word embeddings syntax-aware by introducing a variation of the hinge loss objective function of <ref type="bibr">Collobert and Weston (2008)</ref>, in which the goal is not only to predict the occurrence of a target word in a context, but to also predict the position of the word within that context. A qualitative analysis shows that our vec- tors reflect both semantic and syntactic features in a concise way.</p><p>In all current deep compositional distributional settings, the word embeddings are internal param- eters of the model with no use for any other pur- pose than the task for which they were specifically trained. In this work, one of our main consid- erations is that the joint training step should be generic enough to not be tied in any particular task. In this way the word embeddings and the de- rived compositional model can be learned on data much more diverse than any task-specific dataset, reflecting a wider range of linguistic features. In- deed, experimental evaluation shows that the pro- duced word embeddings can serve as a high qual- ity general-purpose semantic word space, present- ing performance on the Stanford Contextual Word Similarity (SCWS) dataset of <ref type="bibr">Huang et al. (2012)</ref> competitive to and even better of the performance of well-established neural word embeddings sets.</p><p>Finally, we propose a dynamic disambiguation framework for a number of existing deep compo- sitional models of meaning, in which the multi- sense word embeddings and the compositional model of the original training step are further re- fined according to the purposes of a specific task at hand. In the context of paraphrase detection, we achieve a result very close to the current state-of- the-art on the Microsoft Research Paraphrase Cor- pus <ref type="bibr">(Dolan and Brockett, 2005</ref>). An interesting aspect at the sideline of the paraphrase detection experiment is that, in contrast to mainstream ap- proaches that mainly rely on simple forms of clas- sifiers, we approach the problem by following a siamese architecture <ref type="bibr">(Bromley et al., 1993</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and related work 2.1 Distributional models of meaning</head><p>Distributional models of meaning follow the dis- tributional hypothesis <ref type="bibr">(Harris, 1954)</ref>, which states that two words that occur in similar contexts have similar meanings. Traditional approaches for con- structing a word space rely on simple counting: a word is represented by a vector of numbers (usu- ally smoothed by the application of some func- tion such as point-wise mutual information) which show how frequently this word co-occurs with other possible context words in a corpus of text.</p><p>In contrast to these methods, a recent class of distributional models treat word representations as parameters directly optimized on a word predic- tion task ( <ref type="bibr">Bengio et al., 2003;</ref><ref type="bibr">Collobert and Weston, 2008;</ref><ref type="bibr" target="#b10">Mikolov et al., 2013;</ref><ref type="bibr" target="#b17">Pennington et al., 2014</ref>). Instead of relying on observed co- occurrence counts, these models aim to maximize the objective function of a neural net-based ar- chitecture; <ref type="bibr" target="#b10">Mikolov et al. (2013)</ref>, for example, compute the conditional probability of observ- ing words in a context around a target word (an approach known as the skip-gram model). Re- cent studies have shown that, compared to their co-occurrence counterparts, neural word vectors reflect better the semantic relationships between words ( <ref type="bibr" target="#b0">Baroni et al., 2014</ref>) and are more effective in compositional settings ( <ref type="bibr" target="#b11">Milajevs et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Syntactic awareness</head><p>Since the main purpose of distributional models until now was to measure the semantic relatedness of words, relatively little effort has been put into making word vectors aware of information regard- ing the syntactic role under which a word occurs in a sentence. In some cases the vectors are POS- tag specific, so that 'book' as noun and 'book' as verb are represented by different vectors . Furthermore, word spaces in which the context of a target word is de- termined by means of grammatical dependencies <ref type="bibr" target="#b16">(Padó and Lapata, 2007</ref>) are more effective in cap- turing syntactic relations than approaches based on simple word proximity.</p><p>For word embeddings trained in neural settings, syntactic information is not usually taken explic- itly into account, with some notable exceptions. At the lexical level, <ref type="bibr" target="#b6">Levy and Goldberg (2014)</ref> propose an extension of the skip-gram model based on grammatical dependencies. Following a different approach, <ref type="bibr" target="#b13">Mnih and Kavukcuoglu (2013)</ref> weight the vector of each context word depending on its distance from the target word. With regard to compositional settings (discussed in the next section), <ref type="bibr">Hashimoto et al. (2014)</ref> use dependency- based word embeddings by employing a hinge loss objective, while <ref type="bibr">Hermann and Blunsom (2013)</ref> condition their objectives on the CCG types of the involved words.</p><p>As we will see in Section 3, the current paper offers an appealing alternative to those approaches that does not depend on grammatical relations or types of any form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Compositionality in distributional models</head><p>The methods that aim to equip distributional mod- els of meaning with compositional abilities come in many different levels of sophistication, from simple element-wise vector operators such as ad- dition and multiplication <ref type="bibr" target="#b12">(Mitchell and Lapata, 2008)</ref> to category theory ( <ref type="bibr">Coecke et al., 2010)</ref>. In this latter work relational words (such as verbs or adjectives) are represented as multi-linear maps acting on vectors representing their arguments (nouns and noun phrases). In general, the above models are shallow in the sense that they do not have functional parameters and the output is pro- duced by the direct interaction of the inputs; yet they have been shown to capture the compositional meaning of sentences to an adequate degree.</p><p>The idea of using neural networks for compo- sitionality in language appeared 25 years ago in a seminal paper by <ref type="bibr" target="#b18">Pollack (1990)</ref>, and has been recently re-popularized by Socher and colleagues <ref type="bibr" target="#b22">(Socher et al., 2011a;</ref><ref type="bibr" target="#b24">Socher et al., 2012</ref>). The compositional architecture used in these works is that of a recursive neural network (RecNN) <ref type="bibr" target="#b23">(Socher et al., 2011b)</ref>, where the words get com- posed by following a parse tree. A particular variant of the RecNN is the recurrent neural net- work (RNN), in which a sentence is assumed to be generated by aggregating words in sequence ( <ref type="bibr" target="#b9">Mikolov et al., 2010)</ref>. Furthermore, some re- cent work <ref type="bibr" target="#b4">(Kalchbrenner et al., 2014</ref>) models the meaning of sentences by utilizing the concept of a convolutional neural network ( <ref type="bibr" target="#b5">LeCun et al., 1998</ref>), the main characteristic of which is that it acts on small overlapping parts of the input vectors. In all the above models, the word embeddings and the weights of the compositional layers are optimized against a task-specific objective function.</p><p>In Section 3 we will show how to remove the restriction of a supervised setting, introduc- ing a generic objective that can be trained on any general-purpose text corpus. While we focus on recursive and recurrent neural network architec- tures, the general ideas we will discuss are in prin- ciple model-independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Disambiguation in composition</head><p>Regardless of the way they address composition, all the models of Section 2.3 rely on ambiguous word spaces, in which every meaning of a poly- semous word is merged into a single vector. Es- pecially for cases of homonymy (such as 'bank', 'organ' and so on), where the same word is used to describe two or more completely unrelated con- cepts, this approach is problematic: the semantic representation of the word becomes the average of all senses, inadequate to express any of them in a reliable way.</p><p>To address this problem, a prior disambiguation step on the word vectors is often introduced, the purpose of which is to find the word representa- tions that best fit to the given context, before com- position takes place <ref type="bibr" target="#b20">(Reddy et al., 2011;</ref>. This idea has been tested on algebraic and tensor-based compositional func- tions with very positive results. Furthermore, it has been also found to provide minimal benefits for a RecNN compositional architecture in a num- ber of phrase and sentence similarity tasks ( <ref type="bibr">Cheng et al., 2014</ref>). This latter work clearly suggests that explicitly dealing with lexical ambiguity in a deep compositional setting is an idea that is worth to be further explored. While treating disambiguation as only a preprocessing step is a strategy less than optimal for a neural setting, one would expect that the benefits should be greater for an architecture in which the disambiguation takes place in a dy- namic fashion during training.</p><p>We are now ready to start detailing a compo- sitional model that takes into account the above considerations. The issue of lexical ambiguity is covered in Section 4; Section 3 below deals with generic training and syntactic awareness.   </p><formula xml:id="formula_0">p = g(Wx [1:2] + b)<label>(1)</label></formula><p>where x <ref type="bibr">[1:2]</ref> denotes the concatenation of the two vectors, g is a non-linear function, and W, b are the parameters of the model. In the RecNN case, the compositional process continues recursively by following a parse tree until a vector for the whole sentence or phrase is produced; on the other hand, an RNN assumes that a sentence is gener- ated in a left-to-right fashion, taking into consider- ation no dependencies other than word adjacency. We amend the above setting by introducing a novel layer on the top of the compositional one, which scores the linguistic plausibility of the com- posed sentence or phrase vector with regard to both syntax and semantics. Following <ref type="bibr">Collobert and Weston (2008)</ref>, we convert the unsupervised learning problem to a supervised one by corrupt- ing training sentences. Specifically, for each sen- tence s we create two sets of negative examples. In the first set, S , the target word within a given context is replaced by a random word; as in the original C&amp;W paper, this set is used to enforce semantic coherence in the word vectors. Syntac- tic coherence is enforced by a second set of nega- tive examples, S , in which the words of the con- text have been randomly shuffled. The objective function is defined in terms of the following hinge losses:</p><formula xml:id="formula_1">s∈S s ∈S max(0, m − f (s) + f (s )) (2) s∈S s ∈S max(0, m − f (s) + f (s )) (3)</formula><p>where S is the set of sentences, f the composi- tional layer, and m a margin we wish to retain between the scores of the positive training ex- amples and the negative ones. During training, all parameters in the scoring layer, the composi- tional layers and word representations are jointly updated by error back-propagation. As output, we get both general-purpose syntax-aware word representations and weights for the corresponding compositional model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">From words to senses</head><p>We now extend our model to address lexical ambi- guity. We achieve that by applying a gated archi- tecture, similar to the one used in the multi-sense model of <ref type="bibr" target="#b15">Neelakantan et al. (2014)</ref>, but advancing the main idea to the compositional setting detailed in Section 3.</p><p>We assume a fixed number of n senses per word. 1 Each word is associated with a main vector (obtained for example by using an existing vector set, or by simply applying the process of Section 3 in a separate step), as well as with n vectors de- noting cluster centroids and an equal number of sense vectors. Both cluster centroids and sense vectors are randomly initialized in the beginning of the process. For each word w t in a training sen- tence, we prepare a context vector by averaging the main vectors of all other words in the same context. This context vector is compared with the cluster centroids of w t by cosine similarity, and the sense corresponding to the closest cluster is se- lected as the most representative of w t in the cur- rent context. The selected cluster centroid is up- dated by the addition of the context vector, and the associated sense vector is passed as input to the compositional layer. The selected sense vectors for each word in the sentence are updated by back- propagation, based on the objectives of Equations 2 and 3. The overall architecture of our model, as described in this and the previous section, is illus- trated in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Task-specific dynamic disambiguation</head><p>The model of <ref type="figure">Figure 2</ref>  a specific task, and as a consequence from any task-specific training dataset. However, note that by replacing the plausibility layer with a classi- fier trained for some task at hand, you get a task- specific network that transparently trains multi- sense word embeddings and applies dynamic dis- ambiguation on the fly. While this idea of a single- step direct training seems appealing, one consid- eration is that the task-specific dataset used for the training will not probably reflect the linguistic va- riety that is required to exploit the expressiveness of the setting in its full. Additionally, in many cases the size of datasets tied to specific tasks is prohibiting for training a deep architecture. It is a merit of this proposal that, in cases like these, it is possible for one to train the generic model of <ref type="figure">Figure 2</ref> on any large corpus of text, and then use the produced word vectors and compo- sitional weights to initialize the parameters of a more specific version of the architecture. As a result, the trained parameters will be further re- fined according to the task-specific objective. <ref type="figure" target="#fig_4">Fig- ure 3</ref> illustrates the generic case of a composi- tional framework applying dynamic disambigua- tion. Note that here sense selection takes place by a soft-max layer, which can be directly optimized on the task objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">A siamese network for paraphrase detection</head><p>We will test the dynamic disambiguation frame- work of Section 5 in a paraphrase detection task. A paraphrase is a restatement of the meaning of a sentence using different words and/or syntax. The goal of a paraphrase detection model, thus, is to examine two sentences and decide if they express the same meaning. While the usual way to approach this problem is to utilize a classifier that acts (for example) on the concatenation of the two sentence vectors, in this work we follow a novel perspective: specifically, we apply a siamese architecture <ref type="figure" target="#fig_1">(Bromley et al.,  1993)</ref>, a concept that has been extensively used in computer vision ( <ref type="bibr">Hadsell et al., 2006;</ref><ref type="bibr" target="#b25">Sun et al., 2014</ref>). While siamese networks have been also used in the past for NLP purposes (for example, by <ref type="bibr" target="#b27">Yih et al. (2011)</ref>), to the best of our knowledge this is the first time that such a setting is applied for paraphrase detection.</p><p>In our model, two networks sharing the same parameters are used to compute the vectorial rep- resentations of two sentences, the paraphrase rela- tion of which we wish to detect; this is achieved by employing a cost function that compares the two vectors. There are two commonly used cost func- tions: the first is based on the L 2 norm ( <ref type="bibr">Hadsell et al., 2006;</ref><ref type="bibr" target="#b25">Sun et al., 2014</ref>), while the second on the cosine similarity ( <ref type="bibr" target="#b14">Nair and Hinton, 2010;</ref><ref type="bibr" target="#b25">Sun et al., 2014</ref>). The L 2 norm variation is capable of handling differences in the magnitude of the vec- tors. Formally, the cost function is defined as:</p><formula xml:id="formula_2">E f = 1 2 f (s1) − f (s2) 2 2 , if y = 1 1 2 max(0, m − f (s1) − f (s2) 2 ) 2 , o.w.</formula><p>where s 1 , s 2 are the input sentences, f the com- positional layer (so f (s 1 ) and f (s 2 ) refer to sen- tence vectors), and y = 1 denotes a paraphrase re- lationship between the sentences; m stands for the margin, a hyper-parameter chosen in advance. On  the other hand, the cost function based on cosine similarity handles only directional differences, as follows:</p><formula xml:id="formula_3">E f = 1 2 (y − σ(wd + b)) 2<label>(4)</label></formula><formula xml:id="formula_4">where d = f (s 1 )·f (s 2 ) f (s 1 ) 2 f (s 2 ) 2</formula><p>is the cosine similar- ity of the two sentence vectors, w and b are the scaling and shifting parameters to be optimized, σ is the sigmoid function and y is the label. In the experiments that will follow in Section 7.4, both of these cost functions are evaluated. The overall architecture is shown in <ref type="figure" target="#fig_5">Figure 4</ref>.</p><p>In Section 7.4 we will use the pre-trained vec- tors and compositional weights for deriving sen- tence representations that will be subsequently fed to the siamese network. When the dynamic disam- biguation framework is used, the sense vectors of the words are updated during training so that the sense selection process is gradually refined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>We evaluate the quality of the compositional word vectors and the proposed deep compositional framework in the tasks of word similarity and paraphrase detection, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Model pre-training</head><p>In all experiments the word representations and compositional models are pre-trained on the British National Corpus (BNC), a general-purpose text corpus that contains 6 million sentences of written and spoken English. For comparison we train two sets of word vectors and compositional models, one ambiguous and one multi-sense (fix- ing 3 senses per word). The dimension of the em- beddings is set to 300.</p><p>As our compositional architectures we use a RecNN and an RNN. In the RecNN case, the words are composed by following the result of an external parser, while for the RNN the composi- tion takes place in sequence from left to right. To avoid the exploding or vanishing gradient problem ( <ref type="bibr" target="#b1">Bengio et al., 1994)</ref> for long sentences, we em- ploy a long short-term memory (LSTM) network <ref type="bibr">(Hochreiter and Schmidhuber, 1997)</ref>. During the training of each model, we minimize the hinge loss in Equations 2 and 3. The plausibility layer is im- plemented as a 2-layer network, with 150 units at the hidden layer, and is applied at each individ- ual node (as opposed to a single application at the sentence level). All parameters are updated with mini-batches by AdaDelta (Zeiler, 2012) gradient descent method (λ = 0.03, initial α = 0.05).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Qualitative evaluation of the word vectors</head><p>As a first step, we qualitatively evaluate the trained word embeddings by examining the nearest neigh- bours lists of a few selected words. We com- pare the results with those produced by the skip- gram model (SG) of <ref type="bibr" target="#b10">Mikolov et al. (2013)</ref> and the language model (CW) of <ref type="bibr">Collobert and Weston (2008)</ref>. We refer to our model as SAMS (Syntax- Aware Multi-Sense). The results in <ref type="table">Table 1</ref> show clearly that our model tends to group words that are both semantically and syntactically related; for example, and in contrast with the compared mod- els which group words only at the semantic level, our model is able to retain tenses, numbers (singu- lars and plurals), and gerunds.</p><p>The observed behaviour is comparable to that of embedding models with objective functions con- ditioned on grammatical relations between words; <ref type="bibr" target="#b6">Levy and Goldberg (2014)</ref>, for example, present a similar table for their dependency-based extension of the skip-gram model. The advantage of our ap- proach against such models is twofold: firstly, the word embeddings are accompanied by a generic compositional model that can be used for creat- ing sentence representations independently of any specific task; and secondly, the training is quite forgiving to data sparsity problems that in gen- eral a dependency-based approach would intensify (since context words are paired with the grammati- cal relations they occur with the target word). As a result, a small corpus such as the BNC is sufficient for producing high quality syntax-aware word em- beddings.  <ref type="table">Table 1</ref>: Nearest neighbours for a number of words with various embedding models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Word similarity</head><p>We now proceed to a quantitative evaluation of our embeddings on the Stanford Contextual Word Similarity (SCWS) dataset of <ref type="bibr">Huang et al. (2012)</ref>. The dataset contains 2,003 pairs of words and the contexts they occur in. We can therefore make use of the contextual information in order to select the most appropriate sense for each ambiguous word. Similarly to <ref type="bibr" target="#b15">Neelakantan et al. (2014)</ref>, we use three different metrics: globalSim measures the similarity between two ambiguous word vec- tors; localSim selects a single sense for each word based on the context and computes the similarity between the two sense vectors; avgSim represents each word as a weighted average of all senses in the given context and computes the similarity be- tween the two weighted sense vectors. We compute and report the Spearman's corre- lation between the embedding similarities and hu- man judgments <ref type="table">(Table 2</ref>). In addition to the skip- gram and Collobert and Weston models, we also compare against the CBOW model ( <ref type="bibr" target="#b10">Mikolov et al., 2013</ref>) and the multi-sense skip-gram (MSSG) model of <ref type="bibr" target="#b15">Neelakantan et al. (2014)</ref>.  <ref type="table">Table 2</ref>: Results for the word similarity task (Spearman's ρ × 100).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model globalSim localSim avgSim</head><p>Among all methods, only the MSSG model and ours are capable of learning multi-prototype word representations. Our embeddings show top per- formance for localSim and avgSim measures, and performance competitive to that of MSSG and SG for globalSim, both of which use a hierarchical soft-max as their objective function. Compared to the original C&amp;W model, our version presents an improvement of 4.6%-a clear indication for the effectiveness of the proposed learning method and the enhanced objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Paraphrase detection</head><p>In the last set of experiments, the proposed com- positional distributional framework is evaluated on the Microsoft Research Paraphrase Corpus (MSRPC) <ref type="bibr">(Dolan and Brockett, 2005</ref>), which con- tains 5,800 pairs of sentences. This is a binary classification task, with labels provided by human annotators. We apply the siamese network detailed in Section 6.</p><p>While MSRPC is one of the most used datasets for evaluating paraphrase detection models, its size is prohibitory for any attempt of training a deep architecture. Therefore, for our training we rely on a much larger external dataset, the Paraphrase Database (PPDB) ( <ref type="bibr">Ganitkevitch et al., 2013</ref>). The PPDB contains more than 220 million paraphrase pairs, of which 73 million are phrasal paraphrases and 140 million are paraphrase pat- terns that capture syntactic transformations of sen- tences. We use these phrase-and sentence-level paraphrase pairs as additional training contexts to fine-tune the generic compositional model pa- rameters and word embeddings and to train the baseline models. The original training set of the MSRPC is used as validation set for deciding hy- perparameters, such as the margin of the error function and the number of training epochs.</p><p>The evaluations were conducted on various as- pects, and the models are gradually refined to demonstrate performance within the state-of-the- art range.</p><p>Comparison of the two error functions In the first evaluation, we compare the two error func- tions of the siamese network using only ambigu-ous vectors. As we can see in <ref type="table">Table 3</ref>, the co- sine error function consistently outperforms the L 2 norm-based one for both compositional mod- els, providing a yet another confirmation of the already well-established fact that similarity in se- mantic vector spaces is better reflected by length- invariant measures.  <ref type="table">Table 3</ref>: Results with different error functions for the paraphrase detection task (accuracy × 100).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Effectiveness of disambiguation We now pro- ceed to compare the effectiveness of the two com- positional models when using ambiguous vectors and multi-sense vectors, respectively. Our error function is set to cosine similarity, following the results of the previous evaluation. When dynamic disambiguation is applied, we test two methods of selecting sense vectors: in the hard case the vector of the most plausible sense is selected, while in the soft case a new vector is prepared as the weighted average of all sense vectors according to proba- bilities returned by the soft-max layer (see <ref type="figure" target="#fig_4">Figure  3)</ref>. As a baseline we use a simple compositional model based on vector addition. The dynamic disambiguation models and the additive baseline are compared with variations that use a simple prior disambiguation step applied on the word vectors. This is achieved by first se- lecting for each word the sense vector that is the closest to the average of all other word vectors in the same sentence, and then composing the se- lected sense vectors without further considerations regarding ambiguity. The baseline model and the prior disambiguation variants are trained as sepa- rate logistic regression classifiers. The results are shown in  <ref type="table" target="#tab_3">Table 4</ref>: Different disambiguation choices for the paraphrase detection task (accuracy × 100).</p><p>Overall, disambiguated vectors work better than the ambiguous ones, with the improvement to be more significant for the additive model; there, a simple prior disambiguation step produces 1.4% gains. For the deep compositional models, simple prior disambiguation is still helpful with small im- provements, a result which is consistent with the findings of <ref type="bibr">Cheng et al. (2014)</ref>. The small gains of the prior disambiguation models over the am- biguous models clearly show that deep architec- tures are quite capable of performing this elemen- tary form of sense selection intrinsically, as part of the learning process itself. However, the situ- ation changes when the dynamic disambiguation framework is used, where the gains over the am- biguous version become more significant. Com- paring the two ways of dynamic disambiguation (hard method and soft method), the numbers that the soft method gives are slightly higher, produc- ing a total gain of 1.1% over the ambiguous ver- sion for the RecNN case. <ref type="bibr">2</ref> Note that, at this stage, the advantage of us- ing the dynamic disambiguation framework over simple prior disambiguation is still small (0.7% for the case of RecNN). We seek the reason be- hind this in the recursive nature of our architecture, which tends to progressively "hide" local features of word vectors, thus diminishing the effect of the fine-tuned sense vectors produced by the dynamic disambiguation mechanism. The next section dis- cusses the problem and provides a solution.</p><p>The role of pooling One of the problems of the recursive and recurrent compositional architec- tures, especially in grammars with strict branching structure such as in English, is that any given com- position is usually the product of a terminal and a non-terminal; i.e. a single word can contribute to the meaning of a sentence to the same extent as the rest of a sentence on its whole, as below:</p><formula xml:id="formula_5">[[kids] NP [play ball games in the park] VP ] S</formula><p>In the above case, the contribution of the words within the verb phrase to the final sentence rep- resentation will be faded out due to the recursive composition mechanism. Inspired by related work in computer vision ( <ref type="bibr" target="#b25">Sun et al., 2014)</ref>, we attempt to alleviate this problem by introducing an aver- age pooling layer at the sense vector level and adding the resulting vector to the sentence repre- sentation. By doing this we expect that the new sentence vector will reflect local features from all words in the sentence that can help in the clas- sification in a more direct way. The results for the new deep architectures are shown in <ref type="table">Table 5</ref>, where we see substantial improvements for both deep nets. More importantly, the effect of dynamic disambiguation now becomes more significant, as expected by our analysis. <ref type="table">Table 5</ref> also includes results for two models trained in a single step, with word and sense vec- tors randomly initialized at the beginning of the process. We see that, despite the large size of the training set, the results are much lower than the ones obtained when using the pre-training step. This demonstrates the importance of the initial training on a general-purpose corpus: the result- ing vectors reflect linguistic information that, al- though not obtainable from the task-specific train- ing, can make great difference in the result of the classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Ambig  <ref type="table">Table 5</ref>: Results with average pooling for the para- phrase detection task (accuracy × 100).</p><p>Cross-model comparison In this section we propose a method to further improve the perfor- mance of our models, and we present an evaluation against some of the previously reported results. We notice that using distributional properties alone cannot capture efficiently subtle aspects of a sentence, for example numbers or human names. However, even small differences on those aspects between two sentences can lead to a different clas- sification result. Therefore, we train (using the MSPRC training data) an additional logistic re- gression classifier which is based not only on the embeddings similarity, but also on a few hand- engineered features. We then ensemble the new classifier (C1) with the original one. In terms of feature selection, we follow <ref type="bibr" target="#b22">Socher et al. (2011a)</ref> and <ref type="bibr">Blacoe and Lapata (2012)</ref> and add the fol- lowing features: the difference in sentence length, the unigram overlap among the two sentences, fea- tures related to numbers (including the presence or absence of numbers from a sentence and whether or not the numbers in the two sentences are the same). In <ref type="table">Table 6</ref> we report results of the original model and the ensembled model, and we compare with the performance of other existing models.</p><p>In all of the implemented models (including the additive baseline), disambiguation is performed to guarantee the best performance. We see that by ensembling the original classifier with C1, we im- prove the result of the previous section by another 1%. This is the second best result reported so far  <ref type="formula" target="#formula_0">(2013)</ref> 80.4 85.9 <ref type="table">Table 6</ref>: Cross-model comparison in the para- phrase detection task.</p><p>for the specific task, with a 0.6 difference in F- score from the first ( <ref type="bibr">Ji and Eisenstein, 2013)</ref>. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and future work</head><p>The main contribution of this paper is a deep com- positional distributional model acting on linguis- tically motivated word embeddings. <ref type="bibr">4</ref> The effec- tiveness of the syntax-aware, multi-sense word vectors and the dynamic compositional disam- biguation framework in which they are used was demonstrated by appropriate tasks at the lexical and sentence level, respectively, with very posi- tive results. As an aside, we also demonstrated the benefits of a siamese architecture in the context of a paraphrase detection task. While the architec- tures tested in this work were limited to a RecNN and an RNN, the ideas we presented are in prin- ciple directly applicable to any kind of deep net- work. As a future step, we aim to test the proposed models on a convolutional compositional architec- ture, similar to that of <ref type="bibr" target="#b4">Kalchbrenner et al. (2014)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Recursive (a) and recurrent (b) neural networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 shows</head><label>1</label><figDesc>Figure 1 shows the general form of recursive and recurrent neural networks. In architectures of this form, a compositional layer is applied on each pair of inputs x 1 and x 2 in the following way:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 2: Training of syntax-aware multi-sense embeddings in the context of a RecNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Dynamic disambiguation in a generic compositional deep net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A siamese network for paraphrase detection.</figDesc><graphic url="image-250.png" coords="6,83.27,79.28,195.74,135.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>Model 
Ambig. Prior Hard DD Soft DD 
Addition 
69.9 
71.3 
-
-
RecNN 
74.9 
75.3 
75.7 
76.0 
RNN 
74.3 
74.6 
75.1 
75.2 

</table></figure>

			<note place="foot" n="3"> Syntax-based generic training We propose a novel architecture for learning word embeddings and a compositional model to use them in a single step. The learning takes places in the context of a RecNN (or an RNN), and both word embeddings and parameters of the compositional layer are optimized against a generic objective function that uses a hinge loss function.</note>

			<note place="foot" n="1"> Note that in principle the fixed number of senses assumption is not necessary; Neelakantan et al. (2014), for example, present a version of their model in which new senses are added dynamically when appropriate.</note>

			<note place="foot" n="2"> For all subsequent experiments, the reported results are based on the soft selection method.</note>

			<note place="foot" n="3"> Source: ACL Wiki (http://www.aclweb.org/aclwiki), August 2015. 4 Code in Python/Theano and the word embeddings can be found at https://github.com/cheng6076.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the three anony-mous reviewers for their useful comments, as well as Nal Kalchbrenner and Ed Grefenstette for early discussions and suggestions on the paper, and Si-moň Suster for comments on the final draft. Dim-itri Kartsaklis gratefully acknowledges financial support by AFOSR.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Prior disambiguation of word tensors for constructing sentence vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1590" to="1601" />
		</imprint>
	</monogr>
	<note>Seattle, Washington, USA, October. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Separating disambiguation from composition in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 17th Conference on Natural Language Learning (CoNLL)</title>
		<meeting>17th Conference on Natural Language Learning (CoNLL)<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="114" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Resolving lexical ambiguity in tensor regression models of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="212" to="217" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Re-examining machine translation metrics for paraphrase identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="182" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Corpus-based and knowledge-based measures of text semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Corley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="775" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Makuhari, Chiba, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evaluating neural word representations in tensor-based compositional settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrijs</forename><surname>Milajevs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="708" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="236" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient nonparametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dependency-based Construction of Semantic Space Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="199" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recursive distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan B Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="105" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Paraphrase recognition via dissimilarity significance classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="18" to="26" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic and static prototype vectors for semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Klapaftis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="705" to="713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Paraphrase identification with lexicosyntactic graph subsumption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vasile Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mihai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><forename type="middle">S</forename><surname>Lintean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur C</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graesser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FLAIRS conference</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="201" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<title level="m">Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ng</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1988" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Using dependency-based features to take the para-farce out of paraphrase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cécile</forename><surname>Paris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Australasian Language Technology Workshop</title>
		<meeting>the Australasian Language Technology Workshop</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning discriminative projections for text similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fifteenth Conference on Computational Natural Language Learning<address><addrLine>Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="247" to="256" />
		</imprint>
	</monogr>
	<note>Portland. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
