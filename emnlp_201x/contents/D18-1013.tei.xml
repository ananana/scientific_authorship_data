<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">simNet: Stepwise Image-Topic Merging Network for Generating Detailed and Comprehensive Image Captions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenglin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of ICE</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanxin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of ICE</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">simNet: Stepwise Image-Topic Merging Network for Generating Detailed and Comprehensive Image Captions</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="137" to="149"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>137</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The encode-decoder framework has shown recent success in image captioning. Visual attention , which is good at detailedness, and semantic attention, which is good at comprehensive-ness, have been separately proposed to ground the caption on the image. In this paper, we propose the Stepwise Image-Topic Merging Network (simNet) that makes use of the two kinds of attention at the same time. At each time step when generating the caption, the de-coder adaptively merges the attentive information in the extracted topics and the image according to the generated context, so that the visual information and the semantic information can be effectively combined. The proposed approach is evaluated on two benchmark datasets and reaches the state-of-the-art performances. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image captioning attracts considerable attention in both natural language processing and computer vi- sion. The task aims to generate a description in natural language grounded on the input image. It is a very challenging yet interesting task. On the one hand, it has to identify the objects in the im- age, associate the objects, and express them in a fluent sentence, each of which is a difficult sub- task. On the other hand, it combines two impor- tant fields in artificial intelligence, namely, natural language processing and computer vision. More importantly, it has a wide range of applications, including text-based image retrieval, helping visu- ally impaired people see ( <ref type="bibr" target="#b39">Wu et al., 2017)</ref>, human- robot interaction ( <ref type="bibr" target="#b8">Das et al., 2017)</ref>, etc.</p><p>Models based on the encoder-decoder frame- work have shown success in image captioning. According to the pivot representation, they can be * Equal Contributions <ref type="bibr">1</ref> The code is available at https://github.com/ lancopku/simNet Soft-Attention: a open laptop computer sitting on top of a ta- ble ATT-FCN: a dog sitting on a desk with a laptop computer and mouse simNet: a open laptop com- puter and mouse sitting on a ta- ble with a dog nearby <ref type="figure">Figure 1</ref>: Examples of using different attention mecha- nisms. Soft-Attention ( <ref type="bibr" target="#b42">Xu et al., 2015</ref>) is based on vi- sual attention. The generated caption is detailed in that it knows the visual attributes <ref type="bibr">well (e.g. open)</ref>. How- ever, it omits many objects (e.g. mouse and dog). ATT- FCN ( <ref type="bibr" target="#b44">You et al., 2016</ref>) is based on semantic attention. The generated caption is more comprehensive in that it includes more objects. However, it is bad at associ- ating details with the objects (e.g. missing open and mislocating dog). simNet is our proposal that effec- tively merges the two kinds of attention and generates a detailed and comprehensive caption.</p><p>roughly categorized into models based on visual information ( <ref type="bibr" target="#b36">Vinyals et al., 2015;</ref><ref type="bibr" target="#b28">Mao et al., 2014;</ref><ref type="bibr">Li, 2015, 2017)</ref>, and models based on conceptual in- formation <ref type="bibr" target="#b44">You et al., 2016;</ref><ref type="bibr" target="#b38">Wu et al., 2016</ref>). The later explicitly provides the vi- sual words (e.g. dog, sit, red) to the decoder in- stead of the image features, and is more effective in image captioning according to the evaluation on benchmark datasets. However, the models based on conceptual information have a major drawback that it is hard for the model to associate the details with the specific objects in the image, because the visual words are inherently unordered in seman- tics. <ref type="figure">Figure 1</ref> shows an example. For semantic attention, although open is provided as a visual word, due to the insufficient use of visual infor- mation, the model gets confused about what ob- jects open should be associated with and thus dis- cards open in the caption. The model may even associate the details incorrectly, which is the case   for the position of the dog. In contrast, models based on the visual information often are accurate in details but have difficulty in describing the im- age comprehensively and tend to only describe a subregion.</p><p>In this work, we get the best of both worlds and integrate visual attention and semantic attention for generating captions that are both detailed and comprehensive. We propose a Stepwise Image- Topic Merging Network as the decoder to guide the information flow between the image and the extracted topics. At each time step, the decoder first extracts focal information from the image. Then, it decides which topics are most probable for the time step. Finally, it attends differently to the visual information and the conceptual informa- tion to generate the output word. Hence, the model can efficiently merge the two kinds of information, leading to outstanding results in image captioning.</p><p>Overall, the main contributions of this work are:</p><p>• We propose a novel approach that can effec- tively merge the information in the image and the topics to generate cohesive captions that are both detailed and comprehensive. We re- fine and combine two previous competing at- tention mechanisms, namely visual attention and semantic attention, with an importance- based merging gate that effectively combines and balances the two kinds of information.</p><p>• The proposed approach outperforms the state-of-the-art methods substantially on two benchmark datasets, Flickr30k and COCO, in terms of SPICE, which correlates the best with human judgments. Systematic analysis shows that the merging gate contributes the most to the overall improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>A large number of systems have been proposed for image captioning. Neural models based on the encoder-decoder framework have been attract- ing increased attention in the last few years in several multi-discipline tasks, such as neural im- age/video captioning (NIC) and visual question answering (VQA) ( <ref type="bibr" target="#b36">Vinyals et al., 2015;</ref><ref type="bibr" target="#b14">Karpathy and Li, 2015;</ref><ref type="bibr" target="#b35">Venugopalan et al., 2015;</ref><ref type="bibr" target="#b48">Zhao et al., 2016;</ref>. State-of-the- art neural approaches ( <ref type="bibr" target="#b1">Anderson et al., 2018;</ref><ref type="bibr" target="#b24">Liu et al., 2018;</ref><ref type="bibr" target="#b26">Lu et al., 2018)</ref> incorporate the atten- tion mechanism in machine translation ( <ref type="bibr" target="#b2">Bahdanau et al., 2014</ref>) to generate grounded image captions. Based on what they attend to, the models can be categorized into visual attention models and se- mantic attention models. Visual attention models pay attention to the im- age features generated by CNNs. CNNs are typ- ically pre-trained on the image recognition task to extract general visual signals ( <ref type="bibr" target="#b42">Xu et al., 2015;</ref><ref type="bibr" target="#b25">Lu et al., 2017)</ref>. The visual at- tention is expected to find the most relevant image regions in generating the caption. Most recently, image features based on predicted bounding boxes are used ( <ref type="bibr" target="#b1">Anderson et al., 2018;</ref><ref type="bibr" target="#b26">Lu et al., 2018)</ref>. The advantages are that the attention no longer needs to find the relevant generic regions by itself but instead find relevant bounding boxes that are object orientated and can serve as semantic guides. However, the drawback is that predicting bound- ing boxes is difficult, which requires large datasets ( <ref type="bibr" target="#b17">Krishna et al., 2017</ref>) and complex models <ref type="bibr" target="#b30">(Ren et al., 2015</ref><ref type="bibr" target="#b31">(Ren et al., , 2017a</ref>.</p><p>Semantic attention models pay attention to a predicted set of semantic concepts <ref type="bibr" target="#b44">You et al., 2016;</ref><ref type="bibr" target="#b38">Wu et al., 2016</ref>   of approach can be seen as the extension of the earlier template-based slotting-filling approaches <ref type="bibr" target="#b10">(Farhadi et al., 2010;</ref><ref type="bibr" target="#b18">Kulkarni et al., 2013)</ref>.</p><p>However, few work studies how to combine the two kinds of attention models to take advantage of both of them. On the one hand, due to the lim- ited number of visual features, it is hard to provide comprehensive information to the decoder. On the other hand, the extracted semantic concepts are unordered, making it hard for the decoder to por- tray the details of the objects correctly.</p><p>This work focuses on combining the visual at- tention and the semantic attention efficiently to ad- dress their drawbacks and make use of their mer- its. The visual attention is designed to focus on the attributes and the relationships of the objects, while the semantic attention only includes words that are objects so that the extracted topics could be more accurate. The combination is controlled by the importance-based merging mechanism that decides at each time step which kind of informa- tion should be relied on. The goal is to generate image captions that are both detailed and compre- hensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Our proposed model consists of an image encoder, a topic extractor, and a stepwise merging decoder. <ref type="figure" target="#fig_3">Figure 3</ref> shows a sketch. We first briefly introduce the image encoder and the topic extractor. Then, we introduce the proposed stepwise image-topic merging decoder in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Encoder</head><p>For an input image, the image encoder expresses the image as a series of visual feature vectors V = {v 1 , v 2 , . . . , v k }, v i ∈ R g . Each feature cor- responds to a different perspective of the image. The visual features serve as descriptive guides of the objects in the image for the decoder. We use a ResNet152 ( , which is commonly used in image captioning, to generate the visual features. The output of the last convolutional layer is used as the visual information:</p><formula xml:id="formula_0">V = W V,I CNN(I) (1)</formula><p>where I is the input image, and W V,I shrinks the last dimension of the output. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Topic Extractor</head><p>Typically, identifying an object requires a com- bination of visual features, and considering the limited capacity of the visual features, it is hard for the conventional decoder to describe the ob- jects in the image comprehensively. An advance in image captioning is to provide the decoder with the semantic concepts in the image directly so that the decoder is equipped with an overall per- spective of the image. The semantic concepts can be objects (e.g. person, car), attributes (e.g. off, electric), and relationships (e.g. using, sit- ting). We only use the words that are objects in this work, the reason of which is explained later. We call such words topics. The topic ex- tractor concludes a list of candidate topic embed- Different from existing work that uses all the most frequent words in the captions as valid se- mantic concepts or visual words, we only include the object words (nouns) in the topic word list. Existing work relies on attribute words and rela-tionship words to provide visual information to the decoder. However, it not only complicates the ex- tracting procedure but also contributes little to the generation. For an image containing many objects, the decoder is likely to combine the attributes with the objects arbitrarily, as such words are specific to certain objects but are provided to the decoder unordered. In contrast, our model has visual infor- mation as additional input and we expect that the decoder should refer to the image for such kind of information instead of the extracted concepts.</p><formula xml:id="formula_1">dings T = {w 1 , w 2 , . . . , w m }, w i ∈ R e from the image,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Stepwise Image-Topic Merging Decoder</head><p>The essential component of the decoder is the pro- posed stepwise image-topic merging network. The decoder is based on an LSTM <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber, 1997)</ref>. At each time step, it com- bines the textual caption, the attentive visual in- formation, and the attentive conceptual informa- tion as the context for generating an output word. The goal is achieved by three modules, the visual attention, the topic attention, and the merging gate.</p><p>Visual Attention as Output The visual atten- tion attends to attracting parts of the image based on the state of the LSTM decoder. In existing work ( <ref type="bibr" target="#b42">Xu et al., 2015)</ref>, only the previous hidden state h t−1 ∈ R d of the LSTM is used in computation of the visual attention:</p><formula xml:id="formula_2">Z t = tanh(W Z,V V ⊕ W Z,h h t−1 ) (2) α t = softmax(Z t w α,Z )<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">W Z,V ∈ R k×g , W Z,h ∈ R k×d , w α,Z ∈ R k</formula><p>are the learnable parameters. We denote the matrix-vector addition as ⊕, which is calculated by adding the vector to each column of the matrix. α t ∈ R k is the attentive weights of V and the attentive visual input z t ∈ R g is calculated as</p><formula xml:id="formula_4">z t = V α t<label>(4)</label></formula><p>The visual input z t and the embedding of the pre- vious output word y t−1 are the input of the LSTM.</p><formula xml:id="formula_5">h t = LSTM( z t y t−1 , h t−1 )<label>(5)</label></formula><p>However, there is a noticeable drawback that the previous output word y t−1 , which is a much stronger indicator than the previous hidden state h t−1 , is not used in the attention. As z t is used as the input, we call it input attention. To over- come that drawback, we add another attention that incorporates the current hidden state h t , which is based on the last generated word y t−1 :</p><formula xml:id="formula_6">Z t = tanh( W Z,V V ⊕ W Z,h h t ) (6) α t = softmax( Z t w α,Z ) (7) z t = V α t<label>(8)</label></formula><p>The procedure resembles the input attention, and we call it output attention. It is worth mention- ing that the output attention is essentially the same with the spatial visual attention proposed by <ref type="bibr" target="#b25">Lu et al. (2017)</ref>. However, they did not see it from the input-output point of view nor combine it with the input attention.</p><p>The attentive visual output is further trans- formed to r t = tanh(W s,z z t ), W s,z ∈ R e×g , which is of the same dimension as the topic word embedding to simplify the following procedure.</p><p>Topic Attention In an image caption, different parts concern different topics. In the existing work ( <ref type="bibr" target="#b44">You et al., 2016</ref>), the conceptual information is attended based on the previous output word:</p><formula xml:id="formula_7">β t = softmax(T T U y t−1 )<label>(9)</label></formula><p>where U ∈ R e×e , β t ∈ R m . The profound issue is that this approach neglects the visual information. It should be beneficial to provide the attentive vi- sual information when selecting topics. The hid- den state of the LSTM contains both the informa- tion of previous words and the attentive input vi- sual information. Therefore, the model attends to the topics based on the hidden state of the LSTM:</p><formula xml:id="formula_8">Q t = tanh(W Q,T T ⊕ W Q,h h t )<label>(10)</label></formula><formula xml:id="formula_9">β t = softmax(Q t w β,Q )<label>(11)</label></formula><p>where W Q,T ∈ R m×e , W Q,h ∈ R m×d , w β,Q ∈ R m are the parameters to be learned. β t ∈ R m is the weight of the topics, from which the attentive conceptual output q t ∈ R e is calculated:</p><formula xml:id="formula_10">q t = T β t<label>(12)</label></formula><p>The topic attention q t and the hidden state h t are combined as the contextual information s t :</p><formula xml:id="formula_11">s t = tanh(W s,q q t + W s,h h t )<label>(13)</label></formula><p>where W s,q ∈ R e×e , W s,h ∈ R e×d are learnable parameters.</p><p>Merging Gate We have prepared both the visual information r t and the contextual information s t . It is not reasonable to treat the two kinds of in- formation equally when the decoder generates dif- ferent types of words. For example, when generat- ing descriptive words (e.g., behind, red), r t should matter more than s t . However, when generating object words (e.g., people, table), s t is more im- portant. We introduce a novel score-based merg- ing mechanism to make the model adaptively learn to adjust the balance:</p><formula xml:id="formula_12">γ t = σ(S(s t ) − S(r t )) (14) c t = γ t s t + (1 − γ t )r t (15)</formula><p>where σ is the sigmoid function, γ t ∈ [0, 1] in- dicates how important the topic attention is com- pared to the visual attention, and S is the scoring function. The scoring function needs to evaluate the importance of the topic attention. Noticing that Eq. (10) and Eq. <ref type="formula" target="#formula_9">(11)</ref> have a similar purpose, we define S similarly:</p><formula xml:id="formula_13">S(s t ) = tanh(W S,h h t + W S,s s t ) · w S (16) S(r t ) = tanh(W S,h h t + W S,r r t ) · w S (17)</formula><p>where · denotes dot product of vectors, W S,s ∈ R m×e , W S,r ∈ R m×e are the parameters to be learned, and W S,h , w s share the weights of W Q,h , w β,Q from Eq. (10) and Eq. <ref type="formula" target="#formula_9">(11)</ref>, respec- tively. Finally, the output word is generated by:</p><formula xml:id="formula_14">y t ∼ p t = softmax(W p,c c t )<label>(18)</label></formula><p>where each value of p t ∈ R |D| is a probability in- dicating how likely the corresponding word in vo- cabulary D is the current output word. The whole model is trained using maximum log likelihood and the loss function is the cross entropy loss. In all, our proposed approach encourages the model to take advantage of all the available infor- mation. The adaptive merging mechanism makes the model weigh the information elaborately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>We describe the datasets and the metrics used for evaluation, followed by the training details and the evaluation of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Metrics</head><p>There are several datasets containing images and their captions. We report results on the popular Microsoft COCO ( ) dataset and the Flickr30k ( <ref type="bibr" target="#b45">Young et al., 2014</ref>) dataset. They contain 123,287 images and 31,000 images, re- spectively, and each image is annotated with 5 sen- tences. We report results using the widely-used publicly-available splits in the work of <ref type="bibr" target="#b14">Karpathy and Li (2015)</ref>. There are 5,000 images each in the validation set and the test set for COCO, 1,000 im- ages for Flickr30k.</p><p>We report results using the COCO captioning evaluation toolkit ) that reports the widely-used automatic evaluation metrics SPICE, CIDEr, BLEU, METEOR, and ROUGE. SPICE ( <ref type="bibr" target="#b0">Anderson et al., 2016)</ref>, which is based on scene graph matching, and <ref type="bibr">CIDEr (Vedantam et al., 2015)</ref>, which is based on n-gram match- ing, are specifically proposed for evaluating im- age captioning systems. They both incorporate the consensus of a set of references for an example. BLEU ( <ref type="bibr" target="#b29">Papineni et al., 2002</ref>) and METOR (Baner- jee and <ref type="bibr" target="#b3">Lavie, 2005</ref>) are originally proposed for machine translation evaluation. ROUGE ( <ref type="bibr" target="#b21">Lin and Hovy, 2003;</ref><ref type="bibr" target="#b19">Lin, 2004</ref>) is designed for automatic evaluation of extractive text summarization. In the related studies, it is concluded that SPICE corre- lates the best with human judgments with a re- markable margin over the other metrics, and is expert in judging detailedness, where the other metrics show negative correlations, surprisingly; CIDEr and METEOR follows with no particular precedence, followed by ROUGE-L, and BLEU- 4, in that order ( <ref type="bibr" target="#b0">Anderson et al., 2016;</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Settings</head><p>Following common practice, the CNN used is the ResNet152 model (  pre-trained on ImageNet. <ref type="bibr">3</ref> There are 2048 7 × 7 feature maps, and we project them into 512 feature maps, i.e. g is 512. The word embedding size e is 256 and the hidden size d of the LSTM is 512. We only keep caption words that occur at least 5 times in the training set, resulting in 10,132 words for COCO and 7,544 for Flickr30k. We use the topic ex- tractor pre-trained by  for 1,000 concepts on COCO. We only use 568 manually- annotated object words as topics. For an image, only the top 5 topics are selected, which means m is 5. The same topic extractor is used for Flickr30k, as COCO provides adequate general- ity. The caption words and the topic words share the same embeddings. In training, we first train the model without visual attention (freezing the CNN parameters) for 20 epochs with the batch size of 80. The learning rate for the LSTM is 0.0004. Then, we switch to jointly train the full model with a learning rate of 0.00001, which exponen- tially decays with the number of epochs so that it is halved every 50 epochs. We also use momen-   tum of 0.8 and weight decay of 0.999. We use Adam ( <ref type="bibr" target="#b16">Kingma and Ba, 2014</ref>) for parameter opti- mization. For fair comparison, we adopt early stop based on CIDEr within maximum 50 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>We compare our approach with various represen- tative systems on Flickr30k and COCO, including the recently proposed NBT that is the state-of-the- art on the two datasets in comparable settings. Ta- ble 1 shows the result on Flickr30k. As we can see, our model outperforms the comparable sys- tems in terms of all of the metrics except BLEU-4. Moreover, our model overpasses the state-of-the- art with a comfortable margin in terms of SPICE, which is shown to correlate the best with human judgments ( <ref type="bibr" target="#b0">Anderson et al., 2016)</ref>. <ref type="table" target="#tab_4">Table 2</ref> shows the results on COCO. Among the directly comparable models, our model is arguably the best and outperforms the existing models ex- cept in terms of BLEU-4. Most encouragingly, our model is also competitive with Up-Down <ref type="bibr" target="#b1">(Anderson et al., 2018)</ref>, which uses much larger dataset, Visual Genome ( <ref type="bibr" target="#b17">Krishna et al., 2017)</ref>, with dense annotations to train the object detector, and di- rectly optimizes CIDEr. Especially, our model outperforms the state-of-the-art substantially in SPICE and METEOR. Breakdown of SPICE F- scores over various subcategories (see <ref type="table" target="#tab_6">Table 3</ref>) shows that our model is in dominant lead in almost all subcategories. It proves the effectiveness of our approach and indicates that our model is quite data efficient.</p><p>For the methods that directly optimize CIDEr, it is intuitive that CIDEr can improve signifi- cantly. The similar improvement of BLEU-4 is evidence that optimizing CIDEr leads to more n- gram matching. However, it comes to our notice that the improvements of SPICE, METEOR, and ROUGE-L are far less significant, which suggests there may be a gaming situation where the n-gram matching is wrongfully exploited by the model in reinforcement learning. As shown by , it is most reasonable to jointly optimize      all the metrics at the same time.</p><p>We also evaluate the proposed model on the COCO evaluation server, the results of which are shown in Appendix A.1, due to limited space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>In this section, we analyze the contribution of each component in the proposed approach, and give ex- amples to show the strength and the potential im- provements of the model. The analysis is con- ducted on the test set of COCO.</p><p>Topic Extraction The motivation of using ob- jects as topics is that they are easier to identify so that the generation suffers less from erroneous predictions. This can be proved by the F-score of the identified topics in the test set, which is shown in <ref type="table" target="#tab_7">Table 4</ref>. Using top-5 object words is at least as good as using top-10 all words. However, us- ing top-10 all words introduces more erroneous visual words to the generation. As shown in Ta- ble 5, when extracting all words, providing more words to the model indeed increases the caption- ing performance. However, even when top-20 all words are used, the performance is still far behind using only top-5 object words and seems to reach the performance ceiling. It proves that for seman- tic attention, it is also important to limit the abso- lute number of incorrect visual words instead of merely the precision or the recall. It is also inter- esting to check whether using other kind of words can reach the same effect. Unfortunately, in our experiments, only using verbs or adjectives as se- mantic concepts works poorly.</p><p>To examine the contributions of the sub- modules in our model, we conduct a series of ex- periments. The results are summarized in <ref type="table" target="#tab_6">Table 3</ref>.</p><p>To help with the understanding of the differences, we also report the breakdown of SPICE F-scores.</p><p>Visual Attention Our input attention achieves similar results to previous work ( <ref type="bibr" target="#b42">Xu et al., 2015</ref>  if not better. Using only the output attention is much more effective than using only the in- put attention, with substantial improvements in all metrics, showing the impact of information gap caused by delayed input in attention. Combining the input attention and the output attention can fur- ther improve the results, especially in color and size descriptions.</p><p>Topic Attention As expected, compared with visual attention, the topic attention is better at identifying objects but worse at identifying at- tributes. We also apply the merging gate to the topic attention, but it now merges q t and h t in- stead of s t and r t . With the merging gate, the model can balance the information in caption text and extracted topics, resulting in better overall scores. While it overpasses the conventional vi- sual attention, it lags behind the output attention.</p><p>Merging Gate Combing the visual attention and the topic attention directly indeed results in a huge boost in performance, which confirms our moti- vation. However, directly combining them also causes lower scores in attributes, color, count, and size, showing that the advantages are not fully made use of. The most dramatic improvements come from applying the merging gate to the com- bined attention, showing that the proposed balance mechanism can adaptively combine the two kinds of information and is essential to the overall per- formance. The average merging gate value sum- marized in <ref type="figure" target="#fig_7">Figure 4</ref> suggests the same.</p><p>We give some examples in the left plot of Fig- ure 5 to illustrate the differences between the mod- els more intuitively. From the examples, it is clear that the proposed simNet generates the best cap- tions in that more objects are described and many informative and detailed attributes are included, such as the quantity and the color.</p><p>Visualization <ref type="figure" target="#fig_9">Figure 6</ref> shows the visualization of the topic attention and the visual attention with running examples. As we can see, the topic atten- tion is active when generating a phrase containing the related topic. For example, bathroom is always most attended when generating a bathroom. The merging gate learns to direct the information flow efficiently. When generating words such as on and a, it gives lower weight to the topic attention and prefers the visual attention. As to the visual at- tention, the output attention is much more focused than the input attention. As we hypothesized, the conventional input attention lacks the information of the last generated word and does not know what to look for exactly. For example, when generating bathroom, the input attention does not know the previous generated word is a, and it loses its fo- cus, while the output attention is relatively more concentrated. Moreover, the merging gate learns to overcome the erroneous topics, as shown in the second example. When generating chair, the topic attention is focused on a wrong object bed, while the visual attention attends correctly to the chair, and especially the output attention attends to the armrest. The merging gate effectively remedies  the misleading information from the topic atten- tion and outputs a lower weight, resulting in the model correctly generating the word chair.</p><p>Error Analysis We conduct error analysis using the proposed (full) model on the test set to pro- vide insights on how the model may be improved. We find 123 out of 1000 generated captions that are not satisfactory. There are mainly three types of errors, i.e. distance (32, 26%), movement (22, 18%), and object (60, 49%), with 9 (7%) other er- rors. Distance error takes place when there is a lot of objects and the model cannot grasp the fore- ground and the background relationship. Move- ment error means that the model fails to describe whether the objects are moving. Those two kinds of errors are hard to eliminate, as they are funda- mental problems of computer vision waiting to be resolved. Object error happens when there are in- correct extracted topics, and the merging gate re- gards the topic as grounded in the image. In the given example, the incorrect topic is garden. The tricky part is that the topic is seemingly correct according to the image features or otherwise the proposed model will choose other topics. A more powerful topic extractor may help with the prob- lem but it is unlikely to be completely avoided.  <ref type="table" target="#tab_4">-1  BLEU-2  BLEU-3  BLEU-4  METEOR  ROUGE-L  CIDEr   c5  c40  c5  c40  c5  c40  c5  c40  c5  c40  c5  c40  c5  c40</ref> HardAtt ( <ref type="bibr" target="#b42">Xu et al., 2015)</ref> 0.705 0.881 0.528 0.779 0.383 0.658 0.277 0.537 0.241 0.322 0.516 0.654 0.865 0.893 ATT-FCN ( <ref type="bibr" target="#b44">You et al., 2016)</ref> 0.731 0.900 0.565 0.815 0.424 0.709 0.316 0.599 0.250 0.335 0.535 0.682 0.943 0.958 SCA-CNN (  0.712 0.894 0.542 0.802 0.404 0.691 0.302 0.579 0.244 0.331 0.524 0.674 0.912 0.921 LSTM-A ( <ref type="bibr" target="#b43">Yao et al., 2017)</ref> 0.739 0.919 0.575 0.842 0.436 0.740 0.330 0.632 0.256 0.350 0.542 0.700 0.984 1.003 SCN-LSTM ( <ref type="bibr" target="#b11">Gan et al., 2017)</ref> 0.740 0.917 0.575 0.839 0.436 0.739 0.331 0.631 0.257 0.348 0.543 0.696 1.003 1.013 AdaAtt ( <ref type="bibr" target="#b25">Lu et al., 2017</ref>) † 0.748 0.920 0.584 0.845 0.444 0.744 0.336 0.637 0.264 0.359 0.550 0.705 1.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary Material</head><p>A.1 Results on COCO Evaluation Server <ref type="table" target="#tab_11">Table 6</ref> shows the performance on the online COCO evaluation server <ref type="bibr">4</ref> . We put it in the ap- pendix because the results are incomplete and the SPICE metric is not available for our submission, which correlates the best with human evaluation. The SPICE metrics are only available at the leader- board on the COCO dataset website 5 , which, un- fortunately, has not been updated for more than a year. Our submission does not directly optimize CIDEr, use model ensemble, or use extra training data. The three techniques typically result in or- thogonal improvements ( <ref type="bibr" target="#b25">Lu et al., 2017;</ref><ref type="bibr" target="#b33">Rennie et al., 2017;</ref><ref type="bibr" target="#b1">Anderson et al., 2018)</ref>. Moreover, the SPICE results are missing, in which the pro- posed model has the most advantage. Nonethe- less, our model is second only to Up-Down <ref type="bibr" target="#b1">(Anderson et al., 2018)</ref> and surpasses almost all the other models in published work, especially when 40 references are considered.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the main idea. The visual information captured by CNN and the conceptual information in the extracted topics are first condensed by attention mechanisms respectively. The merging gate then adaptively adjusts the weight between the visual information and the conceptual information for generating the caption.</figDesc><graphic url="image-2.png" coords="2,76.82,62.81,208.72,195.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The data flow in the proposed simNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of the proposed approach. In the right plot, we use φ, ψ, χ to denote input attention, output attention, and topic attention, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Flickr30k</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Methods</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Average merging gate values according to word types. As we can see, object words (noun) dominate the high value range, while attribute and relation words are assigned lower values, indicating the merging gate learns to efficiently combine the information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples of the generated captions. The left plot compares simNet with visual attention and topic attention. Visual attention is good at portraying the relations but is less specific in objects. Topic attention includes more objects but lacks details, such as material, color, and number. The proposed model achieves a very good balance. The right plot shows the error analysis of the proposed simNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualization. Please view in color. Here, we give two running examples. The upper part of each example shows the attention weights of each of 5 extracted topics. Deeper color means larger in value. The middle part shows the value of the merging gate that determines the importance of the topic attention. The lower part shows the visualization of visual attention. The attended region is covered with color. The blue shade indicates the output attention. The red shade indicates the input attention.</figDesc><graphic url="image-23.png" coords="9,72.00,191.45,117.92,116.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>). The se- mantic concepts are the most frequent words in the captions, and the extractor can be trained us- ing various methods but typically is only trained on the given image captioning dataset. This kind</figDesc><table>Topic 
Extractor 

Image 
Encoder 
Input Attention 

Output Attention 
… 

… 

&lt;bos&gt; a herd 
grass field 

… 

… 

a 
herd of 
field &lt;eos&gt; 

Input Image 

simNet 

Topic Attention 
… 

… 

… 

(cows) (field) (sheep) 
(water) (grass) 

Topic Bag 

LSTM M-Gate 

FC 

FC 

LSTM M-Gate 

FC 

FC 

LSTM M-Gate 

FC 

LSTM M-Gate 

FC 

FC 

LSTM M-Gate 

FC 

FC 

V 

V 

T 

̃ 

FC 

(a) The overall framework. 




</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance on the COCO Karpathy test split. Symbols,  *  and  † , are defined similarly. Our model 
outperforms the current state-of-the-art Up-Down substantially in terms of SPICE. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>SPICE CIDEr METEOR ROUGE-L BLEU-4 All Objects Attributes Relations Color Count Size</head><label></label><figDesc></figDesc><table>Baseline (Plain Encoder-Decoder Network) 
0.150 0.295 
0.048 
0.039 
0.022 0.004 0.023 0.762 
0.220 
0.495 
0.251 
Up-Down (Anderson et al., 2018)  *   † 
0.214 0.391 
0.100 
0.065 
0.114 0.184 0.032 1.201 
0.277 
0.569 
0.363 

Baseline + Input Att. 
0.164 0.316 
0.060 
0.044 
0.030 0.038 0.024 0.840 
0.233 
0.512 
0.273 
Baseline + Output Att. 
0.181 0.329 
0.094 
0.053 
0.089 0.184 0.044 0.968 
0.253 
0.534 
0.301 
Baseline + Input Att. + Output Att. 
0.187 0.338 
0.101 
0.055 
0.115 0.161 0.048 1.038 
0.259 
0.542 
0.311 

Baseline + Topic Att. 
0.184 0.348 
0.074 
0.051 
0.047 0.064 0.037 0.915 
0.250 
0.517 
0.260 
Baseline + Topic Att. + MGate 
0.189 0.355 
0.080 
0.051 
0.055 0.090 0.033 0.959 
0.256 
0.527 
0.281 

Baseline + Input Att. + Output Att. + Topic Att. 0.206 0.381 
0.091 
0.060 
0.075 0.094 0.045 1.068 
0.273 
0.556 
0.320 

simNet (Full Model) 
0.220 0.394 
0.109 
0.070 
0.088 0.202 0.045 1.135 
0.283 
0.564 
0.332 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results of incremental analysis. For a better understanding of the differences, we further list the break-
down of SPICE F-scores. Objects indicates comprehensiveness, and the others indicate detailedness. Additionally, 
we report the performance of the current state-of-the-art Up-Down for further comparison, which uses extra dense-
annotated data for pre-training and directly optimizes CIDEr. 

Method 
Precision Recall 
F1 

Topics (m=5) 
49.95 
38.91 42.48 

All words (m=5) 
84.01 
17.99 29.49 
All words (m=10) 
70.90 
30.18 42.05 
All words (m=20) 
52.51 
44.53 47.80 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Performance of visual word extraction.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 : Effect of using different visual words.</head><label>5</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Performance on the online COCO evaluation server. The SPICE metric is unavailable for our model, 
thus not reported. c5 means evaluating against 5 references, and c40 means evaluating against 40 references. 
The symbol  *  denotes directly optimizing CIDEr. The symbol  † denotes model ensemble. The symbol  ‡ denotes 
using extra data for training, thus not directly comparable. Our submission does not use the three aforementioned 
techniques. Nonetheless, our model is second only to Up-Down and surpasses almost all the other models in 
published work, especially when 40 references are considered. 

</table></figure>

			<note place="foot" n="2"> For conciseness, all the bias terms of linear transformations in this paper are omitted.</note>

			<note place="foot" n="3"> We use the pre-trained model from torchvision.</note>

			<note place="foot" n="6"> Conclusions We propose the stepwise image-topic merging network to sequentially and adaptively merge the visual and the conceptual information for improved image captioning. To our knowledge, we are the first to combine the visual and the semantic attention to achieve substantial improvements. We introduce the stepwise merging mechanism to efficiently guide the two kinds of information when generating the caption. The experimental results demonstrate the effectiveness of the proposed approach, which substantially outperforms the stateof-the-art image captioning methods in terms of SPICE on COCO and Flickr30k datasets. Quantitative and qualitative analysis show that the generated captions are both detailed and comprehensive in comparison with the existing methods.</note>

			<note place="foot" n="4"> https://competitions.codalab.org/ competitions/3221 5 http://cocodataset.org/ #captions-leaderboard</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by National Natu-ral Science Foundation of China (No. 61673028). We thank all the anonymous reviewers for their constructive comments and suggestions. Xu Sun is the corresponding author of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SPICE: semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2016-14th European Conference, Amsterdam</title>
		<meeting><address><addrLine>The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016-10-11" />
			<biblScope unit="volume">9909</biblScope>
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and VQA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">METEOR: an automatic metric for MT evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005</title>
		<meeting>the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005-06-29" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Temporal-difference learning with sampling baseline for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence<address><addrLine>New Orleans, Louisiana, USA, February 2-7</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SCACNN: spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="6298" to="6306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Microsoft COCO captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1504.00325</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mind&apos;s eye: A recurrent visual representation for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="2422" to="2431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khushi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="1080" to="1089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Hao Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><forename type="middle">N</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="1473" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Every picture tells a story: Generating sentences from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyyed</forename><surname>Mohammad Mohsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Amin</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010, 11th European Conference on Computer Vision</title>
		<meeting><address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010-09-05" />
			<biblScope unit="volume">6314</biblScope>
			<biblScope unit="page" from="15" to="29" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic compositional networks for visual captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchen</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="1141" to="1150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="664" to="676" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BabyTalk: Understanding and generating simple image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Visruth</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagnik</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2891" to="2903" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ROUGE: a package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out: Proceedings of the ACL-04</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Workshop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004-07" />
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="74" to="81" />
			<pubPlace>Barcelona, Spain</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic evaluation of summaries using n-gram cooccurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, HLTNAACL 2003</title>
		<meeting><address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-05-27" />
		</imprint>
	</monogr>
	<note>The Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deconvolution-based global decoding for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-08-20" />
			<biblScope unit="page" from="3260" to="3271" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved image captioning via policy gradient optimization of spider</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="873" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Show, tell and discriminate: Image captioning by self-retrieval with partially labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1803.08314</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Knowing when to look: Adaptive attention via a visual sentinel for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="3242" to="3250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural baby talk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A hierarchical end-to-end model for jointly improving text summarization and sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-13" />
			<biblScope unit="page" from="4251" to="4257" />
		</imprint>
	</monogr>
	<note>ijcai.org</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep captioning with multimodal recurrent neural networks (m-RNN)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>abs/1412.6632</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Philadelphia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Usa</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2002-07-06" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep reinforcement learningbased image captioning with embedding reward</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="1151" to="1159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="1179" to="1195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">CIDEr: consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Ramakrishna Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile, De</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4534" to="4542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Skeleton key: Image captioning by skeleton-attribute decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrison</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="7378" to="7387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">What value do explicit high level concepts have in vision to language problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><forename type="middle">R</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Automatic alt-text: Computergenerated image descriptions for blind users on a social network service</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaomei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wieland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Farivar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Schiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing</title>
		<meeting>the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing<address><addrLine>OR, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-02-25" />
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="1180" to="1192" />
		</imprint>
	</monogr>
<note type="report_type">Portland</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unpaired sentiment-to-sentiment translation: A cycled reinforcement learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018-07-15" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="979" to="988" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A skeleton-based model for promoting coherence among sentences in narrative story generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10-31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France. PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Boosting image captioning with attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="4904" to="4912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multiple instance boosting for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 18 [Neural Information Processing Systems, NIPS 2005</title>
		<editor>Y. Weiss, B. Schölkopf, and J. C. Platt</editor>
		<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1417" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Visual translation embedding network for visual relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zawlin</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="3107" to="3115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Partial multi-modal sparse coding via adaptive similarity structure regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM Conference on Multimedia Conference, MM 2016</title>
		<meeting>the 2016 ACM Conference on Multimedia Conference, MM 2016<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-10-15" />
			<biblScope unit="page" from="152" to="156" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
