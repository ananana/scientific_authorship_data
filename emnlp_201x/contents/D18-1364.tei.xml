<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Implicational Universals in Stochastic Constraint-Based Phonology</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Magri</surname></persName>
							<email>magrigrg@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">University of Paris</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Implicational Universals in Stochastic Constraint-Based Phonology</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3265" to="3274"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3265</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper focuses on the most basic implica-tional universals in phonological theory, called T-orders after Anttila and Andrus (2006). It shows that the T-orders predicted by stochas-tic and categorical Optimality Theory coincide. Analogously, the T-orders predicted by stochastic and categorical Harmonic Grammar coincide. In other words, these stochas-tic constraint-based frameworks do not tamper with the typological structure induced by the corresponding categorical frameworks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Phonology has traditionally focused on alterna- tions revealed by paradigms such as the German fi- nal devoicing examples <ref type="bibr">[ba:t]</ref>/ <ref type="bibr">[bE:d@]</ref> ('bath-SG/PL') and <ref type="bibr">[tsu:k]</ref>/ <ref type="bibr">[tsy:g@]</ref> ('train-SG/PL'). These alterna- tions are usually modeled through phonological grammars which map from underlying represen- tations (URs) to surface representations (SRs) <ref type="bibr" target="#b9">(Chomsky and Halle, 1968)</ref>. Constraint-based im- plementations of this combinatorial phonological theory include Optimality Theory (OT; <ref type="bibr">Smolensky, 1997, 2004</ref>) and Harmonic Grammar (HG; <ref type="bibr" target="#b18">Legendre et al., 1990;</ref><ref type="bibr" target="#b24">Smolensky and Legendre, 2006</ref>), reviewed below in sections 4 and 5.</p><p>More recently, phonology has extended its em- pirical coverage from categorical alternations to patterns of phonologically conditioned variation and gradient phonological (or phonotactic) judge- ments (see for instance <ref type="bibr" target="#b3">Anttila, 2012 and</ref><ref type="bibr" target="#b11">Pater, 2011)</ref>. This extension of the empiri- cal coverage has required a corresponding exten- sion of the theoretical framework. A phonological grammar cannot be construed anymore as a cate- gorical function from URs to SRs. Instead, it must be construed as a function from URs to probability distributions over the entire set of SRs. Constraint- based implementations of this stochastic theory in- clude partial order OT <ref type="bibr" target="#b2">(Anttila, 1997b)</ref>, stochastic OT (SOT; <ref type="bibr" target="#b6">Boersma, 1997</ref><ref type="bibr" target="#b7">Boersma, , 1998</ref>, and stochastic HG (SHG; <ref type="bibr" target="#b8">Boersma and Pater, 2016)</ref>  <ref type="bibr">1</ref> , recalled below in sections 4 and 5. Another framework explored in the recent literature on probabilis- tic constraint-based phonology is MaxEnt (ME; <ref type="bibr" target="#b12">Goldwater and Johnson, 2003;</ref><ref type="bibr" target="#b16">Hayes and Wilson, 2008)</ref>. Its T-orders are discussed in a companion paper <ref type="bibr" target="#b5">(Anttila and Magri, 2018)</ref>.</p><p>How can we investigate and understand the typological structure encoded by a probabilistic phonological framework? In the case of a categor- ical framework such as OT or HG, the predicted typological structure can be investigated directly by exhaustively listing all the grammars predicted for certain constraint and candidate sets. That is possible because the predicted typology of gram- mars is usually finite. The situation is rather dif- ferent for probabilistic frameworks: the predicted typology always consists of an infinite number of probability distributions which therefore cannot be exhaustively listed and directly inspected. A more indirect strategy is needed to chart the predicted typological structure.</p><p>A natural indirect strategy that gets around the problem raised by an infinite typology is to enu- merate, not the individual languages in the typol- ogy, but the set of implicational universals pre- dicted by the typology. An implicational univer- sal is an implication P T −→ P which holds of a given typology T whenever every language in the typology that satisfies the antecedent property P also satisfies the consequent property P ( <ref type="bibr" target="#b13">Greenberg, 1963)</ref>. Since implicational universals take into account every language in the typology, they chart the boundaries and measure the richness of the typological structure predicted by T.</p><p>Which antecedent and consequent properties P and P should we focus on? To start from the sim- plest case, let us consider a typology T of categor- ical phonological grammars, construed tradition- ally as mappings from URs to SRs. Within this categorical framework, the simplest, most basic, most atomic antecedent property P is the property of mapping a certain specific UR x to a certain spe- cific SR y. Analogously, the simplest consequent property P is the property of mapping a certain specific UR x to a certain specific SR y. We thus focus on the following class of implications:</p><formula xml:id="formula_0">Definition 1 The implicational universal (x, y) T → ( x, y)</formula><p>holds relative to a categorical typology T provided each grammar in T which succeeds at the antecedent mapping (i.e., it maps the UR x to the SR y), also succeeds at the consequent mapping (i.e., it maps the UR x to the SR y).</p><p>2</p><p>The relation T → thus defined over mappings is a partial order (under mild additional assumptions). It is called the T-order induced by the typology T ( <ref type="bibr" target="#b4">Anttila and Andrus, 2006</ref>). For example, any dialect of English that deletes t/d at the end of a coda cluster before a vowel also deletes it before a consonant <ref type="bibr" target="#b14">(Guy, 1991;</ref><ref type="bibr" target="#b17">Kiparsky, 1993;</ref><ref type="bibr" target="#b10">Coetzee, 2004</ref>). The implication (/cost.us/, [cos.us]) → (/cost.me/, [cos.me]) thus holds relative to the ty- pology T of English dialects.</p><p>Implicational universals can also be statistical. For instance, in dialects of English where t/d dele- tion applies variably, deletion has been found to be more frequent before consonants than before vow- els. To model these frequency effects, we need to consider a typology T of probabilistic phonologi- cal grammars, construed as functions from URs to probability distributions over SRs. We propose to extend the notion of T-orders from the categorical to the probabilistic setting as follows:</p><formula xml:id="formula_1">Definition 2</formula><p>The implicational universal (x, y) T → ( x, y) holds relative to a probabilistic typology T provided each grammar in T assigns a probabil- ity to the consequent mapping ( x, y) which is at least as large as the probability it assigns to the antecedent mapping (x, y). 2</p><p>To illustrate, the implication (/cost.us/ , [cos.us]) → (/cost.me/ , [cos.me]) also holds relative to the ty- pology T of English dialects with variable dele- tion because the probability of the consequent (/cost.me/, [cos.me]) (i.e., the frequency of dele- tion before a consonant) in any dialect is at least as large as the probability of the antecedent (/cost.us/, [cos.us]) (i.e., the frequency of deletion before a vowel).</p><p>The original categorical definition 1 of T-orders is a special case of the probabilistic definition 2. In fact, suppose that a categorical grammar succeeds on the antecedent mapping (x, y). That grammar construed probabilistically thus assigns probabil- ity 1 to the antecedent mapping. Definition 2 then requires that grammar to also assign probability 1 to the consequent mapping ( x, y). In other words, the grammar construed categorically succeeds on the consequent mapping, as required by the origi- nal definition 1 of categorical T-orders.</p><p>T-orders are defined at the level of mappings from URs to SRs. They thus allow for cross- framework comparisons, even bridging across cat- egorical and probabilistic frameworks. This pa- per (together with the companion Anttila and Magri 2018) thus uses T-orders to compare the probabilistic implementations of constraint-based phonology with the original categorical imple- mentations.</p><p>The main result reported in this paper is that the T-orders predicted by stochastic OT (and by partial order OT) coincide with those predicted by categorical OT, no matter what the candidate and constraint sets look like, as shown in section 4. Analogously, the T-orders predicted by stochastic HG coincide with those predicted by categorical HG, as shown in section 5. In other words, these stochastic frameworks do not tamper with the ty- pological structure induced by the original cate- gorical frameworks, at least when that structure is measured in terms of T-orders. These specific re- sults about OT and HG are derived as a special case of a more general result on stochastic typolo- gies, developed in sections 2 and 3.</p><p>As discussed in a companion paper <ref type="bibr" target="#b5">(Anttila and Magri, 2018</ref>), the situation is very different for ME. Both ME and stochastic HG can be con- strued as probabilistic variants of categorical HG. Stochastic and categorical HG share the same T- orders. The ME T-orders instead obey a rather different underlying convex geometry and turn out to be much sparser. In other words, ME yields a much richer probabilistic extension of categorical HG than stochastic HG does. Section 6 concludes the paper by discussing these results in the context of the recent literature on probabilistic constraint- based phonology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Categorical and stochastic phonology</head><p>We assume a relation Gen which pairs each UR x with a set Gen(x) of candidate SRs. As recalled above, a categorical phonological grammar G takes a UR x and selects a corresponding SR y = G(x) from the candidate set Gen(x). A stochas- tic phonological grammar G instead takes a UR x and returns a probability distribution G(·| x) over Gen(x) which assigns a probability G(y| x) to each candidate SR y in Gen(x). This section il- lustrates a general method to leverage a given ty- pology T of categorical grammars into a typology of stochastic grammars. Sections 4 and 5 will then show that various stochastic frameworks in the re- cent constraint-based literature (such as partial or- der OT, stochastic OT, and stochastic HG) all fit within this general scheme.</p><p>Following common practice in constraint-based phonology, we assume that the categorical typol- ogy T only contains a finite number of grammars. <ref type="bibr">2</ref> We consider a probability mass function p over T. Thus, p assigns to each categorical grammar G in T a nonnegative probability mass p(G) ≥ 0 and these masses sum up to 1, namely G∈T p(G) = 1. We can then define the stochastic grammar G p corresponding to the probability mass function p as the function which takes a UR x and returns the probability distribution G p (·| x) over the candidate set Gen(x) defined as in (1). It says that the proba- bility G p (y | x) that the UR x is mapped to the SR y is he probability mass allocated by p to the region {G ∈ T | G(x) = y} of the typology T consisting of those categorical grammars which succeed on the mapping (x, y).</p><formula xml:id="formula_2">G p (y | x) = {G∈T | G(x)=y} p(G)<label>(1)</label></formula><p>We assume next that each categorical grammar in the typology T returns a unique SR y for each UR x. <ref type="bibr">3</ref> This assumption suffices to ensure that G p is indeed a probability distribution, namely that the sum of the probabilities G p (y | x) over the can- didates y in Gen(x) is equal to 1, as shown in <ref type="formula">(2)</ref>.</p><formula xml:id="formula_3">y∈Gen(x) G p (y | x) (a) = y∈Gen(x) {G∈T | G(x)=y} p(G) (b) = G∈T p(G) (c) = 1 (2)</formula><p>In step (2a), we have used the definition (1) of G p (y | x). In step (2b), we have used the fact that every grammar in T maps x to a unique SR y, so that the sets {G ∈ T | G(x) = y} partition the ty- pology T into disjoint sets as y spans the candidate set Gen(x). In step (2c), we have used the fact that p is a probability mass function over T and thus adds up to 1. A family P of probability mass functions p 1 , p 2 , . . . over the finite categorical typology T thus induces a typology {G p 1 , G p 2 , . . .} of stochastic grammars. It is called the stochastic typology corresponding to the categorical typol- ogy T and the probability family P, and it is de- noted by T P . We denote by T −→ the T-order rel- ative to the categorical typology T in the sense of definition 1 and by T P −→ the T-order relative to the stochastic typology T P in the sense of definition 2. We want to investigate the relationship between these categorical and stochastic T-orders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Relationship between categorical and stochastic T-orders</head><p>Let us suppose that the implication (x, y) T → ( x, y) holds between an antecedent mapping (x, y) and a consequent mapping ( x, y) relative to a categorical typology T. By definition 1, this means that ev- ery categorical grammar G in the typology T that maps the antecedent UR x to the antecedent SR y (namely, G(x) = y) also maps the consequent UR x to the consequent SR y (namely, G( x) = y), yielding the inclusion (3). By (1), this inclusion (3) entails that the proba- bility assigned by G p to the consequent mapping ( x, y) is at least as large as the probability assigned to the antecedent mapping (x, y), as stated in (4). This entailment follows from the sheer fact that probabilities are monotonic relative to set inclu- sion. The entailment from the inclusion (3) to the inequality (4) thus holds under no assump- tions whatsoever on the probability mass function p used to define the stochastic grammar G p .</p><formula xml:id="formula_4">{G ∈ T | G(x) = y} ⊆ {G ∈ T | G( x) = y}</formula><formula xml:id="formula_5">G p (y | x) ≤ G p ( y | x)</formula><p>probability of the antecedent mapping probability of the consequent mapping <ref type="formula">(4)</ref> The latter inequality (4) finally says that the im- plication (x, y)</p><p>T P −→ ( x, y) holds also relative to the stochastic typology T P in the sense of defi- nition 2. In conclusion, a categorical T-order al- ways entails the corresponding stochastic T-order, no matter the shape of the family P of probability mass functions used to derive the stochastic typol- ogy T P from the categorical typology T.</p><p>We now turn to the reverse entailment. Sup- pose that an implication (x, y)</p><p>T P −→ ( x, y) holds between an antecedent mapping (x, y) and a con- sequent mapping ( x, y) relative to the stochastic typology T P . By definition 2, this means in turn that the inequality (4) holds between the probabil- ities G p (y | x) and G p ( y | x) of the antecedent and the consequent mappings relative to any probabil- ity mass function p in the family P. Suppose by contradiction that the corresponding implication (x, y) T −→ ( x, y) relative to the original categori- cal typology T instead fails. By definition 1, this means that the set inclusion (3) fails because there exists some grammar G 0 with the properties in (5): G 0 succeeds on the antecedent mapping, namely it maps x to y; but G 0 fails on the consequent map- ping, namely it maps x to some loser candidate z different from the intended winner candidate y.</p><formula xml:id="formula_6">G 0 (x) = y, G 0 ( x) = z = y<label>(5)</label></formula><p>We would like to derive a contradiction from the assumption (4) that the stochastic implication (x, y) fails. Yet, no contradiction arises in the general case. Indeed, suppose that the probability mass func- tions in the family P all happen to assign zero (or tiny) probability mass to this grammar G 0 which flouts the categorical implication because of (5). This problematic grammar G 0 thus bears no (or only a tiny) effect on the total probabilities G p (y | x) and G p ( y | x) of the two mappings (x, y) and ( x, y). The probability inequality (4) is there- fore not necessarily compromised by the offensive behavior (5) of G 0 , as long as the other grammars in the typology comply.</p><p>In order to derive a contradiction from these two conditions (4) and (5), we need to make some as- sumptions on the family P of probability mass functions. Indeed, the problem just discussed arises when every probability mass function p in P assigns zero (or tiny) probability to the prob- lematic grammar G 0 . We need to rule out this sce- nario. We propose to achieve that through the as- sumption that the family P satisfies the following Definition 3 The family P of probability mass functions over the finite categorical typology T is sufficiently rich in the sense that for every categor- ical grammar G in T and for any two URs x and x, the following inequalities</p><formula xml:id="formula_7">G p (G(x)| x) &gt; 1/2, G p (G( x)| x) &gt; 1/2 (6)</formula><p>hold for some probability mass function p in P. 2</p><p>Here is the intuition behind this definition. Sup- pose that for every categorical grammar G, the family P contains a probability mass function p which assigns all the probability mass to that grammar G. By (1), the corresponding stochastic grammar G p assigns probability 1 to the mappings enforced by G, as stated in <ref type="formula" target="#formula_8">(7)</ref>.</p><formula xml:id="formula_8">G p (G(x)| x) = 1 for every UR x<label>(7)</label></formula><p>In other words, the stochastic grammar G p "co- incides" with the categorical grammar G and the stochastic typology T P thus "contains" or "ex- tends" the original categorical typology T. In this special case, we obviously expect the stochastic implication (x, y)</p><formula xml:id="formula_9">T P −→ ( x, y) to entail the categori- cal implication (x, y) T −→ ( x, y)</formula><p>, as desired. Condition (6) required by definition 3 is a weaker version of the latter condition (7). First, it is weaker because the requirement G p (G(x)| x) = 1 is replaced with the weaker requirement G p (G(x)| x) &gt; 1/2: the probability assigned to the mappings enforced by G needs not be 1, as long as it is large enough, namely larger than 1/2. Second, this requirement G p (G(x)|x) &gt; 1/2 needs not be satisfied by a unique mass p for all URs: it suffices to look at just two URs at the time.</p><p>If the family P is sufficiently rich in the sense of definition 3, the two conditions (4) and (5) are in- deed contradictory. In fact, condition (5) now en- sures that P contains a probability mass function p 0 such that the corresponding stochastic grammar G p 0 maps x to y with probability larger than 1/2 and it maps x to z with probability larger than 1/2. The latter fact means in turn that G p 0 maps x to y with probability smaller than 1/2, because the probabilities of the various candidates y, z, . . . in Gen( x) must add up to 1. In conclusion, we have obtained G p 0 (y | x) &gt; 1/2 and G p 0 ( y | x) &lt; 1/2, in blatant contradiction of (4).</p><p>The preceding reasoning is summarized in the following proposition 1, which says that the T- order relative to a categorical typology T and the T-order relative to the corresponding stochastic ty- pology T P coincide, no matter what the family P of probability mass functions looks like, as long as it is sufficiently rich, in the sense of definition 3. Identity of T-orders holds even when the family P is infinite, so that the stochastic typology T P con- tains an infinite number of stochastic grammars, while the categorical typology T contains only a finite number of grammars.</p><p>Proposition 1 Consider a finite typology T of cat- egorical grammars and a family P of probability mass functions on T. Let T P be the typology of the corresponding stochastic grammars, defined through (1). If P is sufficiently rich in the sense of definition 3, the T-order In the rest of the paper, we apply this result to various categorical and stochastic frameworks for constraint-based phonology.</p><p>4 Categorial OT, partial order OT, and stochastic OT induce the same T-orders</p><p>In this section, we focus on categorical and stochastic OT. We assume a set of n constraints C 1 , . . . , C k , . . . , C n and some candidacy relation Gen. We recall that a constraint C k prefers a map- ping (x, y) to another mapping (x, z) provided C k assigns less violations to the former than to the latter, namely C k (x, y) &lt; C k (x, z). A constraint ranking is an arbitrary linear order over the con- straint set. A constraint ranking prefers a map- ping (x, y) to another mapping (x, z) provided the highest -ranked constraint which distinguishes between the two mappings (x, y) and (x, z) prefers (x, y). The categorical OT grammar correspond- ing to a ranking maps a UR x to that SR y such that prefers the mapping (x, y) to the mapping (x, z) corresponding to any other candidate z in Gen(x) ( <ref type="bibr" target="#b23">Prince and Smolensky, 2004</ref>). We denote by OT → the T-order corresponding to the typology T of the categorical OT grammars corresponding to all constraint rankings, in the sense of definition 2.</p><p>To illustrate, consider the following three con- straints (from <ref type="bibr" target="#b17">Kiparsky, 1993)</ref> for the process of t/d deletion mentioned in section 1: C 1 = SYLLABLEWELLFORMEDNESS (SWF) penalizes codas and tautosyllabic consonant clusters; C 2 = ALIGN penalizes resyllabification across word boundaries; and C 3 = MAX penalizes segment deletion. Suppose that the UR /cost us/ comes with the three candidate SRs <ref type="bibr">[</ref> We now turn to the stochastic counterpart of this categorical framework. A ranking vector θ = (θ 1 , . . . , θ k , . . . , θ n ) ∈ R n assigns a nu- merical ranking value θ k to each constraint C k . The stochastic ranking vector θ + = (θ 1 + 1 , . . . , θ n + n ) is obtained by adding to the rank- ing values θ 1 , . . . , θ n some numbers 1 , . . . , n sampled independently from each other according to some distribution D on R. If the distribution D is continuous, the probability that two stochastic ranking values θ h + h and θ k + k coincide is equal to zero. The stochastic ranking vector θ + thus describes the unique ranking θ+ which respects the relative size of the stochastic ranking values: a constraint C h is ranked above a constraint C k according to θ+ (namely, C h θ+ C k ) if and only if the stochastic ranking value of the former is larger than that of the latter (namely, The typology of stochastic grammars T P obtained as in section 2 from the categorical OT typology T and the family P = {p D θ | θ ∈ R n } of probabil- ity mass functions p D θ corresponding to all ranking vectors θ is called stochastic OT <ref type="bibr">(SOT;</ref><ref type="bibr" target="#b6">Boersma, 1997</ref><ref type="bibr" target="#b7">Boersma, , 1998</ref>. We denote by SOT −→ the T-orders cor- responding to SOT in the sense of definition 2.</p><formula xml:id="formula_10">θ h + h &gt; θ k + k ). A</formula><p>What is the typological structure encoded by SOT's T-orders? Given that the original OT ty- pology is finite (because there are only a finite number of constraint rankings) while the SOT typology is infinite (it contains an infinite num- ber of grammars which assign different proba- bilities), how much of OT's typological structure is preserved in SOT? These questions are cru- cial for phonological theory but technically non- trivial. To illustrate, <ref type="figure" target="#fig_3">figure 1</ref> plots the SOT probability of the mappings (/cost.us/, [cos.us]) and (/cost.me/, [cos.me]) relative to the three con- straints C 1 , C 2 , C 3 listed above as a function of the ranking value θ 1 of constraint C 1 (horizon- tal axis) and the ranking value θ 2 of constraint C 2 (vertical axis) for three choices of the rank- (plotted in the top row). But how can this conjec- ture be checked, given that SOT probabilities seem not to admit a closed-form expression?</p><p>The result obtained in section 3 provides a straightforward solution to this problem. Sup- pose that there exists a positive constant ∆ large enough that the distribution D concentrates most of the probability mass on the interval [−∆, +∆], as stated in (9). This assumption holds in particu- lar when D has a bounded support or it is defined through a density (such as a gaussian, as assumed in <ref type="bibr" target="#b6">Boersma, 1997</ref><ref type="bibr" target="#b7">Boersma, , 1998</ref>.</p><formula xml:id="formula_11">(D([−∆, +∆])) n &gt; 1/2<label>(9)</label></formula><p>For any constraint ranking , consider a rank- ing vector θ such that the top -ranked constraint has the largest ranking value; the second top - ranked constraint has the second largest ranking value; and so on. Furthermore, assume that these ranking values are spaced apart by more than 2∆. Since the numbers 1 , . . . , n are all bounded be- tween −∆ and +∆ with probability at least 1/2 and since the ranking values are spaced apart by more than 2∆, the constraint ranking θ+ cor- responding to the stochastic ranking vector θ + coincides with the original ranking with prob- ability at least 1/2. In other words, the probabil- ity mass function p D θ corresponding to this rank- ing vector θ according to <ref type="bibr">(8)</ref> assigns more than half of the probability mass to the OT grammar corresponding to the ranking . The family P = {p D θ | θ ∈ R n } is therefore sufficiently rich in the sense of definition 3. Proposition 1 thus yields the following Corollary 1 Under the mild assumption (9) on the distribution D, the T-order SOT −→ relative to SOT is identical to the T-order OT −→ relative to categorical OT for any constraint and candidate set. 2</p><p>In conclusion, despite the SOT typology being infinite, SOT induces the same typological struc- ture as categorical OT, at least when typological structure is measured in terms of T-orders. Fur- thermore, the technical problem of computing T- orders relative to SOT is reduced to the much eas- ier problem of computing T-orders relative to cate- gorical OT, which indeed admits an efficient solu- tion <ref type="bibr" target="#b20">(Magri, 2018a)</ref>. This result extends to partial order OT (Anttila, 1997a), as the latter is a special case of SOT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Categorial HG and stochastic HG induce the same T-orders</head><p>This section shows that completely analogous con- siderations hold for HG. A weight vector w = (w 1 , . . . , w k , . . . , w n ) ∈ R n + assigns a nonnega- tive weight w k ≥ 0 to each constraint C k . The w-harmony of a mapping (x, y) is the weighted sum of the constraint violations multiplied by −1, namely − n k=1 w k C k (x, y). Because of the mi- nus sign, mappings with a large harmony have few constraint violations. The categorical HG gram- mar corresponding to a weight vector w maps a UR x to the surface form y such that the map- ping (x, y) has a larger w-harmony than the map- ping (x, z) corresponding to any other candidate z in Gen(x) ( <ref type="bibr" target="#b18">Legendre et al., 1990;</ref><ref type="bibr" target="#b24">Smolensky and Legendre, 2006</ref>). We denote by HG → the T-order corresponding to the typology T of the categori- cal HG grammars corresponding to all nonnega- tive weight vectors, in the sense of definition 2.</p><p>To illustrate, it is easy to verify that the implica- tion (/cost.us/, [cos.us]) HG → (/cost.me/, [cos.me]) considered above holds also relative to the HG typology in the sense of definition 1: every weighting of the three constraints which succeeds on the antecedent mapping (/cost.us/, <ref type="bibr">[cos.us]</ref>) also succeeds on the consequent mapping (/cost.me/, <ref type="bibr">[cos.me]</ref>). In general, the HG typology is a proper superset of the OT typology (when the set of URs is finite). The HG T-order is therefore a subset of the corresponding OT T-order.</p><p>We now turn to the stochastic counterpart of this categorical framework. The stochastic weight vec- tor w + = (w 1 + 1 , . . . , w n + n ) is obtained by adding to the weights w 1 , . . . , w n some num- bers 1 , . . . , n sampled independently from each other according to some distribution D on R. 5 A weight vector w induces the corresponding proba- bility mass function p D w on the categorical HG ty- pology T defined in (10). Obviously, this defini- tion yields a probability mass, namely the sum of the masses p D w (G) over all the categorical gram- mars G in the HG typology T is equal to 1.</p><formula xml:id="formula_12">p D w (G) = the probability of sampling 1 , . . . , n i.i.d.</formula><p>∼ D such that the HG grammar corresponding to the weight vector w + is G <ref type="formula" target="#formula_2">(10)</ref> The typology of stochastic grammars T P obtained as in section 2 from the categorical HG typol- ogy T and the family P = {p D w | w ∈ R n + } of probability mass functions p D w corresponding to all nonnegative weight vectors w is called stochastic HG (SHG; <ref type="bibr" target="#b8">Boersma and Pater, 2016)</ref>. We denote by SHG −→ the T-orders corresponding to SHG in the sense of definition 2.</p><p>To illustrate, <ref type="figure" target="#fig_5">figure 2</ref>  We consider two URs x and x. We assume that x comes with a finite number m + 1 of candidates y, z 1 , . . . , z m and that x comes with a finite number m + 1 of candidates y, z 1 , . . . , z m . This assump- tion is nonrestrictive. In fact, each UR admits only a finite number of optima in HG <ref type="bibr" target="#b21">(Magri, 2018b)</ref>. Candidate sets can thus be assumed to be finite without loss of generality. We consider a categor- ical HG grammar in the typology T and assume that it maps x and x to y and y, respectively. This means that any weight vector w = (w 1 , . . . , w n ) corresponding to this HG grammar assigns a larger harmony to the winner mappings (x, y) and x, y) than to any of the loser mappings (x, z i ) and ( x, z j ) respectively, as stated in <ref type="bibr">(11)</ref>.</p><formula xml:id="formula_13">min i=1,...,m k w k (C k (x, z i ) − C k (x, y)) ξ &gt; 0 min j=1,..., m k w k (C k ( x, z j ) − C k ( x, y)) ξ &gt; 0<label>(11)</label></formula><p>Let B be an upper bound on the constraint vio- lation differences, so that |C(x, z i )−C(x, y)| ≤ B and |C( x, z j ) − C( x, y)| ≤ B for every i = 1, . . . , m and j = 1, . . . , m. Suppose again that there exists a positive constant ∆ large enough that the distribution D concentrates most of the proba- bility mass on [−∆, +∆], in the sense that it sat- isfies the inequality (9). We consider the weight vector λw = (λw 1 , . . . , λw n ) obtained by rescal- ing the weight vector w by a positive scalar λ &gt; 0 sufficiently large, in the sense of (12).</p><formula xml:id="formula_14">λ &gt; max n∆B ξ , n∆B ξ<label>(12)</label></formula><p>Whenever ∈ [−∆, +∆] n , the HG grammar corresponding to the stochastic rescaled weight vector λw + maps the UR x to the SR y, as shown in <ref type="bibr">(13)</ref>. An analogous reasoning shows that it also maps x to y. In step (13a), we have used the definition (11) of ξ. In step (13b), we have lower bounded C(x, z i ) − C(x, y) with −B. In step (13c), we have used the definition (12) of λ.</p><formula xml:id="formula_15">k (λw k + k )(C k (x, z i ) − C k (x, y)) = = λ k w k (C k (x, z i ) − C k (x, y))+ + k k (C k (x, z i ) − C k (x, y)) (a) ≥ λξ + k k (C k (x, z i ) − C k (x, y)) (b) ≥ λξ − n∆B (c) &gt; 0 (13)</formula><p>The intuition behind this reasoning (13) is as fol- lows. The rescaled weight vector λw generates −→ relative to categori- cal HG for any constraint set and any candidate set which assigns a finite number of candidates to each UR (while the number of URs can be infinite). <ref type="bibr">2</ref> </p><formula xml:id="formula_16">(VC, VC) (VC, CVC) (VC, V) (VC, CV) (CVC, CVC) (V, V)) (V, CV) (CVC, CV) (CV, CV)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Phonology has traditionally focused on patterns of categorical alternations modeled within categor- ical frameworks such as OT and HG. More re- cently, phonology has extended its empirical cov- erage to quantitative data such as gradient judg- ments and patterns of variation. This move has required a parallel extension from categorical to stochastic frameworks, such as partial order OT, stochastic OT, and stochastic HG. These stochas- tic frameworks are "extensions" of the original categorical frameworks in the sense discussed in section 3. One might thus expect the stochas- tic frameworks to be typologically less restrictive than the original categorical frameworks. This pa- per has shown that is not the case, at least when typological restrictiveness is measured in terms of the most basic implicational universals, namely T- orders. Indeed, the T-orders induced by partial or- der and stochastic OT coincide with those induced by categorical OT. Analogously, the T-orders in- duced by stochastic HG coincide with those in- duced by categorical HG.</p><p>As discussed in a companion paper <ref type="bibr" target="#b5">(Anttila and Magri, 2018</ref>), the situation is very different in ME. To illustrate, consider the basic syllable system of <ref type="bibr" target="#b23">Prince and Smolensky (2004)</ref>. The set of forms consists of the four syllable types CV, CVC, V, and VC. Each of them is a candidate of each other. The constraint set consists of the four constraints ON- SET, NOCODA, MAX, and DEP. The HG and OT T-orders coincide and consist of 16 entail- ments with a feasible antecedent, plotted in <ref type="figure" target="#fig_6">figure  3</ref>. These entailments extend to SOT and SHG, by virtue of the corollaries 1 and 2 obtained above. ME instead misses the eight dotted entailments. Of the eight entailments which do survive in ME, seven are such that the antecedent and the conse- quent surface form coincide, plus the entailment (VC, VC) → (CV, CV), which is a quirk due to the fact that VC is the most marked syllable type. This restriction to entailments whose antecedent and consequent surface forms coincide is not phono- logically plausible. Anttila and Magri (2018) con- clude that the ME formalism imposes typological restrictions at odds with phonological intuition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>( x, y) holds and the assumption (5) that the categorical implication (x, y) T −→ ( x, y)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>T</head><label></label><figDesc>−→ relative to the cat- egorical typology T and the T-order T P −→ relative to the stochastic typology T P coincide. 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: SOT probabilities of the antecedent mapping (/cost.us/, [cos.us]) and the consequent mapping (/cost.me/, [cos.me]) as a function of θ1 (horizontal axis) and θ2 (vertical axis) for three choices of θ3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>ing value θ 3 of constraint C 3 . 4 These plots sug- gest that the implication (/cost.us/, [cos.us]) SOT → (/cost.me/, [cos.me]) holds in SOT: the probability of the consequent (/cost.me/, [cos.me]) (plotted in the bottom row) seems to be always larger than the probability of the antecedent (/cost.us/, [cos.us])</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: SHG probabilities of the antecedent mapping (/cost.us/, [cos.us]) and the consequent mapping (/cost.me/, [cos.me]) as a function of θ1 (horizontal axis) and θ2 (vertical axis) for three choices of θ3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Solid arrows are entailments that hold in OT, HG, SOT, SHG, and ME; dotted arrows are entailments that fail in ME.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>cost.us] (faithful), [cos.us] (with deletion), and [cos.tus] (with resyl- labification). Analogously, suppose that the UR /cost me/ comes with the three candidate SRs [cost.me], [cos.me], and [cos.tme]. It is easy to verify that the implication (/cost.us/, [cos.us]) OT → (/cost.me/, [cos.me]) holds relative to the OT ty- pology generated by constraints C 1 , C 2 , C 3 in the sense of definition 1: every ranking of the three constraints which succeeds on the antecedent mapping (/cost.us/, [cos.us]) also succeeds on the consequent mapping (/cost.me/, [cos.me]). In other words, t/d deletion before a vowel entails deletion before a consonant.</figDesc><table></table></figure>

			<note place="foot" n="1"> Boersma and Pater (2016) actually use the term &quot;noisy HG&quot; instead of &quot;stochastic HG&quot;. We prefer &quot;stochastic HG&quot; to stress the analogy with Boersma&apos;s earlier framework of stochastic OT. Furthermore, we prefer to use &quot;stochastic&quot; to describe a property of the framework, reserving &quot;noisy&quot; to describe a property of the learning scenario (as opposed to noise-free). Hayes (2017) discusses further stochastic variants of categorical HG.</note>

			<note place="foot" n="2"> This assumption always holds in OT, as the number of constraint rankings is finite. It might fail in HG, but only in rather pathological situations which do not seem germane to natural language phonology. 3 Suppose instead that a categorical grammar were to return two different SRs y1 and y2 for some UR x. How should we interpret such a scenario? Plausibly, we should interpret the two SRs y1 and y2 as free variants with equal probability of 0.5 (while all other candidates have probability 0). But this means that our grammar is stochastic, not categorical.</note>

			<note place="foot" n="4"> These plots (as well as those in figure 2) are obtained by sampling for 10,000 times from each stochastic grammar. The distribution D is a gaussian with mean 0 and variance 2.</note>

			<note place="foot" n="5"> Some component w k + k of the corrupted weight vector w + could be negative. In this case, w + could correspond to no HG grammar in T and the probability mass defined in (10) could therefore add up to less than 1. This problem can be avoided simply by truncating the corrupted weights at zero, namely by replacing w k + k with max{w k + k , 0} in the definition of the corrupted weight vector w + (Magri, 2015).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research reported in this paper has been funded by the Agence National de la Recherche (project title: 'The mathematics of segmental phonotactics'). This paper is part of a larger project on T-orders, developed in collaboration with Arto Anttila. His comments on this paper are gratefully acknowledged.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deriving variation from grammar: A study of Finnish genitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arto</forename><surname>Anttila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Variation, change and phonological theory</title>
		<editor>Frans Hinskens, Roeland van Hout, and Leo Wetzels</editor>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="35" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<ptr target="http://ruccs.rutgers.edu/roa.html" />
	</analytic>
	<monogr>
		<title level="j">John Benjamins, Amsterdam. Rutgers Optimality Archive ROA</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Variation in Finnish phonology and morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arto</forename><surname>Anttila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modeling phonological variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arto</forename><surname>Anttila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Oxford Handbook of Laboratory Phonology</title>
		<editor>Abigail C. Cohn, Cécile Fougeron, and Marie Huffman</editor>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="76" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arto</forename><surname>Anttila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><surname>Andrus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>Stanford</pubPlace>
		</imprint>
	</monogr>
	<note>Manuscript and software</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">T-orders across categorical and probabilistic constraint-based phonology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arto</forename><surname>Anttila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Magri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>Stanford, CNRS</pubPlace>
		</imprint>
	</monogr>
	<note>Manuscript</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">How we learn variation, optionality and probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Boersma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Institute of Phonetic Sciences (IFA) 21</title>
		<meeting>the Institute of Phonetic Sciences (IFA) 21</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="43" to="58" />
		</imprint>
		<respStmt>
			<orgName>University of Amsterdam. Institute of Phonetic Sciences</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Functional Phonology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Boersma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Holland Academic Graphics</publisher>
			<pubPlace>The Netherlands. The Hague</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Amsterdam</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Convergence properties of a gradual learning algorithm for Harmonic Grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Boersma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Pater</surname></persName>
		</author>
		<editor>John McCarthy and Joe Pater, editors</editor>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Harmonic Grammar and Harmonic Serialism. Equinox Press</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The Sound Pattern of English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Chomsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morris</forename><surname>Halle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
			<publisher>Harper and Row</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">What it Means to be a Loser: Non-Optimal Candidates in Optimality Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Andries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coetzee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The place of variation in phonological theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Andries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Coetzee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of phonological theory</title>
		<editor>John Goldsmith, Jason Riggle, and Alan Yu</editor>
		<meeting><address><addrLine>Blackwell, Cambridge</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="401" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning OT constraint rankings using a Maximum Entropy model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Stockholm Workshop on Variation Within Optimality Theory</title>
		<meeting>the Stockholm Workshop on Variation Within Optimality Theory<address><addrLine>Stockholm University</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Universals of Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">H</forename><surname>Greenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1963" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Explanation in variable phonology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Variation and Change</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Varieties of Noisy Harmonic Grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Annual Meeting in Phonology</title>
		<meeting>the 2016 Annual Meeting in Phonology</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Maximum Entropy model of phonotactics and phonotactic learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linguistic Inquiry</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="379" to="440" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An OT perspective on phonological variation. Handout (Stanford)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Kiparsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Harmonic Grammar: A formal multi-level connectionist theory of linguistic well-formedness: Theoretical foundations. In Annual conference of the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Géraldine</forename><surname>Legendre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiro</forename><surname>Miyata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Smolensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science Society</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="388" to="395" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">How to keep the HG weights non-negative: the truncated Perceptron reweighing rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Magri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Language Modeling</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="345" to="375" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Efficient computation of implicational universals in constraint-based phonology through the Hyperplane Separation Theorem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Magri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Manuscript (CNRS</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Finiteness of optima in constraint-based phonology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Magri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>Stanford, CNRS</pubPlace>
		</imprint>
	</monogr>
	<note>Manuscript</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimality: From neural networks to universal grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Smolensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="page" from="1604" to="1610" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Optimality Theory: Constraint Interaction in generative grammar. Blackwell, Oxford. Original version</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Smolensky</surname></persName>
		</author>
		<idno>TR-2</idno>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Colorado at Boulder, and ; Rutgers Center for Cognitive Science, Rutgers University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
	<note>Available from the Rutgers Optimality Archive as ROA 537</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Smolensky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Géraldine</forename><surname>Legendre</surname></persName>
		</author>
		<title level="m">The Harmonic Mind</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
