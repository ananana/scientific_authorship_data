<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Multi-Task Learning with Shared Memory</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Multi-Task Learning with Shared Memory</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="118" to="127"/>
							<date type="published">November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural network based models have achieved impressive results on various specific tasks. However, in previous works, most models are learned separately based on single-task supervised objectives, which often suffer from insufficient training data. In this paper, we propose two deep architectures which can be trained jointly on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed archi-tectures can improve the performance of a task with the help of other related tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural network based models have been shown to achieved impressive results on various NLP tasks ri- valing or in some cases surpassing traditional mod- els, such as text classification <ref type="bibr" target="#b32">Socher et al., 2013;</ref><ref type="bibr" target="#b21">Liu et al., 2015a</ref>), seman- tic matching ( <ref type="bibr" target="#b17">Hu et al., 2014;</ref><ref type="bibr" target="#b23">Liu et al., 2016a</ref>), parser <ref type="bibr" target="#b4">(Chen and Manning, 2014</ref>) and machine translation ( <ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>).</p><p>Usually, due to the large number of parameters these neural models need a large-scale corpus. It is hard to train a deep neural model that generalizes well with size-limited data, while building the large scale resources for some NLP tasks is also a chal- lenge. To overcome this problem, these models often involve an unsupervised pre-training phase. The fi- nal model is fine-tuned on specific task with respect * Corresponding author.</p><p>to a supervised training criterion. However, most pre-training methods are based on unsupervised ob- jectives <ref type="bibr" target="#b6">(Collobert et al., 2011;</ref><ref type="bibr" target="#b35">Turian et al., 2010;</ref><ref type="bibr" target="#b27">Mikolov et al., 2013)</ref>, which is effective to improve the final performance, but it does not directly opti- mize the desired task.</p><p>Multi-task learning is an approach to learn multi- ple related tasks simultaneously to significantly im- prove performance relative to learning each task in- dependently. Inspired by the success of multi-task learning <ref type="bibr" target="#b3">(Caruana, 1997)</ref>, several neural network based models <ref type="bibr" target="#b5">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b22">Liu et al., 2015b</ref>) are proposed for NLP tasks, which uti- lized multi-task learning to jointly learn several tasks with the aim of mutual benefit. The characteristic of these multi-task architectures is they share some lower layers to determine common features. After the shared layers, the remaining layers are split into multiple specific tasks.</p><p>In this paper, we propose two deep architectures of sharing information among several tasks in multi- task learning framework. All the related tasks are in- tegrated into a single system which is trained jointly. More specifically, inspired by Neural Turing Ma- chine (NTM) ( <ref type="bibr" target="#b14">Graves et al., 2014</ref>) and memory network ( <ref type="bibr" target="#b33">Sukhbaatar et al., 2015)</ref>, we equip task- specific long short-term memory (LSTM) neural network <ref type="bibr" target="#b16">(Hochreiter and Schmidhuber, 1997</ref>) with an external shared memory. The external memory has capability to store long term information and knowledge shared by several related tasks. Different with NTM, we use a deep fusion strategy to integrate the information from the external memory into task- specific LSTM, in which a fusion gate controls the information flowing flexibly and enables the model to selectively utilize the shared information.</p><p>We demonstrate the effectiveness of our architec- tures on two groups of text classification tasks. Ex- perimental results show that jointly learning of mul- tiple related tasks can improve the performance of each task relative to learning them independently.</p><p>Our contributions are of three-folds:</p><p>• We proposed a generic multi-task framework, in which different tasks can share information by an external memory and communicate by a reading/writing mechanism. Two proposed models are complementary to prior multi-task neural networks.</p><p>• Different with Neural Turing Machine and memory network, we introduce a deep fu- sion mechanism between internal and external memories, which helps the LSTM units keep them interacting closely without being con- flated.</p><p>• As a by-product, the fusion gate enables us to better understand how the external shared memory helps specific task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Memory Models for Specific Task</head><p>In this section, we briefly describe LSTM model, and then propose an external memory enhanced LSTM with deep fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Long Short-term Memory</head><p>Long short-term memory network (LSTM) <ref type="bibr" target="#b16">(Hochreiter and Schmidhuber, 1997</ref>) is a type of recurrent neural network (RNN) <ref type="bibr" target="#b11">(Elman, 1990)</ref>, and specifi- cally addresses the issue of learning long-term de- pendencies. LSTM maintains an internal memory cell that updates and exposes its content only when deemed necessary. Architecturally speaking, the memory state and output state are explicitly separated by activation gates ( <ref type="bibr" target="#b36">Wang and Cho, 2015)</ref>. However, the limita- tion of LSTM is that it lacks a mechanism to index its memory while writing and reading <ref type="bibr" target="#b7">(Danihelka et al., 2016)</ref>.</p><p>While there are numerous LSTM variants, here we use the LSTM architecture used by <ref type="bibr" target="#b18">(Jozefowicz et al., 2015)</ref>, which is similar to the architecture of (Graves, 2013) but without peep-hole connections.</p><p>We define the LSTM units at each time step t to be a collection of vectors in R d : an input gate i t , a forget gate f t , an output gate o t , a memory cell c t and a hidden state h t . d is the number of the LSTM units. The elements of the gating vectors i t , f t and o t are in <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>.</p><p>The LSTM is precisely specified as follows.</p><formula xml:id="formula_0">    ˜ c t o t i t f t     =     tanh σ σ σ     W p x t h t−1 + b p ,<label>(1)</label></formula><formula xml:id="formula_1">c t = ˜ c t i t + c t−1 f t ,<label>(2)</label></formula><formula xml:id="formula_2">h t = o t tanh (c t ) ,<label>(3)</label></formula><p>where x t ∈ R m is the input at the current time step; W ∈ R 4h×(d+m) and b p ∈ R 4h are parameters of affine transformation; σ denotes the logistic sigmoid function and denotes elementwise multiplication.</p><p>The update of each LSTM unit can be written pre- cisely as follows:</p><formula xml:id="formula_3">(h t , c t ) = LSTM(h t−1 , c t−1 , x t , θ p ). (4)</formula><p>Here, the function LSTM(·, ·, ·, ·) is a shorthand for Eq. <ref type="bibr">(1)</ref><ref type="bibr">(2)</ref><ref type="bibr">(3)</ref>, and θ p represents all the parameters of LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Memory Enhanced LSTM</head><p>LSTM has an internal memory to keep useful in- formation for specific task, some of which may be beneficial to other tasks. However, it is non-trivial to share information stored in internal memory.</p><p>Recently, there are some works to augment LSTM with an external memory, such as neural Turing machine ( <ref type="bibr" target="#b14">Graves et al., 2014</ref>) and memory net- work ( <ref type="bibr" target="#b33">Sukhbaatar et al., 2015)</ref>, called memory en- hanced LSTM (ME-LSTM). These models enhance the low-capacity internal memory to have a capabil- ity of modelling long pieces of text <ref type="bibr" target="#b0">(Andrychowicz and Kurach, 2016)</ref>.</p><p>Inspired by these models, we introduce an ex- ternal memory to share information among several tasks. To better control shared information and un- derstand how it is utilized from external memory, we propose a deep fusion strategy for ME-LSTM. <ref type="figure">Figure 1</ref>: Graphical illustration of the proposed ME- LSTM unit with deep fusion of internal and external memories.</p><formula xml:id="formula_4">tanh tanh     t x 1 t h  1 t c  t r t f t i t o t g t h t c Memory 1 t k  1 t e  1 t a   tanh tanh</formula><p>As shown in <ref type="figure">Figure 1</ref>, ME-LSTM consists the original LSTM and an external memory which is maintained by reading and writing operations. The LSTM not only interacts with the input and output information but accesses the external memory using selective read and write operations.</p><p>The external memory and corresponding opera- tions will be discussed in detail below.</p><p>External Memory The form of external memory is defined as a matrix M ∈ R K×M , where K is the number of memory segments, and M is the size of each segment. Besides, K and M are gener- ally instance-independent and pre-defined as hyper- parameters.</p><p>At each step t, LSTM emits output h t and three key vectors k t , e t and a t simultaneously. k t , e t and a t can be computed as </p><formula xml:id="formula_5"> k t e t a t   =   tanh σ tanh   (W m h t + b m ) (5)</formula><p>where W m and b m are parameters of affine trans- formation.</p><p>Reading The read operation is to read information r t ∈ R M from memory M t−1 .</p><formula xml:id="formula_6">r t = α t M t−1 ,<label>(6)</label></formula><p>where r t denotes the reading vector and α t ∈ R K represents a distribution over the set of segments of memory M t−1 , which controls the amount of infor- mation to be read from and written to the memory.</p><p>Each scalar α t,k in attention distribution α t can be obtained as:</p><formula xml:id="formula_7">α t,k = softmax(g(M t−1,k , k t−1 ))<label>(7)</label></formula><p>where M t−1,k represents the k-th row memory vec- tor, and k t−1 is a key vector emitted by LSTM.</p><p>Here g(x, y) (x ∈ R M , y ∈ R M ) is a align function for which we consider two different alter- natives:</p><formula xml:id="formula_8">g(x, y) = v T tanh(W a [x; y]) cosine(x, y)<label>(8)</label></formula><p>where v ∈ R M is a parameter vector.</p><p>In our current implementation, the similarity mea- sure is cosine similarity.</p><p>Writing The memory can be written by two oper- ations: erase and add.</p><formula xml:id="formula_9">M t = M t−1 (1 − α t e T t ) + α t a T t ,<label>(9)</label></formula><p>where e t , a t ∈ R M represent erase and add vectors respectively.</p><p>To facilitate the following statements, we re-write the writing equation as:</p><formula xml:id="formula_10">M t = f write (M t−1 , α t , h t ).<label>(10)</label></formula><p>Deep Fusion between External and Internal Memories After we obtain the information from external memory, we need a strategy to comprehen- sively utilize information from both external and in- ternal memory.</p><p>To better control signals flowing from external memory, inspired by ( <ref type="bibr" target="#b36">Wang and Cho, 2015)</ref>, we pro- pose a deep fusion strategy to keep internal and ex- ternal memories interacting closely without being conflated.</p><p>In detail, the state h t of LSTM at step t depends on both the read vector r t from external memory, and internal memory c t , which is computed by</p><formula xml:id="formula_11">h t = o t tanh(c t + g t (W f r t )),<label>(11)</label></formula><p>where W f is parameter matrix, and g t is a fusion gate to select information from external memory, which is computed by where W r and W c are parameter matrices. Finally, the update of external memory enhanced LSTM unit can be written precisely as</p><formula xml:id="formula_12">g t = σ(W r r t + W c c t ),<label>(12)</label></formula><formula xml:id="formula_13">x1 x2 x3 xT h (m) 1 h (m) 2 h (m) 3 · · · h (m) T softmax1 y (m) M (s) 0 M (s) 1 M (s) 2 · · · M (s) T −1 h (n) 1 h (n) 2 h (n) 3 · · · h (n) T softmax2 y (n) x1 x2 x3 xT (a) Global Memory Architecture x1 x2 x3 xT h (m) 1 h (m) 2 h (m) 3 · · · h (m) T softmax1 y (m) M (m) 1 M (m) 2 M (m) 3 M (s) 0 M (s) 1 M (s) 2 M (s) T −1 M (n) 1 M (n) 2 M (n) 3 h (n) 1 h (n) 2 h (n) 3 · · · h (n) T softmax2 y (n) x1 x2 x3 xT (b) Local-Global Hybrid Memory Architecture</formula><formula xml:id="formula_14">(h t , M t , c t ) = ME-LSTM(h t−1 , M t−1 , c t−1 , x t , θ p , θ q ),<label>(13)</label></formula><p>where θ p represents all the parameters of LSTM in- ternal structure and θ q represents all the parameters to maintain the external memory. Overall, the external memory enables ME-LSTM to have larger capability to store more information, thereby increasing the ability of ME-LSTM. The read and write operations allow ME-LSTM to cap- ture complex sentence patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deep Architectures with Shared Memory for Multi-task Learning</head><p>Most existing neural network methods are based on supervised training objectives on a single task <ref type="bibr" target="#b6">(Collobert et al., 2011;</ref><ref type="bibr" target="#b32">Socher et al., 2013;</ref>). These methods often suffer from the limited amounts of training data. To deal with this problem, these models often involve an unsupervised pre-training phase. This unsupervised pre-training is effective to improve the final perfor- mance, but it does not directly optimize the desired task.</p><p>Motivated by the success of multi-task learning <ref type="bibr" target="#b3">(Caruana, 1997)</ref>, we propose two deep architectures with shared external memory to leverage supervised data from many related tasks. Deep neural model is well suited for multi-task learning since the features learned from a task may be useful for other tasks. <ref type="figure" target="#fig_0">Figure 2</ref> gives an illustration of our proposed archi- tectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARC-I: Global Shared Memory</head><p>In ARC-I, the input is modelled by a task-specific LSTM and ex- ternal shared memory. More formally, given an input text x, the task-specific output h (m) t of task m at step t is defined as</p><formula xml:id="formula_15">(h (m) t , M (s) t , c (m) t ) = ME-LSTM(h (m) t−1 , M (s) t−1 , c (m) t−1 , x t , θ (m) p , θ (s) q ),<label>(14)</label></formula><p>where x t represents word embeddings of word x t ; the superscript s represents the parameters are shared across different tasks; the superscript m rep- resents that the parameters or variables are task- specific for task m.</p><p>Here all tasks share single global memory M (s) , meaning that all tasks can read information from it and have the duty to write their shared or task- specific information into the memory.</p><formula xml:id="formula_16">M (s) t = f write (M (s) t−1 , α (s) t , h (m) t )<label>(15)</label></formula><p>After calculating the task-specific representation of text h (m) T for task m, we can predict the probability distribution over classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARC-II: Local-Global Hybrid Memory</head><p>In ARC-I, all tasks share a global memory, but can also record task-specific information besides shared information. To address this, we allocate each task a local task-specific external memory, which can further write shared information to a global memory for all tasks. More generally, for task m, we assign each task- specific LSTM with a local memory M (m) , followed by a global memory M (s) , which is shared across different tasks.</p><p>The read and write operations of the local and global memory are defined as <ref type="table" target="#tab_2">Movie  SST-1  Sen.  8544  1101  2210  5  19  18K  SST-2  Sen.  6920  872  1821  2  18  15K  SUBJ  Sen.  9000  - 1000  2  21  21K  IMDB  Doc. 25,000  - 25,000  2</ref>   <ref type="table">Table 1</ref>: Statistics of two multi-task datasets. Each dataset consists of four related tasks.</p><formula xml:id="formula_17">r (m) t = α (m) t M (m) t ,<label>(16)</label></formula><note type="other">Dataset Type Train Size Dev. Size Test Size Class Avg. Length Vocabulary Size</note><formula xml:id="formula_18">M (m) t = f write (M (m) t−1 , α (m) t , h (m) t ),<label>(17)</label></formula><formula xml:id="formula_19">r (s) t = α (s) t−1 M (s) t−1 ,<label>(18)</label></formula><formula xml:id="formula_20">M (s) t = f write (M (s) t−1 , α (s) t , r (m) t ),<label>(19)</label></formula><p>where the superscript s represents the parameters are shared across different tasks; the superscript m represents that the parameters or variables are task- specific for task m.</p><p>In ARC-II, the local memories enhance the capac- ity of memorizing, while global memory enables the information flowing from different tasks to interact sufficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training</head><p>The task-specific representation h (m) , emitted by the deep muti-task architectures, is ultimately fed into the corresponding task-specific output layers.</p><formula xml:id="formula_21">ˆ y (m) = softmax(W (m) h (m) + b (m) ),<label>(20)</label></formula><p>wherê y (m) is prediction probabilities for task m. Given M related tasks, our global cost function is the linear combination of cost function for all tasks.</p><formula xml:id="formula_22">φ = M m=1 λ m L(ˆ y (m) , y (m) )<label>(21)</label></formula><p>where λ m is the weights for each task m respec- tively.</p><p>Computational  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head><p>In this section, we investigate the empirical perfor- mances of our proposed architectures on two multi- task datasets. Each dataset contains several related tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>The used multi-task datasets are briefly described as follows. The detailed statistics are listed in <ref type="table">Table 1</ref>.</p><p>Movie Reviews The movie reviews dataset con- sists of four sub-datasets about movie reviews.</p><p>• SST-1 The movie reviews with five classes in the Stanford Sentiment Treebank 1 <ref type="bibr" target="#b32">(Socher et al., 2013</ref>).</p><p>• SST-2 The movie reviews with binary classes.</p><p>It is also from the Stanford Sentiment Tree- bank.</p><p>• SUBJ The movie reviews with labels of sub- jective or objective ( <ref type="bibr" target="#b28">Pang and Lee, 2004</ref>).</p><p>• IMDB The IMDB dataset 2 consists of 100,000 movie reviews with binary classes <ref type="bibr" target="#b26">(Maas et al., 2011</ref>). One key aspect of this dataset is that each movie review has several sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SST-1 SST-2 SUBJ IMDB Avg∆</head><p>Single <ref type="table">Task</ref>   <ref type="figure" target="#fig_0">(Socher et al., 2012)</ref> 44.4 82.9 - - - RNTN ( <ref type="bibr" target="#b32">Socher et al., 2013)</ref> 45.7 85.4 - - - DCNN (  48.5 86.8 - 89.3 - CNN-multichannel <ref type="bibr" target="#b20">(Kim, 2014)</ref> 47.4 88.1 93.2 - - Tree-LSTM ( <ref type="bibr" target="#b34">Tai et al., 2015)</ref> 50.6 86.9 - - - <ref type="table">Table 3</ref>: Accuracies of our models on movie reviews tasks against state-of-the-art neural models. The last column gives the improvements relative to LSTM and ME-LSTM respectively. NBOW: Sums up the word vectors and applies a non-linearity followed by a softmax classification layer. Product Reviews This dataset 3 , constructed by <ref type="bibr" target="#b2">Blitzer et al. (2007)</ref>, contains Amazon product re- views from four different domains: Books, DVDs, Electronics and Kitchen appliances. The goal in each domain is to classify a product review as ei- ther positive or negative. The datasets in each do- main are partitioned randomly into training data, de- velopment data and testing data with the proportion of 70%, 20% and 10% respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Competitor Methods for Multi-task Learning</head><p>The multi-task frameworks proposed by previous works are various while not all can be applied to the tasks we focused. Nevertheless, we chose two most related neural models for multi-task learning and im- plement them as strong competitor methods .</p><p>• MT-CNN: This model is proposed by Collobert and Weston (2008) with convolutional layer, in which lookup-tables are shared partially while other layers are task-specific.</p><p>• MT-DNN: The model is proposed by <ref type="bibr" target="#b22">Liu et al. (2015b)</ref> with bag-of-words input and multi- layer perceptrons, in which a hidden layer is shared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Hyperparameters and Training</head><p>The networks are trained with backpropagation and the gradient-based optimization is performed using the Adagrad update rule <ref type="bibr" target="#b10">(Duchi et al., 2011</ref>).</p><p>The word embeddings for all of the models are initialized with the 100d GloVe vectors (840B token version, ( <ref type="bibr" target="#b29">Pennington et al., 2014)</ref>) and fine-tuned during training to improve the performance. The other parameters are initialized by randomly sam- pling from uniform distribution in [−0.1, 0.1]. The mini-batch size is set to 16.</p><p>For each task, we take the hyperparameters which achieve the best performance on the development set via an small grid search over combinations of the initial learning rate [0.1, 0.01], l 2 regularization [0.0, 5E−5, 1E−5]. For datasets without develop- ment set, we use 10-fold cross-validation (CV) in- stead. The final hyper-parameters are set as <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Multi-task Learning of Movie Reviews</head><p>We first compare our proposed models with the baseline system for single task classification. <ref type="table">Table  3</ref> shows the classification accuracies on the movie reviews dataset. The row of "Single Task" shows the results of LSTM and ME-LSTM for each individ- ual task. With the help of multi-task learning, the performances of these four tasks are improved by 1.8% (ARC-I) and 2.9% (ARC-II) on average rela- tive to LSTM. We can find that the architecture of local-global hybrid external memory has better per- formances. The reason is that the global memory in ARC-I could store some task-specific information besides shared information, which maybe noisy to other tasks. Moreover, both of our proposed mod- els outperform MT-CNN and MT-DNN, which indi- cates the effectiveness of our proposed shared mech- anism. To give an intuitive evaluation of these re- sults, we also list the following state-of-the-art neu- ral models. With the help of utilizing the shared in- formation of several related tasks, our results out- perform most of state-of-the-art models. Although Tree-LSTM outperforms our method on SST-1, it needs an external parser to get the sentence topologi- cal structure. It is worth noticing that our models are generic and compatible with the other LSTM based models. For example, we can easily extend our mod- els to incorporate the Tree-LSTM model. <ref type="table">Table 4</ref> shows the classification accuracies on the tasks of product reviews. The row of "Single Task" shows the results of the baseline for each individ- ual task. With the help of global shared memory (ARC-I), the performances of these four tasks are improved by an average of 2.9%(2.6%) compared with LSTM(ME-LSTM). ARC-II achieves best per- formances on three sub-tasks, and its average im- provement is 3.7%(3.5%). Compared with MT-CNN and MT-DNN, our models achieve a better perfor- mance. We think the reason is that our models can not only share lexical information but share compli- cated patterns of sentences by reading/writing op- erations of external memory. Furthermore, these re- sults on product reviews are consistent with that on movie reviews, which shows our architectures are robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Multi-task Learning of Product Reviews</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Case Study</head><p>To get an intuitive understanding of what is happen- ing when we use shared memory to predict the class of text, we design an experiment to compare and an- alyze the difference between our models and vanilla LSTM, thereby demonstrating the effectiveness of our proposed architectures.</p><p>We sample two sentences from the SST-2 valida- tion dataset, and the changes of the predicted sen- timent score at different time steps are shown in <ref type="figure" target="#fig_2">Figure 3</ref>, which are obtained by vanilla LSTM and ARC-I respectively. Additionally, both models are bidirectional for better visualization. To get more insights into how the shared external memory in- fluences the specific task, we plot and observe the evolving activation of fusion gates through time, which controls signals flowing from a shared exter- nal memory to task-specific output, to understand the behaviour of neurons.</p><p>For the sentence "It is a cookie-cutter movie, a cut-and-paste job.", which has a negative sentiment, while the standard LSTM gives a wrong predic- tion due to not understanding the informative words "cookie-cutter" and "cut-and-paste".</p><p>In contrast, our model makes a correct prediction and the reason can be inferred from the activation of fusion gates. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>-(c), we can see clearly the neurons are activated much when they take input as "cookie-cutter" and "cut-and-paste", which indicates much information in shared mem- ory has be passed into LSTM, therefore enabling the model to give a correct prediction.</p><p>Another case "If you were not nearly moved to tears by a couple of scenes , you 've got ice water in your veins", a subjunctive clause introduced by "if ", has a positive sentiment.</p><p>As shown in <ref type="figure" target="#fig_2">Figure 3-(b,d)</ref>, vanilla LSTM failed to capture the implicit meaning behind the sentence, while our model is sensitive to the pattern "If ... were not ..." and has an accurate understanding of the sentence, which indicates the shared memory mech- anism can not only enrich the meaning of certain words, but teach some information of sentence struc- ture to specific task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Books DVDs Electronics Kitchen Avg∆</head><p>Single <ref type="table">Task</ref>   <ref type="table">Table 4</ref>: Accuracies of our models on product reviews dataset. The last column gives the improvement relative to LSTM and ME-LSTM respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Neural networks based multi-task learning has been proven effective in many NLP problems <ref type="bibr" target="#b5">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b13">Glorot et al., 2011;</ref><ref type="bibr" target="#b22">Liu et al., 2015b;</ref><ref type="bibr" target="#b24">Liu et al., 2016b</ref>). In most of these models, the lower layers are shared across all tasks, while top layers are task-specific. <ref type="bibr" target="#b5">Collobert and Weston (2008)</ref> used a shared rep- resentation for input words and solved different tra- ditional NLP tasks within one framework. However, only one lookup table is shared, and the other lookup tables and layers are task-specific.</p><p>Liu et al. (2015b) developed a multi-task DNN for learning representations across multiple tasks. Their multi-task DNN approach combines tasks of query classification and ranking for web search. But the input of the model is bag-of-word representation, which loses the information of word order.</p><p>More recently, several multi-task encoder- decoder networks were also proposed for neural machine translation ( <ref type="bibr" target="#b9">Dong et al., 2015;</ref><ref type="bibr" target="#b25">Luong et al., 2015;</ref><ref type="bibr" target="#b12">Firat et al., 2016)</ref>, which can make use of cross-lingual information. Unlike these works, in this paper we design two neural architectures with shared memory for multi- task learning, which can store useful information across the tasks. Our architectures are relatively loosely coupled, and therefore more flexible to ex- pand. With the help of shared memory, we can ob- tain better task-specific sentence representation by utilizing the knowledge obtained by other related tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this paper, we introduce two deep architectures for multi-task learning. The difference with the pre- vious models is the mechanisms of sharing infor- mation among several tasks. We design an external memory to store the knowledge shared by several re- lated tasks. Experimental results show that our mod- els can improve the performances of several related tasks by exploring common features.</p><p>In addition, we also propose a deep fusion strat- egy to integrate the information from the external memory into task-specific LSTM with a fusion gate.</p><p>In future work, we would like to investigate the other sharing mechanisms of neural network based multi-task learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Two architectures for modelling text with multi-task learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>RAE: Recursive Autoencoders with pre-trained word vectors from Wikipedia (Socher et al., 2011). MV-RNN: Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012). RNTN: Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013). DCNN: Dynamic Convolutional Neural Network with dynamic k-max pooling (Kalchbrenner et al., 2014; Denil et al., 2014). CNN-multichannel: Convolutional Neural Network (Kim, 2014). Tree-LSTM: A generalization of LSTMs to tree-structured network topologies (Tai et al., 2015).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>It is a cookie-cutter movie cut-and-you were not nearly moved tears by couple scenes you 've got ice water youris a cookie−cutter movie cut−and−Figure 3 :</head><label>3</label><figDesc>Figure 3: (a)(b) The change of the predicted sentiment score at different time steps. Y-axis represents the sentiment score, while X-axis represents the input words in chronological order. The red horizontal line gives a border between the positive and negative sentiments. (c)(d) Visualization of the fusion gate's activation.</figDesc><graphic url="image-1.png" coords="8,88.87,300.77,162.87,56.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Hyper-parameters of our models.</figDesc><table></table></figure>

			<note place="foot" n="1"> http://nlp.stanford.edu/sentiment. 2 http://ai.stanford.edu/ ˜ amaas/data/ sentiment/</note>

			<note place="foot" n="3"> https://www.cs.jhu.edu/ ˜ mdredze/ datasets/sentiment/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their valuable comments. This work was partially funded by National Natural Science Foundation of China (No. 61532011 and 61672162), the National High Technology Research and Development Pro-gram of China (No. 2015AA015408).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning efficient algorithms with hierarchical attentive memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Kurach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03218</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multitask learning. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Associative long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benigno</forename><surname>Uria</surname></persName>
		</author>
		<idno>abs/1602.03032</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Nal Kalchbrenner, and Alex Graves</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Demiraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.3830</idno>
		<title level="m">Modelling, visualising and summarising documents with a single convolutional neural network</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-task learning for multiple language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="179" to="211" />
		</imprint>
	</monogr>
	<note>Cognitive science</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multi-way, multilingual neural machine translation with a shared attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.01073</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classification</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-timescale long shortterm memory neural network for modelling sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on EMNLP</title>
		<meeting>the Conference on EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Representation learning using multi-task deep neural networks for semantic classification and information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep fusion LSTMs for text semantic matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent neural network for text classification with multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence</title>
		<meeting>International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06114</idno>
		<title level="m">Multi-task sequence to sequence learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semisupervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2431" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03729</idno>
		<title level="m">Largercontext language modelling</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
