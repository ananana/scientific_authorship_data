<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Net Models of Open-domain Discourse Coherence</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Net Models of Open-domain Discourse Coherence</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="198" to="209"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Discourse coherence is strongly associated with text quality, making it important to natural language generation and understanding. Yet existing models of coherence focus on measuring individual aspects of coherence (lexical overlap, rhetorical structure, entity centering) in narrow domains. In this paper, we describe domain-independent neural models of discourse coherence that are capable of measuring multiple aspects of coherence in existing sentences and can maintain coherence while generating new sentences. We study both discriminative models that learn to distinguish coherent from incoherent discourse, and generative models that produce coherent text, including a novel neural latent-variable Markovian generative model that captures the latent discourse dependencies between sentences in a text. Our work achieves state-of-the-art performance on multiple coherence evaluations, and marks an initial step in generating coherent texts given discourse contexts.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modeling discourse coherence (the way parts of a text are linked into a coherent whole) is essen- tial for summarization ( <ref type="bibr">Barzilay and McKeown, 2005</ref>), text planning <ref type="bibr">(Hovy, 1988;</ref><ref type="bibr" target="#b1">Marcu, 1997</ref>) question-answering ( <ref type="bibr" target="#b16">Verberne et al., 2007)</ref>, and even psychiatric diagnosis <ref type="bibr">(Elvevåg et al., 2007;</ref><ref type="bibr">Bedi et al., 2015)</ref>.</p><p>Various frameworks exist, each tackling aspects of coherence. Lexical cohesion <ref type="bibr">(Halliday and Hasan, 1976;</ref><ref type="bibr" target="#b5">Morris and Hirst, 1991</ref>) models chains of words and synonyms. Psychological mod- els of discourse ( <ref type="bibr">Foltz et al., 1998;</ref><ref type="bibr">Foltz, 2007;</ref><ref type="bibr" target="#b2">McNamara et al., 2010)</ref> use LSA embeddings to generalize lexical cohesion. Relational models like RST ( <ref type="bibr" target="#b0">Mann and Thompson, 1988;</ref><ref type="bibr">Lascarides and Asher, 1991)</ref> define relations that hierarchically structure texts. The entity grid model ( <ref type="bibr">Barzilay and Lapata, 2008)</ref> and its extensions 1 capture the referential coherence of entities moving in and out of focus across a text. Each captures only a single aspect of coherence, and all focus on scoring exist- ing sentences, rather than on generating coherent discourse for tasks like abstractive summarization.</p><p>Here we introduce two classes of neural models for discourse coherence. Our discriminative mod- els induce coherence by treating human generated texts as coherent examples and texts with random sentence replacements as negative examples, feed- ing LSTM sentence embeddings of pairs of consec- utive sentences to a classifier. These achieve state- of-the-art (96% accuracy) on the standard domain- specific sentence-pair-ordering dataset ( <ref type="bibr">Barzilay and Lapata, 2008)</ref>, but suffer in a larger open- domain setting due to the small semantic space that negative sampling is able to cover.</p><p>Our generative models are based on augument- ing encoder-decoder models with latent variables to model discourse relationships across sentences, including (1) a model that incorporates an HMM- LDA topic model into the generative model and (2) an end-to-end model that introduces a Markov- structured neural latent variable, inspired by re- cent work on training latent-variable recurrent nets <ref type="bibr">(Bowman et al., 2015;</ref><ref type="bibr" target="#b12">Serban et al., 2016b</ref>). These generative models obtain the best result on a large open-domain setting, including on the difficult task of reconstructing the order of every sentence in a paragraph, and our latent variable generative model significantly improves the coherence of text gener- ated by the model. Our work marks an initial step in building end- to-end systems to evaluate open-domain discourse coherence, and more importantly, generating coher- ent texts given discourse contexts.</p><p>The discriminative model treats cliques (sets of sen- tences surrounding a center sentence) taken from the original articles as coherent positive examples and cliques with random replacements of the center sentence as negative examples. The discriminative model can be viewed as an extended version of <ref type="bibr">Li and Hovy's (2014)</ref> model but is practical at large scale 2 . We thus make this section succinct.</p><p>Notations Let C denote a sequence of coherent texts taken from original articles generated by hu- mans. C is comprised of a sequence of sentences C = {s n−L , ..., s n−1 , s n , s n+1 , ..., s n+L } where L denotes the half size of the context window. Sup- pose each sentence s n consists of a sequence of words w n1 , ..., w nt , ..., w nM , where M is the num- ber of tokens in s n . Each word w is associated with a K dimensional vector h w and each sentence is associated with a K dimensional vector x s .</p><p>Each C contains 2L + 1 sentences, and is as- sociated with a (2L + 1) × K dimensional vector obtained by concatenating the representations of its constituent sentences. The sentence representation is obtained from LSTMs. After word compositions, we use the representation output from the final time step to represent the entire sentence. Another neu- ral network model with a sigmoid function on the very top layer is employed to map the concatena- tion of representations of its constituent sentences to a scalar, indicating the probability of the current clique being a coherent one or an incoherent one.</p><p>Weakness Two problems with the discriminative model stand out: First, it relies on negative sam- pling to generate negative examples. Since the sentence-level semantic space in the open-domain setting is huge, the sampled instances can only cover a tiny proportion of the possible negative candidates, and therefore don't cover the space of possible meanings. As we will show in the experi- ments section, the discriminative model performs competitively in specific domains, but not in the open domain setting. Secondly and more impor- tantly, discriminative models are only able to tell whether an already-given chunk of text is coherent or not. While they can thus be used in tasks like extractive summarization for sentence re-ordering, they cannot be used for coherent text generation in tasks like dialogue generation or abstractive text summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Generative Model</head><p>We therefore introduce three neural generative mod- els of discourse coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model 1: the SEQ2SEQ Model and its Variations</head><p>In a coherent context, a machine should be able to guess the next utterance given the preceding ones. A straightforward way to do that is to train a SEQ2SEQ model to predict a sentence given its contexts ( <ref type="bibr" target="#b14">Sutskever et al., 2014</ref>). Generating sen- tences based on neighboring sentences resembles skip-thought models ( <ref type="bibr">Kiros et al., 2015)</ref>, which build an encoder-decoder model by predicting to- kens in neighboring sentences. As shown in <ref type="figure" target="#fig_0">Figure 1a</ref>, given two consecutive sentences [s i , s i+1 ], one can measure the coher- ence by the likelihood of generating s i+1 given its preceding sentence s i (denoted by uni). This likeli- hood is scaled by the number of words in s i+1 (de- noted by N i+1 ) to avoid favoring short sequences.</p><formula xml:id="formula_0">L(s i , s i+1 ) = 1 N i+1 log p(s i+1 |s i )<label>(1)</label></formula><p>The probability can be directly computed using a pretrained SEQ2SEQ model <ref type="bibr" target="#b14">(Sutskever et al., 2014)</ref> or an attention-based model ( <ref type="bibr">Bahdanau et al., 2015;</ref><ref type="bibr">Luong et al., 2015)</ref>. In a coherent context, a machine should not only be able to guess the next utterance given the pre- ceding ones, but also the preceding one given the following ones. This gives rise to the coherence model (denoted by bi) that measures the bidirec- tional dependency between the two consecutive sentences:</p><formula xml:id="formula_1">L(s i , s i+1 ) = 1 N i log p B (s i |s i+1 ) + log 1 N i+1 p F (s i+1 |s i )<label>(2)</label></formula><p>We separately train two models: a forward model p F (s i+1 |s i ) that predicts the next sentence based on the previous one and a backward model p B (s i |s i+1 ) that predicts the previous sentence given the next sentence. p B (s i |s i+1 ) can be trained in a way similar to p F (s i+1 |s i ) with sources and targets swapped. It is worth noting that p B and p F are separate models and do not share parameters.</p><p>One problem with the described uni and bi mod- els is that sentences with higher language model probability (e.g., sentences without rare words) also tend to have higher conditional probability given their preceding or succeeding sentences. We are in- terested in measuring the informational gain from the contexts rather than how fluent the current sen- tence is. We thus propose eliminating the influence of the language model, which yields the following coherence score:</p><formula xml:id="formula_2">L(s i , s i+1 ) = 1 N i [log p B (s i |s i+1 ) − log p L (s i )] + 1 N i+1 [log p B (s i+1 |s i ) − log p L (s i+1 )]</formula><p>(3) where p L (s) is the language model probability for generating sentence s. We train an LSTM language model, which can be thought of as a SEQ2SEQ model with an empty source. A closer look at Eq. 3 shows that it is of the same form as the mutual information between s i+1 and s i , namely</p><formula xml:id="formula_3">log[p(s i+1 , s i )/p(s i+1 )p(s i )].</formula><p>Generation The scoring functions in Eqs. 1, 2, and 3 are discriminative, generating coherence scores for an already-given chunk of text. Eqs. 2 and 3 can not be directly used for generation pur- poses, since they requires the completion of s i+1 before the score can be computed. A normal strat- egy is to generate a big N-best list using Eq. 1 and then rerank the N-best list using Eq. 2 or 3 ( <ref type="bibr">Li et al., 2015a</ref>). The N-best list can be generated using standard beam search, or other algorithmic variations that promote diversity, coherence, etc. ( <ref type="bibr" target="#b13">Shao et al., 2017</ref>).</p><p>Weakness (1) The SEQ2SEQ model generates words sequentially based on an evolving hidden vector, which is updated by combining the current word representation with previously built hidden vectors. The generation process is thus not exposed to more global features of the discourse like topics. As the hidden vector evolves, the influence from contexts gradually diminishes, with language mod- els quickly dominating. (2) By predicting a sen- tence conditioning only on its left or right neighbor, the model lacks the ability to handle the longer- term discourse dependencies across the sentences of a text.</p><p>To tackle these two issues, we need a model that is able to constantly remind the decoder about the global meaning that it should convey at each word- generation step, a global meaning which can cap- ture the state of the discourse across the sentences of a text. We propose two models of this global meaning, a pipelined approach based on HMM- based topic models ( <ref type="bibr">Blei et al., 2003;</ref><ref type="bibr">Gruber et al., 2007)</ref>, and an end-to-end generative model with variational latent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">HMM-LDA based Generative Models</head><p>(HMM-LDA-GM)</p><p>In Markov topic models the topic depends on the previous topics in context ( <ref type="bibr" target="#b10">Ritter et al., 2010;</ref><ref type="bibr" target="#b6">Paul and Girju, 2010;</ref><ref type="bibr" target="#b17">Wang et al., 2011;</ref><ref type="bibr">Gruber et al., 2007;</ref><ref type="bibr" target="#b7">Paul, 2012)</ref>. The topic for the current sen- tence is drawn based on the topic of the preced- ing sentence (or word) rather than on the global document-level topic distribution in vanilla LDA.</p><p>Our first model is a pipelined one (the HMM- LDA-GM in <ref type="figure" target="#fig_0">Fig. 1b</ref>), in which an HMM-LDA model provides the SEQ2SEQ model with global information for token generation, with two compo- nents:</p><p>(1) Running HMM-LDA: we first run a sentence-level HMM-LDA similar to <ref type="bibr">Gruber et al. (2007)</ref>. Our implementation forces all words in a sentence to be generated from the same topic, and this topic is sampled from a distribution based on the topic from previous sentence. Let t n denote the distribution of topics for the current sentence, where t n ∈ R 1×T . We also associate each LDA topic with a K dimensional vector, representing the semantics embedded in this topic. The topic- representation matrix is denoted by V ∈ R T ×K , where T is the pre-specified number of topics in LDA. V is learned in the word predicting process when training encoder-decoder models.</p><p>(2) Training encoder-decoder models: For the current sentence s n , given its topic distribution t n , we first compute the topic representation z n for s n using the weighted sum of LDA topic vectors:</p><formula xml:id="formula_4">z n = t n × V (4)</formula><p>z n can be thought of as a discourse state vector that stores the information the current sentence needs to convey in the discourse, and is used to guide every step of word generation in s n . We run the encoder- decoder model, which subsequently predicts tokens in s n given s n−1 . This process is the same as the vanilla version of SEQ2SEQ models, the only dif- ference being that z n is incorporated into each step of decoding for hidden vector updates:</p><formula xml:id="formula_5">p(s n |z n , s n−1 ) = M t=1 p(w t |h t−1 , z n ) (5)</formula><p>V is updated along with parameters in the encoder- decoder model. z n influences each time step of decoding, and thus addresses the problem that vanilla SEQ2SEQ models gradually lose global information as the hidden representations evolve. z n is computed based on the topic distribution t n , which is ob- tained from the HMM-LDA model, thus model- ing the global Markov discourse dependency be- tween sentences of the text. <ref type="bibr">3</ref> The model can be adapted to the bi-directional setting, in which we separately train two models to handle the forward probability log p(t n |s n−1 , ...) and the backward one log p(t n |s n+1 ). The bi-directional (bi) strat- egy described in Eq. 3 can also be incorporated to remove the influence of language models.</p><p>Weakness Topic models (either vanilla or HMM versions) focus on word co-occurrences at the document-level and are thus very lexicon-based. Furthermore, given the diversity of topics in a dataset like Wikipedia but the small number of topic clusters, the LDA model usually produces very coarse-grained topics (politics, sports, history, etc.), assigning very similar topic distributions to consecutive sentences. These topics thus capture topical coherence but are too coarse-grained to cap- ture all the more fine-grained aspects of discourse coherence relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Variational Latent Variable Generative</head><p>Models (VLV-GM)</p><p>We therefore propose instead to train an end-to-end system, in which the meaning transitions between sentences can be naturally learned from the data. Inspired by recent work on generating sentences from a latent space ( <ref type="bibr" target="#b12">Serban et al., 2016b;</ref><ref type="bibr">Bowman et al., 2015;</ref><ref type="bibr">Chung et al., 2015)</ref>, we propose the VSV-GM model in <ref type="figure" target="#fig_0">Fig. 1c</ref>. Each sentence s n is again associated with a hidden vector representa- tion z n ∈ R K which stores the global information that the current sentence needs to talk about, but instead of obtaining z n from an upstream model like LDA, z n is learned from the training data. z n is a stochastic latent variable conditioned on all previous sentences and z n−1 :</p><formula xml:id="formula_6">p(z n |z n−1 , s n−1 , s n−2 , ...) = N (µ true zn , Σ true zn ) µ true zn = f (z n−1 , s n−1 , s n−2 , ...) Σ true zn = g(z n−1 , s n−1 , s n−2 , ...)<label>(6)</label></formula><p>where N (µ, Σ) is a multivariate normal distribu- tion with mean µ ∈ R K and covariance matrix Σ ∈ R K×K . Σ is a diagonal matrix. As can be seen, the global information z n for the current sen- tence depends on the information z n−1 for its pre- vious sentence as well as the text of the context sentences. This forms a Markov chain across all sentences. f and g are neural network models that take previous sentences and z n−1 , and map them to a real-valued representation using hierarchical LSTMs ( <ref type="bibr">Li et al., 2015b)</ref>  <ref type="bibr">4</ref> .</p><p>Each word w nt from s n is predicted using the concatenation of the representation previously build by the LSTMs (the same vector used in word prediction in vanilla SEQ2SEQ models) and z n , as shown in Eq.5.</p><p>We are interested in the posterior distribution p(z n |s 1 , s 2 , ..., s n−1 ), namely, the information that the current sentence needs to convey given the pre- ceding ones. Unfortunately, a highly non-linear mapping from z n to tokens in s n results in in-tractable inference of the posterior. A common so- lution is to use variational inference to learn another distribution, denoted by q(z n |s 1 , s 2 , ..., s N ), to ap- proximate the true posterior p(z n |s 1 , s 2 , ..., s n−1 ). The model's latent variables are obtained by max- imizing the variational lower-bound of observing the dataset:</p><formula xml:id="formula_7">log p(s 1 , .., s N ) ≤ N t=1 −D KL (q(z n |s n , s n−1 , ...)||p(z n |s n−1 , s n−2 , ...)) + E q(zn|sn,s n−1 ,...) log p(s n |z n , s n−1 , s n−2 , ...)<label>(7)</label></formula><p>This objective to optimize consists of two parts; the first is the KL divergence between the ap- proximate distribution q and the true posterior p(s n |z n , s n−1 , s n−2 , ...), in which we want to ap- proximate the true posterior using q. The second part E q(zn|sn,s n−1 ,...) log p(s n |z n , s n−1 , s n−2 , ...), predicts tokens in s n in the same way as in SEQ2SEQ models with the difference that it con- siders the global information z n .</p><p>The approximate posterior distribution q(z n |s n , s n−1 , ...) takes a form similar to p(z n |s n−1 , s n−2 , ...):</p><formula xml:id="formula_8">q(z n |s n , s n−1 , ...) = N (µ approx zn , Σ approx zn ) µ approx zn = f q (z n−1 , s n , s n−1 , ...) Σ approx zn = g q (z n−1 , s n , s n−1 , ...)<label>(8)</label></formula><p>f q and g q are of similar structures to f and g, using a hierarchical neural network model to map context tokens to vector representations.</p><p>Learning and Testing At training time, the ap- proximate posterior q(z n |z n−1 , s n , s n−1 , ...), the true distribution p(z n |z n−1 , s n−1 , s n−2 , ...), and the generative probability p(s n |z n , s n−1 , s n−2 , ... ). This sample is used to approximate the expectation E q log p(s n |z n , s n−1 , s n−2 , ...). Using z n , we can update the encoder-decoder model using SGD in a way similar to the standard SEQ2SEQ model, the only difference being that the current token to pre- dict not only depends on the LSTM output h t , but also z n . Given the sampled z n , the KL-divergence can be readily computed, and we update the model using standard gradient decent (details shown in the Appendix).</p><p>The proposed VLV-GM model can be adapted to the bi-directional setting and the bi setting similarly to the way LDA-based models are adapted.</p><p>The proposed model is closely related to many recent attempts in training variational autoencoders (VAE) ( <ref type="bibr">Kingma and Welling, 2013;</ref><ref type="bibr" target="#b9">Rezende et al., 2014</ref></p><note type="other">), variational or latent-variable recurrent nets (Bowman et al., 2015; Chung et al., 2015; Ji et al., 2016; Bayer and Osendorfer, 2014), hierarchical latent variable encoder-decoder models (Serban et al., 2016b,a).</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>In this section, we describe experimental results. We first evaluate the proposed models on discrimi- native tasks such as sentence-pair ordering and full paragraph ordering reconstruction. Then we look at the task of coherent text generation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sentence Ordering, Domain-specific Data</head><p>Dataset We first evaluate the proposed algo- rithms on the task of predicting the correct ordering of pairs of sentences predicated on the assumption that an article is always more coherent than a ran- dom permutation of its sentences ( <ref type="bibr">Barzilay and Lapata, 2008)</ref>. A detailed description of this com- monly used dataset and training/testing are found in the Appendix. We report the performance of the following base- lines widely used in the coherence literature.</p><p>(1) Entity Grid Model: The grid model presented in <ref type="bibr">Barzilay and Lapata (2008)</ref>. Results are directly taken from <ref type="bibr">Barzilay and Lapata's (2008)</ref> paper. We also consider variations of entity grid models, such as Louis and Nenkova (2012) which models the cluster transition probability and the Graph Based Approach which uses a graph to represent the entity transitions needed for local coherence computation ( <ref type="bibr">Guinaudeau and Strube, 2013)</ref>.</p><p>(2) <ref type="bibr">Li and Hovy (2014)</ref>: A recursive neural model computes sentence representations based on parse trees. Negative sampling is used to con- struct negative incoherent examples. Results are from their papers.</p><p>(3) <ref type="bibr">Foltz et al. (1998)</ref> computes the semantic relatedness of two text units as the cosine similarity between their LSA vectors. The coherence of a discourse is the average of the cosine of adjacent sentences. We used this intuition, but with more modern embedding models: (1) 300-dimensional Glove word vectors ( <ref type="bibr" target="#b8">Pennington et al., 2014</ref>), em- beddings for a sentence computed by averaging the embeddings of its words (2) Sentence representa- tions obtained from LDA ( <ref type="bibr">Blei et al., 2003</ref>) with 300 topics, trained on the Wikipedia dataset. Re- sults are reported in <ref type="table" target="#tab_2">Table 2</ref>. The extended version of the discriminative model described in this work significantly outperforms the parse-tree based re- cursive models presented in <ref type="bibr">Li and Hovy (2014)</ref> as well as all non-neural baselines. It achieves al- most perfect accuracy on the earthquake dataset and 93% on the accident dataset, marking a signif- icant advancement in the benchmark. Generative models (both vanilla SEQ2SEQ and the proposed variational model) do not perform competitively on this dataset. We conjecture that this is due to the small size of the dataset, leading the generative model to overfit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluating Ordering on Open-domain</head><p>Since the dataset presented in <ref type="bibr">Barzilay and Lapata (2008)</ref> is quite domain-specific, we propose test- ing coherence with a much larger, open-domain dataset: Wikipedia. We created a test set by ran- domly selecting 984 paragraphs from Wikipedia dump 2014, each paragraph consisting of at least 16 sentences. The training set is 30 million sentences not overlapping with the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Binary Permutation Classification</head><p>We adopt the same strategy as in <ref type="bibr">Barzilay and Lapata (2008)</ref>, in which we generate pairs of sen- tence permutations from the original Wikipedia paragraphs. We follow the protocols described in the subsection and each pair whose original paragraph's score is higher than its permutation is treated as being correctly classified, else incorrectly  classified. Models are evaluated using accuracy. We implement the Entity Grid Model ( <ref type="bibr">Barzilay and Lapata, 2008</ref>) using the Wikipedia training set as a baseline, the detail of which is presented in the Ap- pendix. Other baselines consist of the Glove and LDA updates of the lexical coherence baselines ( <ref type="bibr">Foltz et al., 1998</ref>).</p><formula xml:id="formula_9">Model Accuracy VLV-GM (MMI) 0.873 VLV-GM (bi) 0.860 VLV-GM (uni) 0.839 LDA-HMM-GM (MMI) 0.847 LDA-HMM-GM (bi) 0.837 LDA-HMM-GM (uni) 0.814 SEQ2SEQ (MMI) 0.840 SEQ2SEQ (bi) 0.821 SEQ2SEQ (uni) 0</formula><p>Results <ref type="table" target="#tab_2">Table 2</ref> presents results on the binary classification task. Contrary to the findings on the domain specific dataset in the previous subsection, the discriminative model does not yield compelling results, performing only slightly better than the en- tity grid model. We believe the poor performance is due to the sentence-level negative sampling used by the discriminative model. Due to the huge seman- tic space in the open-domain setting, the sampled instances can only cover a tiny proportion of the possible negative candidates, and therefore don't cover the space of possible meanings. By contrast the dataset in <ref type="bibr">Barzilay and Lapata (2008)</ref> is very domain-specific, and the semantic space is thus rel- atively small. By treating all other sentences in the document as negative, the discriminative strategy's negative samples form a much larger proportion of the semantic space, leading to good performance. Generative models perform significantly better than all other baselines. Compared with the dataset in <ref type="bibr">Barzilay and Lapata (2008)</ref>, overfitting is not an issue here due to the great amount of training data. In line with our expectation, the MMI model outper- forms the bidirectional model, which in turn out- performs the unidirectional model across all three generative model settings. We thus only report MMI results for experiments below. The VLV-GM model outperforms that the LDA-HMM-GM model, which is slightly better than the vanila SEQ2SEQ models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VLV-GM (MMI)</head><p>0.256 LDA-HMM-GM (MMI) 0.237 SEQ2SEQ (MMI) 0.226 Entity Grid Model 0.143 <ref type="bibr">Foltz et al. (1998) (Glove)</ref> 0.084  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Paragraph Reconstruction</head><p>The accuracy of our models on the binary task of detecting the original sentence ordering is very high, on both the prior small task and our large open-domain version. We therefore believe it is time for the community to move to a more difficult task for measuring coherence.</p><p>We suggest the task of reconstructing an origi- nal paragraph from a bag of constituent sentences, which has been previously used in coherence eval- uation <ref type="bibr">(Lapata, 2003)</ref>. More formally, given a set of permuted sentences s 1 , s 2 , ..., s N (N the number of sentences in the original document), our goal is return the original (presumably most coherent) ordering of s.</p><p>Because the discriminative model calculates the coherence of a sentence given the known previous and following sentences, it cannot be applied to this task since we don't know the surrounding context. Hence, we only use the generative model. The first sentence of a paragraph is given: for each step, we compute the coherence score of placing each remaining candidate sentence to the right of the partially constructed document. We use beam search with beam size 10. We use the Entity Grid model as a baseline for both the settings.</p><p>Evaluating the absolute positions of sentences would be too harsh, penalizing orderings that main- tain relative position between sentences through which local coherence can be manifested. We there- fore use <ref type="bibr">Kendall's τ (Lapata, 2003</ref><ref type="bibr">, 2006</ref>), a metric of rank correlation for evaluation. See the Ap- pendix for details of Kendall's τ computation. We observe a pattern similar to the results on the bi- nary classification task, where the VLV-GM model performs the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Adversarial evaluation on Text Generation Quality</head><p>Both the tasks above are discriminative ones. We also want to evaluate different models' ability to generate coherent text chunks. The experiment is set up as follow: each encoder-decoder model is first given a set of context sentences (3 sentences).</p><p>The model then generates a succeeding sentence using beam-search given the contexts. For the uni- directional setting, we directly take the most prob- able sequence and for the bi-directional and MMI, we rerank the N-best list using the backward prob- ability and language model probability.</p><p>We conduct experiments on multi-sentence gen- eration, in which we repeat the generative process described above for N times, where N =1,2,3. At the end of each turn, the context is updated by adding in the newly generated sequence, and this sequence is used as the source input to the encoder- decoder model for next sequence generation. For example, when N is set to 2, given the three con- text sentences context-a, context-b and context-c, we first generate sen-d given the three context sen- tences and then generate sen-e given the sen-d, context-a, context-b and context-c.</p><p>For evaluation, standard word overlap metrics such as BLEU or ROUGE are not suited for our task, and we use adversarial evaluation <ref type="bibr">Bowman et al. (2015)</ref>; <ref type="bibr">Anjuli and Vinyals (2016)</ref>. In ad- versarial evaluation, we train a binary discrimi- nant function to classify a sequence as machine generated or human generated, in an attempt to evaluate the model's sentence generation capabil- ity. The evaluator takes as input the concatenation of the contexts and the generated sentences (i.e., context-a, context-b and context-c, sen-d , sen-e in the example described above), <ref type="bibr">5</ref> and outputs a scalar, indicating the probability of the current text chunk being human-generated. Training/dev/test sets are held-out sets from the one on which gener- ative models are trained. They respectively contain 128,000/12,800/12,800 instances. Since discrimi- native models cannot generate sentences, and thus cannot be used for adversarial evaluation, they are skipped in this section.</p><p>We report Adversarial Success (AdverSuc for short), which is the fraction of instances in which a model is capable of fooling the evaluator. Adver- <ref type="figure">Figure 2</ref>: An overview of training the adversarial evaluator using a hierarchical neural model. Green denotes input con- texts. Red denotes a sentence from human-generated texts, treated as a positive example. Purple denotes a sentence from machine-decoded texts, treated as a negative example.</p><p>Suc is the difference between 1 and the accuracy achieved by the evaluator. Higher values of Ad- verSuc for a dialogue generation model are better. AdverSuc-N denotes the adversarial accuracy value on machine-generated texts with N turns. <ref type="table" target="#tab_4">Table 4</ref> show AdverSuc numbers for different models. As can be seen, the latent variable model VLV-GM is able to generate chunk of texts that are most indistinguishable from coherent texts from humans. This is due to its ability to handle the dependency between neighboring sentences. Per- formance declines as the number of turns increases due to the accumulation of errors and current mod- els' inability to model long-term sentence-level dependency. All models perform poorly on the adver-3 evaluation metric, with the best adversarial success value being 0.081 (the trained evaluator is able to distinguish between human-generated and machine generated dialogues with greater than 90 percent accuracy for all models).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Analysis</head><p>With the aim of guiding future investigations, we also briefly explore our model qualitatively, exam- ining the coherence scores assigned to some artifi- cial miniature discourses that exhibit various kinds of coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case 1: Lexical Coherence</head><p>Pinochet was arrested. His arrest was unexpected. The examples suggest that the model handles lexical coherence, correctly favoring the 1st over the 2nd, and the 3rd over the 4th examples. Note that the coherence score for the final example is negative, which means conditioning on the first sentence actually decreases the likelihood of gener- ating the second one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case 2: Temporal Order</head><p>Washington was unanimously elected president in the first two national elections. He oversaw the creation of a strong, well- financed national government. 1.48 Washington oversaw the creation of a strong, well-financed national government. He was unanimously elected president in the first two national elections. 0.72</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case 3: Causal Relationship</head><p>Bret enjoys video games; therefore, he sometimes is late to appointments. 0.69 Bret sometimes is late to appointments; therefore, he enjoys video games. -0.07</p><p>Cases 2 and 3 suggest the model may, at least in these simple cases, be capable of addressing the much more complex task of dealing with tempo- ral and causal relationships. Presumably this is because the model is exposed in training to the gen- eral preference of natural text for temporal order, and even for the more subtle causal links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case 4: Centering/Referential Coherence</head><p>Mary ate some apples. She likes apples. 3.06 She ate some apples. Mary likes apples. 2.41</p><p>The model seems to deal with simple cases of referential coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example3: 2.40</head><p>John went to his favorite music store to buy a piano. He had frequented the store for many years. He was excited that he could finally buy a piano. He arrived just as the store was closing for the day. Example4: 1.62 John went to his favorite music store to buy a piano. It was a store John had frequented for many years He was excited that he could finally buy a piano.. It was closing just as John arrived.</p><p>In these final examples from Miltsakaki and <ref type="bibr" target="#b4">Kukich (2004)</ref>, the model successfully captures the fact that the second text is less coherent due to rough shifts. This suggests that the discourse em- bedding space may be able to capture a representa- tion of entity focus.</p><p>Of course all of these these qualitative evalu- ations are only suggestive, and a deeper under- standing of what the discourse embedding space is capturing will likely require more sophisticated visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>205</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We investigate the problem of open-domain dis- course coherence, training discriminative models that treating natural texts as coherent and permu- tations as non-coherent, and Markov generative models that can predict sentences given their neigh- bors.</p><p>Our work shows that the traditional evaluation metric (ordering pairs of sentences in small do- mains) is completely solvable by our discrimina- tive models, and we therefore suggest the com- munity move to the harder task of open-domain full-paragraph sentence ordering.</p><p>The proposed models also offer an initial step in generating coherent texts given contexts, which has the potential to benefit a wide range of generation tasks in NLP. Our latent variable neural models, by offering a new way to learn latent discourse-level features of a text, also suggest new directions in discourse representation that may bring benefits to any discourse-aware NLP task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Supplemental Material</head><p>Details for the domain specific dataset ( <ref type="bibr">Barzilay and Lapata, 2008)</ref> The corpus consists of 200 articles each from two domains: NTSB airplane ac- cident reports (V=4758, 10.6 sentences/document) and AP earthquake reports (V=3287, 11.5 sen- tences/document), split into training and testing. For each document, pairs of permutations are gen- erated <ref type="bibr">6</ref> . Each pair contains the original document order and a random permutation of the sentences from the same document.</p><p>Training/Testing details for models on the do- main specific dataset We use reduced versions of both generative and discriminative models to allow fair comparison with baselines. For the dis- criminative model, we generate noise negative ex- amples from random replacements in the training set, with the only difference that random replace- ments only come from the same document. We use 300 dimensional embeddings borrowed from GLOVE ( <ref type="bibr" target="#b8">Pennington et al., 2014</ref>) to initialize word embeddings. Word embeddings are kept fixed dur- ing training and we update LSTM parameters using AdaGrad <ref type="bibr">(Duchi et al., 2011)</ref>. For the generative model, due to the small size of the dataset, we train a one layer SEQ2SEQ model with word dimension- ality and number of hidden neurons set to 100. The model is trained using SGD with AdaGrad <ref type="bibr" target="#b19">(Zeiler, 2012)</ref>.</p><p>The task requires a coherence score for the whole document, which is comprised of multiple cliques. We adopt the strategy described in <ref type="bibr">Li and Hovy (2014)</ref> by breaking the document into a series of cliques which is comprised of a sequence of consecutive sentences. The document-level coher- ence score is attained by averaging its constituent cliques. We say a document is more coherent if it achieves a higher average score within its con- stituent cliques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation of Entity Grid Model</head><p>For each noun in a sentence, we extract its syntactic role (subject, object or other). We use a wikipedia dump parsed using the Fanse Parser ( <ref type="bibr" target="#b15">Tratz and Hovy, 2011</ref>). Subjects and objects are extracted based on nsubj and dobj relations in the depen- dency trees. ( <ref type="bibr">Barzilay and Lapata, 2008</ref>) define two versions of the Entity Grid Model, one us- ing full coreference and a simpler method using only exact-string coreference; Due to the difficulty of running full coreference resolution tens of mil- lions of Wikipedia sentences, we follow other re- searchers in using Barzilay and Lapata's simpler method <ref type="bibr">(Feng and Hirst, 2012;</ref><ref type="bibr">Burstein et al., 2010;</ref><ref type="bibr">Barzilay and Lapata, 2008)</ref>. <ref type="bibr">7</ref> Kendall's τ Kendall's τ is computed based on the number of inversions in the rankings as follows:</p><formula xml:id="formula_10">τ = 1 − 2# of inversions N × (N − 1)<label>(9)</label></formula><p>where N denotes the number of sentences in the original document and inversions denote the num- ber of interchanges of consecutive elements needed to reconstruct the original document. Kendall's τ can be efficiently computed by counting the num- ber of intersections of lines when aligning the orig- inal document and the generated document. We refer the readers to <ref type="bibr">Lapata (2003)</ref> for more details.</p><p>Derivation for Variation Inference For sim- plicity, we use µ post and Σ approx to denote µ approx (z n ) and Σ approx (z n ), µ true and Σ true to denote µ true (z n ) and Σ true (z n ). The KL- divergence between the approximate distribution q(z n |z n−1 , s n , s n−1 , ...) and the true distribution p(z n |z n−1 , s n−1 , s n−2 , ...) in the variational infer- ence is given by:</p><p>D KL (q(z n |z n−1 , s n , s n−1 , ...)||p(z n |z n−1 , s n−1 , s n−2 , ...) </p><p>7 Our implementation of the Entity Grid Model is built upon public available code at https://github.com/ karins/CoherenceFramework. where k denotes the dimensionality of the vector. Since z n has already been sampled and thus known, µ approx , Σ approx , µ true , Σ true and consequently Eq10 can be readily computed. The gradient with respect to µ approx , Σ approx , µ true , Σ true can be respectively computed, and the error is then back- propagated to the hierarchical neural models that are used to compute them. We refer the readers to <ref type="bibr">Doersch (2016)</ref> for more details about how a general VAE model can be trained.</p><p>Our generate models offer a powerful way to rep- resent the latent discourse structure in a complex embedding space, but one that is hard to visualize.</p><p>To help understand what the model is doing, we examine some relevant examples, annotated with the (log-likelihood) coherence score from the MMI generative model, with the goal of seeing (qualita- tively) the kinds of coherence the model seems to be representing. (The MMI can be viewed as the in- formational gain from conditioning the generation of the current sentence on its neighbors.) 209</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the proposed generative models for discourse coherence modeling.</figDesc><graphic url="image-1.png" coords="3,326.41,62.81,180.00,233.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>) are trained jointly by maximizing the variational lower bound with respect to their parameters: a sample z n is first drawn from the posterior dis- tribution q, namely N (µ approx zn , Σ approx zn</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>true Σ approx ) − k + log detΣ true detΣ approx +(µ true − µ approx ) −1 Σ −1 true (µ true − µ approx ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Performance on the open-domain binary classification dataset of 984 Wikipedia paragraphs.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 : Performances of the proposed models on the open- domain paragraph reconstruction dataset.</head><label>3</label><figDesc></figDesc><table>Model 
adver-1 adver-2 adver-3 
VLV-GM (MMI) 
0.174 
0.120 
0.054 
LDA-HMM-GM (MMI) 
0.130 
0.104 
0.043 
SEQ2SEQ (MMI) 
0.120 
0.090 
0.039 
SEQ2SEQ (bi) 
0.108 
0.078 
0.030 
SEQ2SEQ (uni) 
0.101 
0.068 
0.024 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 : Adversarial Success for different models.</head><label>4</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Adding coreference (Elsner and Charniak, 2008), named entities (Eisner and Charniak, 2011), discourse relations (Lin et al., 2011) and entity graphs (Guinaudeau and Strube, 2013). 198</note>

			<note place="foot" n="2"> Li and Hovy&apos;s (2014) recursive neural model operates on parse trees, which does not support batched computation and is therefore hard to scale up.</note>

			<note place="foot" n="3"> This pipelined approach is closely related to recent work that incorporates LDA topic information into generation models in an attempt to leverage context information (Ghosh et al., 2016; Xing et al., 2016; Mei et al., 2016)</note>

			<note place="foot" n="4"> Sentences are first mapped to vector representations using a LSTM model. Another level of LSTM at the sentence level then composes representations of the multiple sentences to a single vector.</note>

			<note place="foot" n="5"> The model uses a hierarchical neural structure that first maps each sentence to a vector representation, with another level of LSTM on top of the constituent sentences, producing a single vector to represent the entire chunk of texts.</note>

			<note place="foot" n="6"> Permutations downloaded from people.csail.mit. edu/regina/coherence/CLsubmission/.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Ernst Althaus, Nikiforos Karamanis, and Alexander</p><p>Koller. 2004. Computing locally coherent dis-courses. In Proceedings of ACL 2004.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rhetorical structure theory: Toward a functional theory of text organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="281" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">From local to global coherence: A bottom-up approach to text planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI/IAAI. Citeseer</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="629" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cohmetrix: Capturing linguistic features of cohesion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><forename type="middle">S</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><forename type="middle">M</forename><surname>Louwerse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">M</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><forename type="middle">C</forename><surname>Graesser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discourse Processes</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="292" to="330" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Coherent dialogue with attention-based language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Walter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06997</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluation of text coherence for electronic essay scoring systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Kukich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="25" to="55" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lexical cohesion computed by thesaural relations as an indicator of the structure of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="48" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A twodimensional topic-aspect model for discovering multi-faceted topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Urbana</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mixed membership markov models for unsupervised conversation modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="94" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
		<title level="m">Stochastic backpropagation and approximate inference in deep generative models</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised modeling of twitter conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="172" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multiresolution recurrent neural networks: An application to dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Talamadupula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00776</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06069</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Kurzweil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03185</idno>
		<title level="m">Generating long and diverse responses with neural conversation models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A fast, accurate, non-projective, semantically-enriched parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tratz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1257" to="1268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evaluating discoursebased answer extraction for why-question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzan</forename><surname>Verberne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lou</forename><surname>Boves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelleke</forename><surname>Oostdijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter-Arno</forename><surname>Coppen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 30th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="735" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Structural topic model for latent topical structure analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1526" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Topic augmented neural response generation with a joint attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08340</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
