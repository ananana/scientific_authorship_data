<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transition-based Dependency Parsing Using Two Heterogeneous Gated Recursive Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqian</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transition-based Dependency Parsing Using Two Heterogeneous Gated Recursive Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recently, neural network based dependency parsing has attracted much interest, which can effectively alleviate the problems of data sparsity and feature engineering by using the dense features. However , it is still a challenge problem to sufficiently model the complicated syntactic and semantic compositions of the dense features in neural network based methods. In this paper, we propose two heterogeneous gated recursive neu-ral networks: tree structured gated re-cursive neural network (Tree-GRNN) and directed acyclic graph structured gated recursive neural network (DAG-GRNN). Then we integrate them to automatically learn the compositions of the dense features for transition-based dependency parsing. Specifically, Tree-GRNN models the feature combinations for the trees in stack, which already have partial dependency structures. DAG-GRNN models the feature combinations of the nodes whose dependency relations have not been built yet. Experiment results on two prevalent benchmark datasets (PTB3 and CTB5) show the effectiveness of our proposed model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transition-based dependency parsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP com- munity. The traditional discriminative dependency parsing methods have achieved great success ( <ref type="bibr" target="#b20">Koo et al., 2008;</ref><ref type="bibr" target="#b17">He et al., 2013;</ref><ref type="bibr" target="#b3">Bohnet, 2010;</ref><ref type="bibr" target="#b18">Huang and Sagae, 2010;</ref><ref type="bibr" target="#b37">Zhang and Nivre, 2011;</ref><ref type="bibr" target="#b23">Martins et al., 2009;</ref><ref type="bibr" target="#b25">McDonald et al., 2005;</ref><ref type="bibr" target="#b28">Nivre et al., 2006;</ref><ref type="bibr" target="#b21">Kübler et al., 2009</ref>; Goldberg and Nivre, is the standard RNN for con- stituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the depen- dency relations between the nodes; (c) is DAG- GRNN for the nodes without given topological structure.</p><p>2013; <ref type="bibr" target="#b9">Choi and McCallum, 2013;</ref><ref type="bibr" target="#b0">Ballesteros and Bohnet, 2014</ref>). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering <ref type="bibr" target="#b4">(Chen and Manning, 2014)</ref>. Recently, distributed representations have been widely used in a variety of natural language pro- cessing (NLP) tasks <ref type="bibr" target="#b11">(Collobert et al., 2011;</ref><ref type="bibr" target="#b12">Devlin et al., 2014;</ref><ref type="bibr" target="#b31">Socher et al., 2013;</ref><ref type="bibr" target="#b35">Turian et al., 2010;</ref><ref type="bibr" target="#b27">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b2">Bengio et al., 2003)</ref>. Specific to the transition-based parsing, the neu- ral network based methods have also been increas- ingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance ( <ref type="bibr" target="#b22">Le and Zuidema, 2014;</ref><ref type="bibr" target="#b34">Stenetorp, 2013;</ref><ref type="bibr" target="#b1">Bansal et al., 2014;</ref><ref type="bibr" target="#b4">Chen and Manning, 2014;</ref><ref type="bibr" target="#b38">Zhu et al., 2015)</ref>.</p><p>However, most of the existing neural network based methods still need some efforts in feature engineering. For example, most methods often se- lect the first and second leftmost/rightmost chil- dren of the top nodes in stack, which could miss some valuable information hidden in the unchosen nodes. Besides, the features of the selected nodes are just simply concatenated and then fed into neu- ral network. Since the concatenation operation is relatively simple, it is difficult to model the com-plicated feature combinations which can be man- ually designed in the traditional discrete feature based methods.</p><p>To tackle these problems, we use two het- erogeneous gated recursive neural networks, tree structured gated recursive neural network (Tree- GRNN) and directed acyclic graph gated struc- tured recursive neural network (DAG-GRNN), to model each configuration during transition based dependency parsing. The two proposed GRNNs introduce the gate mechanism ( <ref type="bibr" target="#b10">Chung et al., 2014)</ref> to improve the standard recursive neural network (RNN) <ref type="bibr" target="#b31">(Socher et al., 2013;</ref><ref type="bibr">Socher et al., 2014</ref>), and can model the syntactic and semantic compo- sitions of the nodes during parsing. <ref type="figure" target="#fig_0">Figure 1</ref> gives a rough sketch for the standard RNN, Tree-GRNN and DAG-GRNN. Tree-GRNN is applied to the partial-constructed trees in stack, which have already been constructed according to the previous transition actions. DAG-GRNN is ap- plied to model the feature composition of nodes in stack and buffer which have not been labeled their dependency relations yet. Intuitively, Tree-GRNN selects and merges features recursively from chil- dren nodes into their parent according to their de- pendency structures, while DAG-GRNN further models the complicated combinations of extracted features and explicitly exploits features in differ- ent levels of granularity.</p><p>To evaluate our approach, we experiment on two prevalent benchmark datasets: English Penn Treebank 3 (PTB3) and Chinese Penn Treebank 5 (CTB5) datasets. Experiment results show the ef- fectiveness of our proposed method. Compared to the parser of <ref type="bibr" target="#b4">Chen and Manning (2014)</ref>, we re- ceive 0.6% (UAS) and 0.9% (LAS) improvement on PTB3 test set, while we receive 0.8% (UAS) and 1.3% (LAS) improvement on CTB5 test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Network Based Transition</head><p>Dependency Parsing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transition Dependency Parsing</head><p>In this paper, we employ the arc-standard tran- sition systems <ref type="bibr" target="#b29">(Nivre, 2004</ref>) and examine only greedy parsing for its efficiency. <ref type="figure">Figure 2</ref> gives an example of arc-standard transition dependency parsing.</p><p>In transition-based dependency parsing, the consecutive configurations of parsing process can be defined as c (i) = (s (i) , b (i) , A (i) ) which con- sists of a stack s, a buffer b, and a set of dependency arcs A. Then, the greedy pars- ing process consecutively predicts the actions based on the features extracted from the corre- sponding configurations. For a given sentence w 1 , . . . , w n , parsing process starts from a initial configuration c (0) = ( <ref type="bibr">[ROOT ]</ref>, [w 1 , . . . , w n ], ∅), and terminates at some configuration c (2n) = ( <ref type="bibr">[ROOT ]</ref>, ∅, A (2n) ], where n is the length of the given sentence w 1:n . As a result, we derive the parse tree of the sentence w 1:n according to the arcs set A (2n) .</p><p>In arc-standard system, there are three types of actions: Left-Arc, Right-Arc and Shift. Denot- ing s j (j = 1, 2, . . . ) as the j th top element of the stack, and b j (j = 1, 2, . . . ) as the j th front ele- ment of the buffer, we can formalize the three ac- tions of arc-standard system as:</p><p>• Left-Arc(l) adds an arc s 2 ← s 1 with label l and removes s 2 from the stack, resulting a new arc l(s 1 , s 2 ). Precondition: |s| ≥ 3 (The ROOT node cannot be child node).</p><p>• Right-Arc(l) adds an arc s 2 → s 1 with label l and removes s 1 from the stack, resulting a new arc l(s 2 , s 1 ). Precondition: |s| ≥ 2.</p><p>• Shift removes b 1 from the buffer, and adds it to the stack. Precondition: |b| ≥ 1.</p><p>The greedy parser aims to predict the correct transition action for a given configuration. There are two versions of parsing: unlabeled and labeled versions. The set of possible action candidates T = 2n l + 1 in the labeled version of parsing, and T = 3 in the unlabeled version, where n l is number of different types of arc labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Neural Network Based Parser</head><p>In neural network architecture, the words, POS tags and arc labels are mapped into distributed vectors (embeddings). Specifically, given the word embedding matrix E w ∈ R de×nw , each word w i is mapped into its corresponding column e w w i ∈ R de of E w according to its index in the dictionary, where d e is the dimensionality of embeddings and n w is the dictionary size. Likewise, The POS and arc labels are also mapped into embeddings by the POS embedding matrix E t ∈ R de×nt and arc la- bel embedding matrix E l ∈ R de×n l respectively, where n t and n l are the numbers of distinct POS tags and arc labels respectively. Correspondingly, embeddings of each POS tag t i and each arc label l i are e t t i ∈ R de and e l l i ∈ R de extracted from E t and E l respectively.   <ref type="figure">Figure 2</ref>: An example of arc-standard transition dependency parsing.</p><formula xml:id="formula_0">s3 s2 s1 b1 b2 b3 s2.lc1 s2.lc2 s2.rc2 s2.rc1 … … … … s2.rc2.lc2 s2.rc2.lc1 s2.rc2.rc1 s2.rc2.rc2 Stack Buffer s2.lc1 s1 …… b2 s2.rc2.rc1 s2.lc2.rc1 S L R … … Concatenate h=tanh(W1x+b1) p=softmax(W2h+b2) Input x</formula><p>Hidden units h Probability of each action p Sub tree  Following <ref type="bibr" target="#b4">Chen and Manning (2014)</ref>, a set of el- ements S from stack and buffer (e.g.</p><formula xml:id="formula_1">S = {s 2 .lc 2 .rc 1 , s 2 .lc 1 , s 1 , b 2 , s 2 .rc 2 .rc 1 , . . . })</formula><p>is chosen as input. Specifically, the information (word, POS or label) of each element in the set S (e.g.</p><formula xml:id="formula_2">{s 2 .lc 2 .rc 1 .t, s 2 .lc 1 .l, s 1 .w, s 1 .t, b 2 .w, . . . })</formula><p>are extracted and mapped into their corresponding embeddings. Then these embeddings are concate- nated as the input vector x ∈ R ˆ d . A special token NULL is used to represent a non-existent element.</p><p>We perform a standard neural network using one hidden layer with d h hidden units followed by a softmax layer as:</p><formula xml:id="formula_3">h = g(W 1 x + b 1 ),<label>(1)</label></formula><formula xml:id="formula_4">p = softmax(W 2 h + b 2 ),<label>(2)</label></formula><p>where</p><formula xml:id="formula_5">W 1 ∈ R d h × ˆ d , b 1 ∈ R d h , W 2 ∈ R |T |×d h , b 2 ∈ R |T | .</formula><p>Here, g is a non-linear function which can be hyperbolic tangent, sigmoid, cube (Chen and Manning, 2014), etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Recursive Neural Network</head><p>Recursive neural network (RNN) is one of classi- cal neural networks, which performs the same set of parameters recursively on a given structure (e.g. syntactic tree) in topological order <ref type="bibr" target="#b30">(Pollack, 1990;</ref><ref type="bibr" target="#b31">Socher et al., 2013</ref>).</p><p>In the simplest case, children nodes are com- bined into their parent node using a weight matrix W which is shared across the whole network, fol- lowed by a non-linear function g(·). Specifically, given the left child node vector h L ∈ R d and right child node vector h R ∈ R d , their parent node vec- tor h P ∈ R d will be formalized as:</p><formula xml:id="formula_6">h P = g W h L h R ,<label>(3)</label></formula><p>where W ∈ R d×2d and g is a non-linear function as mentioned above.</p><formula xml:id="formula_7">s2.lc1 s2.lc2 s2.rc2 s2.rc1 … … s3 s2 s1 b1 b2 b3 … S L R Softmax … … s2.rc2.lc2 s2.rc2.lc1</formula><p>s2.rc2.rc1 s2.rc2.rc2</p><p>Stack Buffer</p><p>Sub tree</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAG-GRNN</head><p>Tree-GRNN <ref type="figure">Figure 4</ref>: Architecture of our proposed depen- dency parser using two heterogeneous gated recur- sive neural networks.</p><p>(DAG-GRNN). Tree-GRNN is applied to the sub- trees with partial dependency relations in stack which have already been constructed according to the previous transition actions. DAG-GRNN is employed to model the feature composition of nodes in stack and buffer which have not been la- beled their dependency relations yet. <ref type="figure">Figure 4</ref> shows the whole architecture of our model, which integrates two different GRNNs to predict the action for each parsing configuration. The detailed descriptions of two GRNNs will be discussed in the following two subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tree Structured Gated Recursive Neural Network</head><p>It is a natural way to merge the information from children nodes into their parent node recursively according to the given tree structures in stack. Al- though the dependency relations have been built, it is still hard to apply the recursive neural net- work (as Eq. 3) directly for the uncertain num- ber of children of each node in stack. By aver- aging operation on children nodes <ref type="formula">(</ref>  In Tree-GRNN, each node p of trees in stack is composed of three components: state vector of left children nodes v p l ∈ R dc , state vector of current node v p n ∈ R dn and state vector of right children nodes v p r ∈ R dc , where d n and d c indicate the cor- responding vector dimensionalities. Particularly, we represent information of node p as a vector</p><formula xml:id="formula_8">v p =   v p l v p n v p r   ,<label>(4)</label></formula><p>where v p ∈ R q and q = 2d c +d n . Specifically, v p n contains the information of current node including its word form p.w, pos tag p.t and label type p.l as shown in Eq. 5, and v p l and v p r are initialized by zero vectors 0 ∈ R dc , then update as Eq. 6.</p><formula xml:id="formula_9">v p n = tanh     e w p.w e t p.t e l p.l     ,<label>(5)</label></formula><p>where word embedding e w p.w ∈ R de , pos embed- ding e t p.t ∈ R de and label embedding e l p.l ∈ R de are extracted from embedding matrices E w , E t and E l according to the indices of the correspond- ing word p.w, pos p.t and label p.l respectively. Specifically, in the case of unlabeled attachment parsing, we ignore the last term e l p.l in Eq. 5. Thus, the dimensionality d n of v p n varies. In la- beled attachment parsing case, we set a special to- ken NULL to represent label p.l if not available (e.g. p is the node in stack or buffer).</p><p>By given node p and its left children nodes p.lc i and right children nodes p.rc i , we update the left</p><note type="other">children information v p l and right children infor- mation v p r as</note><formula xml:id="formula_10">v p l = tanh(W l 1 NL(p) i o p.lc i v p.lc i + b l ), v p r = tanh(Wr 1 NR(p) i o p.rc i v p.rc i + br),<label>(6)</label></formula><note type="other">where o p.lc i and o p.rc i are the reset gates of the nodes p.lc i and p.rc i respectively as shown in Eq. 7. In addition, functions N L (p) and N R (p) re- sult the numbers of left and right children nodes of node p respectively. The operator indicates element multiplication here. W l ∈ R dc×q and W r ∈ R dc×q are weight matrices. b l ∈ R dc and b r ∈ R dc are bias terms.</note><p>The reset gates o p.lc i and o p.rc i can be formal- ized as</p><formula xml:id="formula_11">o p.lc i = σ(W o v p.lc i v p n + b o ), o p.rc i = σ(W o v p.rc i v p n + b o ),<label>(7)</label></formula><p>where σ indicates the sigmoid function, W o ∈ R q×(q+dn) and b o ∈ R q . By the mechanism above, we can summarize the whole information into the stack recursively from children nodes to their parent using the partial-built tree structure. Intuitively, the gate mechanism can selectively choose the crucial fea- tures of a child node according to the gate state which is derived from the current child node and its parent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Directed Acyclic Graph Structured Gated Recursive Neural Network</head><p>Previous neural based parsing works feed the ex- tracted features into a standard neural network with one hidden layer. Then, the hidden units are fed into a softmax layer, outputting the probability vector of available actions. Actually, it cannot well model the complicated combinations of extracted features. As for the nodes, whose dependency relations are still unknown, we propose another recursive neural network namely directed acyclic graph structured gated recursive neural network (DAG-GRNN) to better model the interactions of features. Intuitively, the DAG structure models the com- binations of features by recursively mixing the in- formation from the bottom layer to the top layer  as shown in <ref type="figure">Figure 4</ref>. The concatenation opera- tion can be regraded as a mix of features in differ- ent levels of granularity. Each node in the directed acyclic graph can be seen as a complicated feature composition of its governed nodes. Moreover, we also use the gate mechanism to better model the feature combinations by introduc- ing two kinds of gates, namely "reset gate" and "update gate". Intuitively, each node in the net- work seems to preserve all the information of its governed notes without gates, and the gate mech- anism similarly plays a role of filter which de- cides how to selectively exploit the information of its children nodes, discovering and preserving the crucial features.</p><p>DAG-GRNN structure consists of minimal structures as shown in <ref type="figure" target="#fig_6">Figure 6</ref>. Vectors h P , h L , h R and h ˆ P ∈ R q denote the value of the parent node P , left child node L, right child node R and new activation nodê P respectively. The value of parent node h P ∈ R q is computed as:</p><formula xml:id="formula_12">h P = z ˆ P h ˆ P + z L h L + z R h R ,<label>(8)</label></formula><p>where z ˆ P , z L and z R ∈ R q are update gates for new activation nodê P , left child node L and right child node R respectively. Operator indicates element-wise multiplication.</p><p>The update gates z can be formalized as:</p><formula xml:id="formula_13">z =   z ˆ P z L z R   ∝ exp(W z   h ˆ P h L h R   ),<label>(9)</label></formula><p>which are constrained by:</p><formula xml:id="formula_14">     [z ˆ P ] k + [zL] k + [zR] k = 1, 1 ≤ k ≤ q, [z ˆ P ] k ≥ 0, 1 ≤ k ≤ q, [zL] k ≥ 0, 1 ≤ k ≤ q, [zR] k ≥ 0, 1 ≤ k ≤ q,<label>(10)</label></formula><p>where W z ∈ R 3q×3q is the coefficient of update gates.</p><p>The value of new activation node h ˆ P is com- puted as:</p><formula xml:id="formula_15">h ˆ P = tanh(W ˆ P r L h L r R h R ),<label>(11)</label></formula><p>where W ˆ P ∈ R q×2q , r L ∈ R q , r R ∈ R q . r L and r R are the reset gates for left child node L and right child node R respectively, which can be formalized as:</p><formula xml:id="formula_16">r = r L r R = σ(W r h L h R ),<label>(12)</label></formula><p>where W r ∈ R 2q×2q is the coefficient of two reset gates and σ indicates the sigmoid function. Intuitively, the reset gates r partially read the information from the left and right children, out- putting a new activation node h ˆ P , while the up- date gates z selectively choosing the information among the the new activation nodê P , the left child node L and the right child node R. This gate mechanism is effective to model the combinations of features.</p><p>Finally, we concatenate all the nodes in the DAG-GRNN structure as input x of the architec- ture described in Section 2.2, resulting the proba- bility vector for all available actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Inference</head><p>We use greedy decoding in parsing. At each step, we apply our two GRNNs on the current config- uration to extract the features. After softmax op- eration, we choose the feasible transition with the highest possibility, and perform the chosen tran- sition on the current configuration to get the next configuration state.</p><p>In practice, we do not need calculate the Tree- GRNN over the all trees in the stack on the current configuration. Instead, we preserve the represen- tations of trees in the stack. When we need apply a new transition on the configuration, we update the relative representations using Tree-GRNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Training</head><p>We use the maximum likelihood (ML) criterion to train our model. By extracting training set (x i , y i ) from gold parse trees using a shortest stack oracle which always prefers Left-Arc(l) or Right-Arc(l) action over Shift, the goal of our model is to mini- mize the loss function with the parameter set θ:</p><formula xml:id="formula_17">J(θ) = − 1 m m i=1 log p(y i |x i ; θ)+ λ 2m θ 2 2 , (13)</formula><p>where m is number of extracted training examples which is as same as the number of all configura- tions. Following <ref type="bibr" target="#b31">Socher et al. (2013)</ref>, we use the diag- onal variant of AdaGrad ( <ref type="bibr" target="#b13">Duchi et al., 2011</ref>) with minibatch strategy to minimize the objective. We also employ dropout strategy to avoid overfitting.</p><p>In practice, we perform DAG-GRNN with two hidden layers, which gets the best perfor- mance. We use the approximated gradient for Tree-GRNN, which only performs gradient back propagation on the first two layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head><p>To evaluate our proposed model, we experiment on two prevalent datasets: English Penn Treebank 3 (PTB3) and Chinese Penn Treebank 5 (CTB5) datasets.</p><p>• English For English Penn Treebank 3 (PTB3) dataset, we use sections 2-21 for training, section 22 and section 23 as de- velopment set and test set respectively. We adopt CoNLL Syntactic Dependencies (CD) <ref type="bibr" target="#b19">(Johansson and Nugues, 2007</ref>) using the LTH Constituent-to-Dependency Conversion Tool.</p><p>• Chinese For Chinese Penn Treebank 5 (CTB5) dataset, we follow the same split as described in <ref type="bibr" target="#b36">(Zhang and Clark, 2008)</ref>. De- pendencies are converted by the Penn2Malt tool with the head-finding rules of <ref type="bibr" target="#b36">(Zhang and Clark, 2008</ref>).</p><p>Embedding size de = 50 Dimensionality of child node vector dc = 50 Initial learning rate α = 0.05 Regularization λ = 10 −8 Dropout rate p = 20% <ref type="table">Table 1</ref>: Hyper-parameter settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experimental Settings</head><p>For parameter initialization, we use random ini- tialization within (-0.01, 0.01) for all parameters except the word embedding matrix E w . Specifi- cally, we adopt pre-trained English word embed- dings from <ref type="bibr" target="#b11">(Collobert et al., 2011</ref>). And we pre- train the Chinese word embeddings on a huge un- labeled data, the Chinese Wikipedia corpus, with word2vec toolkit (Mikolov et al., 2013a). <ref type="table">Table 1</ref> gives the details of hyper-parameter set- tings of our approach. In addition, we set mini- batch size to 20. In all experiments, we only take s 1 , s 2 , s 3 nodes in stack and b 1 , b 2 , b 3 nodes in buffer into account. We also apply dropout strat- egy here, and only dropout at the nodes in stack and buffer with probability p = 20%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results</head><p>The experiment results on PTB3 and CTB5 datasets are list in <ref type="table" target="#tab_3">Table 2 and 3</ref> respectively. On all datasets, we report unlabeled attachment scores (UAS) and labeled attachment scores (LAS). Con- ventionally, punctuations are excluded in all eval- uation metrics.</p><p>To evaluate the effectiveness of our approach, we compare our parsers with feature-based parser and neural-based parser. For feature-based parser, we compare our models with two prevalent parsers: MaltParser ( <ref type="bibr" target="#b28">Nivre et al., 2006</ref>) and MSTParser ( <ref type="bibr" target="#b24">McDonald and Pereira, 2006</ref>). For neural-based parser, we compare our results with parser of <ref type="bibr" target="#b4">Chen and Manning (2014)</ref>. Compared with parser of <ref type="bibr" target="#b4">Chen and Manning (2014)</ref>, our parser with two heterogeneous gated recursive neural networks (Tree-GRNN+DAG-GRNN) re- ceives 0.6% (UAS) and 0.9% (LAS) improvement on PTB3 test set, and receives 0.8% (UAS) and 1.3% (LAS) improvement on CTB5 test set.</p><p>Since that speed of algorithm is not the focus of our paper, we do not optimize the speed a lot. On CTB (UAS), it takes about 2 days to train Tree- GRNN+DAG-GRNN model with CPU only. The testing speed is about 2.7 sentences per second. All implementation is based on Python.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Effects of Gate Mechanisms</head><p>We adopt five different models: plain parser, Tree-RNN parser, Tree-GRNN parser, Tree-RNN+DAG-GRNN parser, and Tree- GRNN+DAG-GRNN parser. The experiment results show the effectiveness of our proposed two heterogeneous gated recursive neural networks.</p><p>Specifically, plain parser is as same as parser of <ref type="bibr" target="#b4">Chen and Manning (2014)</ref>. The difference between them is that plain parser only takes the nodes in stack and buffer into account, which uses a simpler feature template than parser of <ref type="bibr" target="#b4">Chen and Manning (2014)</ref>. As plain parser omits all children nodes of trees in stack, it performs poorly compared with parser of <ref type="bibr" target="#b4">Chen and Manning (2014)</ref>.</p><p>In addition, we find plain parser outperforms MaltParser (standard) on PTB3 dataset making about 1% progress, while it performs poorer than MaltParser (standard) on CTB5 dataset. It shows that the children nodes of trees in stack is of great importance, especially for Chinese. Moreover, it also shows the effective- ness of neural network based model which could represent complicated features as compacted em- beddings. Tree-RNN parser additionally exploits all the children nodes of trees in stack, which is a simplified version of Tree-GRNN without incor- porating the gate mechanism described in Section 4.1. In anther word, Tree-RNN omits the gate terms o p.lc i and o p.rc i in Eq. 6. As we can see, the results are significantly boosted by utilizing the all information in stack, which again shows the importance of children nodes of trees in stack. Although the results of Tree-RNN are compara- ble to results of <ref type="bibr" target="#b4">Chen and Manning (2014)</ref>, it not outperforms parser of <ref type="bibr" target="#b4">Chen and Manning (2014)</ref> in all cases (e.g. UAS on CTB5), which implies that exploiting all information without selection might lead to incorporate noise features. More- over, Tree-GRNN parser further boosts the perfor- mance by incorporating the gate mechanism. In- tuitively, Tree-RNN who exploits all the informa- tion of stack without selection cannot well capture the crucial features, while Tree-GRNN with gate mechanism could selectively choose and preserve the effective features by adapting the current gate state.</p><p>We also experiment on parsers using two heterogeneous gated recursive neural networks: Tree-RNN+DAG-GRNN parser and Tree- GRNN+DAG-GRNN parser. The similarity of   two parsers is that they all employ the DAG structured recursive neural network with gate mechanism to model the combination of features extracted from stack and buffer. The difference between them is the former one employs the Tree-RNN without gate mechanism to model the features of stack, while the later one employs the gated version (Tree-GRNN). Again, the perfor- mance of these two parsers is further boosted, which shows DAG-GRNN can well model the combinations of features which is summarized by Tree-(G)RNN structure. In addition, we find the performance does not drop a lot in almost cases by turning off the gate mechanism of Tree-GRNN, which implies that the DAG-GRNN can help selecting the information from trees in stack, even it has not been selected by gate mechanism of Tree-GRNN yet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Convergency Speed</head><p>To further analyze the convergency speed of our approach, we compare the UAS results on devel- opment sets of two datasets for first ten epoches as shown in <ref type="figure" target="#fig_7">Figure 7</ref> and 8. As plain parser only take the nodes in stack and buffer into ac-  count, the performance is much poorer than the rest parsers. Moreover, Tree-GRNN converges slower than Tree-RNN, which shows that it might be more difficult to learn this gate mechanism. By introducing the DAG-GRNN, both Tree-RNN and Tree-GRNN parsers become faster to converge, which shows that the DAG-GRNN is of great help in boosting the convergency speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Many neural network based methods have been used for transition based dependency parsing.  and <ref type="bibr" target="#b1">Bansal et al. (2014)</ref> used the dense vectors (embeddings) to represent words or features and found these representations are complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embeddings) of words or features.</p><p>Stenetorp (2013) first used RNN for transition based dependency parsing. He followed the stan- dard RNN and used the binary combination to model the representation of two linked words. But his model does not achieve the performance of the traditional method. <ref type="bibr" target="#b22">Le and Zuidema (2014)</ref> proposed a genera- tive re-ranking model with Inside-Outside Recur- sive Neural Network (IORNN), which can pro- cess trees both bottom-up and top-down. How- ever, IORNN works in generative way and just es- timates the probability of a given tree, so IORNN cannot fully utilize the incorrect trees in k-best candidate results. Besides, IORNN treats depen- dency tree as a sequence, which can be regarded as a generalization of simple recurrent neural net- work (SRNN) <ref type="bibr" target="#b15">(Elman, 1990)</ref>.</p><p>Although the two methods also used RNN, they just deal with the binary combination, which is un- natural for dependency tree. <ref type="bibr" target="#b38">Zhu et al. (2015)</ref> proposed a recursive convolu- tional neural network (RCNN) architecture to cap- ture syntactic and compositional-semantic repre- sentations of phrases and words in a dependency tree. Different with the original recursive neu- ral network, they introduced the convolution and pooling layers, which can model a variety of com- positions by the feature maps and choose the most informative compositions by the pooling layers. <ref type="bibr" target="#b4">Chen and Manning (2014)</ref> improved the transition-based dependency parsing by represent- ing all words, POS tags and arc labels as dense vectors, and modeled their interactions with neu- ral network to make predictions of actions. Their method only relies on dense features, and is not able to automatically learn the most useful feature conjunctions to predict the transition action.</p><p>Compared with <ref type="bibr" target="#b4">(Chen and Manning, 2014</ref>), our method can fully exploit the information of all the descendants of a node in stack with Tree-GRNN. Then DAG-GRNN automatically learns the com- plicated combination of all the features, while the traditional discrete feature based methods need manually design them. <ref type="bibr" target="#b14">Dyer et al. (2015)</ref> improved the transition-based dependency parsing using stack long short term memory neural network and received significant improvement on performance. They focused on exploiting the long distance dependencies and in- formation, while we aims to automatically model the complicated feature combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we pay attention to the syntac- tic and semantic composition of the dense fea- tures for transition-based dependency parsing. We propose two heterogeneous gated recursive neu- ral networks, Tree-GRNN and DAG-GRNN. Each hidden neuron in two proposed GRNNs can be re- garded as a different combination of the input fea- tures. Thus, the whole model has an ability to sim- ulate the design of the sophisticated feature com- binations in the traditional discrete feature based methods.</p><p>Although the two proposed GRNNs are only used for the greedy parsing based on arc-standard transition system in this paper, it is easy to gen- eralize them to other transition systems and graph based parsing. In future work, we would also like to extend our GRNNs for the other NLP tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sketch of three recursive neural networks (RNN). (a) is the standard RNN for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Configurations</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Architecture of neural network based transition dependency parsing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 gives</head><label>3</label><figDesc>Figure 3 gives the architecture of neural network based parser. Following Chen and Manning (2014), a set of elements S from stack and buffer (e.g. S = {s 2 .lc 2 .rc 1 , s 2 .lc 1 , s 1 , b 2 , s 2 .rc 2 .rc 1 ,. .. }) is chosen as input. Specifically, the information (word, POS or label) of each element in the set S (e.g. {s 2 .lc 2 .rc 1 .t, s 2 .lc 1 .l, s 1 .w, s 1 .t, b 2 .w,. .. }) are extracted and mapped into their corresponding embeddings. Then these embeddings are concatenated as the input vector x ∈ R</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Minimal structure of tree structured gated recursive neural network (Tree-GRNN). The solid arrow denotes that there is a weight matrix on the link, while the dashed one denotes none.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Minimal structure of directed acyclic graph structured gated recursive neural network (DAG-GRNN). The solid arrow denotes that there is a weight matrix on the link, while the dashed one denotes none.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Performance of different models on PTB3 development set. UAS: unlabeled attachment score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Performance of different models on CTB5 development set. UAS: unlabeled attachment score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance of different models on PTB3 
dataset. UAS: unlabeled attachment score. LAS: 
labeled attachment score. 

models 
Dev 
Test 
UAS LAS UAS LAS 
Malt:standard 
82.4 80.5 82.4 80.6 
Malt:eager 
91.2 79.3 80.2 78.4 
MSTParser 
84.0 82.1 83.0 81.2 
Chen's Parser 
84.0 82.4 83.9 82.4 
Plain 
81.6 79.3 81.1 78.8 
Tree-RNN 
83.5 82.5 83.8 82.7 
Tree-GRNN 
84.2 82.5 84.3 83.1 
Tree-RNN+DAG-GRNN 84.5 83.3 84.5 83.1 
Tree-GRNN+DAG-GRNN 84.6 83.6 84.7 83.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Performance of different models on CTB5 dataset. UAS: unlabeled attachment score. LAS: labeled attachment score.</figDesc><table></table></figure>

			<note place="foot" n="4"> Architecture of Two Heterogeneous Gated Recursive Neural Networks for Transition-based Dependency Parsing In this paper, we apply the idea of recursive neural network (RNN) to dependency parsing task. RNN needs a pre-defined topological structure. However, in each configuration during parsing, just partial dependency relations have been constructed, while the remains are still unknown. Besides, the standard RNN can just deal with the binary tree. Therefore we cannot apply the standard RNN directly. Here, we propose two heterogeneous recursive neural networks: tree structured gated recursive neural network (Tree-GRNN) and directed acyclic graph structured gated recursive neural network</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous review-ers for their valuable comments. This work was partially funded by the National Natural Science Foundation of China (61472088, 61473092), Na-tional High Technology Research and Develop-ment Program of China (2015AA015408), Shang-hai Science and Technology Development Funds (14ZR1403200).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic feature selection for agenda-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics</title>
		<meeting>the 25th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tailoring continuous word representations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Very high accuracy and fast dependency parsing is not a contradiction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="89" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feature embedding for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08" />
			<biblScope unit="page" from="816" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gated recursive neural network for Chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sentence modeling with gated recursive neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with selectional branching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1052" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.08075</idno>
		<title level="m">Transitionbased dependency parsing with stack long shortterm memory</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Training deterministic parsers with non-deterministic oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="403" to="414" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dynamic feature selection for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1455" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic programming for linear-time incremental parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Extended constituent-to-dependency conversion for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Nugues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th Nordic Conference of Computational Linguistics</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="105" to="112" />
		</imprint>
		<respStmt>
			<orgName>University of Tartu</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simple semi-supervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The insideoutside recursive neural network model for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="729" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Concise integer linear programming formulations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="342" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Online learning of approximate dependency parsing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Online large-margin training of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Maltparser: A data-driven parser-generator for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2216" to="2219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Incrementality in deterministic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together</title>
		<meeting>the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recursive distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan B Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="105" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL conference</title>
		<meeting>the ACL conference</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing using recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="562" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with rich non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="188" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A re-ranking model for dependency parser with recursive convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
