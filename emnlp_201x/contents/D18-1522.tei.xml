<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Concept Abstractness Using Weak Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ella</forename><surname>Rabinovich</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Noam Slonim † † IBM Research Dept. of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sznajder</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Noam Slonim † † IBM Research Dept. of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Spector</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Noam Slonim † † IBM Research Dept. of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Shnayderman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Noam Slonim † † IBM Research Dept. of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranit</forename><surname>Aharonov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Noam Slonim † † IBM Research Dept. of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Konopnicki</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Noam Slonim † † IBM Research Dept. of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Concept Abstractness Using Weak Supervision</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4854" to="4859"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4854</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce a weakly supervised approach for inferring the property of abstractness of words and expressions in the complete absence of labeled data. Exploiting only minimal linguistic clues and the contextual usage of a concept as manifested in textual data, we train sufficiently powerful classifiers, obtaining high correlation with human labels. The results imply the applicability of this approach to additional properties of concepts, additional languages, and resource-scarce scenarios.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>During the last decades, the influence of psy- cholinguistic properties of words on cognitive pro- cesses has become a major topic of scientific in- quiry. Among the most studied psycholinguistic attributes are concreteness, familiarity, imagery, and average age of acquisition. Abstractness (the opposite of concreteness) quantifies the degree to which an expression denotes an entity that can be directly perceived by human senses.</p><p>Word abstractness ratings were first collected by <ref type="bibr" target="#b20">Spreen and Schulz (1966)</ref> and <ref type="bibr" target="#b16">Paivio et al. (1968)</ref>, and made available in the MRC database <ref type="bibr" target="#b2">(Coltheart, 1981</ref>) for 4,292 English words. Since its release, this database has stimulated research in a wide range of linguistic tasks, as well as artificial intelligence and cognitive studies. Despite their evident usefulness, resources providing abstract- ness ratings are relatively rare and of limited size. Here, we address the task of automatically infer- ring the abstractness rating of a concept by ap- plying a weakly supervised approach that exploits minimal linguistic clues.</p><p>Studies on derivational morphological pro- cesses indicate that word meaning is often entailed by its morphology. As an example, word suffixa- tion by -ant or -ent is used to denote a person, as * *Work done while the author was at <ref type="bibr">IBM Research.</ref> in assistant, while the suffix -hood yields nouns meaning "condition of being", as in childhood. A wide range of word-formation processes was de- scribed by <ref type="bibr" target="#b7">Huddleston and Pullum (2002)</ref>; in par- ticular, the authors detail categories of suffixes that are used to derive words, broadly perceived as ab- stract, e.g., -ism as in feminism, or -ness as in agreeableness.</p><p>Concept abstractness indicators are also likely to be manifested in its contextual usage. Consider the two sentences below, each embedding abstract and concrete words -one describing feminism and the other screwdriver -respectively:</p><p>Second-and third-wave feminism in China in- volved a reexamination of women's roles dur- ing the communist revolution and other reform movements, and new discussions about whether women's equality has been fully achieved.</p><p>Many screwdriver handles are not smooth and often not round, but have bumps or other irreg- ularities to improve grip and to prevent the tool from rolling when on a flat surface.</p><p>We hypothesize that the immediate neighbor- hood of a word as reflected in embedding sen- tences captures the signal of abstractness. In the examples above, several potential clues for the de- gree of word abstractness are underlined.</p><p>Correspondingly, we propose a method for in- ferring the degree of abstractness of concepts in the complete absence of labeled data, by exploit- ing (1) a minimal set of morphological word- formation clues; and (2) a text corpus for learning the context in which words tend to appear.</p><p>We demonstrate that this method allows us to infer the abstractness ratings of unigram, bigram and trigram Wikipedia concepts (titles) -the task that, to the best of our knowledge, was only ad- dressed through manual labeling so far <ref type="bibr" target="#b1">(Brysbaert et al., 2014</ref>). The main contribution of this work is, therefore, in the proposal and evaluation of a weakly supervised methodology for inferring the abstractness rating of concepts, potentially appli- cable to additional languages. The suggested ap- proach may also be applicable for predicting other word and concept properties, when those are man- ifested in both morphology and context. Finally, we release a dataset of 300K Wikipedia concepts automatically rated for their degree of abstract- ness, and additional 1500 unigram, bigram and trigram concepts annotated with both manual and predicted scores. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>A large body of research addressed the relations of word abstractness and cognitive processes <ref type="bibr" target="#b3">(Connell and Lynott, 2012;</ref><ref type="bibr" target="#b5">Gianico-Relyea and Altarriba, 2012;</ref><ref type="bibr" target="#b12">Oliveira et al., 2013;</ref><ref type="bibr" target="#b11">Nishiyama, 2013;</ref><ref type="bibr" target="#b15">Paivio, 2013;</ref><ref type="bibr" target="#b0">Barber et al., 2013</ref>). Computational investigation of word abstractness and concrete- ness has been a prolific field of recent research, laying out an empirical foundation for the theoret- ically motivated hypotheses on the characteristics of these properties. A ranker trained on psycholin- guistic features extracted from the MRC database (in combination with other features) reached first place in the English Lexical Simplification task at SemEval 2012 <ref type="bibr" target="#b8">(Jauhar and Specia, 2012)</ref>. <ref type="bibr" target="#b6">Hill and Korhonen (2014)</ref>  A comprehensive survey of psycholinguistic and memory research on word concreteness is pre-sented in <ref type="bibr" target="#b1">Brysbaert et al. (2014)</ref> (BWK), who con- ducted a large-scale manual annotation of con- creteness ratings for over 40K concepts, further used by <ref type="bibr" target="#b18">Rothe et al. (2016)</ref> to infer concreteness ratings for the whole Google News lexicon. To the best of our knowledge, our work is the first attempt to automatically infer the property of concept ab- stractness in the complete absence of labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Predicting concept abstractness 3.1 Abstractness indicators</head><p>Nominalization is a word-formation process that involves the formation of nouns from bases of other classes by means of affixation. As an ex- ample, a derivational suffix can be added to an ad- jective (capable+ity for capability) or a verb (re- act+tion for reaction) to create a noun. Various word-formation processes often enrich words with meaning associated with certain semantic group- ing. <ref type="bibr" target="#b7">Huddleston and Pullum (2002)</ref> detail nomi- nalization processes that serve to form nouns de- noting a "state" or "condition of being", which in turn are broadly associated with abstractness. As such, the suffixes -ety, -ity and -ness carry over the general meaning of "quality or state of being" and the suffix -ism is used to form nouns denoting a range of doctrines, beliefs and movements <ref type="bibr" target="#b7">(Huddleston and Pullum, 2002</ref>). Additional suffixes that tend to form English nouns with high degree of abstractness include -ance, -ence, -ation, -ution, -dom, -hood, -ship and -y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset</head><p>We used the English Wikipedia 2 article titles as a proxy for retrieving frequently used single-and multi-word expressions, thereby associating over 5M Wikipedia titles with concepts.</p><p>Training data We chose two abstractness sig- nals, manifested by the suffixes -ism and -ness, representing different types of abstract meanings. We extracted 1,040 potentially abstract unigram Wikipedia titles suffixed by either of the two (the positive class). The -admittedly noisy -concrete (negative) class was generated by randomly select- ing the same number of unigram concepts from the complementary set of titles.In both cases, we set a threshold 3 on the frequency of a concept in the corpus, and filtered out non-alphabetic unigrams and unigrams containing special characters. We assessed the quality of the positive and negative weakly-labeled training unigrams by manual an- notation of their level of abstractness, obtaining abstractness prior of 93% in the set of presumably abstract concepts, and concreteness prior of 81% for the opposite class.</p><p>Given this set of weakly-labeled positive and negative concepts, we randomly selected a set of Wikipedia sentences that include any of these con- cepts (equally split by positive and negative un- igrams), to be used in the training phase, while limiting sentence length to the range of 10 to 70 tokens. This step resulted in about 400K train sentences in each class, 800K in total. The final preprocessing phase involved masking a sentence concept with a generic token, aiming to prevent the classifier from training on the concept itself, and instead training on its contextual usage.</p><p>Evaluation data A randomly selected set of 1500 Wikpedia concepts (with the minimum of 500 occurrences per concept), split equally be- tween unigrams, bigrams and trigrams, and dis- tinct from the training set, was used for testing prediction. We henceforth refer to this set of con- cepts as the evaluation set. Each of these con- cepts was manually annotated for abstractness on the 1-7 scale by seven in-house labelers, using an adaptation of the guidelines by <ref type="bibr" target="#b20">Spreen and Schulz (1966)</ref> to the multi-word scenario:</p><p>Words or phrases may refer to persons, places and things that can be seen, heard, felt, smelled or tasted or to more abstract concepts that can- not be experienced by our senses. The purpose of this task is to rate a list of concepts with respect to "concreteness" in terms of sense-experience. Any expression that refers to objects, materials or persons should receive a high concreteness rat- ing; any expression that refers to an abstract con- cept that cannot be experienced by the senses should receive a low concreteness rating. Con- crete concepts typically have physical or concrete existence, while abstract do not. Think of the con- cepts "onion" and "nationalism" -"onion" can be experienced by our senses and therefore should be rated as concrete (1); "nationalism" cannot be experienced by the senses as such and therefore should be rated as abstract <ref type="formula">(7)</ref>.</p><p>Word polysemy is a common challenge in tasks related to lexical semantics. As such, our percep- tion of the concreteness rate of the concept bank may vary depending on whether a financial institu- tion or a river bank is concerned. While we could not avoid this issue altogether (since working with pre-trained word representations that do not carry disambiguation information), we ensured that all in-house labelers annotated the same word sense by providing them with Wikipedia definition of the most frequent sense of a concept.</p><p>The final abstractness score was computed as the average over individual annotations. The av- erage pairwise weighted Kappa agreement 4 on the entire set of 1500 concepts was 0.65.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Classification models</head><p>We hypothesize that words that share similar de- gree of abstractness tend to share certain similari- ties in their contextual usage; that, in contrast to concepts that exhibit opposite abstractness rate. Indeed, a statistical significance test applied to the (weak) positive and negative training data (Sec- tion 3.2) reveals markers such as {parish, move- ment, century, spiritual, life, doctrine, nature, regime} sharing excessive frequency in sentences containing abstract concepts. The very essence of this phenomenon is captured by distributed word representations ( <ref type="bibr" target="#b10">Mikolov et al., 2013;</ref><ref type="bibr" target="#b17">Pennington et al., 2014</ref>), a.k.a. word embeddings, learned based on the contextual usage of words. We there- fore trained three classifiers, each exploiting dif- ferent language properties, as described below.</p><p>Naive Bayes (NB) Using solely word counts in textual data, we used a simple probabilistic Naive Bayes classifier, with a bag-of-words feature set extracted from the 800K sentences containing pos- itive and negative training concepts. Given a sen- tence containing a test concept, its degree of ab- stractness was defined as the posterior probability assigned by the classifier. Aiming at robust clas- sification, we retrieved 500 sentences containing each test concept from the corpus. Consequently, the final abstractness score of a concept was calcu- lated by averaging the predictions assigned by the classifier to individual sentences.</p><p>Nearest neighbor We used the nearest neigh- bors algorithm, specifically, its radius-based ver- sion (NN-RAD), using the pre-trained GloVe em- beddings ( <ref type="bibr" target="#b17">Pennington et al., 2014</ref>). This classifier estimates the degree of concept abstractness given only its distributional representation.</p><p>The abstractness score of a test concept was computed by the ratio of its abstract neighbors to the total number of concepts within the predefined radius, where the entire set of neighbors is lim- ited to the concepts in the weakly-labeled train- ing set. The proximity threshold (radius) was set to 0.25, w.r.t. the cosine similarity between two embedding vectors. <ref type="bibr">5</ref> Multi-word concepts were subject to more careful processing, where the clas- sifier computed a multi-word concept representa- tion as an average of representations of its individ- ual words, and further estimated the abstractness score of the obtained embedding. In case that one of a concept constituents was not found in embed- dings, we excluded the concept from computation.</p><p>RNN Aiming at exploiting both embeddings and textual data, we utilized a bidirectional recurrent neural network (RNN) with one layer of forward and backward LSTM cells. Each cell has width of 128, and is wrapped by a dropout wrapper with keep probability 0.85. An attention layer was cre- ated in order to weigh words according to their proximity to the train/test concept. The output of the LSTM cells is passed to the attention layer which reduces it to the size of 100. The output of the attention layer is passed to a fully connected layer which produces the final prediction of the ab- stractness level of a concept. GloVe embeddings with 300 dimensions were used as word represen- tations. Given a set of sentences containing a test concept, its final abstractness score was computed by applying the averaging procedure described for the Naive Bayes classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We demonstrate that trained models discover lin- guistic patterns associated with abstract meaning (beyond those known at training), and further- more yield abstractness scores that correlate sig- nificantly with human annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Revealing abstractness markers</head><p>We automatically scored 100K unigram Wikipedia concepts for abstractness with all classifiers and extracted the set of suffixes that share excessive frequency in the top-k abstract concepts using the statistical proportion test. More specifically, <ref type="bibr">5</ref> The radius was tuned on the set of 500 unigrams. we applied the test to the exhaustive list of all three-character English suffixes (e.g., -aaa, -aab), counting their occurrences in the subset of con- cepts with the highest abstractness scores 6 (the population under test) and in the remainder (the background). Our hypothesis was that suffixes associated with abstract meaning in the literature will be over-represented in the population of con- cepts ranked as abstract by the classifiers. The top-10 suffixes, scored by their statistical signif- icance p-value 7 were {-ism, -ity, -ion, -sis, -ics, -ess, -phy, -nce, -ogy, -ing} -suffixes broadly as- sociated with abstractness in the literature (where all suffixes but two are distinct from the training data). The underlying concept examples included {illegalism, modernity, antireligion, henosis, poli- tics, lawlessness, ecosophy, conscience, ideology, enabling} -words broadly perceived as abstract.  <ref type="table" target="#tab_2">Table 2</ref> presents the Pearson correlation be- tween the abstractness scores as assigned by the classifiers and the manual annotations over the evaluation set. We also present the correlation of scores produced by our classifiers to the set of Wikipedia concepts from the manually annotated MRC database (MRC-seed, Section 1), and to the set of 5883 noun concepts 8 from manually anno- tated BWK dataset <ref type="bibr" target="#b1">(Brysbaert et al., 2014</ref>  with human annotations. Notably, the simple Naive Bayes, utilizing only textual data, yields re- sults of reasonable quality; the broad implication of this outcome lies in the potential applicabil- ity of this approach to resource-scarce scenarios where high quality word embeddings are not avail- able. Interestingly, while using Google word2vec embeddings (instead of Glove) yielded similar re- sults, utilizing fastText pre-trained representations ( <ref type="bibr" target="#b9">Joulin et al., 2016</ref>) obtained more accurate rank- ing, e.g., the NN-RAD classifier yielded correla- tion of 0.688 for the BWK dataset, compared to 0.622 obtained using Glove <ref type="table" target="#tab_2">(Table 2)</ref>. We attribute this improvement to the fact that fastText embed- dings better capture morphological word proper- ties and cover more extensive vocabulary. The relatively low correlation obtained with tri- gram concepts can be explained by the inherent complexity introduced by the multi-word scenario, challenging still further the subjective human per- ception of abstractness. While inter-labeler agree- ment for unigrams and bigrams was 0.72 and 0.66, respectively, it only reached 0.54 for trigrams, sup- porting the aforementioned hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Abstractness rating</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Varying the size of a test set</head><p>How many sentences containing a test concept suf- fice for a reliable prediction? We address this question by limiting the number of (randomly cho- sen) sentences used for rating. While the correla- tion obtained by RNN with 500 sentences contain- ing a test concept reached 0.740 <ref type="table" target="#tab_2">(Table 2)</ref>, as lit- tle as 10, and even 5 sentences yielded correlation of 0.706 and 0.675, respectively, implying the effi- ciency and effectiveness of the presented approach in the availability of only little data. The plot in <ref type="figure" target="#fig_1">Figure 1</ref> presents the correlation of the RNN and NB classifiers to label as function of number of (randomly sampled) sentences used for evalu- ation. Each such experiment (e.g., using 1, 5, 10 sentences) was averaged over 50 runs; the aver- age correlation to label, as well as standard devi- ation, are plotted on the chart. The constant cor- relation yield by the (text-independent) NN-RAD algorithm is illustrated by the vertical line. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison to supervised models</head><p>Tsvetkov et al. (2013) used supervised learning al- gorithm to propagate abstractness scores to words using pre-trained word representations. Utilizing vector elements as features, they trained a super- vised classifier, and predicted the degree of ab- stractness for unseen words. Abstractness rank- ings from the MRC database were used as a train- ing set, and the classifier predictions were bina- rized into abstract-concrete boolean indicators us- ing predefined thresholds. The authors obtained 94% accuracy when tested on held-out data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We presented a weakly supervised approach for inferring the degree of concept abstractness. Our results demonstrate that a minimal morphologi- cal signal and a textual corpus are sufficient to train classifiers that yield relatively accurate pre- dictions, that in turn can be used to unravel ad- ditional linguistic patterns indicative of the same property. Our future plans include exploring the value of the proposed methodology with other lan- guages and additional properties.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>achieved state-of-the-art perfor- mance in Semantic Composition and Semantic Modification prediction by including concreteness in the set of features used by the model. Along the years, several works extended the seed MRC dataset by employing various super- vised machine learning techniques, further utiliz- ing the extended dataset for tasks of lexical sim- plification (Paetzold and Specia, 2016b,a), cross- lingual metaphor detection (Tsvetkov et al., 2013), literal and metaphorical sense identification (Tur- ney et al., 2011), as well as readability assessment of Brazilian Portuguese (dos Santos et al., 2017). Feng et al. (2011) exploited word attributes from WordNet, properties extracted from the CELEX database, and Latent Semantic Analysis over a large text corpus for building a linear regression model predicting abstractness rate; the model ac- counted for 64% variance of human annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Average correlation (and standard deviation) to manual annotation as function of number of sentences used for evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 presents</head><label>1</label><figDesc></figDesc><table>a few examples of abstract and 
concrete concepts, as identified by manual anno-
tation, along with their abstractness score as pre-
dicted by the RNN classifier (Section 3.3). 

abstract 
concrete 
concept 
score concept 
score 
marxism 
0.972 plywood 
0.000 
islamophobia 
0.969 Wiltshire 
0.000 
affirmative action 
0.844 moonlight 0.058 
absolute monarchy 0.842 convoy 
0.112 
sincerity 
0.836 gadget 
0.120 

Table 1: Examples of concepts found as abstract/concrete 

(above/below the average score of 0.5) via manual annota-
tion, along with their score as predicted by RNN. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Correlation of abstractness scores assigned by the 

classifiers to manual annotations. 

</table></figure>

			<note place="foot" n="1"> The datasets are available for download at https://www.research.ibm.com/haifa/dept/ vst/debating_data.shtml</note>

			<note place="foot" n="2"> We used the Wikipedia May 2017 dump. 3 The minimum of 20 occurrences for a concept.</note>

			<note place="foot" n="4"> We used the implementation in http:// scikit-learn.org, with &quot;quadratic&quot; scheme.</note>

			<note place="foot" n="6"> We used the set of 18% highest ranked concepts-the fraction of abstract concepts in a sample population, as estimated by manual labeling. 7 In all cases the obtained p-value was practically zero. 8 Only concepts that can be mapped to a corresponding Wikipedia page were considered.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to Dafna Sheinwald and Shuly Wintner for much advise and helpful suggestions. We also thank our anonymous reviewers for their constructive feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Concreteness in word processing: ERP and behavioral effects in a lexical decision task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Horacio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavroula-Thaleia</forename><surname>Otten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriella</forename><surname>Kousta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vigliocco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain and language</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="53" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brysbaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><forename type="middle">Beth</forename><surname>Warriner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Kuperman</surname></persName>
		</author>
		<title level="m">Concreteness ratings for 40 thousand generally known English word lemmas. Behavior research methods</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="904" to="911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The MRC psycholinguistic database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Coltheart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="497" to="505" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Strength of perceptual experience predicts word processing performance better than concreteness or imageability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louise</forename><surname>Connell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dermot</forename><surname>Lynott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="452" to="465" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simulating human ratings on word concreteness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shi Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><forename type="middle">S</forename><surname>Crossley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcnamara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the FLAIRS Conference</title>
		<meeting>the FLAIRS Conference</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Word concreteness as a moderator of the tip-of-the-tongue effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeanette</forename><surname>Jennifer L Gianico-Relyea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Altarriba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Psychological Record</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="763" to="776" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Concreteness and subjectivity as dimensions of lexical meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="725" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The cambridge grammar of English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodney</forename><surname>Huddleston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">K</forename><surname>Pullum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">UOWSHEF: Simplex-lexical simplicity ranking based on contextual and psycholinguistic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Sujay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the First Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="477" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in neural information processing systems</title>
		<meeting>Advances in neural information processing systems</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dissociative contributions of semantic and lexical-phonological information to immediate recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryoji</forename><surname>Nishiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="642" to="648" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The roles of word concreteness and cognitive load on interhemispheric processes of recognition. Laterality: Asymmetries of Body</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ladera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gamito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain and Cognition</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="215" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Collecting and exploring everyday language for predicting psycholinguistic properties of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Paetzold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1669" to="1679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inferring psycholinguistic properties of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Paetzold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="435" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dual coding theory, word abstractness, and emotion: a critical review of kousta</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paivio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of experimental psychology</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">282</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Concreteness, imagery, and meaningfulness values for 925 nouns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Paivio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen A</forename><surname>John C Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Madigan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of experimental psychology</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1p2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
		<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07572</idno>
		<title level="m">Ultradense word embeddings by orthogonal transformation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A lightweight regression method to infer psycholinguistic properties for Brazilian Portuguese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leandro</forename><forename type="middle">B</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magali</forename><forename type="middle">Sanches</forename><surname>Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Siegle Hartmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaldo</forename><surname>Candido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><forename type="middle">Henrique</forename><surname>Paetzold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">Maria</forename><surname>Aluisio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Text, Speech, and Dialogue</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="281" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Parameters of abstraction, meaningfulness, and pronunciability for 329 nouns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otfried</forename><surname>Spreen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Verbal Learning and Verbal Behavior</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="459" to="468" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cross-lingual metaphor detection using common semantic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Mukomel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatole</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Metaphor in NLP</title>
		<meeting>the First Workshop on Metaphor in NLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="45" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Literal and metaphorical sense identification through concrete and abstract context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Peter D Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Neuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohai</forename><surname>Assaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="680" to="690" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
