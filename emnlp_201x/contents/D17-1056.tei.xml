<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Refining Word Embeddings for Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chih</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Management</orgName>
								<orgName type="institution">Yuan Ze University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Innovation Center for Big Data and Digital Convergence</orgName>
								<orgName type="institution">Yuan Ze University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Yuan Ze University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Innovation Center for Big Data and Digital Convergence</orgName>
								<orgName type="institution">Yuan Ze University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<settlement>Yunnan</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Robert</forename><surname>Lai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Yuan Ze University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Innovation Center for Big Data and Digital Convergence</orgName>
								<orgName type="institution">Yuan Ze University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejie</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<settlement>Yunnan</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Refining Word Embeddings for Sentiment Analysis</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="534" to="539"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Word embeddings that can capture semantic and syntactic information from contexts have been extensively used for various natural language processing tasks. However , existing methods for learning context-based word embeddings typically fail to capture sufficient sentiment information. This may result in words with similar vector representations having an opposite sentiment polarity (e.g., good and bad), thus degrading sentiment analysis performance. Therefore, this study proposes a word vector refinement model that can be applied to any pre-trained word vectors (e.g., Word2vec and GloVe). The refinement model is based on adjusting the vector representations of words such that they can be closer to both semantically and sentimentally similar words and further away from sentimentally dissimilar words. Experimental results show that the proposed method can improve conventional word embeddings and outperform previously proposed sentiment embeddings for both binary and fine-grained classification on Stanford Sentiment Treebank (SST).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word embeddings are a technique to learn con- tinuous low-dimensional vector space representa- tions of words by leveraging the contextual in- formation from large corpora. Examples include C&amp;W <ref type="bibr" target="#b1">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b2">Collobert et al., 2011</ref>), <ref type="bibr">Word2vec (Mikolov et al., 2013a;</ref><ref type="bibr">2013b)</ref> and <ref type="bibr">GloVe (Pennington et al., 2014</ref>). In addition to the contextual information, character- level subwords ( <ref type="bibr" target="#b0">Bojanowski et al., 2016</ref>) and se- mantic knowledge resources <ref type="bibr" target="#b4">(Faruqui et al., 2015;</ref><ref type="bibr">Kiela et al., 2015</ref>) such as WordNet <ref type="bibr">(Miller, 1995)</ref> are also useful information for learning word embeddings. These embeddings have been successfully used for various natural language processing tasks.</p><p>In general, existing word embeddings are se- mantically oriented. They can capture semantic and syntactic information from unlabeled data in an unsupervised manner but fail to capture suffi- cient sentiment information. This makes it diffi- cult to directly apply existing word embeddings to sentiment analysis. Prior studies have reported that words with similar vector representations (similar contexts) may have opposite sentiment polarities, as in the example of happy-sad men- tioned in <ref type="bibr">(Mohammad et al., 2013)</ref> and good-bad in ( <ref type="bibr">Tang et al., 2016)</ref>. Composing these word vec- tors may produce sentence vectors with similar vector representations but opposite sentiment po- larities (e.g., a sentence containing happy and a sentence containing sad may have similar vector representations). Building on such ambiguous vectors will affect sentiment classification per- formance.</p><p>To enhance the performance of distinguishing words with similar vector representations but op- posite sentiment polarities, recent studies have suggested learning sentiment embeddings from labeled data in a supervised manner <ref type="bibr">(Maas et al., 2011;</ref><ref type="bibr">Labutov and Lipson, 2013;</ref><ref type="bibr">Lan et al., 2016;</ref><ref type="bibr">Ren et al., 2016;</ref><ref type="bibr">Tang et al., 2016)</ref>. The common goal of these methods is to capture both seman- tic/syntactic and sentiment information such that sentimentally similar words have similar vector representations. They typically apply an objective function to optimize word vectors based on the sentiment polarity labels (e.g., positive and nega- tive) given by the training instances. The use of such sentiment embeddings has improved the per- formance of binary sentiment classification. great (7.50) bad (3.24) terrific (7.12) decent <ref type="bibr">(6.27)</ref> nice <ref type="formula" target="#formula_1">(</ref> Ranked by cosine similarity <ref type="figure">Figure 1</ref>: Example of nearest neighbor rank- ing.</p><p>This study adopts another strategy to obtain both semantic and sentiment word vectors. Instead of building a new word embedding model from labeled data, we propose a word vector refinement model to refine existing semantically oriented word vectors using sentiment lexicons. That is, the proposed model can be applied to the pre-trained vectors obtained by any word representation learning models (e.g., <ref type="bibr">Word2vec</ref> and GloVe) as a post-processing step to adapt the pre-trained vec- tors to sentiment applications. The refinement model is based on adjusting the pre-trained vector of each affective word in a given sentiment lexi- con such that it can be closer to a set of both se- mantically and sentimentally similar nearest neighbors (i.e., those with the same polarity) and further away from sentimentally dissimilar neigh- bors (i.e., those with an opposite polarity).</p><p>The proposed refinement model is evaluated by examining whether our refined embeddings can improve conventional word embeddings and out- perform previously proposed sentiment embed- dings. To this end, several deep neural network classifiers that performed well on the Stanford Sentiment Treebank (SST) <ref type="bibr">(Socher et al., 2013)</ref> are selected, including convolutional neural net- works (CNN) <ref type="bibr">(Kim, 2014)</ref>, deep averaging net- work (DAN) <ref type="bibr">(Iyyer et al., 2015</ref>) and long-short term memory (LSTM) <ref type="bibr">(Tai et al., 2015;</ref><ref type="bibr">Looks et al., 2017</ref>). The conventional word embeddings used in these classifiers are then replaced by our refined versions and previously proposed senti- ment embeddings to re-run the classification for performance comparison. The SST is chosen be- cause it can show the effect of using different word embeddings on fine-grained sentiment clas- sification, whereas prior studies only reported bi- nary classification results.</p><p>The rest of this paper is organized as follows. Section 2 describes the proposed word vector re- finement model. Section 3 presents the evaluation results. Conclusions are drawn in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Word Vector Refinement</head><p>The refinement procedure begins by giving a set of pre-trained word vectors and a sentiment lexi- con annotated with real-valued sentiment scores. Our goal is to refine the pre-trained vectors of the affective words in the lexicon such that they can capture both semantic and sentiment information. To accomplish this goal, we first calculate the se- mantic similarity between each affective word (target word) and the other words in the lexicon based on the cosine distance of their pre-trained vectors, and then select top-k most similar words as the nearest neighbors. These semantically simi- lar nearest neighbors are then re-ranked according to their sentiment scores provided by the lexicon such that the sentimentally similar neighbors can be ranked higher and dissimilar neighbors lower. Finally, the pre-trained vector of the target word is refined to be closer to its semantically and senti- mentally similar nearest neighbors and further away from sentimentally dissimilar neighbors. The following subsections provide a detailed de- scription of the nearest neighbor ranking and re- finement model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Nearest Neighbor Ranking</head><p>The sentiment lexicon used in this study is the ex- tended version of Affective Norms of English Words (E-ANEW) ( <ref type="bibr">Warriner et al., 2013</ref>). It con- tains 13,915 words and each word is associated with a real-valued score in <ref type="bibr">[1,</ref><ref type="bibr">9]</ref> for the dimen- sions of valence, arousal and dominance. The va- lence represents the degree of positive and nega- tive sentiment, where values of 1, 5 and 9 respec- tively denote most negative, neutral and most pos- itive sentiment. In <ref type="figure">Fig. 1</ref>, good has a valence score of 7.89, which is greater than 5, and thus can be considered positive. Conversely, bad has a va- lence score of 3.24 and is thus negative. In addi- tion to the E-ANEW, other lexicons such as Sen- tiWordNet ( <ref type="bibr" target="#b3">Esuli and Fabrizio, 2006</ref>), SoCal (Taboada et al., 2011), SentiStrength ( <ref type="bibr">Thelwall et al., 2012</ref>), Vader ( <ref type="bibr">Hutto et al., 2014</ref>), ANTUSD ( <ref type="bibr">Wang and Ku, 2016)</ref> and SCL-NMA ( <ref type="bibr">Kiritchenko and Mohammad, 2016</ref>) also provide real-valued sentiment intensity or strength scores like the valence scores.</p><p>For each target word to be refined, the top-k semantically similar nearest neighbors are first se- lected and ranked in descending order of their co- sine similarities. In <ref type="figure">Fig. 1</ref>, the left ranked list shows the top 10 nearest neighbors for the target word good. The semantically ranked list is then sentimentally re-ranked based on the absolute dif- ference of the valence scores between the target word and the words in the list. A smaller differ- ence indicates that the word is more sentimentally similar to the target word, and thus will be ranked higher. As shown in the right ranked list in <ref type="figure">Fig. 1</ref>, the re-ranking step can rank the sentimentally similar neighbors higher and the dissimilar neigh- bors lower. In the refinement model, the higher ranked sentimentally similar neighbors will re- ceive a higher weight to refine the pre-trained vec- tor of the target word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Refinement Model</head><p>Once the word list ranked by both cosine similari- ty and valence scores for each target word is ob- tained, its pre-trained vector will be refined to be (1) closer to its sentimentally similar neighbors, (2) further away from its dissimilar neighbors, and (3) not too far away from the original vector.</p><p>Let V = {v1, v2, …, vn} be a set of the pre- trained vectors corresponding to the affective words in the sentiment lexicon. For each target to be refined, the refinement model iteratively mini- mizes the distance between the target word and its top-k nearest neighbors. The objective function Φ(V) can thus be defined as</p><formula xml:id="formula_0">1 1 ( ) ( , ) n k ij i j i j V w dist v v = = Φ = ∑∑ (1)</formula><p>where n denotes the total number of vectors in V to be refined, vi denotes the vector of a target word, vj denotes the vector of one of its nearest neighbors in the ranked list, dist(vi, vj) denotes the distance between vi and vj, and wij denotes the weight of the target word's nearest neighbor, de- fined as the reciprocal rank of a ranked list. For example, excellent in <ref type="figure">Fig. 1</ref> will receive a weight of 1, great will receive a weight of 1/2, and so on. A word ranked higher will receive a higher weight. This weight is used to control the move- ment direction of the target word towards to its nearest neighbors. That is, the target word will be moved closer to the higher-ranked sentimentally similar neighbors and further away from lower- ranked dissimilar neighbors, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. To prevent too many words being moved to the same location and thereby producing too many similar vectors, we add a constraint to keep each pre-trained vector within a certain range from its original vector. The objective function is thus divided as two parts: denotes the distance between the vector of the target word in step t and t+1, i.e., the distance between the refined vector and its original vector. The later one represents the dis- tance between the vector of the target word and that of its neighbors (similar to Eq. <ref type="formula" target="#formula_1">(1)</ref>). The pa- rameters α and β together are used as a ratio to control how far the refined vector can be moved away from its original vector and toward its near- est neighbors. A greater ratio indicates a stronger constraint on keeping the refined vector closer to its original vector. For the extreme case of α=1 and β=0, the target word will not be moved (re- fined). As the ratio decreases, the constraint de- creases accordingly and the refined vector can be moved closer to its nearest neighbors. The setting of α=0 and β=1 means that the constraint is disa- bled.</p><formula xml:id="formula_1">1 1 1 1 arg min ( )= argmin ( , )<label>( ,</label></formula><p>To facilitate the calculation of the partial de- rivative of Φ(V), dist(vi, vj) in the above equa- tions is measured by the squared Euclidean dis- tance, defined as  </p><formula xml:id="formula_2">( , ) ( ) D d d i j i j d dist v v v v = = − ∑ (3)</formula><p>where D is the dimensionality of the word vectors.</p><p>The global optimal solution of Φ(V) can be found by using an iterative update method. To do so, we solve the partial derivation of Eq. <ref type="formula" target="#formula_1">(2)</ref>  </p><p>Through the iterative procedure, the vector representation of each target word will be iteratively updated until the change of the location of the target word's vector is converged. The refinement process will be terminated when all target words are refined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>This section evaluates the proposed refinement model, conventional word embeddings and previ- ously proposed sentiment embeddings using sev- eral deep neural network models for binary and fine-grained sentiment classification.</p><p>Dataset. SST was adopted as the evaluation cor- pus <ref type="bibr">(Socher et al., 2013</ref>). The binary classification subtask (positive and negative) contains 6920/872/1821 samples for the train/dev/test sets, while the fine-grained ordinal classification sub- task (very negative, negative, neutral, positive, and very positive) contains 8544/1101/2210 sam- ples of the train/dev/test sets.</p><p>Word Embeddings. The word embeddings used for comparison included two conventional word embeddings (GloVe and Word2vec), our refined versions (Re(GloVe) and Re(Word2vec)), and previously proposed sentiment embeddings (Hy- Rank) ( <ref type="bibr">Tang et al., 2016)</ref>. We used the same di- mensionality of 300 for all word embeddings.</p><p> GloVe and Word2vec: The respective GloVe and Word2vec (skip-gram) were pre-trained on Common Crawl 840B 1 and Google- News 2 .</p><p>1 http://nlp.stanford.edu/projects/glove/ 2 https://code.google.com/archive/p/word2vec/  Re(Glove) and Re(Word2vec): Both the pre- trained GloVe and Word2vec were refined using E-ANEW ( <ref type="bibr">Warriner et al., 2013)</ref>. Each affective word was refined by its top k=10 nearest neighbors with parameters of α:β=0.1 (1:10) (see Eq. <ref type="formula" target="#formula_1">(2)</ref>).</p><p> HyRank: It was trained using SST, NRC Sentiment140 and IMDB datasets. We com- pared this method because its code is public- ly accessible 3 .</p><p>Classifiers. After the proposed refinement model was applied, both the pre-trained Word2vec and GloVe were improved. The Re(Word2vec) and Re(GloVe) re- spectively improved Word2vec and GloVe by 1.7% and 1.5% averaged over all classifiers for binary classification, and both 1.6% for fine- grained classification. In addition, both Re(GloVe) and Re(Word2vec) outperformed the sentiment embeddings HyRank for all classifiers on both bi- nary and fine-grained classification, indicating that the real-valued intensity scores used by the proposed refinement model are more effective than the binary polarity labels used by the previ- ously proposed sentiment embedings. The proposed method yielded better perfor- mance because it can remove semantically similar but sentimentally dissimilar nearest neighbors for the target words by refining their vector represen- tations. To demonstrate the effect, we define a measure noise@k to calculate the percentage of top k nearest neighbors with an opposite polarity (i.e., noise) to each word in E-ANEW. For in- stance, in <ref type="figure">Fig. 1</ref>, the noise@10 for good is 20% because there are two words with an opposite po- larity to good among its top 10 nearest neighbors. <ref type="table">Table 2</ref> shows the average noise@10 for different word embeddings. For the two semantic-oriented word vectors, GloVe and Word2vec, on average around 24% of the top 10 nearest neighbors for each word are noisy words. After refinement, both Re(GloVe) and Re(Word2vec) can reduce noise@10 to around 14%. The HyRank also yielded better performance than both GloVe and Word2vec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This study presents a word vector refinement model that requires no labeled corpus and can be applied to any pre-trained word vectors. The pro- posed method selects a set of semantically similar nearest neighbors and then ranks the sentimentally similar neighbors higher and dissimilar neighbors lower based on a sentiment lexicon. This ranked list can guide the refinement procedure to itera- tively improve the word vector representations.</p><p>Experiments on SST show that the proposed method yielded better performance than both con- ventional word embeddings and sentiment em- beddings for both binary and fine-grained senti- ment classification. In addition, the performances of various deep neural network models have also been improved. Future work will evaluate the proposed method on another datasets. More ex- periments will also be conducted to provide more in-depth analysis.   <ref type="table">Table 2</ref>: Average percentages of noisy words in the top 10 nearest neighbors for different word embeddings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Conceptual diagram of word vector refinement.</figDesc><graphic url="image-1.png" coords="3,309.88,77.51,213.46,163.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Method</head><label></label><figDesc></figDesc></figure>

			<note place="foot" n="3"> http://ir.hit.edu.cn/~dytang/ 4 https://github.com/yoonkim/CNN_sentence 5 https://github.com/miyyer/dan 6 https://github.com/stanfordnlp/treelstm 7 https://github.com/tensorflow/fold</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the Ministry of Sci-ence and Technology, Taiwan, ROC, under Grant No. MOST 105-2221-E-155-059-MY2 and MOST 105-2218-E-006-028. The authors would like to thank the anonymous reviewers and the ar-ea chairs for their constructive comments. The effect of negators, modals, and degree adverbs on sentiment composition. In Proceedings of the 7th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis at NAACL-HLT 2016, pages 43-52. Igor <ref type="bibr">Labutov and Hod Lipson. 2013. Re-embedding</ref> words. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL-13), pages 489-493. Man Lan, Zhihua Zhang, Yue Lu, and Ju Wu. 2016.</p><p>Three convolutional neural network-based models for learning sentiment word vectors towards sentiment analysis. </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the 2015 Conference of the North American Chapter of the Association for Computa- tional Linguistics -Human Language Technolo- gies (NAACL/HLT-15), pages 1606-1615. Clayton J. Hutto and Eric Gilbert. 2014. VADER: A parsimonious rule-based model for sentiment anal- ysis of social media text. In Proceedings of 8th In- ternational AAAI Conference on Weblogs and So- cial Media, pages 216-225. Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, and Hal Daumé III. 2015. Deep unordered composition rivals syntactic methods for text classification. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL-15), pages 1681-1691. Douwe Kiela, Felix Hill and Stephen Clark. 2015. Specializing word embeddings for similarity or relatedness. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP-15), pages 2044-2048. Yoon Kim. 2014. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural</head><p>Language Processing <ref type="bibr">(EMNLP-14)</ref>, pages 1746- 1751. Svetlana <ref type="bibr">Kiritchenko and Saif M. Mohammad. 2016.</ref></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<title level="m">Enriching word vectors with subword information</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning (ICML-08)</title>
		<meeting>the 25th International Conference on Machine Learning (ICML-08)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L´eon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SentiWordNet: A publicly available lexical resource for opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC-06)</title>
		<meeting>the 5th International Conference on Language Resources and Evaluation (LREC-06)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="417" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sujay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
