<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Integrating Order Information and Event Relation for Script Event Prediction *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqing</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yun</forename><surname>Chang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Integrating Order Information and Event Relation for Script Event Prediction *</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="57" to="67"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>There has been a recent line of work automatically learning scripts from unstruc-tured texts, by modeling narrative event chains. While the dominant approach group events using event pair relations, L-STMs have been used to encode full chains of narrative events. The latter has the advantage of learning long-range temporal orders 1 , yet the former is more adap-tive to partial orders. We propose a neu-ral model that leverages the advantages of both methods, by using LSTM hidden states as features for event pair modelling. A dynamic memory network is utilized to automatically induce weights on existing events for inferring a subsequent event. Standard evaluation shows that our method significantly outperforms both methods above, giving the best results reported so far.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Frequently recurring sequences of events in pro- totypical scenarios, such as visiting a restauran- t and driving to work, are a useful source of world knowledge. Two examples are shown in <ref type="figure" target="#fig_0">Figure 1</ref>, which are different variations of the "restaurant visiting" scenario, where events are partially or- dered and can be flexible. Such knowledge is useful for natural language understanding because texts typically do not include event details when mentioning a scenario. For example, the reader is expected to infer that the narrator could have been driving or cycling given the text "I got flat tire". Another typical use of event chain knowledge is to help infer what is likely to happen next given a previous event sequence in a scenario. We inves- tigate the modeling of stereotypical event chains, which is remotely similar to language modeling, but with events being more sparse and flexibly or- dered than words.</p><p>Our work follows a recent line of NLP re- search on script learning. Stereotypical knowl- edge about partially-ordered events, together with their participant roles such as "customer", "wait- er", and "table", is conventionally referred to as scripts ( <ref type="bibr" target="#b34">Schank et al., 1977)</ref>. NLP algorithms have been investigated for automatically inducing scripts from unstructured texts <ref type="bibr" target="#b25">(Mooney and DeJong, 1985;</ref><ref type="bibr" target="#b3">Chambers and Jurafsky, 2008)</ref>. In par- ticular, <ref type="bibr" target="#b3">Chambers and Jurafsky (2008)</ref> made a first attempt to learn scripts from test inducing event chains by grouping events based on their narrative coherence, calculated based on Pairwise Mutual Information (PMI). <ref type="bibr" target="#b16">Jans et al. (2012)</ref> showed that the method can be improved by calculating even- t relations using skip bi-gram probabilities, which explicitly model the temporal order of pairs even- t. <ref type="bibr" target="#b16">Jans et al. (2012)</ref>'s model is adopted by a line of subsequent methods on inducing event chains from text ( <ref type="bibr" target="#b27">Orr et al., 2014;</ref><ref type="bibr" target="#b28">Pichotta and Mooney, 2014;</ref><ref type="bibr" target="#b32">Rudinger et al., 2015</ref>).</p><p>While the above methods are statistical, neural network models have recently been used for event sequence modeling. <ref type="bibr" target="#b12">Granroth-Wilding and Clark (2016)</ref> used a Siamese Network instead of PMI to calculate the coherence between two events. <ref type="bibr" target="#b32">Rudinger et al. (2015)</ref> extended the idea of Jan- s et al. <ref type="formula" target="#formula_0">(2012)</ref> by using a log-bilinear neural lan- guage model <ref type="bibr" target="#b22">(Mnih and Hinton, 2007)</ref> to calcu- late event probabilities. By learning embeddings for reducing sparsity, the above models give much better results compared to the models of <ref type="bibr" target="#b3">Chambers and Jurafsky (2008)</ref> and <ref type="bibr" target="#b16">Jans et al. (2012)</ref>. Simi- lar in spirit, <ref type="bibr" target="#b23">Modi (2016)</ref> predicted the probability of an event belonging to a certain event chain by modeling known events in the chain as a bag of vectors, showing that it outperforms discrete sta- tistical methods. These neural methods are con- sistent with the earlier statistical models in lever- aging event-pair relations.</p><p>Pichotta and Mooney (2016) experimented with LSTM for script learning, using an existing se- quence of events to predict the probability of a next event, which outperformed strong discrete baselines. One advantage of LSTMs is that they can encode unbounded time sequences without losing long-term historical information. LSTMs capture significantly more order information com- pared to the methods of <ref type="bibr" target="#b12">Granroth-Wilding and Clark (2016)</ref>, <ref type="bibr" target="#b32">Rudinger et al. (2015), and</ref><ref type="bibr" target="#b23">Modi (2016)</ref>, which model the temporal order of only pairs of events. On the other hand, a strong-order LSTM model can also suffer the disadvantage of over-fitting, given the flexible order of event chain- s in a script, as demonstrated by the cases of Fig- ure 1. In this aspect, event-pair models are more adaptive for flexible orders. However, no direc- t comparisons have been reported between LSTM and various existing neural network methods that model event-pairs. We make such comparisons using the same benchmark, finding that the method of Pichotta and Mooney (2016) does not necessarily outper- form event-pair models, such as <ref type="bibr" target="#b12">Granroth-Wilding and Clark (2016)</ref>. LSTM temporal ordering and event pair modeling have their respective strength. To leverage the advantages of both methods, we propose to integrate chain temporal order infor- mation into event relation measuring. In partic- ular, we calculate event pair relations by repre- senting events in a chain using LSTM hidden s- tates, which encode temporal information. The L- STM over-fitting issue is mitigated by using the temporal-order in a chain as a feature for event pair modeling, rather than the direct model out- put. In addition, observing that the importance of existing events can vary for inferring a subse- quent event, we use a dynamic memory network model to automatically induce event weights for each event for inferring the next event. In con- trast, previous methods give equal weights to ex- isting events <ref type="bibr" target="#b3">(Chambers and Jurafsky, 2008;</ref><ref type="bibr" target="#b23">Modi, 2016;</ref><ref type="bibr" target="#b12">Granroth-Wilding and Clark, 2016)</ref>.</p><p>Results on a multi-choice narrative cloze bench- mark show that our model significantly outper- forms both <ref type="bibr" target="#b12">Granroth-Wilding and Clark (2016)</ref> and <ref type="bibr" target="#b29">Pichotta and Mooney (2016)</ref>, improving the state-of-the-art accuracy from 49.57% to 55.12%. Our contributions can be summarized as follows:</p><p>• We make a systematic comparison of LSTM and pair-based event sequence learning meth- ods using the same benchmarks.</p><p>• We propose a novel dynamic memory net- work model, which combines the advantages of both LSTM temporal order learning and traditional event pair coherence learning.</p><p>• We obtain the best results in the standard multi-choice narrative cloze test.</p><p>Our code is released at https://github. com/wangzq870305/event_chain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Scripts have been a traditional subject in AI re- search ( <ref type="bibr" target="#b34">Schank et al., 1977)</ref>, where event se- quences are manually encoded in knowledge bases, and used for end tasks such as inference. They are also connected with research in linguis- tics and psychology, and sometimes referred to as frames <ref type="bibr" target="#b21">(Minsky, 1975;</ref><ref type="bibr" target="#b9">Fillmore, 1982)</ref> and schemata <ref type="bibr" target="#b33">(Rumelhart, 1975)</ref>. The same concept is also studied as templates in information extrac- tion <ref type="bibr" target="#b36">(Sundheim, 1991)</ref>. <ref type="bibr" target="#b3">Chambers and Jurafsky (2008)</ref> pioneered the recent line of work on script induction ( <ref type="bibr" target="#b16">Jans et al., 2012;</ref><ref type="bibr" target="#b29">Pichotta and Mooney, 2016;</ref><ref type="bibr" target="#b12">Granroth-Wilding and Clark, 2016)</ref>, where the focus is on modeling narrative event chains, a crucial subtask for script modeling from raw text. Below we summarize such investigations.</p><p>With respect to event representation, Cham- bers and Jurafsky (2008) casted narrative events as triples of the form event, dependency, where the event is typically represented by a verb and the dependency represents typed dependency relations between the event and a protagonist, such as "sub- ject" and "object". <ref type="bibr" target="#b3">Chambers and Jurafsky (2008)</ref> organized narrative chains around a central actor, or protagonist, mining events that share a common protagonist from texts by using a syntactic parser and a coreference resolver. <ref type="bibr" target="#b1">Balasubramanian et al. (2013)</ref> observed that the protagonist representa- tion of event chains can suffer from weaknesses such as lack of coherence, and proposed to repre- sent events as arg 1 , relation, arg 2 , where arg 1 and arg 2 represent the subject and object, respec- tively. Such representation is inspired by open in- formation extraction <ref type="bibr" target="#b18">(Mausam et al., 2012)</ref>, and offers richer features for event pair modeling. Pi- chotta and Mooney (2014) adpoted a similar idea, using v(e s , e o , e p ) to represent an event, where v is a verb lemma, e s is the subject, e o is the ob- ject, and e p is an entity with prepositional relation to v. Their representation is used by subsequent work such as <ref type="bibr" target="#b23">Modi (2016)</ref> and <ref type="bibr" target="#b12">Granroth-Wilding and Clark (2016)</ref>. We follow <ref type="bibr" target="#b29">Pichotta and Mooney (2016)</ref> in our event representation form.</p><p>With respect to modeling, existing methods can be classified into two main categories, namely weak-order models, which calculate relations be- tween pairs of events, and strong-order models, which consider the temporal order of events in a full sequence. Event-pair models have so far been the dominant method in the literature. Earlier work used discrete event representations and esti- mated event relations by statistical counting. As mentioned earlier, <ref type="bibr" target="#b3">Chambers and Jurafsky (2008)</ref> used PMI to calculate event relations, and Jan- s et al. (2012) used skip bigram probabilites to the same end, which is order-sensitive. Most sub- sequent methods followed <ref type="bibr" target="#b16">Jans et al. (2012)</ref> in using skip n-grams ( <ref type="bibr" target="#b28">Pichotta and Mooney, 2014;</ref><ref type="bibr" target="#b32">Rudinger et al., 2015</ref>).</p><p>Events being multi-argument structures, counting-based methods can suffer from sparsity issues. Recent work employed embeddings to address this disadvantage. <ref type="bibr" target="#b32">Rudinger et al. (2015)</ref> learned event embeddings as a by-product of training a log-bilinear language model for events; <ref type="bibr" target="#b12">Granroth-Wilding and Clark (2016)</ref> leveraged the skip-gram model of <ref type="bibr" target="#b20">Mikolov et al. (2013)</ref> for training the embeddings of event and arguments by ordering them into a pseudo sentence. Modi (2016) utilized word embeddings of verbs and arguments directly, using a hidden layer to au- tomatically consolidate word embedding into a single structured event embeddings. We follow <ref type="bibr" target="#b23">Modi (2016)</ref> and use a hidden layer to learn event argument compositions given word embeddings, training the composition function as a part of the event chain learning process.</p><p>Mitigating the sparsity issue of event represen- tations, neural methods can capture temporal or- ders between events beyond skip n-grams. Our model integrates the advantages of strong-order learning and event-pair learning by using LSTM hidden states as feature representation of existing events in the calculation of event pair relationship- s. In addition, we use a memory network model to weigh existing events, which gives better results compared to the equal weighting method of exist- ing models.</p><p>With respect to evaluation, Chambers and Ju- rafsky <ref type="bibr">(2008)</ref> proposed the Narrative Cloze Test, which asks for a missing event in a given even- t chain with a gap. The task has been adopted by various subsequent work for comparing result- s with Chambers and Jurafsky (2008) ( <ref type="bibr" target="#b16">Jans et al., 2012;</ref><ref type="bibr" target="#b28">Pichotta and Mooney, 2014;</ref><ref type="bibr" target="#b32">Rudinger et al., 2015)</ref>. One issue of the narrative cloze test is that there can sometimes be multiple plausible answer- s, but only one gold-standard answer, which can make it overly expensive to manually evaluate sys- tem outputs. To address this issue, <ref type="bibr" target="#b23">Modi (2016)</ref> proposed the Adversarial Narrative Cloze (ANC) task, which is to discriminate between pairs of real and corrupted event chains. <ref type="bibr" target="#b12">Granroth-Wilding and Clark (2016)</ref> proposed the Multi-Choice Narrative Cloze (MCNC) task, which is to choose the most likely next event from a set of candidates given a chain of events. We choose MCNC for comparing different models.</p><p>Other related work includes learning tempo- ral relations of events ( <ref type="bibr" target="#b24">Modi and Titov, 2014</ref>; Uz-X = Customer, Y = Waiter walk(X, restaurant), seat(X), order(X, food), serve(Y, food) eat(X, food), make(X, payment), Figure 2: Multiple choice narrative cloze. The gold subsequent event is marked in bold. <ref type="bibr">Zaman et al., 2013;</ref><ref type="bibr" target="#b0">Abend et al., 2015</ref>), evaluat- ed using different metrics. There has also been work using graph models to induce frames, which emphasize more on learning event structures and less on temporal orders <ref type="bibr" target="#b2">(Chambers, 2013;</ref><ref type="bibr" target="#b4">Cheung et al., 2013</ref>). The above methods focus on one of the two subtasks we consider here. <ref type="bibr" target="#b10">Frermann et al. (2014)</ref> used a Bayesian model to jointly clus- ter web collections of explicit event sequence and learn input event-pair temporal orders. However, their work is under a different input setting <ref type="bibr" target="#b30">(Regneri et al., 2010)</ref>, not learning event chains from texts. <ref type="bibr" target="#b26">Mostafazadeh et al. (2016)</ref> proposed the s- tory close task (SCT), which is to predict the end- ing given a unfinished story. Our narrative chain prediction task can be regarded as a sub task in the story close task, which can contribute as a major approach. On the other hand, information beyond event chains can be useful for the story close task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Definition</head><p>As shown in <ref type="figure">Figure 2</ref>, given a chain of narrative events e 1 , e 2 , ..., e n−1 , our work is to predict the likelihood of a next event candidate e n . Formally, an event e is a structure v(a 0 , a 1 , a 2 ), where v is a verb describing the event, a 0 and a 1 are its subject and direct object, respectively, and a 2 is a prepo- sitional object. For example, given the sentence "John brought Marry to the restaurant", an even- t bring{John, Marry, to the restaurant} can be extracted.</p><p>We follow the standard script induction set- ting ( <ref type="bibr" target="#b3">Chambers and Jurafsky, 2008;</ref><ref type="bibr">GranrothWilding and Clark, 2016)</ref>, extracting events from a text corpus using a syntactic parser and a named entity resolver. A neural network is used to mod- el chains of extracted events for script learning.</p><p>In particular, we model the probability of a sub- sequent event given a chain of events. For eval- uation, we solve the multi-choice narrative cloze task: given a chain of events and a set of candidate next events, the most likely candidate is chosen as the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>The overall structure of our model is shown in <ref type="figure">Fig- ure 3</ref>, which has three main components. First, given an event v(a 0 , a 1 , a 2 ), a representation lay- er is used to compose the embeddings of v, a 0 , a 1 , and a 2 into a single event vector e. Second, a L- STM is used to map a sequence of existing events e 1 , e 2 , ..., e n−1 into a sequence of hidden vectors h 1 , h 2 , ..., h n−1 , which encode the temporal or- der. Given a next event candidate e c , the recurrent network takes one further step from h n−1 to de- rive its hidden vector h c , which encodes e c . Third, h c is paired with h 1 , h 2 , ..., h n−1 individually, and passed to a dynamic memory network to learn the relatedness score s. s is used to denote the connectedness between the candidate subsequent event and the context event chain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Event Representation</head><p>We learn vector representations of standard events by composing pre-trained word embeddings of its verb and arguments. The skipgram mod- el ( <ref type="bibr" target="#b20">Mikolov et al., 2013</ref>) is used to train word vec- tors. For arguments that consist of more than one word, we use the averaged word for the represen- tation. OOV words are represented simply using zero vectors. For events with less than 3 argu- ments, such as "John fell", where v = fall, a 0 = John, a 1 = NULL, and a 2 = NULL, the NULL arguments are represented using all-zero vectors.</p><p>Denoting the embeddings of v, a 0 , a 1 , and a 2 as e(v), e(a 0 ), e(a 1 ), and e(a 2 ), respectively, the embedding of e is calculated using a tanh compo- sition layer</p><formula xml:id="formula_0">e(e) = tanh(W v e · e(v) + W 0 e · e(a 0 )+ W 1 e · e(a 1 ) + W 2 e · e(a 2 ) + b e )<label>(1)</label></formula><p>Here W v e , W 0 e , W 1 e , W 2 e , and b are model parameters, which are randomly initialized and tuned during the training of the main network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Modeling Temporal Orders</head><p>Given the embeddings of the existing chain of events e 1 , e 2 , ..., e n−1 , we use a standard LST- M <ref type="bibr" target="#b15">(Hochreiter and Schmidhuber, 1997</ref>  <ref type="figure">Figure 4</ref>: Temporal order modeling. coupled input and forget gates or peephole con- nections to model the temporal order. We ob- tain a sequence of hidden state vectors h 1 , h 2 , ..., h n−1 by recurrently feeding e(e 1 ), e(e 2 ), ..., e(e n−1 ) as inputs to the LSTM, where h i = LSTM(e(e i ), h i−1 ). The initial state h s and al- l stand LSTM parameters are randomly initialized and tuned during training. Now for each candidate next event e c , we ob- tain its vector representation e(e c ) in the same way as for e 1 to e n−1 . e(e c ) is then appended to the existing event chain to obtain a temporal-order- sensitive feature vector h c , by advancing the re- current encoding process for one step from h n−1 : h c = LSTM(e(e c ), h n−1 ). With multiple next event candidates e 1 c , e 2 c , ..., e m c (m ∈ [1, ∞]), m feature vectors are obtained, as shown in <ref type="figure">Figure 4</ref>, each being used as a basis for estimating the prob- ability of the corresponding event candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Modeling Pairwise Event Relations</head><p>After obtaining the hidden states for events, we model event pair relations using these hidden s- tate vectors. A straightforward approach to model the relation between two events is using a Siamese network <ref type="bibr" target="#b12">(Granroth-Wilding and Clark, 2016)</ref>. The order-sensitive LSTM features for existing events h 1 , h 2 , ..., h n−1 and the candidate event h c are used as event representations. Given a pair of events h i (i ∈ [1..n − 1]) and h c , the relatedness score is calculated by</p><formula xml:id="formula_1">s i = sigmoid(W si h i + W sc h c + b s ), (2)</formula><p>where W si , W sc and b s are model parameters.</p><p>Given the relation score s i between h c and each existing event h i , the likelihood of e c given e 1 , e 2 , ..., e n−1 can be calculated as the average of s i :</p><formula xml:id="formula_2">s = n−1 i=1 s i n − 1<label>(3)</label></formula><p>Weighting existing events. The drawback of above approach is that it considers the contribu- tion of each event on the chain is same. How- ever, given a chain of existing events, some are more informative for inferring a subsequent event than others. For example, given the events "wait in queue", "getting seated" and "order food", "or- der food" is more relevant for inferring "eat food" compared with the other two given events. Given information over the full event chain, this link can be more evident since the scenario is likely restau- rant visiting.</p><p>We use an attentional neural network to calcu- late the relative importance of each existing event according to the subsequent event candidate, using h i (i ∈ [1..n−1]) and h c for event representations:</p><formula xml:id="formula_3">u i = tanh(W ei h i + W c h c + b u )<label>(4)</label></formula><formula xml:id="formula_4">α i = exp(u i ) j exp(u j )<label>(5)</label></formula><p>where α i ∈ [0, 1] is the weight of h i , and i α t i = 1. W ei , W c , and b u are model parameters.</p><p>After obtaining the weight α i of each existing event h i , the relatedness of e c with the existing events can be calculated as:</p><formula xml:id="formula_5">s = n−1 i=1 α i · s i<label>(6)</label></formula><p>Multi-layer attention using Deep memory network. Memory network ( <ref type="bibr" target="#b40">Weston et al., 2014;</ref><ref type="bibr" target="#b19">Mikolov et al., 2014</ref>) has been used for explor- ing deep semantic information for semantic tasks. Such as question answering <ref type="bibr" target="#b35">(Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b17">Kumar et al., 2016)</ref> and reading comprehen- sion ( <ref type="bibr" target="#b13">Hermann et al., 2015;</ref>. Our task is analogous to such semantic tasks in <ref type="figure">Figure 5</ref>: Memory network at hop t. h i is the hid- den variable of the existing event chain, v t is the semantic representation between context events and candidate event. a t is the weight of contex- t events, and g is the gated recurrent network on Eq.10.</p><formula xml:id="formula_6">h 1 h 2 h n-1 v (t) Attention h e (t) g v (t+1) ∑ a (t) … …</formula><p>the sense that deep semantic information can be necessary for making the most rational inference. Hence, we are motivated to use a deep memory network model to refine event weight and event relation calculation by recurrently modeling more abstract representations of the scenario. Different from the previous researches, we use the memo- ry network to model the event chain, refining the attention mechanism used to explore the pair-wise relation between events.</p><p>The memory model consists of multiple dynam- ic computational layers (hops). For the first layer (hop 1), the weights α for existing events e 1 , e 2 , ..., e n−1 can be calculated using the same attention mechanism as Eq.4 and Eq.5. Given the weights α, we build a consolidated representation of con- text event chain e 1 , e 2 , ..., e n−1 as a weighted sum of h 1 , h 2 , ..., h n−1 :</p><formula xml:id="formula_7">h e = n−1 i−1 α i · h i<label>(7)</label></formula><p>The event candidate h c and the new represen- tation of the existing chain h e can be further in- tegrated to deduce a deeper representation of the full event chain hypothesis to the next layer (hop 2), denoted as v. v contains deeper semantic infor- mation compared with h c , which encode the tem- poral order of the event chain [h 1 , h 2 , ..., h n−1 , h c ] without differentiating the weights of each event.</p><p>As a result, in the next hop, better event weights can potentially be deduced by using v instead of h c in the calculation of attention:</p><formula xml:id="formula_8">u t i = tanh(W ei h i + W v v t + b u )<label>(8)</label></formula><formula xml:id="formula_9">α t i = exp(u t i ) j exp(u t j )<label>(9)</label></formula><p>In the same way, we stack multiple hops and repeat the steps multiple times, so that more ab- stract evidences can be extracted according to the chain of existing events. The above process can be performed recurrently, by taking h c as an ini- tial scenario representation v 0 , and then repeated- ly calculating h t e given h 1 , h 2 , ..., h n−1 and v t , and using h t e and v t to find a deeper scenario rep- resentation v t+1 . Following <ref type="bibr" target="#b5">Chung et al. (2014)</ref> and <ref type="bibr" target="#b37">Tran et al. (2016)</ref>, a gated recurrent network is used to this end:</p><formula xml:id="formula_10">z = σ(W z h t e + U z v t ) r = σ(W r h t e + U r v t ) ˆ h = tanh(W h t e + U (r v t )) v t+1 = (1 − z) v t + z ˆ h<label>(10)</label></formula><p>At any step, if the value of |v t+1 −v t | is less than the threshold µ, we consider that the progress has reached convergence. <ref type="figure">Figure 5</ref> shows an overview of the memory network at hop t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training</head><p>Given a set of event chains, each with a gold- standard subsequent event and a number of non- subsequent events, our training objective is to minimize the cross-entropy loss between the gold subsequent event and the set of non-subsequent events. The loss function of event chain predic- tion is that:</p><formula xml:id="formula_11">L(Θ) = N i=1 (s i − y i ) 2 + λ 2 ||Θ|| 2 (11)</formula><p>where s i is the relation score, y i is the label of the candidate (y i = 1 for positive sample, and y i = 0 for negative sample), Θ is the set of model param- eters and λ is a parameter for L2 regularization. We apply online training, where model parameter- s are optimized by using AdaGrad <ref type="bibr" target="#b8">(Duchi et al., 2011</ref>). We train word embedding using the Skip- gram algorithm (Mikolov et al., 2013) 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>Following Granroth-Wilding and Clark (2016), we extract events from the NYT portion of the Gi- gaword corpus ( <ref type="bibr" target="#b11">Graff et al., 2003</ref>). The C&amp;C tools ( <ref type="bibr" target="#b6">Curran et al., 2007)</ref> are used for POS tag- ging and dependency parsing, and OpenNLP 3 for phrase structure parsing and coreference resolu- tion. The training set consists of 1,500,000 even- t chains. We follow <ref type="bibr" target="#b12">Granroth-Wilding and Clark (2016)</ref> and use 10,000 event chains as the test set, and 1,000 event chains for development. There are 5 choices of output event for event input chain, which are given by <ref type="bibr" target="#b12">Granroth-Wilding and Clark (2016)</ref>. This dataset is referred to as G&amp;C16.</p><p>We also adapt the Chambers and Jurafsky (2008)'s dataset to the multiple choice setting, and use this dataset as the second benchmark. The dataset contains 69 documents, with 346 multiple choice event chain samples. We randomly sample 4 negative subsequent events for each event chain to make multiple-choice candidates. This dataset is referred to as C&amp;J08. For both datasets, accu- racy (Acc.) of the chosen subsequent event is used to measure the performance of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Hyper-parameters</head><p>There are several important hyper-parameters in our models, and we tune their values using the development dataset. We set the regularization weight λ = 10 −8 and the initial learning rate to 0.01. The size of word vectors is set to 300, and the size of hidden vectors in LSTM to 128. In order to avoid over-fitting, dropout <ref type="bibr" target="#b14">(Hinton et al., 2012</ref>) is used for word embedding with a ratio of 0.2. The neighbor similarity threshold η is set to 0.25. The threshold µ of the memory network sets to 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Development Experiments</head><p>We conduct a set of development experiments on the G&amp;C16 development set to study the influence of event argument representations and network configurations of the proposed MemNet model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Influence of Event Structure</head><p>Existing literature discussed various structures to denote events, such as v(a 0 , a 1 ) and v(a 0 , a 1 , a 2 ). We investigate the influence of integrating argu- ment values of the subject a 0 , object a 1 and prepo- sition a 2 , by doing ablation experiments on the development data. The results are shown in Ta- ble 1, where the system using all arguments gives a 54.36% accuracy. By removing a 2 , which exists in 17.6% of the events in our developmental data, the   <ref type="table">Table 2</ref>: Analysis of network structure.</p><p>Attention" with "-Attention, -LSTM", one can find that temporal order information over the w- hole event chain does have significant influence on the results (p − value &lt; 0.01 using t-test).</p><p>On the other hand, using LSTM to directly pre- dict the subsequent event ("LSTM-only") does not give better accuracies compared to model even- t pairs ("-Attention, -LSTM"). This confirms our intuition that strong-oder modelling and event-pair modelling each have their own strength.</p><p>Influence of Attention.</p><p>Comparison be- tween "-Attention" and "-Hop", and between "- Attention, -LSTM" and "-Hop, -LSTM" shows that giving different weights to different events does lead to improving results. Our analysis in Section 4.3 gives more intuitions to this obser- vation. Finally, comparison between "-Hop" and "MemNet" and between "-Hop, -LSTM" and "- LSTM" shows that a multi-hop deep memory net- work can indeed enhance the model with single level attention by offering more effective semantic representation of the scenarios. <ref type="table" target="#tab_3">Table 3</ref> shows the final results on the C&amp;C 16 and C&amp;J08 datasets, respectively. We compare the re- sults of our final model with the following base- lines:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Final Results</head><p>• PMI is the co-occurrence based model of <ref type="bibr" target="#b3">Chambers and Jurafsky (2008)</ref>, who calcu- late event pair relations based on Pointwise Mutual Information (PMI), scoring each can- didate event e c by the sum of PMI scores be- tween the given events e 0 , e 1 , ..., e n−1 and the candidate.</p><p>• Bigram is the counting based model of <ref type="bibr" target="#b16">Jans et al. (2012)</ref>, calculating event pair relation- s based on skip bigram probabilities, trained using maximum likelihood estimation.  • Event-Comp is the neural event relation model proposed by <ref type="bibr" target="#b12">Granroth-Wilding and Clark (2016)</ref>. They learn event representa- tions by calculating pair-wise event scores using a Siamese network.</p><p>• RNN is the method of <ref type="bibr" target="#b29">Pichotta and Mooney (2016)</ref>, who model event chains by directly using h c in Section 4.2 to predict the output, rather than taking them as features for event pair relation modeling.</p><p>• MemNet is the proposed deep memory net- work model.</p><p>Our reimplementation of PMI and Bigrams fol- lows <ref type="bibr" target="#b12">(Granroth-Wilding and Clark, 2016)</ref>. It can be seen from the table that the statistical counting-based models PMI and Bigram signif- icantly underperform the neural network models Event-Comp, RNN and MemNet, which is largely due to their sparsity and lack of semantic repre- sentation power. Under our event representation, Bigram does not outperform PMI significantly ei- ther, although considering the order of event pairs. This is likely due to sparsity of events when all arguments are considered.</p><p>Direct comparison between Event-Comp and RNN shows that the event-pair model gives com- parable results to the strong-order LSTM model. Although <ref type="bibr" target="#b12">Granroth-Wilding and Clark (2016)</ref> and <ref type="bibr" target="#b29">Pichotta and Mooney (2016)</ref> both compared with statistical baselines, they did not make direct com- parisons between their methods, which represen- t two different approaches to the task. Our re- sults show that they each have their unique ad- vantages, which confirm our intuition in the in- troduction. By considering both pairwise rela- tions and chain temporal orders, our method sig- nificantly outperform both Event-Comp and RNN (p − value &lt; 0.01 using t-test), giving the best reported results on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed a dynamic memory network to inte- grate chain order information into event relation measuring, calculating event pair relations by rep- resenting events in a chain using LSTM hidden s- tates, which encode temporal orders, and using a dynamic memory model to automatically induce event weights for each event. Standard evaluation showed that our method significantly outperforms state-of-the-art event pair models and event chain models, giving the best results reported so far.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Event sequences for restaurant visiting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>c 1</head><label>1</label><figDesc>: receive(X, response) c 2 : drive(X, mile) c 3 : seem(X) c 4 : discover(X, truth) c 5 : leave(X, restaurant) ? Entities Context(e i )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Method</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>) without</head><label>without</label><figDesc></figDesc><table>Event composition 

LSTM Temporal Order Learning 

v a 0 a 1 a 2 

e 0 

Event composition 

v a 0 a 1 a 2 

e c 
e 1 
e n-1 

… 

Dynamic Memory Network for Relation Measuring 

h 0 
h 1 
h n-1 
h c 

… 

Prob(e c |e 0 ,e 1 ,…,e n-1 ) 

candidate next event 
existing event sequence 

Event composition 

v a 0 a 1 a 2 

Event composition 

v a 0 a 1 a 2 

Figure 3: Overview of proposed model. 

e 0 
e 1 
e n-1 

e c1 

e c2 

e cm 

h n-1 
h 2 
h 1 

h c1 

h c2 

h cm 

… 

… 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Final results. 

</table></figure>

			<note place="foot">* This work has been done when the first author worked at SUTD. 1 The term &quot;temporal order&quot; is used throughout this work to indicate the narrative order in texts, following Chambers and Jurafsky (2008). Strictly speaking, the event order we extract is the narrative order.</note>

			<note place="foot" n="2"> https://code.google.com/p/word2vec/</note>

			<note place="foot" n="3"> https://opennlp.apache.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The corresponding author is Yue Zhang. We are grateful for the help of Fei Dong for his initial dis-cussion. We thank our anonymous reviewers for their constructive comments, which helped to im-prove the paper. This work is supported by the Temasek Lab grant IGDST1403012 at Singapore University of Technology and Design.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Acc. (%) MemNet</head><p>54.36 -verb 42.63 -(a 0 , a 1 ) 52.32 -(a 0 ) 53.43 -(a 1 ) 53.57 -(a 2 ) 54.02 <ref type="table">Table 1</ref>: Influence of event arguments.</p><p>accuracy drops to 54.02%. In contrast, by remov- ing a 0 and a 1 , which exist in 87.6% and 64.6% of the events in the development data, respective- ly, the accuracies drop to 53.43% and 53.57%, re- spectively, which demonstrates the relative impor- tance of a 0 (i.e., the subject) and a 1 (i.e., the ob- ject) for event modelling. While most previous work ( <ref type="bibr" target="#b3">Chambers and Jurafsky, 2008;</ref><ref type="bibr" target="#b1">Balasubramanian et al., 2013;</ref><ref type="bibr" target="#b28">Pichotta and Mooney, 2014</ref>) modelled only a 0 and a 1 , recent work (Pichotta and Mooney, 2016; Granroth-Wilding and Clark, 2016) modelled a 2 also. By removing both a 1 and a 2 , the accuracy drops further to 53.32%. Interestingly, by removing the verb while keeping only the arguments, the accu- racy drops to 42.63%. While this demonstrates the central value of the verb in denoting a event, it al- so suggests that the arguments themselves play a useful role in inferring the stereotypical scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Influence of Network Configurations</head><p>We study the influence of various network config- urations by performing ablation experiments, as shown in <ref type="table">Table 2</ref>. MemNet is the full model of this paper; -LSTM denotes ablation of the LSTM layer, using e(e 1 ), e(e 2 ), ..., e(e n−1 ) instead of h 1 , h 2 , ..., h n−1 to represent events; -Hop denotes ab- lation of the dynamic network model, using on- ly attention mechanism to calculate the weights of each existing event; -Attention denotes ablation of the attention mechanism, using the same weight on each existing event when inferring e c . The model "-Attention, -LSTM" is hence similar to the method of <ref type="bibr" target="#b12">Granroth-Wilding and Clark (2016)</ref>, al- though we used a different way of deriving even- t embeddings. The model "LSTM-only" shows a based by using LSTM hidden vector h n−1 to di- rectly predict the next event, which is similar to the method of <ref type="bibr" target="#b29">Pichotta and Mooney (2016)</ref>.</p><p>Influence of Temporal Order. By compar- ing "MemNet" and "-LSTM", and comparing "-</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lexical event ordering with an edge-factored model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-31" />
			<biblScope unit="page" from="1161" to="1171" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2015</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generating coherent event schemas at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Grand Hyatt Seattle, Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="page" from="1721" to="1731" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Event schema induction with a probabilistic entity-driven model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Grand Hyatt Seattle, Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="page" from="1797" to="1807" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised learning of narrative event chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2008, Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Columbus, Ohio, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1520" />
			<biblScope unit="page" from="789" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Probabilistic frame induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie Chi Kit</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings</title>
		<meeting><address><addrLine>Westin Peachtree Plaza Hotel, Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06-09" />
			<biblScope unit="page" from="837" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Linguistically motivated large-scale NLP with c&amp;c and boxer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2007, Proceedings of the 45th</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Frame semantics. Linguistics in the morning calm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Fillmore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="page" from="111" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A hierarchical bayesian model for unsupervised induction of script knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lea</forename><surname>Frermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<idno>EA- CL 2014</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-26" />
			<biblScope unit="page" from="49" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">English gigaword. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What happens next? event prediction using a compositional neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Granroth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Wilding</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-02-12" />
			<biblScope unit="page" from="2727" to="2733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Kocisk´ykocisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015</title>
		<imprint>
			<publisher>Quebec</publisher>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno>abs/1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Ilya Sutskever, and Ruslan Salakhutdinov</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Skip n-grams and ranking functions for predicting script events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bram</forename><surname>Jans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariefrancine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL 2012, 13th Conference of the European Chapter of the Association for Computational Linguistics, Avignon</title>
		<meeting><address><addrLine>France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-04-23" />
			<biblScope unit="page" from="336" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning, ICML 2016</title>
		<meeting>the 33nd International Conference on Machine Learning, ICML 2016<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19" />
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Open language learning for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07-12" />
			<biblScope unit="page" from="523" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning longer memory in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno>abs/1412.7753</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>Proceedings of a meeting held</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A framework for representing knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Minsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the TwentyFourth International Conference (ICML 2007), Corvallis</title>
		<meeting><address><addrLine>Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06-20" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Event embeddings for semantic script modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Modi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-11" />
			<biblScope unit="page" from="75" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Inducing neural models of script knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-26" />
			<biblScope unit="page" from="49" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning schemata for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Dejong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="page">61801</biblScope>
			<pubPlace>Urbana, 51</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A corpus and cloze evaluation for deeper understanding of commonsense stories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">F</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning scripts as hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">Walker</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janardhan</forename><surname>Rao Doppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth AAAI Conference on Artificial Intelligence<address><addrLine>Québec City, Québec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-07-27" />
			<biblScope unit="page" from="1565" to="1571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Statistical script learning with multi-argument events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pichotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2014</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2014<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-26" />
			<biblScope unit="page" from="220" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning statistical scripts with LSTM recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pichotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-02-12" />
			<biblScope unit="page" from="2800" to="2806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning script knowledge with web experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2010, Proceedings of the 48th</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="979" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Script induction as language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="1681" to="1686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Notes on a schema for stories. Representation and understanding: Studies in cognitive science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David E Rumelhart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
			<biblScope unit="volume">211</biblScope>
			<biblScope unit="page">45</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Scripts, plans, goals and understanding; an inquiry into human knowledge structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Schank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Schank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert P</forename><surname>Abelson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-712" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Third message understanding evaluation and conference (muc-3): Phase 1 status report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beth</forename><surname>Sundheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recurrent memory networks for language 66 modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="321" to="331" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semeval-2013 task 1: Tempeval3: Evaluating time expressions, events, and temporal relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naushad</forename><surname>Uzzaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><surname>Llorens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">F</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Verhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Pustejovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2013</title>
		<meeting>the 7th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2013<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06-14" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards ai-complete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>abs/1502.05698</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno>abs/1410.3916</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
