<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual Decomposition Inference for Graphical Models over Strings *</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dual Decomposition Inference for Graphical Models over Strings *</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We investigate dual decomposition for joint MAP inference of many strings. Given an arbitrary graphical model, we decompose it into small acyclic sub-models, whose MAP configurations can be found by finite-state composition and dynamic programming. We force the solutions of these subproblems to agree on overlapping variables, by tuning Lagrange multi-pliers for an adaptively expanding set of variable-length n-gram count features. This is the first inference method for arbitrary graphical models over strings that does not require approximations such as random sampling, message simplification, or a bound on string length. Provided that the inference method terminates, it gives a certificate of global optimality (though MAP inference in our setting is undecid-able in general). On our global phonolog-ical inference problems, it always terminates , and achieves more accurate results than max-product and sum-product loopy belief propagation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graphical models allow expert modeling of com- plex relations and interactions between random variables. Since a graphical model with given pa- rameters defines a probability distribution, it can be used to reconstruct values for unobserved vari- ables. The marginal inference problem is to com- pute the posterior marginal distributions of these variables. The MAP inference (or MPE) prob- lem is to compute the single highest-probability joint assignment to all the unobserved variables.</p><p>Inference in general graphical models is NP- hard even when the variables' values are finite dis- crete values such as categories, tags or domains. In this paper, we address the more challenging setting * This material is based upon work supported by the Na- tional Science Foundation under Grant <ref type="bibr">No. 1423276.</ref> where the variables in the graphical models range over strings. Thus, the domain of the variables is an infinite space of discrete structures.</p><p>In NLP, such graphical models can deal with large, incompletely observed lexicons. They could be used to model diverse relationships among strings that represent spellings or pronunciations; morphemes, words, phrases (such as named enti- ties and URLs), or utterances; standard or variant forms; clean or noisy forms; contemporary or his- torical forms; underlying or surface forms; source or target language forms. Such relationships arise in domains such as morphology, phonology, his- torical linguistics, translation between related lan- guages, and social media text analysis.</p><p>In this paper, we assume a given graphical model, whose factors evaluate the relationships among observed and unobserved strings. <ref type="bibr">1</ref> We present a dual decomposition algorithm for MAP inference, which returns a certifiably optimal so- lution when it converges. We demonstrate our method on a graphical model for phonology pro- posed by . We show that the method generally converges and that it achieves better results than alternatives.</p><p>The rest of the paper is arranged as follows: We will review graphical models over strings in sec- tion 2, and briefly introduce our sample problem in section 3. Section 4 develops dual decompo- sition inference for graphical models over strings. Then our experimental setup and results are pre- sented in sections 5 and 6, with some discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Graphical Models Over Strings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Factor Graphs and MAP Inference</head><p>To perform inference on a graphical model (di- rected or undirected), one first converts the model to a factor graph representation ( <ref type="bibr" target="#b17">Kschischang et al., 2001)</ref>. A factor graph is a finite bipartite </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Underlying morphemes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concatenation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Underlying words</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phonology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Surface words</head><p>Figure 1: A fragment of the factor graph for the directed graphical model of , displaying a possible assignment to the variables (ellipses). The model explains each observed surface word as the result of applying phonology to a concatenation of underlying morphemes. Shaded variables show the observed surface forms for four words: resignation, resigns, damns, and damnation. The underlying pronunciations of these words are assumed to be more similar than their surface pronunciations, because the words are known to share latent morphemes. The factor graph encodes what is shared. Each observed word at layer 3 has a latent underlying form at layer 2, which is a deterministic concatenation of latent morphemes at layer 1. The binary factors between layers 2 and 3 score each (underlying,surface) pair for its phonological plausibility. The unary factors at layer 1 score each morpheme for its lexical plausibility. See  for discussion of alternatives.</p><p>graph over a set X = {X 1 , X 2 , . . .} of variables and a set F of factors. An assignment to the vari- ables is a vector of values x = (x 1 , x 2 , . . .). Each factor F ∈ F is a real-valued function of x, but it depends on a given x i only if F is connected to X i in the graph. Thus, a degree d-factor scores some length-d subtuple of x. The score of the whole joint assignment simply sums over all factors:</p><formula xml:id="formula_0">score(x) def = F ∈F F (x).<label>(1)</label></formula><p>We seek the x of maximum score that is con- sistent with our partial observation of x. This is a generic constraint satisfaction problem with soft constraints. While our algorithm does not de- pend on a probabilistic interpretation of the fac- tor graph, 2 it can be regarded as peforming max- imum a posteriori (MAP) inference of the unob- served variables, under the probability distribution p(x) def = (1/Z) exp score(x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The String Case</head><p>Graphical models over strings have enjoyed some attention in the NLP community. Tree-shaped graphical models naturally model the evolution- ary tree of word forms <ref type="bibr" target="#b2">(Bouchard-Côté et al., 2007;</ref><ref type="bibr" target="#b3">Bouchard-Côté et al., 2008;</ref><ref type="bibr" target="#b10">Hall and Klein, 2010;</ref><ref type="bibr" target="#b11">Hall and Klein, 2011</ref>). Cyclic graphical 2 E.g., it could be used for exactly computing the separa- tion oracle when training a structural SVM ( <ref type="bibr" target="#b27">Tsochantaridis et al., 2005;</ref><ref type="bibr" target="#b9">Finley and Joachims, 2007)</ref>. Another use is mini- mum Bayes risk decoding-computing the joint assignment having minimum expected loss-if the loss function does not decompose over the variables, but a factor graph can be con- structed that evaluates the expected loss of any assignment. models have been used to model morphological paradigms <ref type="bibr" target="#b7">(Dreyer and Eisner, 2009;</ref><ref type="bibr" target="#b8">Dreyer and Eisner, 2011</ref>) and to reconstruct phonological un- derlying forms of words ( .</p><p>The variables in such a model are strings of un- bounded length: each variable X i is permitted to range over Σ * where Σ is some fixed, finite al- phabet. As in previous work, we assume that a degree-d factor is a d-way rational relation, i.e., a function of d strings that can be computed by a d-tape weighted finite-state machine (WFSM) ( <ref type="bibr" target="#b20">Mohri et al., 2002;</ref><ref type="bibr" target="#b12">Kempe et al., 2004</ref>). Such a machine is called an acceptor (WFSA) if d = 1 or a transducer (WFST) if d = 2. <ref type="bibr">3</ref> Past work has shown how to approximately sample from the distribution over x defined by such a model <ref type="bibr" target="#b2">(Bouchard-Côté et al., 2007)</ref>, or ap- proximately compute the distribution's marginals using variants of sum-product belief propaga- tion (BP) <ref type="bibr" target="#b7">(Dreyer and Eisner, 2009)</ref> and expecta- tion propagation (EP) (Cotterell and Eisner, 2015).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Finite-State Belief Propagation</head><p>BP iteratively updates messages between factors and variables. Each message is a vector whose el- ements score the possible values of a variable.</p><p>Murphy et al. (1999) discusses BP on cyclic ("loopy") graphs. For pedagogical reasons, sup- pose momentarily that all factors have degree ≤ 2 (this loses no power). Then BP manipulates only vectors and matrices-whose dimensionality de- pends on the number of possible values of the vari-ables. In the string case, they have infinitely many rows and columns, indexed by possible strings. <ref type="bibr" target="#b7">Dreyer and Eisner (2009)</ref> represented these in- finite vectors and matrices by WFSAs and WF- STs, respectively. They observed that the simple linear-algebra operations used by BP can be im- plemented by finite-state constructions. The point- wise product of two vectors is the intersection of their WFSAs; the marginalization of a matrix is the projection of its WFST; a vector-matrix prod- uct is computed by composing the WFSA with the WFST and then projecting onto the output tape. For degree &gt; 2, BP's rank-d tensors become d- tape WFSMs, and these constructions generalize.</p><p>Unfortunately, except in small acyclic models, the BP messages-which are WFSAs-usually become impractically large. Each intersection or composition involves a cross-product construc- tion. For example, when finding the marginal distribution at a degree-d variable, intersecting d WFSA messages having m states each may yield a WFSA with up to m d states. (Our models in section 6 include variables with d up to 156.) Combining many cross products, as BP iteratively passes messages along a path in the factor graph, leads to blowup that is exponential in the length of the path-which in turn is unbounded if the graph has cycles <ref type="bibr" target="#b7">(Dreyer and Eisner, 2009)</ref>, as ours do.</p><p>The usual solution is to prune or otherwise ap- proximate the messages at each step. In particu- lar,  gave a principled way to approximate the messages using variable- length n-gram models, using an adaptive variant of Expectation Propagation <ref type="bibr" target="#b19">(Minka, 2001</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Dual Decomposition Inference</head><p>In section 4, we will present a dual decomposition (DD) method that decomposes the original com- plex problem into many small subproblems that are free of cycles and high degree nodes. BP can solve each subproblem without approximation. <ref type="bibr">4</ref> The subproblems "communicate" through La- grange multipliers that guide them towards agree- ment on a single global solution. This information is encoded in WFSAs that score possible values of a string variable. DD incrementally adjusts the WFSAs so as to encourage values that agree with the variable's average value across subproblems.</p><p>Unlike BP messages, the WFSAs in our DD method will be restricted to be variable-length n-gram models, similar to . They may still grow over time; but DD of- ten halts while the WFSAs are still small. It halts when its strings agree exactly, rather than when it has converged up to a numerical tolerance, like BP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Switching Between Semirings</head><p>Our factors may be nondeterministic WFSMs. So when F ∈ F scores a given d-tuple of string val- ues, it may accept that d-tuple along multiple dif- ferent WFSM paths with different scores, corre- sponding to different alignments of the strings.</p><p>For purposes of MAP inference, we define F to return the maximum of these path scores. That is, we take the WFSMs to be defined with weights in the (max, +) semiring ( <ref type="bibr" target="#b20">Mohri et al., 2002</ref>). Equivalently, we are seeking the "best global solu- tion" in the sense of choosing not only the strings x i but also the alignments of the d-tuples. <ref type="bibr">5</ref> To do so, we must solve each DD subprob- lem in the same sense. We use max-product BP. This still applies the Dreyer-Eisner method of sec- tion 2.3. Since these WFSMs are defined in the (max, +) semiring, the method's finite-state oper- ations will combine weights using max and +.</p><p>MAP inference in our setting is in general com- putationally undecidable. <ref type="bibr">6</ref> However, if DD con- verges (as in our experiments), then its solution is guaranteed to be the true MAP assignment.</p><p>In section 6, we will compare DD with (loopy) max-product BP and (loopy) sum-product BP. These respectively approximate MAP inference and marginal inference over the entire factor graph. Marginal inference computes marginal string probabilities that sum (rather than maxi- mize) over the choices of other strings and the choices of paths. Thus, for sum-product BP, we re-interpret the factor WFSMs as defined over the (logadd, +) semiring. This means that the expo- nentiated score assigned by a WFSM is the sum of the exponentiated scores of the accepting paths.  <ref type="figure">Figure 2</ref>: To apply dual decomposition, we choose to decom- pose 1 into one subproblem per surface word. Dashed lines connect two or more variables from different subproblems that correspond to the same variable in the original graph. The method of Lagrange multipliers is used to force these variables to have identical values. An additional unary factor attached to each subproblem variable (not shown) is used to incorporate its Lagrangian term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Sample Task: Generative Phonology</head><p>Before giving the formal details of our DD method, we give a motivating example: a recently proposed graphical model for morphophonology.  defined a Bayesian network to describe the generative process of phonological words. Our <ref type="figure">Figure 1</ref> shows a conversion of their model to a factor graph and explains what the vari- ables and factors mean. Inference on this graph performs unsupervised discovery of latent strings. Given observed surface representations of words (SRs), inference aims to recover the underlying representations (URs) of the words and their shared constituent morphemes. The latter can then be used to predict held-out SRs.</p><p>Notice that the 8 edges in the first layer of Fig- ure 1 form a cycle; such cycles make BP inexact. Moreover, the figure shows only a schematic frag- ment of the graphical model. In the actual exper- iments, the graphical models have up to 829 vari- ables, and the variables representing morpheme URs are connected to up to 156 factors (because many words share the same affix).</p><p>To handle the above challenges without ap- proximation, we want to decompose the original problem into subproblems where each subproblem can be solved efficiently. In particular, we want the subproblems to be free of cycles and high- degree nodes. In our phonology example, each observed word along with its correspondent latent URs forms an ideal subproblem. This decomposi- tion is shown in <ref type="figure">Figure 2</ref>.</p><p>While the subproblems can be solved efficiently in isolation, they may share variables, as shown by the dashed lines in <ref type="figure">Figure 2</ref>. DD repeatedly modifies and re-solves the subproblems until they agree on their shared variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dual Decomposition</head><p>Dual decomposition is a general technique for solving constrained optimization problems. It has been widely used for MAP inference in graphi- cal models ( <ref type="bibr" target="#b15">Komodakis et al., 2007;</ref><ref type="bibr" target="#b14">Komodakis and Paragios, 2009;</ref><ref type="bibr" target="#b16">Koo et al., 2010;</ref><ref type="bibr" target="#b18">Martins et al., 2011;</ref><ref type="bibr" target="#b25">Sontag et al., 2011;</ref><ref type="bibr" target="#b24">Rush and Collins, 2014</ref>). However, previous work has focused on variables X i whose values are in R or a small fi- nite set; we will consider the infinite set Σ * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Review of Dual Decomposition</head><p>To apply dual decomposition, we must partition the original problem into a union of K subprob- lems, each of which can be solved exactly and ef- ficiently (and in parallel). For example, our exper- iments partition <ref type="figure">Figure 1</ref> as shown in <ref type="figure">Figure 2</ref>.</p><p>Specifically, we partition the factors into K sets F 1 , . . . , F K . Each factor F ∈ F appears in ex- actly one of these sets. This lets us rewrite the score (1) as</p><formula xml:id="formula_1">k F ∈F k F (x)</formula><p>. Instead of simply seeking its maximizer x, we equivalently seek</p><formula xml:id="formula_2">argmax x 1 ,...,x K K k=1 F ∈F k F (x k ) s.t. x 1 = · · · = x K<label>(2)</label></formula><p>If we dropped the equality constraint, (2) could be solved by separately maximizing F ∈F k F (x k ) for each k. This "subproblem" is itself a MAP problem which considers only the factors F k and the variables X k adjacent to them in the original factor graph. The subproblem ob- jective does not depend on the other variables.</p><p>We now attempt to enforce the equality con- straint indirectly, by adding Lagrange multipli- ers that encourage agreement among the subprob- lems. Assume for the moment that the variables in the factor graph are real-valued (each x k i is in R). Then consider the Lagrangian relaxation of <ref type="formula" target="#formula_2">(2)</ref>,</p><formula xml:id="formula_3">max x 1 ,...,x K K k=1 F ∈F k F (x k ) + i λ k i · x k i<label>(3)</label></formula><p>This can still be solved by separate maximizations. For any choices of λ k i ∈ R having (∀i) k λ k i = 0, it upper-bounds the objective of (2). Why? The solution to (2) achieves the same value in (3), yet (3) may do even better by considering solutions that do not satisfy the constraint. Our goal is to find λ k i values that tighten this upper bound as much as possible. If we can find λ k i values so that the optimum of (3) satisfies the equality constraint, then we have a tight bound and a solution to (2). To improve the method, recall that subproblem k considers only variables X k . It is indifferent to the value of X i if X i / ∈ X k , so we just leave x k i un- defined in the subproblem's solution. We treat that as automatically satisfying the equality constraint;</p><note type="other">thus we do not need any Lagrange multiplier λ k i to force equality. Our final solution x ignores unde- fined values, and sets x i to the value agreed on by the subproblems that did consider X i . 7 4.2 Substring Count Features But what do we do if the variables are strings? The Lagrangian term λ k i ·x k i in (3) is now ill-typed. We replace it with λ k i · γ(x k i ), where γ(·) extracts a real-valued feature vector from a string, and λ k i is a vector of Lagrange multipliers.</note><p>This corresponds to changing the constraint in (2). Instead of requiring</p><formula xml:id="formula_4">x 1 i = · · · = x K i for each i, we are now requiring γ(x 1 i ) = · · · = γ(x K i ), i</formula><p>.e., these strings must agree in their features.</p><p>We want each possible string to have a unique feature vector, so that matching features forces the actual strings to match. We follow <ref type="bibr" target="#b22">Paul and Eisner (2012)</ref> and use a substring count feature for each w ∈ Σ * . In other words, γ(x) is an infinitely long vector, which maps each w to the number of times that w appears in x as a substring. <ref type="bibr">8</ref> Computing λ k i · γ(x k i ) in (3) remains possi- ble because in practice, λ k i will have only finitely many nonzeros. This is so because our feature vector γ(x) has only finitely many nonzeros for any string x, and the subgradient algorithm in sec- tion 4.3 below always updates λ k i by adding mul- tiples of such γ(x) vectors.</p><p>We will use a further trick below to prevent rapid growth of this finite set of nonzeros. Each variable X i maintains an active set of features, W i . Only these features may have nonzero La- grange multipliers. While the active set can grow over time, it will be finite at any given step.</p><p>Given the Lagrange multipliers, subproblem k of (3) is simply MAP inference on the factor graph consisting of the variables X k and factors F k as well as an extra unary factor G k i at each X i ∈ X k :</p><p>7 Without this optimization, the Lagrangian term λ k i · x k i would have driven x k i to match that value anyway. 8 More precisely, the number of times that w appears in BOS x EOS, where BOS, EOS are distinguished boundary sym- bols. We allow w to start with BOS and/or end with EOS, which yields prefix and suffix indicator features.</p><formula xml:id="formula_5">G k i (x k ) def = λ k i · γ(x k i )<label>(4)</label></formula><p>These unary factors penalize strings according to the Lagrange multipliers. They can be encoded as WFSAs ( <ref type="bibr" target="#b0">Allauzen et al., 2003;</ref>, Appendices B.1-B.5), allowing us to solve the subproblem by max-product BP as usual.</p><p>The topology of the WFSA for G k i depends only on W i , while its weights come from λ k i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Projected Subgradient Method</head><p>We aim to adjust the collection λ of Lagrange multipliers to minimize the upper bound (3). Fol- lowing <ref type="bibr" target="#b15">Komodakis et al. (2007)</ref>, we solve this con- vex dual problem using a projected subgradient method. We initialize λ = 0 and compute (3) by solving the K subproblems. Then we take a step to adjust λ, and repeat in hopes of eventually sat- isfying the equality condition.</p><p>The projected subgradient step is</p><formula xml:id="formula_6">λ k i := λ k i + η · µ i − γ(x k i )<label>(5)</label></formula><p>where η &gt; 0 is the current step size, and µ i is the mean of γ(x k i ) over all subproblems k that con- sider X i . This update modifies (3) to encourage solutions x k such that γ(x k i ) comes closer to µ i . For each i, we update all λ k i at once to preserve the property that (∀i) k λ k i = 0. However, we are only allowed to update components of the λ k i that correspond to features in the active set W i . To ensure that we continue to make progress even af- ter we agree on these features, we first expand W i by adding the minimal strings (if any) on which the x k i do not yet all agree. For example, we will add the abc feature only when the x k i already agree on their counts of its substrings ab and bc. <ref type="bibr">9</ref> Algorithm 1 summarizes the whole method. Ta- ble 1 illustrates how one active set W i (section 4.3) evolves, in our experiments, as it tries to enforce agreement on a particular string x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Past Work: Implicit Intersection</head><p>Our DD algorithm is an extension of one that Paul and Eisner (2012) developed for the simpler im- plicit intersection problem. Given many WFSAs F 1 , . . . , F K , they were able to find the string x with maximum total score K k=1 F k (x). (They ap- plied this to solve instances of the NP-hard Steiner Algorithm 1 DD for graphical models over strings 1: initialize the active set Wi for each variable Xi ∈ X 2: initialize λ k i = 0 for each Xi and each subproblem k 3: for t = 1 to T do max number of iterations 4:</p><p>for k = 1 to K do solve all primal subproblems 5:</p><p>if any of the λ k i have changed then 6:</p><p>run max-product BP on the acyclic graph de- fined by variables X k and factors F k and G k i 7: extract MAP strings: ∀i with Xi ∈ X k , x k i is the label of the max-scoring accepting path in the WFSA that represents the belief at Xi 8:</p><p>for each Xi ∈ X do improve dual bound 9:</p><p>if the defined strings x k i are not all equal then 10:</p><p>Expand active feature set Wi section 4.3 11:</p><p>Update each λ k i equation <ref type="formula" target="#formula_6">(5)</ref>  string problem, i.e., finding the string x of mini- mum total edit distance to a collection of K ≈ 100 given strings.) The naive solution to this problem would be to find the highest-weighted path in the intersection F 1 ∩ · · · ∩ F K . Unfortunately, the in- tersection of WFSAs takes the Cartesian product of their state sets. Thus materializing this inter- section would have taken time exponential in K.</p><p>To put this another way, inference is NP-hard even on a "trivial" factor graph: a single variable X 1 attached to K factors. Recall from section 2.3 that BP would solve this via the expensive inter- section above. Paul and Eisner (2012) instead ap- plied DD with one subproblem per factor. We generalize their method to handle arbitrary factor graphs, with multiple latent variables and cycles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Block Coordinate Update</head><p>We also explored a possible speedup for our algo- rithm. We used a block coordinate update vari- ant of the algorithm when performing inference on the phonology problem and observed an empiri- cal speedup. Block coordinate updates are widely used in Lagrangian relaxation and have also been explored specifically for dual decomposition.</p><p>In general, block algorithms minimize the ob- jective by holding some variables fixed while up- dating others. <ref type="bibr" target="#b25">Sontag et al. (2011)</ref> proposed a so- phisticated block method called MPLP that con- siders all values of variable X i instead of the ones obtained from the best assignments for the sub- problems. However, it is not clear how to apply their technique to string-valued variables. Instead, the algorithm we propose here is much simpler-it</p><formula xml:id="formula_7">Iter# x 1 i x 2 i x 3 i x 4 i ∆Wi 1 ∅ 3 g g g g</formula><p>∅ 4 gris griz griz griz {s, z, is, iz, s$ z$ } 5 gris grizo griz griz {o, zo, o$ } 14 griz grizo griz griz ∅ 17 griz griz griz griz ∅ 18 griz griz grize griz { e, ze, e$ } 19 gris griz griz griz ∅ 31 griz griz griz griz ∅ divides the primal variables into groups and up- dates each group's associated dual variables in turn, using a single subgradient step (5). Note that this way of partitioning the dual variables has the nice property that we can still use the projected subgradient update we gave in (5) and preserve the property that (∀i) k λ k i = 0. In the graphical model for generative phonol- ogy, there are two types of underlying morphemes in the first layer: word stems and word affixes. Our block coordinate update algorithm thus alternates between subgradient updates to the dual variables for the stems and the dual variables for the affixes. Note that when performing block coordinate up- date on the dual variables, the primal variables are not held constant, but rather are chosen by opti- mizing the corresponding subproblem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We compare DD to belief propagation, using the graphical model for generative phonology dis- cussed in section 3. Inference in this model aims to reconstruct underlying morphemes. Since our fo- cus is inference, we will evaluate these reconstruc- tions directly (whereas  eval- uated their ability to predict novel surface forms using the reconstructions).</p><p>Our factor graphs have a similar topology to the pedagogical fragment shown in <ref type="figure">Figure 1</ref>. How-ever, they are actually derived from datasets con- structed by , which are avail- able with full descriptions at http://hubal.cs. jhu.edu/tacl2015/. Briefly: EXERCISE Small datasets of Catalan, English, Maori, and Tangale, drawn from phonology textbooks. Each dataset contains 55 to 106 surface words, formed from a collection of 16 to 55 morphemes. CELEX Larger datasets of German, English, and Dutch, drawn from the CELEX database ( <ref type="bibr" target="#b1">Baayen et al., 1995)</ref>. Each dataset contains 1000 surface words, formed from 341 to 381 underlying morphemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Scheme</head><p>We compared three types of inference:</p><p>DD Use DD to perform exact MAP inference. SP Perform approximate marginal inference by sum-product loopy BP with pruning . MP Perform approximate MAP inference by max-product loopy BP with pruning. DD and SP improve this baseline in different ways.</p><p>DD predicts a string value for each variable. For SP and MP, we deem the prediction at a variable to be the string that is scored most highly by the belief at that variable.</p><p>We report the fraction of predicted morpheme URs that exactly match the gold-standard URs proposed by a human ( ). We also compare these predicted URs to one another, to see how well the methods agree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Parameterization</head><p>The model of  has two fac- tor types whose parameters must be chosen. <ref type="bibr">10</ref> The first is a unary factor M φ . Each underlying- morpheme variable (layer 1 of <ref type="figure">Figure 1</ref>) is con- nected to a copy of M φ , which gives the prior dis- tribution over its values. The second is a binary factor S θ . For each surface word (layer 3), a copy of S θ gives its conditional distribution given the corresponding underlying word (layer 2). M φ and S θ respectively model the lexicon and the phonol- ogy of the specific language; both are encoded as WFSMs.</p><p>M φ is a 0-gram generative model: at each step it emits a character chosen uniformly from the al- phabet Σ with probability φ, or halts with proba- bility 1 − φ. It favors shorter strings in general, but φ determines how weak this preference is.</p><p>S θ is a sequential edit model that produces a word's SR by stochastically copying, inserting, substituting, and deleting the phonemes of its UR. We explore two ways of parameterizing it.</p><p>Model 1 is a simple model in which θ is a scalar, specifying the probability of copying the next character of the underlying word as it is transduced to the surface word. The remaining probability mass 1−θ is apportioned equally among insertion, substitution and deletion operations. 11 This mod- els phonology as "noisy concatenation"-the min- imum necessary to account for the fact that surface words cannot quite be obtained as simple concate- nations of their shared underlying morphemes.</p><p>Model 2 is a replication of the much more complicated parametric model of , which can handle linguistic phonology. Here the factor S θ is a contextual edit FST <ref type="bibr" target="#b5">(Cotterell et al., 2014</ref>). The probabilities of competing edits in a given context are determined by a log- linear model with weight vector θ and features that are meant to pick up on phonological phenomena.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Training</head><p>When evaluating an inference method from sec- tion 5.2, we use the same inference method both for prediction and within training.</p><p>We train Model 1 by grid search. Specifically, we choose φ ∈ [0.65, 1) and θ ∈ [0.25, 1) such that the predicted forms maximize the joint score (1) (always using the (max, +) semiring).</p><p>For Model 2, we compared two methods for training the φ and θ parameters (θ is a vector):</p><p>Model 2S Supervised training, which observes the "true" (hand-constructed) values of the URs. This idealized setting uses the best pos- sible parameters (trained on the test data). Model 2E Expectation maximization (EM), whose E step imputes the unobserved URs.</p><p>EM's E step calls for exact marginal inference, which is intractable for our model. So we substi- tute the same inference method that we are test-ing. This gives us three approximations to EM, based on DD, SP and MP. Note that DD specif- ically gives the Viterbi approximation to EM- which sometimes gets better results than true EM ( <ref type="bibr" target="#b26">Spitkovsky et al., 2010</ref>). For MP (but not SP), we extract only the 1-best predictions for the E step, since we study MP as an approximation to DD.</p><p>As initialization, our first E step uses the trained version of Model 1 for the same inference method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Inference Details</head><p>We run SP and MP for 20 iterations (usually the predictions converge within 10 iterations). We run DD to convergence (usually &lt; 600 iterations). DD iterations are much faster since each variable con- siders d strings, not d distributions over strings. Hence DD does not intersect distributions, and many parts of the graph settle down early because discrete values can converge in finite time. <ref type="bibr">12</ref> We follow Paul and Eisner (2012, section 5.1) fairly closely. In particular: Our stepsize in <ref type="formula" target="#formula_6">(5)</ref> is η = α/(t + 500), where t is the iteration num- ber; α = 1 for Model 2S and α = 10 otherwise. We proactively include all 1-gram and 2-gram sub- string features in the active sets W i at initializa- tion, rather than adding them only as needed. At it- erations 200, 400, and 600, we proactively add all 3-, 4-, and 5-gram features (respectively) on which the counts still disagree; this accelerates conver- gence on the few variables that have not already converged. We handle negative-weight cycles as Paul and Eisner do. If we had ever failed to con- verge within 2000 iterations, we would have used their heuristic to extract a prediction anyway.</p><p>Model 1 suffers from a symmetry-breaking problem. Many edits have identical probability, and when we run inference, many assignments will tie for highest scoring configuration. This can prevent DD from converging and makes per- formance hard to measure. To break these ties, we add "jitter" separately to each copy of M φ in <ref type="figure">Figure 1</ref>. Specifically, if F i is the unary fac- tor attached to X i , we expand our 0-gram model  </p><formula xml:id="formula_8">F i (x) = log((p/|Σ|) |x| · (1 − p)) to become F i (x) = log( c∈Σ p |x|c c,i · (1 − p)),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Convergence and Speed of DD</head><p>As linguists know, reconstructing an underlying stem or suffix can be difficult. We may face insuf- ficient evidence or linguistic irregularity-or reg- ularity that goes unrecognized because the phono- logical model is impoverished (Model 1) or poorly trained (early EM iterations on Model 2). DD may then require extensive negotiation to resolve disagreements among subproblems. Furthermore, DD must renegotiate as conditions change else- where in the factor graph <ref type="table" target="#tab_2">(Table 1)</ref>. DD converged in all of our experiments. Note that DD (section 4.3) has converged when all the equality constraints in (2) are satisfied. In this case, we have found the true MAP configuration.</p><p>In section 4.5, we discussed a block coordi- nate update variation (BCDD) of our DD algo- rithm. <ref type="figure" target="#fig_2">Figure 3</ref> shows the convergence behavior of BCDD against the naive projected subgradi- ent algorithm (NVDD) on the four EXERCISE lan- guages under Model 1. The dual objective (3) al- ways upper-bounds the primal score (i.e., the score (1) of an assignment derived heuristically from the current subproblem solutions). The dual de- creases as the algorithm progresses. When the two objectives meet, we have found an optimal solu- tion to the primal problem. We can see in <ref type="figure" target="#fig_2">Figure 3</ref> that our DD algorithm converges quickly on the four EXERCISE languages and BCDD converges consistently faster than NVDD. We use BCDD in the remaining experiments.</p><p>When DD runs fast, it is competitive with the  other methods. It is typically faster on the EXER- CISE data, and a few times slower on the CELEX data. But we stop the other methods after 20 it- erations, whereas DD runs until it gets an exact answer. We find that this runtime is unpredictable and sometimes quite long. In the grid search for training Model 1, we observed that changes in the parameters (φ, θ) could cause the runtime of DD inference to vary by 2 orders of magnitude. Sim- ilarly, on the CELEX data, the runtime on Model 1 (over 10 different N = 600 subsets of English) varied from about 1 hour to nearly 2 days. <ref type="bibr">13</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparison of Inference</head><p>For each language, we constructed several differ- ent unsupervised prediction problems. In each problem, we observe some size-N subset of the words in our dataset, and we attempt to predict the URs of the morphemes in those words. For each CELEX language, we took N = 600, and used three of the size-N training sets from ( ). For each EXERCISE language, we took N to be one less than the dataset size, and used all N + 1 subsets of size N , again similar to ( . We report the unweighted macro-average of all these accuracy numbers.</p><p>We compare DD, SP, and MP inference on each language under different settings. <ref type="table" target="#tab_4">Table 2</ref> shows aggregate results, as an unweighted aver- age over multiple languages and training sets. We present various additional results at http://cs. jhu.edu/ ˜ npeng/emnlp2015/, including a per- language breakdown of the results, runtime num- bers, and significance tests.</p><p>The results for Model 1 are shown in <ref type="table" target="#tab_4">Tables 2a  and 2b</ref>. As we can see, in both datasets, dual decomposition performed the best at recovering the URs, while MP performed the worst. Both DD and MP are doing MAP inference, so the dif- ferences reflect the search error in MP. Interest- ingly, DD agrees more with SP than with MP, even though SP uses marginal inference.</p><p>Although the aggregate results on the EXER- CISE dataset show a large improvement of DD over both of the BP algorithms, the gain all comes from the English language. SP actually does better than DD on Catalan and Maori, and MP also gets better results than DD on Maori, tying with SP.</p><p>For Model 2S, all inference methods achieved 100% accuracy on the EXERCISE dataset, so we do not show a table. The results on the CELEX dataset are shown in <ref type="table" target="#tab_4">Table 2c</ref>. Here both DD and MP performed equally well, and outperformed BP-a result like ( <ref type="bibr" target="#b26">Spitkovsky et al., 2010)</ref>. This trend is consistent over all three languages: DD and MP always achieve similar results and both outperform SP. Of course, one advantage of DD in the setting is that it actually finds the true MAP prediction of the model; the errors are known to be due to the model, not the search procedure.</p><p>For Model 2E, we show results on the EXER- CISE dataset in <ref type="table" target="#tab_4">Table 2d</ref>. Here the results resemble the pattern of Model 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>We presented a general dual decomposition algo- rithm for MAP inference on graphical models over strings, and applied it to an unsupervised learn- ing task in phonology. The experiments show that our DD algorithm converges and gets better results than both max-product and sum-product BP.</p><p>Techniques should be explored to speed up the DD method. Adapting the MPLP algorithm <ref type="bibr" target="#b25">(Sontag et al., 2011</ref>) to the string-valued case would be a nontrivial extension. We could also explore other serial update schemes, which generally speed up message-passing algorithms over parallel update.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>where |x| c denotes the count of character c in string x, and p c,i ∝ (p/|Σ|) · exp ε c,i where ε c,i ∼ N (0, 0.01) and we preserve c∈Σ p c,i = p. 12 A variable need not update λ if its strings agree; a sub- problem is not re-solved if none of its variables updated λ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The primal-dual curve of NVDD v.s. BCDD on 4 EXERCISE languages. BCDD always converges faster.</figDesc><graphic url="image-3.png" coords="8,310.00,159.80,113.38,85.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>One variable's active set as DD runs. This variable is 
the unobserved stem morpheme shared by the Catalan words 
gris, grizos, grize, grizes. The second column shows 
the current set of solutions from the 4 subproblems having 
copies of this variable. The third column shows the new sub-
strings that are then added to the active set, to try to enforce 
agreement via their Lagrange multipliers. The table does not 
show iterations in which these columns have not changed. 
However, those iterations still update the Lagrange multipli-
ers to more strongly encourage agreement (if needed). Al-
though agreement is achieved at iterations 1, 3, and 17, it 
is then disrupted-the subproblems' solutions change be-
cause of Lagrange-multiplier pressures on their other vari-
ables (suffixes that do not agree yet). At iteration 31, the vari-
able returns to agreement on griz, and never changes again. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Pairwise agreement (on morpheme URs) of DD, SP, 
MP and the gold standard, for each group of inference prob-
lems. Boldface is highest accuracy (agreement with gold). 

</table></figure>

			<note place="foot" n="1"> In some task settings, it is also necessary to discover the model topology along with the model parameters. In this paper we do not treat that structure learning problem. However, both structure learning and parameter learning need to call inference-such as the method presented here-in order to evaluate proposed topologies or improve their parameters.</note>

			<note place="foot" n="3"> Finite-state software libraries often support only these cases. Accordingly, Cotterell and Eisner (2015, Appendix B.10) explain how to eliminate factors of degree d &gt; 2.</note>

			<note place="foot" n="4"> Such small BP problems commonly arise in NLP. In particular, using finite-state methods to decode a composition of several finite-state noisy channels (Pereira and Riley, 1997; Knight and Graehl, 1998) can be regarded as BP on a graphical model over strings that has a linear-chain topology.</note>

			<note place="foot" n="5"> This problem is more specifically called MPE inference. 6 The trouble is that we cannot bound the length of the latent strings. If we could, then we could encode them using a finite set of boolean variables, and solve as an ILP problem. But that would allow us to determine whether there exists a MAP assignment with score ≥ 0. That is impossible in general, because it would solve Post&apos;s Correspondence Problem as a simple special case (see Dreyer and Eisner (2009)).</note>

			<note place="foot" n="9"> In principle, we should check that they also (still) agree on a, b, and c, but we skip this check. Our active set heuristic is almost identical to that of Paul and Eisner (2012).</note>

			<note place="foot" n="10"> The model also has a three-way factor, connecting layers 1 and 2 of Figure 1. This represents deterministic concatenation (appropriate for these languages) and has no parameters.</note>

			<note place="foot" n="11"> That is, probability mass of (1 − θ)/3 is divided equally among the |Σ| possible insertions; another (1 − θ)/3 is divided equally among the |Σ|−1 possible substitutions; and the final (1 − θ)/3 is allocated to deletion.</note>

			<note place="foot" n="13"> Note that our implementation is not optimized; e.g., it uses Python (not Cython).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generalized algorithms for constructing statistical language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="40" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Piepenbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Gulikers</surname></persName>
		</author>
		<title level="m">The CELEX lexical database on CDROM</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A probabilistic approach to diachronic phonology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="887" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A probabilistic approach to language change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Penalized expectation propagation for graphical models over strings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT<address><addrLine>Denver</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="932" to="942" />
		</imprint>
	</monogr>
	<note>Supplementary material (11 pages) also available</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stochastic contextual edit distance and probabilistic FSTs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Baltimore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling word forms using latent underlying morphs and phonology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="433" to="447" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graphical models over multiple strings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-08" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discovering morphological paradigms from plain text using a Dirichlet process mixture model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Edinburgh</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="616" to="627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parameter learning for loopy markov random fields with structural support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Constrained Optimization and Structured Output Spaces</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Finding cognate groups using phylogenies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-scale cognate recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A note on join and auto-intersection of n-ary rational relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><surname>Kempe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Champarnaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<idno>04-40</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eindhoven FASTAR Days</title>
		<editor>Loek Cleophas and Bruce Watson</editor>
		<meeting>the Eindhoven FASTAR Days<address><addrLine>Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-12" />
			<biblScope unit="page" from="64" to="78" />
		</imprint>
		<respStmt>
			<orgName>Computer Science ; Department of Mathematics and Computer Science, Technische Universiteit Eindhoven</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Machine transliteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Graehl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Beyond pairwise energies: Efficient optimization for higherorder MRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2985" to="2992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MRF optimization via dual decomposition: Message-passing revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Paragios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tziritas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dual decomposition for parsing with non-projective head automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1288" to="1298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Factor graphs and the sum-product algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Kschischang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Loeliger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="498" to="519" />
			<date type="published" when="2001-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An augmented lagrangian approach to constrained map inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mário</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Expectation propagation for approximate Bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">P</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UAI</title>
		<meeting>UAI</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="362" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weighted finite-state transducers in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="88" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Loopy belief propagation for approximate inference: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UAI</title>
		<meeting>UAI</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="467" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Implicitly intersecting weighted automata using dual decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="232" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Speech recognition by composition of weighted finite automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Finite-State Language Processing</title>
		<editor>Emmanuel Roche and Yves Schabes</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A tutorial on dual decomposition and Lagrangian relaxation for inference in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<idno type="arXiv">arXiv.orgasarXiv:1405.5208</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Technical report available from</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Introduction to dual decomposition for inference. Optimization for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="219" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Viterbi training improves unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><forename type="middle">I</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL<address><addrLine>Uppsala</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page">917</biblScope>
		</imprint>
	</monogr>
	<note>Sweden</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1453" to="1484" />
			<date type="published" when="2005-09" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
